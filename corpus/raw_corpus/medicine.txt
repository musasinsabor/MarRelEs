
Patología molecular

La patología molecular intenta explicar por qué un determinado cambio genético acaba provocando un fenotipo clínico particular.
En el Capítulo 10 revisamos la naturaleza y los mecanismos de las mutaciones (brevemente resumidas en el Recuadro 15.1 ); en este capítulo nos centraremos en sus efectos.

La patología molecular requiere que descubramos cuál es el efecto de una mutación en la cantidad o en la función del producto génico, y que expliquemos por qué el cambio resulta o no patogénico para los diferentes tipos celulares o para una fase determinada del desarrollo.

Dada la complejidad de las interacciones genéticas no debe sorprender que la patología molecular sea, en la actualidad, una ciencia muy imperfecta.
Los síntomas clínicos suelen ser el resultado final de una larga serie de casualidades, y con demasiada frecuencia somos incapaces de predecirlos e incluso de entenderlos dado el conocimiento de que disponemos actualmente.
No obstante, a medida que el énfasis del Proyecto Genoma Humano se desplaza del simple catalogado de genes a la comprensión de su función, el estudio de la patología molecular ha pasado a ocupar una posición central, y el conocimiento avanza.
A cada vez es mayor el número de genes bien estudiados para los que es posible predecir el efecto fenotípico de un determinado cambio en la secuencia de su DNA.

Una de las principales ventajas de estudiar seres humanos frente a organismos de laboratorio es que los sistemas sanitarios de todo el mundo actúan como una continua y gigantesca criba de mutaciones.
En consecuencia, para la mayor parte de las enfermedades hereditarias cuyo gen ha sido identificado se conocen muchas mutaciones diferentes.
No podemos experimentar o criar humanos, pero los seres humanos continuamente proporcionan oportunidades únicas para observar los efectos fenotípicos de muchos cambios diferentes en un gen determinado.
Esto genera hipótesis, que han de ser comprobadas posteriormente en animales.
Así, las investigaciones de las mutaciones humanas que aparecen naturalmente son complementadas por los estudios de mutaciones específicas en animales transgénicos (ver página 570).

- Recuadro 15.1: Principales clases de mutaciones
- Deleciones que van desde 1 bp hasta las megabases
- Inserciones , incluyendo duplicaciones 
- Mutaciones de sentido equivocado ( missense mutations ), que sustituyen un aminoácido por otro en el producto génico
- Mutaciones carentes de sentido (nonsense) , que sustituyen un codón de aminoácido por un codón de paro
- Mutaciones de sitio de corte y empalme , que crean o destruyen señales de corte y empalme

- Desplazamientos de pauta de lectura , que pueden resultar de deleciones, inserciones o errores en el corte y empalme

Nomenclatura y bases de datos de mutaciones

La nomenclatura de preferencia para los genes y sus alelos es delineada por las comisiones de nomenclatura en los diversos encuentros de Cartografiado del Genoma Humano.
Las variantes alélicas se coleccionan en bases de datos como la OMIM (ver página 66) y se numeran en serie, empleando el nombre del locus seguido de un asterisco y un número de serie, por ejemplo HBB*0403 .
En marzo de 1995 la revista Trends in Genetics publicó un valioso suplemento recogiendo un sumario de la nomenclatura genética de muchos organismos diferentes, incluyendo el hombre (ver Bibliografía adicional).

Para poder describir las mutaciones (más que sencillamente elaborar listas de ellas)hace falta una nomenclatura que identifique el cambio de secuencia.
La Tabla 15.1 resume las convenciones recomendadas.
Algunos detalles sobre las mutaciones se almacenan en la OMIM, y otros han sido compilados de manera muy útil por Cooper y Krawczak (1995). 

Clasificación de mutaciones: pérdida de función versus ganancia de función 

La clasificación de alelos en A y a esconde una enorme diversidad de cambios en la secuencia del DNA.
Se han descrito unos 500 alelos mutantes de fibrosis quística, y un número similar en el gen de la xxx-globina .
No existe razón alguna por la cual estas mutaciones deban ajustarse a unas pocas categorías.
Sin embargo, en principio la mutación de un gen puede provocar un cambio fenotípico que vaya en uno de los dos sentidos siguientes:
el producto puede tener una función reducida o perderla completamente (mutación de pérdida de función) ; 
el producto puede funcionar de manera anormal (mutación de ganancia de función) .

En algunos casos se conocen mutaciones de ambos tipos, y unas y otras producen fenotipos muy diferentes (página 457).
En general, las mutaciones de pérdida de función producen fenotipos recesivos.
Esto se debe a que, para la mayoría de los productos génicos, la cantidad precisa no es crucial, y se puede salir del paso con la mitad de la cantidad normal.
Así, la mayor parte de los errores hereditarios del metabolismo son recesivos. 
Sin embargo, en algunas ocasiones el 50% del nivel normal no resulta suficiente para la función normal, y se observan efectos de dosis de carácter dominante (ver Tabla 15.3 ).
Las mutaciones de ganancia de función también pueden generar fenotipos dominantes.
Algunas pocas mutaciones hacen que el producto realice algo completamente novedoso.
Más a menudo observamos una desviación respecto de los controles normales -el gen normal provoca un fenotipo anormal debido a que funciona en la célula equivocada, en el momento equivocado o en exceso (ver página 446).
Se dice que hay un efecto dominante negativo cuando el producto matado no solamente pierde su propia función sino que también evita que el producto del alelo normal funcione correctamente en los individuos heterozigotos.
Los efectos dominantes negativos se observan particularmente en aquellas proteínas que funcionan como dímeros o multímeros (ver más adelante, página 448).

Es inevitable que algunas mutaciones no puedan ser clasificadas fácilmente como de pérdida o de ganancia de función. 
Algunas veces el problema es básicamente semántico -¿un canal jónico que esté siempre abierto ha perdido la función de cerrarse o ha ganado la de abrirse de manera inadecuada? Otras veces la dificultad es más fundamental, como ocurre con los efectos dominantes negativos. 

No obstante, la distinción entre pérdida y ganancia de función constituye una primera aproximación bastante útil a la clasificación de las mutaciones. 

Los cambios pueden ser cuantitativos o cualitativos

Las hemoglobinopatías (ver página 457) ilustran la diferencia entre alteraciones cuantitativas y cualitativas en un producto génico.
En las talasemias, el problema es la existencia de una cantidad no adecuada de globina, mientras que en otras hemoglobinopatías el problema es una globina anormal.
En alguna época pasada se pensó que esta distinción podía reflejar dos clases fundamentales de mutaciones: las que introducían cambios cualitativos (anomalías estructurales) provocados por mutaciones en las secuencias codificantes, y las que introducían cambios cuantitativos, provocados por mutaciones en las secuencias de control.
Sin embargo, actualmente sabemos que muchas talasemias se deben a anomalías estructurales que hacen que el mRNA o la proteína sean inestables, por lo que en general se debe examinar cada mutación en particular para determinar cuál es su más probable efecto patogénico. 

Cuando las mutaciones puntuales que afectan a los genes provocan el mismo cambio patológico que las deleciones es probable que nos encontremos ante casos de pérdida de función

Las evidencias puramente genéticas, sin estudios bioquímicos, pueden sugerir si un fenotipo es consecuencia de una pérdida o de una ganancia de función.
Cuando un fenotipo clínico resulta de la pérdida de función de un gen, cabría esperar que todo cambio que inactive al producto génico genere el mismo resultado clínico.
Deberíamos ser capaces de encontrar mutaciones puntuales que tengan el mismo efecto que las mutaciones que delecionen o rompan el gen.
El síndrome de Waardenburg, tipo 1 (MIM 193500) es un ejemplo.
Como se muestra en la Figura 15.1 , entre las mutaciones en el gen PAX3 que provocan el síndrome se incluyen sustituciones, desplazamientos de pauta de lectura, mutaciones en lugares de corte empalme y, en algunos pacientes, la deleción completa de la secuencia de PAX3 .
Estas observaciones demuestran que el síndrome de Waardenburg se debe a una pérdida de función PAX3 .

Cuando una patología determinada es causada únicamente por una mutación específica es probable que nos hallemos ante un caso de ganancia de función 

La ganancia de función tiende a requerir cambios mucho más específicos que la pérdida.
En concordancia con esta idea, cabría esperar que el espectro de mutaciones en condiciones de ganancia de función sea más restringido, y la deleción o la rotura del gen no debería producir el fenotipo de la enfermedad de Huntington (HD, MIM 143100)podría ser un ejemplo de ello.
Como hemos visto anteriormente en el Capítulo 10 (página 288), la única mutación en el gen HD asociada con HD es la expansión de la repetición del trinucleótido.
No se han encontrado mutaciones sin sentido, ni de sentido equivocado, ni de desplazamiento de pauta, ni de lugar de corte y empalme. 
Existe información sobre un caso que confirma que la pérdida de función de una de las copias del gen HD no provoca HD : Ambrose et al., (1994) encontraron un individuo fenotípicamente normal con un punto de translocación que interrumpía el gen HD .
Los pacientes con deleciones cromosómicas que incluyen al gen HD tampoco manifiestan HD, aunque a diferencia del caso de la translocación estos pacientes presentan otras anomalías que podrían enmascarar la manifestación de HD, incluso cuando estuviera presente.
Un punto adicional en favor de una ganancia de función para HD es que los homozigotos son clínicamente idénticos a los heterozigotos.
Sin embargo, la demostración de que la HD es una enfermedad de ganancia de función requerirá la demostración bioquímica de la función que se gana.

Tabla 1

Nomenclatura para la descripción de mutaciones

Sustitución de aminoácidos.


Utiliza el código de una letra: A, alanina; C, cisteína; D, ácido aspártico; E, ácido glutámico; F, fenilalanina, G, glicina; H, histidina; I, isoleucina; K, lisina; L, leucina; M, metionina; N, asparragina; P, prolina; Q, glutamina; R, arginina; S, serina; T, treonina; V, valina; W, triptófano; Y, tirosina; X significa un codón de paro.


R117H - el aminoácido 117 cambiado de arginina a histidina.


G452X - la glicina 542 sustituida por un codón de paro (estos cambios suelen representarse también como Argl 17- His y Gly542- Stop, respectivamente).


Sustitución de nucleótidos.


La nomenclatura de nucleótidos suele ir precedida de nt (nucleótido) o np (par de nucleótidos).
La numeración se suele referir a la hebra con sentido del cDNA (excepto para las mutaciones mitocondriales).
Dado que los extremos 5' de muchos cDNA no están bien definidos, puede ser necesario numerar en referencia a una secuencia publicada específica.
Los nucleótidos que están en el interior de los intrones (que no estarán listados ni numerados en la secuencia del cDNA) se pueden especificar en relación al exón más próximo.


1162(G-A) o np1162(G-A) - una sustitución de la guanina de la posición 1162 del cDNA por adenina.


621+1(G-T) - sustitución de guanina por timina en la primera posición del intrón inmediatamente después de la posición 621 (que es el último nucleótido del exón).


1781-5(C-A)- sustitución de citosina por adenina en un intrón, cinco bases hacia 5' de la posición 1781, la primera del siguiente exón .


Deleciones e inserciones.


Delta- F508 o AF508 - deleción del codón de fenilalanina 508.


nt6232(del5) - deleción de cinco nucleótidos, el primero de los cuales ocupa la posición 6232.


nt409 ( insC )- inserción de citosina tras el nucleótido 409.


Figura 1

Mutaciones de pérdida de función en el gen PAX3 

El gen PAX3 tiene ocho exones.
Las secuencias conocidas cruciales para la función del producto génico están señaladas con los recuadros con trama.
Todas las mutaciones que se muestran fueron encontradas en pacientes con síndrome de Waardenburg.
La S señala una mutación en un sitio de corte y empalme.
Las mutaciones que están escritas en la parte superior del diagrama deberían generar un producto génico truncado, debido a la terminación prematura tras un corrimiento de la pauta de lectura o a una mutación de corte y empalme; las mutaciones situadas en la parte inferior deberían desembocar en una proteína completa pero con sustituciones de aminoácidos.
Algunos pacientes con síndrome de Waardenburg tienen delecionado todo el gen.
Dado que todos estos cambios producen el mismo resultado clínico, es probable que la causa del síndrome sea la pérdida de función del gen.

Mutaciones de pérdida de función 

La pérdida de función puede ser parcial o total.
Los alelos que no generan producto suelen ser denominados alelos nulos .
Algunas veces es posible detectar inmunológicamente el producto génico, aun cuando éste no es funcional, indicando que la proteína sigue siendo sintetizada y transportada.
Estos mutantes reciben la denominación de CRM(iniciales de la frase inglesa positive for cross-reacting material , literalmente positivos para material de reacción cruzada).

Las mutaciones que provocan la deficiencia de una proteína no están necesariamente situadas dentro del gen que la codifica

La agamaglobulinemia (falta de inmunoglobulinas, que lleva a una inmunodeficiencia clínica) suele ser mendeliana. 
Es natural asumir que la causa esté en mutaciones que afecten a los genes de inmunoglobulinas.
Sin embargo, los loci de inmunoglobulinas están situados en los cromosomas 2, 14 y 22 ( Tabla 7.13 ), y las agamaglobulinemias no han sido situadas en estos cromosomas.
Muchas de las formas están ligadas al cromosoma X. Recordando los muchos pasos necesarios para que un polipéptido de nueva síntesis se transforme en una proteína que funcione correctamente (páginas 26-28), esta falta de correspondencia uno a otro entre la mutación y el gen de la proteína estructural no debe constituir una sorpresa.
Tanto los fallos en el procesamiento de los genes de las inmunoglobulinopatías, como en la maduración del linfocito B o en procesos más fundamentales del sistema inmune, todos pueden producir inmunodeficiencia. 

Algunas veces un mismo defecto génico puede producir múltiples defectos enzimáticos.
La enfermedad de las células I, o mucolipidosis II (MIM 252500) se caracteriza por deficiencias en varias enzimas lisosomales.
El defecto, primario no se encuentra en el gen estructural de ninguna de estas enzimas, sino en una N -acetilglucosamina- 1 -fosfotransferasa, que fosforila los residuos de manosa de las enzimas glucosiladas. 
La fosfomanosa es una señal que canaliza las enzimas a los lisosomas; si falta los lisosomas se ven desprovistos de un complemento completo de enzimas.
La lección que se desprende de todos estos ejemplos es que no debemos pecar de ingenuos al especular sobre el defecto génico que subyace a un síndrome clínico.

La función de un gen puede ser destruida por cambios de muchas clases diferentes

La Tabla 15.2 ilustra los principales mecanismos por los que los cambios en un gen pueden destruir o reducir su función.
En general, es difícil predecir si el resultado de una mutación particular será neutro o provocará una pérdida de función parcial o total.
Las predicciones tienen un cierto grado de fiabilidad cuando se trata de roturas o deleciones.
En otros casos sólo es posible establecer predicciones cuando se conocen (bioquímica y clínicamente) los efectos en el gen de muchas otras mutaciones.
Existe una complicación adicional, derivada del hecho de que la mayoría de la gente con trastornos recesivos y cuyos progenitores no están emparentados son heterozigotos compuestos , que presentan dos mutaciones diferentes.
Si ambas mutaciones causan una simple pérdida de función, aunque con diferente grado, el alelo menos severo es el que dicta la pérdida de función global, y por tanto es el responsable del efecto clínico.

Deleciones e inserciones

Los efectos de las pequeñas deleciones o inserciones dependen parcialmente de que generen o no desplazamientos de pauta de lectura (es decir, de si el número de nucleótidos perdidos o ganados es o no múltiplo de 3).
Por ejemplo, dos tercios de las mutaciones del gen de la distrofina son deleciones de uno o más exones, pero algunas de estas deleciones provocan la letal distrofia muscular de Duchenne (DMD), mientras que otras provocan la distrofia muscular de Becker (BMD), un trastorno más benigno.
La explicación no radica en el tamaño de las deleciones (algunas deleciones en la BMD son más grandes que las que provocan la DMD), ni en su posición dentro del gen de la distrofina (algunas deleciones BMD y DMD se superponen), sino en que provoquen o no un desplazamiento de la pauta de lectura.
El gen de la distrofina consiste de 79 exones pequeños ( Figura 7.11 ).
Cuando se clasifica a los exones de acuerdo con la posición de sus fronteras en relación al marco de lectura, se observa que casi todas las deleciones que provocan DMD implican desplazamientos de la pauta, mientras que casi todas las deleciones BMD no la modifican ( Figura 15.2 ).
Las hibridaciones de transferencias Western utilizando anticuerpos apoyan esta interpretación, ya que en las biopsias musculares de pacientes de DMD no se detecta distrofina, mientras que en los pacientes de BMD se observa distrofina de peso molecular reducido.

Variantes de corte y empalme

La supresión de un sitio de corte y empalme o la activación de un sitio críptico ( Figura 10.11 ) suprime, normalmente, la función de los genes afectados.
Los exones son ignorados, o la secuencia de los intrones se retiene en el mRNA maduro.
Estos cambios suelen cambiar la pauta de lectura, cosa que agrava sus efectos. 
En ocasiones, la función se rescata gracias a la existencia de corte y empalme alternativo.
Algunas veces el corte y empalme alternativo también rescata otros tipos de mutación.
Ésta podría ser la explicación para el 1,5% de pacientes con deleciones en el gen de la distrofina que no presentan la correlación esperada con la gravedad de la enfermedad (DMD o BMD) de acuerdo a la hipótesis del desplazamiento de la pauta descrita anteriormente.
También se tienen evidencias de que las mutaciones sin sentido pueden estimular que el exón afectado no sea procesado y su secuencia no pase al mRNA, aunque se desconoce completamente el mecanismo.

Sustituciones de aminoácidos

Predecir el efecto de una mutación de sentido equivocado sin realizar experimentos bioquímicos es difícil. 
Cabe esperar que las sustituciones provoquen pérdida de función cuando: Implican un cambio de aminoácidos no conservador (ver Recuadro 10.2 , página 266).

Afectan a una parte de la proteína importante para la función.
Por ejemplo, las sustituciones en la Figura 15.1 , que provocan todas ellas pérdida de función, se encuentran concentradas sobre el dominio paired y el homeodomain de la proteína PAX3, mientras que las mutaciones que truncan a la proteína se encuentran más esparcidas a lo largo de todo el gen.

Implican un aminoácido que está conservado entre las diferentes especies o entre los miembros de una familia génica (página 201).

Efectos sobre la transcripción, el procesamiento del RNA y la traducción

Cuando se piensa en el posible efecto de una mutación en una secuencia codificante, es importante considerar todas las fases de la expresión génica.
Un cambio que provoque una sustitución de aminoácidos aparentemente inocua o incluso una sustitución sinónima de codones puede abolir un sitio de corte y empalme, o activar un sitio críptico (ver Figura 10.11 ).
En un paciente con síndrome de Waardenburg se encontró una sustitución de aminoácidos A196T (la alanina de la posición 196 sustituida por una treonina) en el gen PAX3 ( Figura 15.1 ).
Esto indicaba que el cambio suprime la función de la proteína PAX3.
Sin embargo, A196T reside fuera de las partes con función importante conocida de la proteína, y además no se trata de un residuo muy conservado, por lo que el resultado fue inesperado.
Sin embargo, si se inspecciona la secuencia genómica se observa que el nucleótido cambiado es el último de un axón.
Los nucleótidos de la posición 3' de los exones suelen ser G ( Figura 1.15 ), y se sabe que una sustitución G-C en esta posición (que genera la sustitución A196T en el caso que nos ocupa al nivel de aminoácidos) afecta la eficiencia del corte y empalme.
Otro ejemplo es la mutación D26K (aspártico en la posición 26 sustituido por lisina) en la xxx-globina de la hemoglobina E.
Esta mutación causa xxx-talasemia , un efecto clínico inesperado.
En el DNA, el codón 26 está cerca de un sitio dador de corte y empalme, situado en el codón 30 ( Figura 1.19 ), y el cambio G-A que provoca la sustitución de aminoácidos D26K reduce la eficiencia de la reacción de procesamiento. 

Tabla 2

Quince formas de reducir o suprimir la función de un producto génico

Cambio.
Ejemplo.


Deleción total del gen.
La mayoría de las mutaciones de la xxx-talasemia .


Deleción parcial del gen.
60% de las mutaciones de la distrofia muscular de Duchenne.


Alteración de la estructura del gen.


1) por una translocación.
Translocaciones X-autosoma en mujeres con distrofia muscular de Duchenne.


2) por una inversión Inversión en el gen F8 .


Inserción de una secuencia en el gen.
Inserción de una secuencia repetitiva LINE-1 (página 293) en el gen F8 en la hemofilia A.


Inhibición o prevención de la transcripción .
Mutación completa del X-frágil.


Mutación en el promotor que reduce la producción de mRNA.
Mutación -29(A-G) en la, xxx-globina .


Disminución de la estabilidad del mRNA.
Hb Constant Spring.


Inactivación de sitio dador en el corte y empalme(provocando que se lea el intrón como parte de la secuencia codificante) .
Mutación 451+1 (G-T) en PAX3.


Inactivación de sitio dador o aceptor (provocando que el exón no sea reconocido como tal y no se procese).
Mutación 452-2(A-G) en PAX3.


Activación de sitio críptico (provocando la pérdida o la ganancia de secuencia codificante).
Mutación 1-110(G-A) en intrón de la xxx- globina que provoca la xxx-talasemia mediterránea.


Introducción de un desplazamiento en la pauta de traducción .
Mutación 874ins(G) en PAX3.


Conversión de un codón codificante en un codón de paro.
Mutación Q254X en PAX3.


Sustitución de un aminoácido esencial.
Mutación R271C en PAX3.


Impedir el procesamiento postraduccional.
Proinsulina resistente al corte en hiperproinsulinemia familiar.


Impedir la localización celular correcta de un producto .
Mutación AF508 en fibrosis quística .


Figura 2

Las deleciones en la parte central del gen de la distrofina están asociadas con las distrofias musculares de Becker y de Duchenne

Mutaciones somáticas y cáncer 

Las mutaciones somáticas no sólo son frecuentes, son inevitables.
Las tasas de mutación en humanos son de xxx a xxx por gen y por generación, y nuestros cuerpos contienen probablemente xxx células. 
De esto se deduce que cada uno de nosotros debe ser un mosaico somático de innumerables enfermedades genéticas (ver página 262).
Esto no debe preocuparnos.
Si una célula de un dedo marta al genotipo HD, o una célula de una oreja sufre una mutación de CF, esto no tiene consecuencia alguna para el individuo afectado.
Únicamente cuando una mutación somática resulta en la aparición de un clon importante de células mutantes se pone en riesgo a todo el organismo.
Esto puede ocurrir de dos maneras:

- mutación causa la proliferación anormal de una célula que normalmente se replica poco o nada, por lo que se acaba generando un clon de células mutantes.

- mutación aparece en las fases iniciales del embrión, afectando a una célula que es la progenitura de una fracción significativa de todo el organismo.


En este capítulo consideraremos las dos clases de mutaciones somáticas, comenzando con las que provocan cáncer.
Ver el Recuadro 18.1 para un resumen de las diferencias genéticas entre las células de un mismo individuo (o de un par de gemelos idénticos).

Cáncer: selección natural entre las células somáticas de un organismo

Las células de los organismos pluricelulares colaboran para el bien común -pero sólo hasta un cierto punto. 
La evolución por selección natural también se aplica a las células constituyentes de un organismo, además de al organismo completo.
En última instancia las células pueden dejar progenie sólo si colaboran a que el organismo que forman pueda reproducirse, por lo que su genoma ha evolucionado necesariamente para la cooperación. 
Sin embargo, a corto plazo, si una célula tiene una mutación que le confiere una ventaja reproductiva, se transformará en un clon exitoso.
Sus descendientes ocuparán el organismo, a no ser que controles superiores evolucionados para proteger el interés del organismo completo lo impidan.

El cáncer es el resultado natural de esta selección entre células somáticas.
Probablemente, el cáncer es el estado final normal de todo organismo pluricelular que viva lo suficiente -pero los organismos que triunfan luchan para posponer este destino.
Durante un millardo de años de evolución han evolucionado muchos niveles de sofisticados controles, de manera que el cáncer clínico sólo aparece cuando varios controles independientes se han perdido. 
El principal nivel de control entre los de orden superior es un mecanismo por el cual las células potencialmente cancerosas se suicidan entrando en un programa de muerte celular programada ( apoptosis ).
Las células de cáncer que tienen éxito deben haber desarrollado sistemas para inactivar o sortear este mecanismo (página 508).

Hace varias décadas los estudios sobre la dependencia del cáncer respecto de la edad sugirieron que se necesita un promedio de seis a siete mutaciones sucesivas para convertir una célula normal en un carcinoma invasivo.
La probabilidad de que una misma célula sufra tantas mutaciones independientes es despreciable; sin embargo, existen dos mecanismos generales que pueden hacer que la progresión sea más probable (Recuadro 17.1) .

En estudios recientes se han identificado tres grupos de genes que resultan frecuentemente matados en el cáncer: 

- (página 495 en adelante).
Estos genes ejercen una acción positiva en la promoción de la proliferación celular.
Las versiones normales no mutantes se llaman adecuadamente protooncogenes .
Las versiones mutantes son excesiva o inadecuadamente activas.
Un solo alelo mutante puede afectar al fenotipo de la célula.

- supresores de tumores (TS, de tumor supressor , pág. 502 en adelante).
Los productos de los genes TS inhiben la proliferación celular.
Las versiones mutantes en las células cancerosas han perdido su función.
Para que se altere el comportamiento de la célula se deben inactivar los dos alelos de un gen TS.

- matadores (pág. 508 en adelante).
Estos genes son responsables del mantenimiento de la integridad del genoma y de la fidelidad de la transferencia de información. 
Cuando se pierde la función de ambos alelos la célula pasa a ser susceptible a errores.
Entre los errores al azar pueden ocurrir mutaciones que afecten a oncogenes y genes TS.


Haciendo un símil con un autobús, uno se podría imaginar que los oncogenes son como el acelerador y los genes TS son el freno.
Apretar el acelerador a fondo (una mutación dominante de ganancia de función en un oncogén) o perder los frenos (una mutación recesiva de pérdida de función en un gen TS) harán que el autobús se acelere fuera de control.
Alternativamente, podría ocurrir que un saboteador se dedicara sencillamente a aflojar tornillos y tuercas al azar y esperar a que ocurriera un desastre (una mutación en un gen matador).

Dos maneras de hacer que una serie de mutaciones sucesivas sea más probable

Para que una célula normal se vuelva cancerosa maligna hacen falta que coincidan en la misma célula probablemente hasta seis mutaciones específicas.
Dadas las tasas típicas de mutación, de xxx por gen por célula, resulta extremadamente improbable que una célula en particular sufra tantas mutaciones a la vez (y esto es lo que permite que la mayoría de nosotros sigamos vivos). 
La probabilidad relativa de que esto ocurra a alguna de las xxx células que nos forman es xxx , es decir, xxx .
No obstante, el cáncer ocurre, y esto puede explicarse por una combinación de dos mecanismos:

- mutaciones estimulan la proliferación celular, creando una población expandida de células diana para la siguiente mutación (Figura 17.1) .

- mutaciones afectan a la estabilidad de todo el genoma, aumentando la tasa de mutación global.


Figura 1

Evolución multifásica del cáncer

Cada nueva mutación confiere una ventaja de crecimiento a la célula, de manera que en cada fase se expande un clon proveniente de la fase anterior, presentando una mayor diana para la próxima mutación.

Oncogenes

Los virus de tumores animales proporcionaron la primera evidencia de la existencia de oncogenes

Por muchos años se había sabido que algunas leucemias, linfomas y cánceres animales eran provocados por virus.
También se conocen algunos ejemplos en humanos ( Tabla 17.1 ).
Los virus de tumores pueden clasificarse en tres amplias categorías: 

- de DNA, que normalmente infectan líticamente a las células.
Provocan tumores mediante la rara integración anómala de su DNA en el DNA de una célula huésped no permisiva (esto es, una célula que no permite la infección lítica habitual).
De alguna manera la integración del genoma vírico implanta las señales de activación transcripcional o de replicación del virus en el genoma del huésped y dispara la proliferación celular.
Algunos de los genes víricos implicados han sido identificados, como los del antígeno T de xxx o las proteínas E1A y E1B de los adenovirus.
A diferencia de los oncogenes retrovíricos clásicos (ver página siguiente), estos genes son específicos del virus y no tienen equivalentes celulares exactos.

- cuyo genoma es de RNA.
Se replican a través de un intermediario de DNA, que se fabrica empleando una transcriptasa inversa vírica ( Figura 17.2 ).
Normalmente, estos virus no matan la célula huésped (HIV es una excepción), y raramente la transforman.
El genoma de un retrovirus típico está formado por tres genes, gag , pal y env( Figura 17.3A ). 

- que provocan transformación aguda, que son partículas retrovíricas que, a diferencia de los retrovirus normales, transforman la célula huésped rápidamente y con alta eficiencia.
Sus genomas incluyen un gen adicional, el oncogén ( Figura 1 7.3B ).
Habitualmente el oncogén sustituye uno o más genes víricos esenciales, por lo que estos virus son de replicación defectuosa.
Para poder propagarlos hay que cultivarlos en células que a la vez están infectadas con un virus colaborador capaz de replicarse, que es el que proporciona las funciones perdidas.
Los estudios realizados sobre esta clase de retrovirus han revelado más de 50 oncogenes diferentes.


Un ensayo de transfección in vitro confirmó que las células cancerosas contienen oncogenes activados

Un ensayo de transformación celular fue la fuente de una vía completamente independiente de descubrimiento de oncogenes.
La línea celular de ratón NIH-3T3 puede transformarse fácilmente in vitro , probablemente porque ha adquirido ya algunos de los cambios genéticos sucesivos en la ruta del cáncer, y sólo un cambio adicional basta para transformarla.
En el test de 3T3 las células son transformadas con fragmentos aleatorios de DNA derivados de células cancerosas humanas. 
Los oncogenes potenciales pueden identificarse seleccionando a las células transformadas y recuperando el DNA humano que contienen ( Figura 17.4 ).
Sólo se obtienen transformantes cuando se usa DNA derivado de células tumorales, no de células no tumorales.
Así, las células tumorales, incluso de tumores no víricos, contienen varios oncogenes activados.
Este método permitió la identificación del mismo grupo de genes que se encontraron con los retrovirus de transformación aguda.

Los oncogenes son versiones matadas de genes implicados en diversas funciones celulares normales 

Pronto se hizo aparente que las células normales poseían equivalentes normales de todos los oncogenes víricos ( Tabla 17.2 ), y que los genes v-onc eran, de hecho, genes celulares transducidos.
Con pocas excepciones, los productos de los genes v-onc difieren de sus equivalentes c-onc(protooncogenes) en sustituciones o interrupciones, que sirven para activar al protooncogén.

La comprensión funcional de los oncogenes comenzó al descubrir que el oncogén vírico v-sis provenía del gen celular normal PFGFB, que codifica el factor de crecimiento derivado de plaquetas B.
La sobreexpresión incontrolada de un factor de crecimiento sería una causa obvia de hiperproliferación celular.
En la actualidad se han dilucidado las funciones de muchos oncogenes celulares (que en rigor deberíamos llamar protooncogenes, Tabla 17.2 ).
Todos ellos controlan exactamente el tipo de funciones celulares que podrían predecirse que estuvieran perturbadas en el cáncer.
Se pueden distinguir cinco amplias clases:

- de crecimiento secretados (por ejemplo, SIS )
- receptores de superficie celular (por ejemplo, ERBB FMS )
- de sistemas intracelulares de transducción de señales (por ejemplo, la familia RAS, ABL )
- nucleares de unión a DNA, incluyendo factores de transcripción (por ejemplo, MYC, JUN )
- de la red de ciclinas, quinasas dependientes de ciclinas e inhibidores de quinasas que gobiernan el progreso a través del ciclo celular, por ejemplo PRAD1 .

Tabla 1

Virus tumorales animales y humanos

Especie.
Enfermedad.
Virus.


Mono.
Sarcoma.
SV40 .


Ratón.
(transformación in vitro .
Adenovirus.


Hombre.
Cáncer cervical.
Papilomavirus HPV16.


Hombre.
Cáncer nasofaríngeo.
Virus de Epstein- Barr.


Hombre.
Leucemia de linfocitos T.
HTLV-1, HTLV- 2.


Hombre.
Sarcoma de Kaposi.
HHV8 o KSAAV .


Gallina.
Sarcoma.
Virus del sarcoma de Rous.


Rata.
Sarcoma.
Virus del sarcoma de Harvey de rata.


Ratón.
Leucemia.
Virus de la leucemia de Abelson.


Mono.
Sarcoma.
Virus del sarcoma simio.


Gallina.
Eritroleucemia.
Virus de la eritroleucemia.


Gallina.
Sarcoma.
Virus 17 del sarcoma aviar.


Ratón.
Osteosarcoma .
Osteosarcoma FBJ.


Gato.
Sarcoma.
Virus felino del sarcoma de McDonough.


Gallina.
Mielocitoma.
Virus de la mielocitomatosis aviar.


Figura 2

Ciclo vital de un retrovirus

La partícula vírica (arriba) contiene el genoma de RNA y una transcriptasa inversa rodeados de una cubierta lipoproteica más externa y una cápside proteica interna.
Una copia de doble cadena de DNA del genoma vírico se integra en el DNA del huésped. 
Desde aquí dirige la síntesis de RNA y proteínas víricas, que se autoensamblan y salen de la célula desprendiéndose como por gemación a partir de la membrana.
La célula huésped no muere.

Figura 3

Retrovirus normal y retrovirus capaz de transformación aguda

El genoma de RNA contiene repeticiones terminales (R), secuencias subterminales únicas (U5, U3) y tres genes, gag , poly env.
A través de un complicado esquema de corte y ensamble y de procesamiento postraduccional se generan varios productos proteicas diferentes.
En un retrovirus capaz de transformación aguda (abajo), uno o más de los genes víricos es sustituido por una secuencia celular transducida, el oncogén.

Inicialmente se traduce a una proteína de fusión. 

Figura 4

El ensayo de NIH-3T3

Se transfectan células 3T3 de ratón con fragmentos aleatorios del DNA de un tumor humano.
Se aíslan todas las células transformadas (identificadas por su crecimiento alterado) y a partir de su DNA se construyen bibliotecas de DNA en fagos.
Los fagos se criban buscando la repetición Alu , especifica humana, para identificar aquellos que contienen fragmentos de DNA humano, que potencialmente contienen oncogenes.

Tabla 2

Oncogenes víricos y celulares 

Enfermedad vírica.
Función .


Sarcoma simio.
Subunidad B del factor de crecimiento derivado de plaquetas.


Eritroleucemia de pollo.
Receptor del factor de crecimiento epidérmico.


Sarcoma felino de McDonough .
Receptor del factor estimulador de colonias de macrófagos .


Sarcoma de Harvey de rata.
Componente p21 de la transducción de señales por proteínas G.


Leucemia de Abelson de ratón.
Tirosina quinasa de proteínas .


Sarcoma 17 aviar.
Factor de transcripción AP-1.


Mielocitomatosis aviar.
Proteína de unión a DNA (factor de transcripción) .


Osteosarcoma de ratón.
Factor de transcripción que se une a DNA.


Activación de protooncogenes

Algunos de los ejemplos más gráficos de la patología molecular en acción provienen de las diferentes formas de activación de los protooncogenes. 
La activación implica una ganancia de función. 
Ésta puede ser cuantitativa (un aumento de producción de un producto inalterado) o cualitativa (producción de un producto sutilmente modificado por una mutación o producción de un producto nuevo a partir de un gen quimérico creado por una reordenación cromosómica).
Estos cambios son dominantes y normalmente afectan sólo a uno de los alelos del gen.

Las mutaciones activadores de oncogenes (a diferencia de las mutaciones en los genes TS, ver más adelante) son prácticamente siempre somáticas.
Las mutaciones constitucionales serían probablemente letales.
Sin embargo, cabe destacar que esto sólo vale para mutaciones que activan a protooncogenes.
Otras mutaciones pueden heredarse constitucionalmente, y pueden tener efectos completamente no relacionados con el cáncer.
Por ejemplo, las mutaciones que inactivan el oncogén kit producen piel moteada ( piebaldism , MIM172800), mientras que las mutaciones de pérdida de función en el oncogén RET producen la enfermedad de Hirschsprung.
Ambos trastornos se heredan con un patrón mendeliano dominante (con poca penetrancia en el caso de las mutaciones RET ).

Sólo se conoce un caso de un oncogén activado que puede heredarse de modo que cause cáncer familiar. 
Se trata el gen RET , implicado en múltiples neoplasias endocrinas y cáncer de tiroides familiar (ver página 500).
Este comportamiento es tan atípico que cabría plantearse si RET debe ser o no clasificado como un oncogén.
De hecho, RET ilustra las limitaciones de forzar la clasificación de todos los genes que gobiernan la proliferación celular en sólo dos categorías, oncogenes o genes TS.
Otro gen difícil de clasificar es TP53 (ver página 507); sin embargo, TP53 se ajusta mucho mejor a la categoría TS que a la de los oncogenes.

Algunos oncogenes se pueden activar por amplificación

Muchas células cancerosas contienen muchas copias de oncogenes estructuralmente normales.
En el cáncer de mama se suele amplificar ERBB2 y algunas veces MYC ; un gen parecido, NMYC , suele amplificarse en las fases tardías de los neuroblastomas.
En estos casos pueden encontrarse cientos de copias extra.
Estas copias pueden existir como pequeños cromosomas independientes (pares de cromosomas diminutos , o double minute chromosomes ) o como inserciones dentro de cromosomas normales (regiones de tinción homogénea, HSR ).
Los sucesos genéticos que producen las HSR pueden ser bastante complejos, debido a que habitualmente estas regiones contienen secuencias derivadas de varios cromosomas distintos.
En ciertas condiciones se pueden observar amplificaciones génicas similares en células no cancerosas (cuando se las somete a condiciones de selección muy estrictas, por ejemplo, se puede observar amplificación de genes de la dihidrofolato reductasa en células seleccionadas por su resistencia a metotrexato).
En todos los casos, el resultado es un gran aumento del nivel de la expresión del gen amplificado.

Existe una técnica alternativa, la hibridación comparativa de genomas , o CGH , desarrollada por Kallioniemi et al. , 1992, que en principio puede revelar todas las regiones de amplificación en un solo experimento, junto con cualquier región donde haya pérdida de alelos o aneuploidía, lo cual podría apuntar a genes TS (ver más adelante).
La prueba CGH ( Figura 17.5 ) emplea una mezcla de DNA de células normales y tumorales equivalentes en una FISH competitiva.
Con la ayuda de programas de procesamiento de imágenes se pueden detectar regiones cromosómicas donde la relación de señales FISH del DNA normal y del DNA del tumor se desvía de lo esperado.
Dependiendo del sentido de la desviación, estas regiones indican amplificación o pérdida de alelos en el tumor.

Algunos oncogenes se activan por mutaciones puntuales

El gen HRAS ( Tabla 17.2 ) pertenece a la familia de genes ras , que codifican proteínas p21 que participan en la transducción de señales a partir de receptores acoplados a proteínas G.
En estos sistemas de transducción de señales la señal que proviene del receptor dispara la unión de GTP a la proteína RAS, y GTP-RAS transmite la señal a otras dianas dentro de la célula.
Las proteínas RAS tienen actividad GTPasa, por lo que GTP-RAS se convierte rápidamente en GDP-RAS, que es inactiva.
Es frecuente encontrar mutaciones puntuales específicas que afectan a genes ras en células de muchos tumores distintos, incluyendo cáncer de colon, pulmón, mama y vejiga.
Estas mutaciones llevan a sustituciones de aminoácidos que disminuyen la actividad GTPasa de la proteína RAS.
En consecuencia, la señal GTP-RAS se inactiva más lentamente, lo que provoca una respuesta celular excesiva a la señal que proviene del receptor.

Otro oncogen que se activa por mutaciones puntuales es RET .
Este gen codifica una tirosina quinasa de superficie celular, que es un receptor para un ligando actualmente desconocido.
En la neoplasia endocrina múltiple tipo 2 y en cáncer medular de la tiroides se encuentran mutaciones que llevan a sustituciones de ciertos residuos de cisteína.
El gen RET es interesante como ejemplo de un gen en el que diferentes mutaciones causan diferentes fenotipos (página 457).

Figura 5

Hibridación comparativa de genomas

Los DNA del tumor y del control normal fueron marcados con fluorescencia verde y roja, respectivamente, e hibridados in situ conjuntamente y a concentraciones iguales a los cromosomas de una célula normal.
Las curvas muestran barridos generados por ordenador de la proporción de la intensidad de fluorescencia verde y roja a lo largo de tres cromosomas.
(A) Cromosoma 1: la línea celular xxx , de cáncer de mama, es portadora de una copia extra de xxx , y de una posible delación intersticial del proximal.
(B) Cromosoma 8: la línea celular xxx , de cáncer de colon, presenta una amplificación de la región que contiene a MYC, en xxx .
(C) Cromosoma 2: la línea celular NIH-H69, de cáncer microcítico de pulmón, presenta tres regiones amplificadas.
NMYC es amplificado en xxx ; el contenido de las otras dos regiones amplificadas es desconocido.

Figura 6

Cariotipo de una célula cancerosa

Enfermedades complejas

Decidir si un carácter no mendeliano es genético: el papel de los estudios familiares, de los estudios sobre gemelos y de los estudios sobre adopción 

A nadie se le ocurre disputar el papel de los genes en un carácter que consistentemente da patrones de herencia mendeliana o que está asociado a anomalías cromosómicas. 
Sin embargo, cuando se trata de caracteres no mendelianos, ya sean continuos (cuantitativos) o bien discontinuos (dicotómicos), es necesario demostrar la determinación genética. 
La estrategia más obvia es demostrar que el carácter se hereda.
La medida del grado de asociación familiar es xxx que mide la relación entre el riesgo de los parientes de los pacientes y la prevalencia poblacional. 
Para los diferentes grados de parentesco se calculan parámetros xxx distintos (es decir, habrá un xxx para hermanos, uno para hijos, etc.).
En la Tabla 18.1 se muestran resultados de estudios iniciales sobre esquizofrenia.
La asociación familiar es evidente si se consideran los elevados valores de xxx , y tal como era de esperar los valores xxx se aproximan de nuevo a 1 para parentescos más distantes - pero esta disminución vale sólo para estos datos, no para lo que sería predecible si se tratara de un carácter puramente genético. 

Importancia de un ambiente familiar compartido

Los genetistas no deben olvidar que los seres humanos, aparte de sus genes, dan a sus hijos un ambiente.
Muchos caracteres se transmiten de manera familiar debido a que existe un ambiente común -por ejemplo, si la lengua nativa es inglés o chino.
En consecuencia siempre se debe plantear si un carácter familiar puede explicarse debido a la existencia de un ambiente común.
Esto resulta particularmente importante para atributos comportamentales como el cociente de inteligencia (CI) o la esquizofrenia, que dependen al menos parcialmente de la educación, pero no puede ser ignorado incluso para caracteres físicos o defectos de nacimiento: una familia puede compartir una dieta inusual o alguna medicina tradicional que puede provocar defectos de desarrollo. 

La teoría poligénica (ver página 518 en adelante) permite predecir el grado con que los parientes deben parecerse entre sí, y el análisis de segregación (página 528) puede intentar estimar esto variando las hipótesis entre causas genéticas y ambientales, aunque en la práctica todos estos análisis raramente llevan a la demostración no ambigua y no controvertida de la existencia de factores genéticos.
Así, para demostrar que un carácter no mendeliano está sometido a control genético suele hacer falta algo más que una tendencia familiar.

Desafortunadamente estas reservas no quedan tan claramente explícitas en la literatura médica como quizás fuera deseable.

Los estudios sobre gemelos tienen muchas limitaciones

Francis Galton, que sentó gran parte de las bases de la genética cuantitativa, destacó el valor que los gemelos tienen para la genética humana.
Los gemelos monozigóticos (MZ) son clones genéticamente idénticos y necesariamente serán concordantes (coincidirán) en todos los caracteres determinados genéticamente, con unas pocas excepciones ( Recuadro 18.1 ).
Esto es cierto con independencia del patrón de herencia o del número de genes implicados en la producción del carácter; las únicas excepciones son las de caracteres que dependen de mutaciones somáticas postzigóticas (Capítulo 17).
Los gemelos dizigóticos (DZ) comparten un promedio de la mitad de sus genes, lo mismo que cualquier par de hermanos.
Los caracteres genéticos deberían, por tanto, mostrar una mayor concordancia entre gemelos MZ que entre gemelos DZ, y muchos caracteres lo hacen ( Tabla 18.2 ).

También es posible observar mayor concordancia en gemelos MZ que en DZ para caracteres determinados por factores ambientales.
Para comenzar, la mitad de los gemelos DZ son de sexo distinto, mientras que todos los gemelos MZ son del mismo sexo.
Incluso si restringiéramos la comparación a los gemelos DZ del mismo sexo (como en los estudios de la Tabla 18.2 ), se puede argumentar, al menos para caracteres comportamentales, que es más probable que los gemelos MZ sean mucha más parecidos entre sí, y que sean vestidos y tratados igual, por lo que compartirán más factores ambientales que los gemelos DZ.

Los gemelos MZ que son separados en el momento del nacimiento y criados en ambientes completamente independientes serían el experimento ideal (Francis Crick sugirió una vez informalmente que un miembro de cada par de gemelos debería ser donado a la ciencia para este propósito).
Separaciones como ésta ocurrían en el pasado con mayor frecuencia de la que uno esperaría, debido a que el nacimiento de gemelos era algunas veces la gota que colmaba el vaso de los problemas para una madre sobresaturada.
Se pueden hacer fascinantes programas de televisión sobre gemelos reencontrados tras 40 años de separación, que descubren que tienen trabajos similares, se visten de manera parecida y tienen los mismos gustos musicales.
Sin embargo, como material de investigación los gemelos separados tienen muchas limitaciones: 

- Existen muy pocos casos, por lo que toda investigación se basa en un número pequeño de individuos probablemente excepcionales.

- A menudo la separación no era total -los gemelos solían separarse algún tiempo tras el nacimiento, y ser criados por parientes.

- Existe un sesgo en la evaluación -todo el mundo quiere saber sobre gemelos separados extraordinariamente parecidos, pero los gemelos separados que son muy diferentes no son una noticia atractiva.

- Aunque sólo sea en principio, la investigación sobre gemelos separados no puede distinguir entre causas ambientales intrauterinas y causas genéticas.
Esto puede ser importante, por ejemplo en estudios sobre orientación sexual (el «gen gay»), donde algunos individuos han sugerido que las hormonas maternas pueden afectar al feto in utero de modo que influyan en su futura orientación sexual.


Así, a pesar de toda la fascinación anecdótica que generan, los gemelos separados han contribuido relativamente poco a la investigación en genética humana.

Los estudios sobre adopción son la manera más poderosa de desenmarañar los factores genéticos y los ambientales

Si los gemelos separados son una vía no práctica para desenmarañar la herencia del ambiente familiar, la adopción es mucho más prometedora.
- Encontrar individuos adoptados que sufren de una enfermedad particular transmitida familiarmente, y ver si se transmite en sus familias biológicas o en sus familias adoptivas.

- Alternativamente, comenzar con sujetos afectados cuyos hijos han sido adoptados y llevados lejos del entorno familiar, y ver si la adopción disminuye el riesgo de la enfermedad en estos niños.


Un celebrado (y controvertido) estudio de Rosenthal y Kety (1968) utilizó el primero de estos diseños para comprobar factores genéticos en esquizofrenia.
Los criterios para el diagnóstico empleados en este estudio han sido criticados; también se ha dicho (cosa que se disputa) que todos los diagnósticos se hicieron a ciegas. 
Sin embargo, un reanálisis independiente utilizando criterios diagnósticos del DSM-III, llegó esencialmente a las mismas conclusiones.
La Tabla 18.3 muestra los resultados de una extensión posterior de este estudio.

El principal obstáculo en los estudios de adopción es la falta de información sobre la familia biológica. 
Esto se ve agravado porque no es deseable acercarse a ellos con preguntas.
Un problema secundario es la colocación selectiva, es decir, que la agencia de adopción, en interés del niño, elige a una familia que probablemente sea parecida a la familia biológica.
Así, la organización de un estudio puede ser muy difícil.
No obstante, en los países con registros de adopción eficientes, éste constituye sin duda el método más poderoso para comprobar si un carácter está determinado genéticamente.
También se pueden estudiar los caracteres cuantitativos, comparando la correlación entre un adoptado y sus parientes adoptivos.

La próxima sección describe la principal teoría utilizada para explicar cómo se pueden determinar genéticamente los caracteres no mendelianos.

Tabla 1

Estudios pioneros de riesgo familiar en esquizofrenia

- repertorio de anticuerpos y de receptores de linfocitos T (debido a las reordenaciones epigenéticas y a mutaciones somáticas, ver página 83).
- número de moléculas de DNA mitocondrial (reparto epigenético).
- somáticas en general (Capítulo 17).
- patrón de inactivación del cromosoma X, si es mujer.


Tabla 2

Estudios de gemelos en esquizofrenia 

Tabla 3

Un estudio de adopción y esquizofrenia

Teoría poligénica de los caracteres cuantitativos

Como vimos en el Capítulo 3, página 85, los primeros años de este siglo estuvieron marcados por la divergencia entre dos tradiciones en la genética.
Los seguidores de Bateson vieron la genética como el estudio de la transmisión y segregación de genes mendelianos, incluso si las variantes fenotípicas implicadas eran raras o triviales, mientras que la escuela de los biométricos, fundada por Francis Galton, veía la genética como el estudio estadístico de la variación de importancia evolutiva, centrada normalmente en caracteres cuantitativos. 
Fisher, en 1918, logró la unificación teórica, al demostrar que los caracteres propugnados por los biométricos podían ser descritos en términos mendelianos si eran poligénicos (es decir, gobernados por la acción simultánea de varios loci génicos). 

Todo carácter variable que dependa de la acción aditiva de un gran número de causas individualmente pequeñas e independientes tendrá una distribución poblacional normal (gaussiana).
Esto puede apreciarse en el muy simplificado ejemplo de la Figura 18.1 .
Suponemos primero que el carácter depende de alelos de un único loci, después de dos loci, luego de tres.
A medida que se incluyen más loci se aprecian dos consecuencias: 

- (i) Desaparece la relación simple uno a uno entre genotipo y fenotipo: con excepción de los fenotipos extremos, es imposible inferir el genotipo a partir del fenotipo. 

- A medida que aumenta el número de loci, la distribución se parece cada vez más a una campana de Gauss.
La adición de un poco de variación ambiental suavizaría la distribución de tres loci dándole la apariencia de una buena campana de Gauss.


Si se realiza un tratamiento más sofisticado, introduciendo dominancia y variando las frecuencias génicas, se llega a las mismas conclusiones.
Dado que los parientes comparten genes, sus fenotipos están correlacionados, y el artículo de Fisher predijo la magnitud de la correlación para los diferentes parentescos.

Los fenotipos en los que los parientes comparten algunos, pero no todos los determinantes, presentarán regresión a la media, sean los determinantes genéticos o ambientales

La regresión a la media es una característica largamente incomprendida, tanto en relación a los datos biométricos como a la teoría poligénica. 
A efectos de la discusión utilizaremos el ejemplo del cociente de inteligencia (CI).
A pesar de su poco interés intrínseco (mide sólo la capacidad y voluntad de una persona de resolver el test del CI, no la inteligencia de esa persona en sentido general, y mucho menos su valor como tal persona), las discusiones acerca de la base genética del CI se presentan al gran público por razones políticas. 
Basándonos en las suposiciones genéticas más sencillas, si uno analizara las madres con un CI de 120, sus hijos tendrían un CI promedio de 110, a mitad de camino entre el valor materno y la media poblacional.
Las madres con un CI de 80 tendrían hijos de CI promedio de 90.
La regresión a la media suele ser malinterpretada (ver Recuadro 18.2 ).

En la Figura 18.2 se muestra la regresión en nuestro modelo simplificado de dos loci.
Para cada clase de madres, el CI promedio de sus hijos se encuentra a mitad de camino entre el valor materno y la media poblacional.

Sin embargo, esto depende de una suposición tácita de este modelo, que el apareamiento sea aleatorio.
El modelo supone que para cada clase de madres el promedio del CI de sus parejas será de 100.
Así, el CI promedio de sus hijos es el promedio del CI de los progenitores, como sugeriría el sentido común.
En el mundo real, las mujeres muy inteligentes tienden a casarse con hombres de inteligencia superior a la media (apareamiento clasificador), y en estos casos no cabría esperar regresión a la mitad de camino respecto a la media poblacional, incluso si el CI fuera un carácter exclusivamente genético. 

El modelo simplificado de la Figura 18.2 supone que no hay dominancia.
Así, el fenotipo de cada persona es la suma de la contribución de cada alelo en el locus relevante.
Si se introduce dominancia, el efecto de alguno de los genes de los padres quedará enmascarado por los alelos dominantes y será invisible en el fenotipo.
Sin embargo, estos genes pueden ser transmitidos y pueden afectar al fenotipo de los hijos.
Una vez que se introduce la dominancia, ya no cabe esperar que el niño presente la media de los progenitores.
Nuestra mejor predicción sobre el efecto fenotípico más probable de los alelos recesivos enmascarados se obtiene de la observación del resto de la población.
Por lo tanto, el fenotipo esperado del niño quedará desplazado del valor medio de los progenitores a la media poblacional. 
La magnitud del desplazamiento depende de la importancia de la dominancia en la determinación del fenotipo.

La heredabilidad es la proporción de la varianza debida a efectos genéticos aditivos

Las campanas de Gauss son especificadas sólo por dos parámetros, la media y la varianza (o la desviación típica, que es la raíz cuadrada de la varianza).
Las varianzas tienen la útil propiedad de ser aditivas cuando se deben a causas independientes ( Recuadro 18.3 ).
De esta forma, la varianza global del fenotipo Vp es la suma de las varianzas debidas a las causas individuales de la variación -la varianza ambiental, VE Y la varianza genética, VG. 
A SU vez, se puede diseccionar VG a una varianza VA debida a efectos genéticos aditivos simples y a un término extra, VD, debido a efectos de dominancia.
La heredabilidad (h2) de un carácter es la proporción de la varianza total que tiene base genética, es decir, VG/VP.
Para los criadores de animales interesados en desarrollar vacas lecheras de elevado rendimiento ésta es una medida importante de cuán lejos pueden llegar con su programa de cría para crear un rebaño en el que el animal promedio se parece al mejor que existe en la actualidad.

Estrictamente, VG/VP es la heredabilidad en sentido amplio. 
La varianza de la dominancia no puede fijarse a través del programa de cría, de manera que la respuesta a la selección viene en realidad determinada por la heredabilidad restricta, VA/VP En los caracteres humanos las heredabilidades se suelen estimar como parte del análisis de segregación (ver página 528 y Tabla 18.5 ).

El término «heredabilidad» suele confundirse. 
La heredabilidad es bastante diferente del patrón o modo de herencia.
El modo o patrón de herencia (autosómica dominante, poligénica, etc.) es una propiedad fija de un carácter, pero la heredabilidad no lo es.
La «heredabilidad del CI» es, en realidad, la heredabilidad de las variaciones en el CI.
En circunstancias sociales diferentes se tendrá una diferente heredabilidad del CI.
En una sociedad igualitaria cabría esperar que el CI tuviera una mayor heredabilidad que en una sociedad donde el acceso a la educación dependiera de las circunstancias del nacimiento.
Si todos tienen las mismas oportunidades desaparecen muchas diferencias ambientales entre individuos. 
En consecuencia, las diferencias restantes en el CI serán debidas en mayor medida a diferencias genéticas entre los individuos.
Contrástense las siguientes dos preguntas:

- ¿Hasta qué punto es genético el CI? La pregunta carece de sentido.

- proporción de las diferencias de CI entre las personas se debe a sus diferencias genéticas, y qué proporción se debe a sus diferentes ambientes e historias vitales? Esta pregunta tiene sentido, aunque sea difícil de contestar.


En muchos caracteres comportamentales humanos no se puede dividir la varianza en componentes genéticos y ambientales.
Los padres dan a sus hijos tanto sus genes como su ambiente, y los factores genéticos y ambientales suelen estar correlacionados.
La desventaja genética y la desventaja social tienden a ir de la mano.
Vp no es igual a VG + VE y los investigadores se ven forzados a incluir varianzas de interacciones adicionales en sus análisis. 
La proliferación de varianzas puede reducir rápidamente el poder explicativo de los modelos, y en general ésta ha sido una difícil área para trabajar.

Figura 1

Aproximaciones sucesivas a una distribución normal

Las gráficas muestran la distribución poblacional de un carácter hipotético con un valor medio de 1 00 unidades.
El carácter está determinado por los efectos aditivos (codominantes) de varios alelos.
Cada alelo en mayúscula añade 5 unidades al valor, y cada alelo en minúscula resta 5 unidades. 
Todas las frecuencias alélicas son 0,5.
(A) El carácter está determinado por un solo locus. 
(B)Dos loci.
(C) Tres loci: la adición de una cantidad mínima de variación «aleatoria» (ambiental o poligénica) produce la campana de Gauss típica de la distribución normal (D).

Dos conceptos equivocados frecuentes sobre la regresión a la media

- Tras unas pocas generaciones todos serán exactamente iguales.
- regresión a la media es un fenómeno genético. Si un carácter presenta regresión a la media, debe ser genético.


La Figura 18.2 demuestra que la primera de estas creencias es incorrecta.
En un modelo genético sencillo:

- La distribución global es la misma entre generaciones.

- La regresión funciona en ambos sentidos: para cada clase de niños, la media de sus madres se encuentra a medio camino entre el valor del niño en concreto y la media poblacional.
Esto puede sonar paradójico, pero I puede confirmarse inspeccionando, por ejemplo, la columna de la derecha del histograma de la parte inferior de la Figura 18.2 (niños con un CI de 120).
Un cuarto de sus madres tienen un CI de 120, la mitad de 110 y un cuarto de 100, lo cual hace una media de 110.


En relación al segundo de los conceptos equivocados, la regresión a la media no es un mecanismo genético sino un fenómeno puramente estadístico.
Cualesquiera sean los determinantes del CI (genéticos, ambientales o cualquier combinación de ambos), si tomamos un grupo excepcional de madres (por ejemplo las que tienen un CI de 120), estas madres deben haber tenido un conjunto excepcional de determinantes.
Si tomamos un segundo grupo que comparta con ellas la mitad de sus determinantes (sus hijos, sus hermanos o cualquiera de sus padres), el fenotipo medio de este segundo grupo se desviará de la media poblacional la mitad de lo que se desvía el fenotipo del grupo excepcional.
La genética hace que el número sea la mitad, pero no hace el principio de regresión.

Figura 2

Regresión a la media

El mismo carácter de la Figura 18.1B: de media 100, determinado por los alelos codominantes A, a, B y b en dos loci, todos con frecuencias génicas de 0,5.

Parte superior: distribución en una serie de madres. 
Parte central: distribuciones en los hijos de cada clase de madres, suponiendo emparejamientos al azar.
Parte inferior: distribución conjunta en los hijos.
Obsérvese que: (a) la distribución en los hijos es la misma que en las madres; (b) para cada clase de madres, la media de sus hijos se encuentra a medio camino entre el valor materno y el valor poblacional 100; y (c) para cada clase de hijos (abajo), la media de sus madres se encuentra a medio camino entre el valor de los hilos y la media poblacional.

Partición de la varianza

- Varianza del fenotipo ( Vp) = Varianza genética (VG) + Varianza ambiental (VE)
- VG = Varianza debida a efectos genéticos aditivos (VA) + Varianza debida a efectos dominantes (VD)-VP = VA + VD + VE 
- Heredabilidad (amplia) = VG/VP 
- Heredabilidad (restrictiva) = VA/VP 
- 


Dedos de cinc

Desempeñan un papel fundamental en la regulación de la actividad génica de muchas especies, desde las levaduras hasta el hombre.

No hace ni diez años, todavía se desconocía su existencia

Uno de los problemas más fascinantes de la biología actual es el de averiguar cómo se expresan los genes. 
Para que un gen se active, cierto grupo de proteínas, los factores de transcripción, deben unirse a determinado segmento del gen, el promotor.
Se dicta así la orden de "puesta en marcha", que es la señal para que una enzima transcriba otro segmento genético de ADN en ARN.
En la mayoría de los casos, la molécula de ARN que resulta sirve de molde para la síntesis de una proteína específica; otras veces, el propio ARN es el producto final.
Pero, ¿cómo se las arregla un factor de transcripción para reconocer su diana específica en un promotor, con la exuberancia de ADN que hay en la célula?

Muchos factores de transcripción contienen pequeñas protuberancias, o dedos de cinc, perfectamente adaptados para reconocer ADN.
En nuestro laboratorio del Consejo de Investigaciones Médicas en Cambridge, se identificó el primer dedo de cinc en 1985.
Lo portaba un factor de transcripción de rana.
Desde entonces, se han descubierto más de 200 proteínas, muchas de ellas factores de transcripción, con dedos de cinc.
Otros factores de transcripción alojan estructuras o motivos similares.
Varios laboratorios, el nuestro entre ellos, han comenzado a descifrar el mecanismo que emplean los dedos de cinc y otras estructuras parecidas para reconocer y unirse a sus dianas específicas en el ADN. 

Los dedos de cinc no agotan los medios de interacción entre el factor y el ADN.

Recuérdense otras estructuras: motivos hélice-giro-hélice (descubiertos en 1981), dominios homeóticos y cremalleras de leucina.

Pero el dedo de cinc es, con diferencia, la vía de unión al ADN más frecuente.

Las investigaciones en el campo del reconocimiento del ADN apuntan, en última instancia, hacia una presa de mayor vuelo:
¿cómo se produce el desarrollo en los organismos pluricelulares?
Aunque todas las células del embrión portan los mismos genes, unas células se diferencian en neuronas y otras irán a formar parte de la epidermis.
Sus destinos divergen porque, a medida que el embrión avanza en su desarrollo, van activándose combinaciones diferentes de genes, sintetizándose proteínas especializadas que confieren a las células diferenciadas sus propiedades características.
Saber cómo reconocen los factores de transcripción sus dianas específicas en el ADN resulta imprescindible para comprender la activación selectiva de los genes.

Nuestro descubrimiento de los dedos de cinc hundía sus raíces en los trabajos de Roberto G. Roeder y Donald D. Brown.
En 1980, los equipos encabezados por Roeder y Brown resolvieron las etapas del proceso de transcripción de un gen de la rana Xenopus laevis .
Demostraron que el factor de transcripción IIIA (FTIIIA) era uno de los tres factores, por lo menos, requeridos para activar el gen que determina la síntesis del ARN SS, que forma parte de los ribosomas, lugar donde las moléculas de ARN mensajero (el producto típico de la transcripción de los genes) se traduce en Foteína.

Más adelante se vio que el FTIIIA se unía a un segmento de ADN, de unos 45 pares de bases.

Las bases son los "peldaños" de esa familiar "escalera" que es el ADN. (El ADN consta de dos cadenas de nucleótidos, compuestos a su vez de azúcar desoxirribosa, un grupo fosfato y una de las cuatro bases: adenina, timina, guanina o citosina.
Las dos cadenas se mantienen unidas por enlaces que se producen entre sus bases, de suerte que adenina siempre se empareja con timina y citosina con guanina).

Nos sorprendió la longitud del sitio reconocido por FTIIIA, habida cuenta del tamaño pequeño del propio FTIIIA.
Factores de transcripción de igual talla, identificados ya en bacterias, reconocían fragmentos de ADN mucho menores, de unos 15 pares de bases.
¿En virtud de qué el FTIIIA, una molécula pequeña, abarcaba tamaño segmento de ADN?
La pregunta, obvia, parecía poder responderse.
Aunque los factores de transcripción se suelen producir en cantidades pequeñas, el FTIIIA abunda en los ovarios de las ranas inmaduras.
Se almacena formando complejo con el ARN SS, en cuya síntesis interviene.
Su abundancia nos llevó a pensar que podríamos recolectar suficiente complejo FTIIIA-ARN para aislar la proteína.
Una vez conseguido, podríamos hurgar en la organización tridimensional de la proteína y en la unión a su diana en el gen del ARN SS.

El plan era bueno, pero pronto encontramos una dificultad, que a la postre sería afortunada, ya que nos conduciría al descubrimiento de los dedos de cinc.
En 1982, Jonathan Miller aplicó en nuestro laboratorio una conocida técnica para extraer de los ovarios de rana el complejo FTIIIA-ARN SS. El resultado podía calificarse de desastre, pues obtuvo muy poco.
El método empleado eliminaba un metal necesario para mantener unido al complejo.
Una vez que Miller modificó el procedimiento de extracción, y consiguió una cantidad aceptable del complejo, comprobó que el metal que se perdía era el cinc.
Cada unidad FTIIIA-ARN SS incorporaba entre 7 y 11 iones de cinc, una cifra altísima.
Otros experimentos nos condujeron posteriormente a los dedos de cinc.

Cuando se trataba el FTIIIA con una proteasa, el factor se fragmentaba en trozos cada vez menores, que disminuían de tres en tres kilodalton (una medida del peso molecular).
Las unidades de tres kilodalton resistían el ataque, por la razón verosímil de su fuerte plegamiento. 
Adivinábase así un FTIIIA constituido, casi en su totalidad, por una sucesión de segmentos de 3 kd.(equivalente a unos 30 aminoácidos por cada segmento) plegados alrededor de un ion de cinc, formando un dominio compacto y pequeño de unión al ADN.

Si la intuición era certera, habíamos encontrado un nuevo tipo de factor de transcripción.
Los hasta entonces estudiados interaccionaban con el ADN constituidos en dímeros, o parejas, en los que cada subunidad del dímero contactaba con el ADN a través de un exclusivo motivo de unión.
A tenor de nuestro hallazgo, el FTIIIA podía extenderse por la doble hélice, asiéndola en varios puntos, y no en uno solo o en dos.
Estos múltiples contactos explicarían también el tamaño insólito del FTIIIA.

Mientras especulábamos con nuestro modelo, el grupo de Roeder publicó la secuencia de aminoácidos de FTIIIA.
En esa secuencia encontramos el apoyo necesario para nuestra propuesta:
las primeras tres cuartas partes de la proteína forman una hilera continua de nueve unidades similares, de unos 30 aminoácidos.
Además, en cada subunidad, y en posiciones casi idénticas, se encuentra una pareja de cisteínas y otra de histidinas. 

Este último descubrimiento estaba en consonancia con la idea de que cada unidad debía contener sus propios iones de cinc, ya que en las proteínas el cinc suele darse ligado a cuatro aminoácidos, cisteínas por lo común o combinación de cisteínas e histidinas.

En 1985, esos resultados determinaron que uno de nosotros ( Klug) propusiese que esas cisteínas e histidinas invariantes (fijas) servían para que cada subunidad se plegase independientemente, formando un minidominio de unión a ADN.
Más tarde se le llamó dedo de cinc por su función: la de aferrarse a la doble hélice de ADN.
Avanzó también que la pareja de cisteínas próxima a uno de los extremos de la unidad, y la pareja de histidinas próxima al otro extremo, atraparían el mismo átomo de cinc, con lo que el segmento de aminoácidos situado entre ellas adoptaría una configuración en lazo o bucle.
De cada unidad de 30 aminoácidos, unos 25 se plegaban y constituían un dominio estructurado (un dedo);
los aminoácidos restantes servirían para unir unos dedos con otros.

Poco después, Gregory P. Diakun demostraba que cada una de las nueve subunidades contenía un ion de cinc unido a dos cisteínas y dos histidinas.
En la práctica, TFIIIA es un conjunto de nueve dedos de cinc casi consecutivos. 
Todos tienen la misma arquitectura básica, pero son químicamente distintos porque difieren en los aminoácidos que no participan en la construcción del módulo del dedo.

¿Establece contacto real cada dedo de cinc al ADN, tal como se había predicho?
Para averiguarlo, Louise Fairall, de nuestro equipo, y también otros grupos investigaron las "huellas moleculares".
Se permite que una proteína se una al ADN.
A continuación se trata con enzimas u otros agentes que atacan al ADN.
Se supone que los sitios que resisten la agresión están protegidos por la proteína, y son, por ende, lugares de interacción ADN-proteína.
En 1986 los datos de huellas moleculares confirmaban que TFIIIA interaccionaba varias veces con el ADN. 

Por tanto, TFIIIA era la novedad que nosotros sospechábamos. 
Se unía a una región específica de ADN utilizando varios módulos independientes que contactan con el ADN.

La idea de economía implícita en la organización modular era de una belleza singular.

Ya se sabía que las células conseguían un repertorio amplio de conmutadores genéticos permutando un conjunto limitado de factores de transcripción.
Esto es, para activar un gen se deben combinar las proteínas a, b y c, mientras que para activar otro se precisan las a, b y d.
Con este tipo de estrategia, los organismos evitan tener que producir factores de transcripción exclusivos para cada uno de los numerosos genes que hay que activar en cada célula. 

Los estudios sobre los dedos de cinc revelaban que la lógica combinatoria podía operar también en el interior del propio factor de transcripción.
Una célula puede producir una gran cantidad de factores de transcripción variando el tipo, orden y número de módulos independientes de unión al ADN en cada proteína.
La combinación particular de dedos de cinc en un factor de transcripción concreto le permite a éste reconocer una secuencia específica de ADN y no otra.

La eficacia de la estrategia basada en combinaciones nos llevó a sugerir que el motivo "dedo de cinc" podía estar presente en muchas proteínas.
Y efectivamente es asombrosa la cantidad de proteínas de organismos eucarióticos que lo tienen (los eucariotas son organismos más avanzados que las bacterias).
Peter F. R. Little estima que el 1% del ADN en células humanas especifica dedos de cinc.
En el cromosoma 19 este valor llega hasta un 8 %.
Las proteínas con dedos de cinc identificadas hasta el momento contienen desde 2 hasta 30 dedos en tándem.

Para entender cómo se las ingenia el dedo de cinc para reconocer determinada secuencia de bases -que adopta una conformación específica- es preciso conocer la estructura tridimensional de aquél.
La mayoría de las proteínas contiene regiones locales de estructura "secundaria", que se pliegan unas con otras hasta producir la forma tridimensional global de la proteína.
Las estructuras secundarias más comunes son las hélices alfa (donde el esqueleto de la proteína se enrolla en una espiral característica) y la cadena beta (donde el esqueleto aparece desplegado).

Jeremy M. Berg descifró los rasgos principales de la arquitectura tridimensional en términos teóricos en 1988, pero su modelo no se confirmó hasta 1989.
Fue cuando Peter E. Wright determinó la estructura de un dedo de cinc de la proteína Xfin de Xenopus.

Recurrió a la espectroscopía de resonancia magnética nuclear (RMN), una técnica que se utiliza para resolver la estructura tridimensional de pequeñas proteínas en solución.
Le secundaron otros laboratorios, el nuestro incluido, en la identificación del mismo diseño en otras proteínas con dedos de cinc.

Como Berg pronosticó, la secuencia de aminoácidos característica del dedo de cinc se pliega en una forma compacta y crea dos subestructuras prominentes a todo lo largo. 
Una parte de la secuencia (correspondiente a la mitad izquierda de una protuberancia vertical) adopta la figura de una "hoja" beta.
La hoja se forma cuando una cadena beta se pliega sobre la otra cadena beta.
La parte restante de la secuencia (la mitad "derecha") se retuerce en una hélice alfa.
Las dos cisteínas se alojan en la porción inferior de la hoja beta y las dos histidinas en la porción inferior de la hélice.
Los cuatro aminoácidos permanecen unidos gracias a un átomo de cinc, cuya función fundamental es mantener juntas la hoja beta y la hélice. 

El análisis por RMN ayudaba también a esclarecer el papel de ciertos aminoácidos.

Cuando examinamos la secuencia del FTIIIA, observamos que los putativos dedos de cinc incluían una serie de tres aminoácidos hidrofóbicos, casi siempre en idénticas posiciones.
(Las sustancias hidrofóbicas suelen asociarse en el interior de la proteína para "huir" del agua del medio.)
Esa invariancia sugería un papel estructural importante para esos aminoácidos.
Aun cuando en la representación lineal de la secuencia de aminoácidos esos residuos quedaban lejos unos de otros, pensamos que podían interaccionar en el espacio tridimensional y, por tanto, facilitar el plegamiento del minidominio.
Tal como indicaba el modelo de Berg, los resultados de la RMN demostraban que, cuando el módulo del dedo de cinc se pliega, los aminoácidos hidrofóbicos quedan en estrecho contacto, lo que permite su interacción. 
Crean así un núcleo hidrofóbico que ayuda a mantener la forma del módulo.

Algunos grupos teníamos en mente un problema más general.
Muchos experimentos llevaban a la conclusión de que los dedos de cinc presentes en el FTIIIA, prácticamente toda la proteína, eran los únicos responsables de que el factor reconociese al promotor del gen del ARN SS. Pero cada vez se descubrían más proteínas en las que sólo había unos pocos dedos de cinc insertos en una macroproteína.
¿Podían esos escasos dedos de cinc dirigir tales proteínas hacia los promotores, sin ayuda de otras partes de la proteína? 

Para despejar el interrogante, nos centramos en un factor de transcripción de levadura con tres dedos de cinc, el SWITCHS (SWIS).
Con nuestro colega Kyoshi Nagai, aislamos la región que contiene los dedos y la pusimos en contacto con el promotor de su gen diana.
La porción de proteína se engarzaba con avidez en el promotor, señal de que los dedos de cinc eran los únicos responsables de la unión al ADN.
Encontramos también que, para que la proteína SWIS se uniese con una fuerza razonable a su diana, se necesitaban al menos dos dedos de cinc juntos.
Aplicando la técnica de la RMN a los dos primeros dedos del SWIS confirmamos, con David Neuhaus y Yukinobu Nakeseko, que los dedos de cinc adyacentes no se fusionan entre sí.
Son verdaderas "cabezas lectoras", unidas de manera independiente por conectores flexibles.

Quedaban por identificar, no obstante, los puntos precisos de contacto entre los dedos de cinc y el ADN.
Nikola P. Pavletich y Carl 0. Pabo dieron los pasos iniciales.
Obtuvieron en primer lugar cristales del complejo formado por el ADN y el dominio de unión al ADN Zif268 , un factor de transcripción.
Con análisis de cristalografía de rayos X determinaron la estructura del complejo Zif268 , que igual que SWIS contiene un grupo de tres dedos de cinc, participa en el desarrollo temprano del ratón.

Los análisis de rayos X revelaban que la región de Zif268 que contenía los dedos de cinc ceñía un segmento de ADN de una longitud cercana a una vuelta de hélice (más o menos formando una letra "C"), encajándose en el surco mayor.
(El surco mayor es la más ancha de las dos "vaguadas" paralelas que rodean, en espiral, el eje longitudinal de la doble hélice de ADN.)
Los contactos de los dedos con el ADN se establecen a través de secuencias sucesivas de tres pares de bases, y se orientan de forma similar con respecto al ácido nucleico.
La hélice alfa de cada dedo apunta hacia el surco mayor, apoyándose en una de sus paredes.

Los dedos primero y tercero de Zif268 se unen al ADN de un modo casi idéntico:
un aminoácido de la primera vuelta de la hélice alfa contacta con el primer par de bases del correspondiente sitio de unión en el ADN, y un aminoácido de la tercera vuelta de hélice contacta con el tercer par de bases del mismo sitio de unión. 
El segundo dedo establece también dos contactos a través de la hélice alfa, pero esta vez los aminoácidos de la primera y segunda vuelta se traban con los pares de bases primero y segundo del correspondiente sitio de unión. 
(En cada caso, un aminoácido se ancla en una de las bases del par.)
Además, la hélice alfa y la hoja beta del dedo se enlazan con los grupos fosfato de la cadena de azúcar y fosfato que conforman cada uno de los dos "largueros" de la escalera que es el ADN.
Estos ligámenes adicionales ayudan a estabilizar el engarce del dedo de cinc con el ADN. 

Hasta la fecha, la cristalografía de rayos X no ha resuelto ningún otro complejo de dedos de cinc-ADN. 
Pese a lo cual, Grant H. Jacobs ha recabado pruebas que sugieren que muchos dedos de cinc se unen al ADN a la manera de Zif268 :
tras comparar la secuencia de aminoácidos de más de 1000 dedos de cinc, observó una notable variación entre los aminoácidos presentes en tres sitios determinados, justamente los sitios que utiliza Zif268 para establecer contactos, esto es, los ubicados en la la, 2a y 3a vuelta de la hélice alfa.
Esa circunstancia avala la esperanza de que podamos diseñar módulos con dedos de cinc capaces de reconocer secuencias de ADN seleccionadas, de obvio interés en el estudio de la regulación génica y en medicina.

Ni que recordar tiene que existen límites a la extrapolación del modelo de Zif268 y de los análisis estadísticos. 
Cabría esperar que las proteínas con muchos dedos de cinc interaccionen con el ADN de forma algo distinta. 
En efecto, si el patrón de unión de Zif268 se aplicase a FTIIIA, esta proteína, dotada de nueve dedos, ceñiría el ADN en una extensión equivalente a tres vueltas de hélice, como un hilo sobre un carrete.
Semejante envoltura, tan fuerte, podría impedir que el factor se soltase del ADN cuando se lo requiriese; 
los datos de "huellas moleculares" obtenidos por varios grupos, entre ellos el nuestro, sugieren, en efecto, que el FTIIIA no se arrolla sin solución de continuidad a lo largo del ADN.

Casi con toda seguridad, los primeros tres dedos del TFIIIA se aprietan en una sola vuelta de hélice del ADN, y es muy probable que los tres últimos hagan lo mismo.
Pero el grueso de la proteína se sitúa en una cara de la doble hélice.

Por tanto, la proteína cruza el surco menor, más estrecho, al menos dos veces.
La variedad de patrones de unión con el ADN que existe entre regiones distintas del FTIIIA refleja otra diferencia, a buen seguro:
la diversidad que reina entre secuencias de aminoácidos de los dedos de cinc del TFIIIA, superior a la disparidad entre ésta y las secuencias de los dedos de las proteínas tipo Zif268 .

Desde una perspectiva evolutiva, hay una buena razón para pensar que los dominios de unión al ADN con muchos dedos surgieron por duplicación de algún gen ancestral que cifraba una proteína de una treintena de aminoácidos; proteína que bien pudo ser una de las primeras en evolucionar. 
Una proteína así habría sido fácil de producir;
sintetizada, podría atrapar cinc (un metal bastante inerte) de sus alrededores con facilidad y seguridad, y se plegaría sin ninguna ayuda, adoptando una configuración estable.
Una vez plegada, adquiriría la capacidad de unirse al ADN o al ARN.
En ese marco hallaría fácil explicación la abundancia de dedos de cinc en los reinos animal y vegetal.

Cualquier secuencia que adquiriese la determinación genética de un dedo con autonomía para plegarse, conseguiría instantáneamente la capacidad de unirse a un nuevo segmento de ADN.

Dicha propiedad originaría nuevas funciones celulares, como transcribir genes silenciados y codificar, por ende, una enzima nueva o cualquier otra proteína de interés. 

Mientras ahondábamos en la estructura y función de los dedos de cinc clásicos, se realizaron algunos experimentos que venían a sugerir que el motivo inicialmente descubierto en FTIIIA no era la única estructura relacionada con el cinc dedicada al reconocimiento del ADN.

En 1987 se determinaron las secuencias de aminoácidos de varios miembros de una extensa familia de factores de transcripción: 
los receptores nucleares de hormonas.
Estos factores han de unirse a un esteroide, hormona tiroidea o vitamina antes de actuar como activadores génicos.
El análisis de las secuencias mostró que todas ellas contenían un dominio conservado (muy similar), de unos 80 aminoácidos. 
El dominio incluía dos, y siempre dos, unidades cuya secuencia de aminoácidos recordaba a los dedos de cinc.
Igual que en éstos, cada unidad o motivo alojaba dos parejas de aminoácidos con capacidad potencial para atrapar cinc.
En este caso, sin embargo, eran sólo cisteínas.
Por el parecido de las secuencias con los dedos de FTIIIA cabía suponer que el segmento de 80 aminoácidos rico en cisteínas presente en tales factores era el dominio de unión al ADN.

Pierre Chambon y Stephen Green confirmaron la hipótesis a finales de los ochenta.
Poco tiempo después, los equipos de Paul B. Sigler y Keith R. Yamamoto establecieron que en el segmento del dominio de unión al ADN incorporaba un átomo de cinc.
Era, pues, de esperar que las configuraciones de los dos motivos se parecieran, lo mismo que en los dedos de cinc del tipo FTIIIA, y que generaran módulos independientes de unión al ADN.

Pero la suposición resultó ser errónea en parte.
Los análisis estructurales acabaron por evidenciar el plegamiento similar de las dos subunidades.
Mas ya antes la bioquímica había mostrado que las subunidades no actuaban como cabezas lectoras independientes. 
Chambon y los equipos de Ronald M. Evans y Gordon M. Ringold realizaron experimentos en los que sustituían unos aminoácidos por otros y a continuación examinaban el efecto de dicha sustitución sobre la unión al ADN;
descubrieron que el primer motivo operaba a modo de unidad de reconocimiento primario del ADN.
Por aquel entonces, Evans y Kazuhiko Umesono, de nuevo aplicando el método de la sustitución, descubrieron al menos una función para el segundo motivo.
Para entender esa función, hemos de traer aquí ciertas ideas generales sobre la interacción entre el ADN y los receptores de esteroides.

Esos receptores se unen al ADN en parejas de moléculas idénticas.
Cada proteína del dímero reconoce una mitad del sitio de unión, que en conjunto es un palíndromo.
Esto es, las dos mitades son idénticas leídas en un sentido y en su contrario, en cadenas de ADN antagónicas.
La secuencia de bases del sitio reconocido por un tipo de factor de transcripción (el receptor del estrógeno) puede ser idéntica a la que reconoce otro factor (el receptor de la hormona tiroidea). 
La única diferencia entre los dos sitios de unión estriba en el número de pares d e bases que separa las dos mitades que constituyen el palíndromo.

Por tanto, para que un factor de transcripción encuentre el sitio por donde anclarse al ADN, la proteína debe contener regiones encargadas de reconocer dos regiones con secuencias de bases específicas y de medir la distancia que existe entre esas dos regiones.
Evans y Umesono descubrieron que una parte del segundo motivo es la encargada de medir la longitud del espaciador.

Para explicar cómo se produce el reconocimiento importa conocer la configuración tridimensional de los dominios de unión al ADN y ver así en qué parte de la estructura se hallan los aminoácidos involucrados. 
En 1990, el grupo de Robert Kaptein resolvió, por RMN, la estructura del dominio de unión al ADN del receptor de glucocorticoides (cortisona) de ratas.
Poco después, John W. R. Schwabe y Neuhaus, junto con uno de los autores ( Rhodes), resolvieron la del receptor de estrógenos humano .

Los dominios de unión al ADN de ambos receptores adoptan casi la misma configuración.

Uno y otro motivo digital inscritos en el dominio constan de dos partes:
un segmento de aminoácidos en bucle irregular (en vez de la hoja beta de los dedos de cinc clásicos), seguido por una hélice alfa.
El bucle porta dos de los sitios de unión con el cinc;
los otros dos están al comienzo de la hélice que le sigue.
Pero los dos motivos no están separados, sino que aparecen fusionados en una sola unidad estructural.

Las hélices se cortan perpendicularmente por sus puntos medios, una configuración creada por la atracción mutua de aminoácidos hidrofóbicos invariantes del todo o bastante.

Conocida la estructura tridimensional del dominio de unión al ADN, procedimos a acotar aquellos aminoácidos que, según sabíamos, eran críticos para reconocer al ADN.

Grupos dirigidos por Chambon, Evans y Ringold habían identificado tres aminoácidos en el primer motivo digitiforme que eran responsables del reconocimiento de una de las dos mitades del sitio de unión.
Resultó que esos aminoácidos residían en un lateral de la hélice del motivo, subestructura que pasó a llamarse "hélice de reconocimiento del ADN".

¿Qué decir de la función del segundo motivo?
Al cruzar la hélice de reconocimiento del primer motivo, la hélice del segundo se constituía en punto de apoyo para mantener en su lugar a la hélice de reconocimiento.

La separación de funciones entre los dos motivos sugiere que el segundo dedo apareció por duplicación del primero - una vez formado, la selección le asignó nuevas tareas.

La cartografía tridimensional nos reveló también el modo en que el segundo motivo mide la espaciación entre las dos secuencias que conforman el sitio de unión. 
Evans y Umesono establecieron que los aminoácidos responsables de esa misión se hallan entre las primeras dos cisteínas del segundo motivo.
En la configuración tridimensional, tales aminoácidos quedan en el bucle que precede a la hélice, listos para servir de enlace entre las dos moléculas del dímero.

El modelo obtenido por computador de la interacción entre ADN y regiones de unión al ADN de los receptores de glucocorticoides y estrógenos nos permitió comprobar, a Kaptein y a nosotros, respectivamente, que la unión de la forma predicha entre las dos proteínas orientaría al dímero.
Las dos hélices de reconocimiento del dímero se dispondrían de suerte tal, que el intervalo espacial entre ellas se correspondería con el hiato entre los dos hemisitios del ADN.

Sigler y Ben F. Luisi, de Yale, junto con Yamamoto y Leonard P. Freedman, de San Francisco, han confirmado ya esta predicción por cristalografía de rayos X.
Han averiguado también que cada proteína del dímero establece varios engarces con los fosfatos de ambos lados del surco mayor.

Gracias a dichos anclajes, la hélice de reconocimiento puede profundizar en el surco mayor y formar enlaces con algunos pares de bases del hemisitio de unión.
Las investigaciones sobre los dedos de cinc de los receptores nucleares de hormonas indican que, a pesar de algunas semejanzas estructurales con los dedos de cinc del tipo FTIIIA, esos motivos se parecen más, desde el punto de vista funcional, a los motivos de reconocimiento del ADN de otros factores de transcripción, como la hélice-giro-hélice y las cremalleras de leucina.

Esto es, plegándose unos con otros, los motivos contribuyen a que los receptores de hormonas nucleares formen los dímeros que permiten a dichos factores reconocer sus sitios de unión específicos en el ADN.

El conocimiento de la estructura y función de una molécula es el punto de partida para pensar en su aplicación médica.
En el asunto que nos ocupa, sabemos que cierto cáncer renal, el tumor de Wilm, tiene su origen en una mutación genética que entorpece la unión al ADN de determinado dedo de cinc.
Algunos de los síntomas que revelan una ingestión insuficiente de ese metal en la dieta, como el retraso en el desarrollo sexual, pueden ahora atribuirse a la imposibilidad que tienen los receptores de estrógenos y andrógenos de plegarse adecuadamente en ausencia de cinc.

Los dos tipos de dedos que hemos analizado difieren en su estructura y en su interacción con el ADN.
Aparecerán, sin duda, otras variedades de proteínas con dedos de cinc. 
Cada vez se conocen más secuencias de aminoácidos que incluyen lo que parecen ser motivos con dedos de cinc, aunque las distancias entre las parejas de cisteínas o histidinas, o el número de parejas, difieren de las supuestas normales. 
Un ejemplo insólito es la proteína GAL4 de levaduras.
Contiene seis cisteínas que se pliegan alrededor de dos átomos de cinc.
Esperamos también encontrar dedos de cinc, o similares, implicados en funciones alejadas de la transcripción, como el transporte, maduración o cualquier proceso que implique una acción sobre el ADN, o incluso el ARN.
No se olvide que el FTIIIA se une lo mismo a uno que a otro.
Nos queda aún mucho que aprender.

Figura 1

TRES DEDOS DE CINC sobresalen de un factor de transcripción, o proteína reguladora de la actividad génica (rojo).

Se fijan al surco mayor de la molécula de ADN (doble hélice).
Los dedos de cinc son el puente entre los factores de transcripción y sus genes diana.
Se unen al ADN por secuencias específicas de pares de bases:
los "peldaños" de la "escalera" que es el ADN.
Se llaman dedos de cinc porque asen al ADN y porque el cinc (bolas amarillas) desempeña un papel fundamental en la determinación de su estructura.


Implantes celulares

Un nuevo enfoque terapéutico combina células vivas con membranas plásticas que las protegen del ataque inmunitario.

En 1994, cierto paciente que sufría un dolor intenso y persistente se convirtió en uno de los primeros voluntarios sometidos al ensayo de un método terapéutico radicalmente nuevo.
Mientras yacía anestesiado, el cirujano adosó un tubito de plástico en la columna vertebral. 
El tubo, sellado, medía cinco centímetros de longitud y era fino como alambre de clip; contenía células de ternera secretoras de una mezcla de analgésicos. 

Si las cosas no se torcían, las secreciones celulares rezumarían a través de los poros del tubo, para difundirse luego por la médula espinal.
Entre tanto, los nutrientes y el oxígeno pasarían del líquido cefalorraquídeo a la cápsula, para alimentar a las células.
El tubito impediría también la entrada de macromoléculas.
Se evitaría así que las células y los anticuerpos del sistema inmunitario (de un tamaño notable ambos) establecieran contacto con las células bovinas y las destruyeran al reconocer su carácter foráneo.

A través de ese medio se buscaba aliviar el sufrimiento, lo que se conseguía al cortar el flujo de señales de dolor que transitaban de la médula a los centros cerebrales de detección.
Pero el ensayo de 1994 fue de puro tanteo.
Se diseñó para comprobar si las células implantadas sobrevivían y liberaban sus analgésicos durante meses.
Lo lograron.
Un éxito similar se repitió en otros pacientes y justificó la realización de una prueba más ambiciosa, ahora en marcha, para evaluar por vía directa el control del dolor.

Aquellos resultados comportaron, sin embargo, consecuencias de mayor alcance.
Alimentaron el creciente optimismo, basado en exhaustivos experimentos con animales, respecto del uso de combinaciones de células vivas y membranas sintéticas protectoras para corregir un amplio abanico de patologías. 

Cinco años después, el revuelo que ha producido esta estrategia -conocida asimismo por tratamiento con células encapsuladas, terapia biohíbrida o inmunoaislamiento- parece justificado.
Como en el caso del implante analgésico, un sistema de soporte hepático biohíbrido ha alcanzado la fase de ensayo controlado en humanos, en el que toman parte numerosos pacientes y diversos hospitales.
Pruebas de carácter restringido en humanos y estudios en mamíferos de cierto porte han de dilucidar la eficacia de terapias de inmunoaislamiento en casos de hemofilia, anemia, retraso del crecimiento y trastornos neurodegenerativos (enfermedades de Parkinson y de Huntington).
En roedores se han iniciado los ensayos de la terapia de la degeneración macular, una causa frecuente de ceguera, y otros tipos de enfermedades oculares.

Las aplicaciones en boga implican la implantación de células encapsuladas en el sitio escogido del organismo. 
Algunas -el tratamiento del hígado, por ejemplo- precisan el concurso de dispositivos externos que recuerdan los aparatos de diálisis.

Porque supera limitaciones importantes de los implantes de células libres, la terapia de inmunoaislamiento despierta especial atracción.
Lo mismo que las libres, las células encapsuladas en membranas se hacen cargo de funciones cruciales que correspondían a las lesionadas o perdidas, pueden suministrar analgésicos y otros elementos "extra" e incluso facilitar la terapia génica, segregando las proteínas codificadas por los genes que los expertos hayan introducido en las células.
Por el contrario, las células libres están expuestas a la celada del sistema inmunitario, salvo que procedan de los mismos sujetos o de sus gemelos.
De ahí que los pacientes suelan necesitar fármacos inmunosupresores.
El bloqueo mecánico contra los ataques inmunitarios, que se consigue a través de la encapsulación de las células insertas dentro de membranas plásticas, haría superfluo este tipo de medicinas, que predisponen a las infecciones, cáncer (linfomas) y disfunción renal.

La protección inmunitaria conseguida con las membranas plásticas debería también permitir el trasplante celular de animal a hombre.
Las células animales sin encapsular no son una opción viable porque los fármacos inmunosupresores no evitan por completo el rechazo de los xenotrasplantes (implantes procedentes de otras especies).
El recurso a células animales paliaría la escasez de donaciones de tejidos.
Por último, las células implantadas dentro de un envase plástico pueden retirarse si fuera preciso, tarea harto difícil con las células libres. 

El empeño puesto ahora para encapsular células para usos médicos no se entiende sin el trabajo pionero de William L. Chick a mediados de los años setenta.
Chick había dirigido sus esfuerzos a la curación de la diabetes de tipo I (dependiente de insulina), peculiar de las personas jóvenes.
Se presenta la disfunción cuando el páncreas deja de sintetizar insulina, una hormona que segrega en la cantidad adecuada para controlar la concentración de glucosa en sangre.
Las inyecciones diarias de insulina salvan vidas, pero no repiten el patrón natural de secreción pancreática de la hormona.
En consecuencia, ciertos tejidos pueden quedar expuestos a cantidades excesivas de glucosa. 
Con el paso de los años, esa demasía genera complicaciones asociadas a la diabetes, como ceguera y disfunciones renales.

Chick pensó que la implantación de cápsulas llenas de islotes pancreáticos -las agregaciones de células que segregan la insulina- podría restaurar el patrón de liberación de esta sustancia adecuado sin tener que administrar fármacos inmunosupresores. 
El empleo de islotes procedentes de cerdos (por aquel entonces la fuente principal de insulina inyectada) garantizaría un suministro abundante de células.

Diversos estudios realizados en roedores desde mediados de los setenta sugerían que andaba en lo cierto.
Por desgracia, ciertos obstáculos técnicos han impedido que la terapia de inmunoaislamiento cuajara en la diabetes Chick murió en 1998 sin ver cumplidos sus deseos.
Pero su feliz anticipación ha estimulado el progreso en otros frentes, como el diseño de dispositivos. 

Los sistemas de encapsulación, aunque polimórficos en su presentación, todos incluyen los mismos ingredientes básicos: las células (capaces de segregar productos útiles), una matriz donde se prenden las células, contribuyendo a su supervivencia y funcionamiento, y una membrana semipermeable.
Los bioingenieros han aprendido que las células de un implante funcionan mal o mueren si distan más de 500 micrometros de los vasos sanguíneos u otras fuentes de alimentación.

Los diseños vasculares (o de paso de flujo) fueron los primeros en entrar en fase de pruebas, en concreto para tratar la diabetes en roedores.
Estos dispositivos desvían la sangre del sistema circulatorio del paciente hacia un tubo de plástico y luego la devuelven al organismo.
Las células secretoras se colocan en una cámara cerrada que rodea a un segmento ligeramente poroso del tubo, como la rosquilla y su agujero.
Cuando la sangre fluye por esta parte del circuito, puede absorber las sustancias segregadas por las células terapéuticas y proporcionar oxígeno y nutrientes a las células.
Si los islotes pancreáticos se colocan en el interior de la cámara, la concentración de insulina que liberan se acomodará al nivel de glucosa en sangre.
En otras aplicaciones podría optarse por células emisoras de un determinado producto a una velocidad constante.

Aunque se pueden preparar prototipos implantables de estos aparatos vasculares, es muy probable que encuentren su aplicación principal como dispositivos externos.
Una implantación requiere cirugía vascular y la administración prolongada de anticoagulantes para impedir la formación de trombos sanguíneos en el interior del tubo.
Si un tubo implantado se rompiera, se produciría una hemorragia interna. 

A finales de los años setenta, durante su búsqueda de métodos menos agresivos, se introdujo la técnica de "microencapsulación".
Las microcápsulas se fabrican colocando un islote pancreático o unos cuantos miles de células en una gota de solución acuosa donde hay polímeros ligeramente dotados de carga.
A continuación, se sumerge la gota en una solución de polímeros con carga opuesta.
Los polímeros entran en reacción y crean una película de unos 500 micrometros de diámetro a su alrededor.

Las microcápsulas, cuya preparación no entraña mayor dificultad, resultan muy útiles para realizar experimentos rápidos, pero presentan graves limitaciones.
Primera, su fragilidad.
Una vez instalada, no será fácil dar con ella para retirarla, un problema delicado si arrastra efectos indeseables.
Es más, el volumen necesario para corregir un trastorno puede ser demasiado grande para ubicarlo convenientemente en el sitio idóneo.

El formato más práctico parece ser el de las macrocápsulas preformadas, unas unidades inicialmente vacías que se cargan con la matriz y todas las células necesarias para el tratamiento.
Algunas macrocápsulas son discos del tamaño de una moneda de cinco pesetas, otras tienen una forma y tamaño similares a un ojal de camisa.
Las macrocápsulas para uso en humanos acostumbran adoptar la forma de un tubo sellado, o capilar, de varios centímetros de longitud y diámetro comprendido entre los 500 y 1000 micrometros.

Las macrocápsulas son bastante más fuertes y duraderas que las microcápsulas.
Contienen refuerzos internos, se puede comprobar que estén bien cerradas antes de la implantación y se pueden diseñar para que permitan nuevos rellenos en el interior del organismo.
Es más, pueden retirarse con facilidad.
Su principal limitación es el número de células que pueden acomodar: hasta unos cinco millones en un tubo y entre 50 y 100 millones en un disco u hoja plana.
Esas cifras son adecuadas para muchas aplicaciones, pero no para todas.
Si se hacen más largas es más probable que se doblen, lo que facilita su rotura.
Además, los bordes de las regiones dobladas estimulan la fibrosis, una retracción local de los tejidos que puede obstaculizar los intercambios de las células encapsuladas.

Los fabricantes de diseños vasculares, microcápsulas y macrocápsulas intentan conseguir que las membranas tengan poros de un tamaño que permita la difusión de moléculas de hasta 50.000 dalton, unidad de masa atómica.
Los poros de este tamaño son lo bastante pequeños para bloquear la entrada de células y moléculas inmunitarias, pero lo bastante grandes para permitir la entrada de nutrientes y oxígeno, así como la salida de las proteínas segregadas por las células del implante. 
Sin embargo, los poros de las membranas reales acaban por tener una gama de tamaños amplia, por lo que algunas moléculas grandes del sistema inmunitario llegan al implante atravesando las membranas.
Por fortuna, este fenómeno no debilita a la mayoría de los implantes.

Hasta finales de los años ochenta, la mayoría de los instrumentos biohíbridos se apoyaban en células primarias, o sea, las obtenidas directamente del tejido donador. 
Las células primarias son útiles para realizar estudios reducidos en animales pequeños, pero obtener las cantidades necesarias para un animal grande (incluido el hombre) o para un número elevado de receptores puede ser problemático.
Además, al tener cada donador su propio historial, garantizar la seguridad de las células primarias puede llegar a ser una empresa colosal.
Por eso, a comienzos de los noventa, algunos equipos se encaminaron hacia las líneas celulares.

Estas líneas consisten en conjuntos de células inmortales, que se dividen indefinidamente, capaces de multiplicarse en cultivos celulares sin perder su capacidad para realizar funciones especializadas, como la secreción de sustancias útiles. 
Debido a que muchas células primarias apenas se reproducen en cultivos celulares o adolecen de otras carencias, los expertos han alterado a menudo las versiones originales. 
Sin embargo, una vez establecidas, las líneas celulares pueden proporcionar un suministro inagotable de células uniformes para trasplantes.

La utilidad potencial de las líneas celulares para la terapia de encapsulación quedó bien clara en las pruebas animales que comenzamos a realizar en 1991.
Se sabía que la conocida línea PC-12, derivada de un tumor suprarrenal de roedor (o feocromocitoma), segregaba grandes cantidades de dopamina, una molécula señal cuya síntesis se suspende en los cerebros afectados por la enfermedad de Parkinson.
Para comprobar si valía la pena estudiar el uso de injertos de estas células en el tratamiento del Parkinson, introdujimos tubitos de las células en cerebros de animales cuyas células productoras de dopamina habíamos dañado por medios químicos para provocar los síntomas característicos del Parkinson. 
En muchos individuos, entre ellos los primates no humanos, el procedimiento eliminó los síntomas.

Fue significativo observar que las células no proliferaban de manera incontrolada ni perforaban la cápsula.
Tan sólo reemplazaban las células muertas y no dejaban que su población excediese la capacidad del implante. 
Los estudios también mitigaron los temores de que las células inmortales engendraran tumores en caso de fuga.
La inmortalización es un paso hacia la conversión de una célula normal en cancerosa.
Para ser realmente malignas, las células tienen que adquirir la capacidad de invadir el tejido vecino, desarrollar su propio suministro sanguíneo y propagarse hasta lugares distantes. 
La formación de tumores es una preocupación potencial en los trasplantes de células inmortales de la misma especie, pero los trasplantes de especies distintas resultan ser menos preocupantes: la introducción de células PC-12 de rata sin encapsular en cerebros de primates no generó tumores.
De hecho, ni siquiera sobrevivieron;
el sistema inmunitario del receptor se encargó de destruirlas rápidamente.

El trabajo con la línea PC-12 no prosiguió en los pacientes de Parkinson.
Se adelantaron otros tratamientos prometedores.
A pesar de todo, esos estudios demostraron la viabilidad del despliegue de linajes celulares en terapias de inmunoaislamiento.

El éxito con los linajes celulares abrió también la puerta a la utilización de células genéticamente manipuladas, debido a que las células en proceso de división se hallan más proclives a incorporar genes introducidos y sintetizar las proteínas cifradas por ellos.
En otras palabras, la técnica de inmunoaislamiento se convirtió de la noche a la mañana en una nueva vía de la terapia génica.
Los biólogos moleculares insertarían genes para la producción de proteínas terapéuticas en líneas celulares capaces de fabricarlas y estas células se incorporarían en implantes encapsulados en plástico. 

En terapia génica, suelen extraerse células del propio paciente para insertar en ellas los genes deseados; tras su multiplicación, las células alteradas tornan al organismo, con la esperanza de que las nuevas proteínas se sinteticen en las cantidades necesarias.
En comparación, la producción de cápsulas llenas de células alteradas genéticamente puede medirse antes de introducir los implantes en el paciente.
Si hubiera que proceder a ello no hay luego dificultad en quitar las cápsulas. 

No se ha resuelto todavía si las líneas celulares candidatas a encapsulación deben provenir de animales o de humanos.
Las células primarias, tomadas directamente de donantes, habrán de ser, casi con toda certeza, de animales, ante la escasez de donaciones de tejidos humanos.
Algunos investigadores prefieren líneas derivadas de animales porque las células que se escapen de un implante, al ser extrañas al receptor, se encontrarán con una destrucción inmediata por parte del sistema inmunitario. 

Para aportar proteínas terapéuticas humanas a un receptor, los citoingenieros podrían dotar a las células animales con los genes humanos que las cifran. 
Otros expertos se inclinan por las líneas de células humanas; no en vano suelen comportarse mejor en el interior de las cápsulas.
Y se evita el riesgo de que gérmenes patógenos propios de los animales se transmitan al hombre. 
Para mayor seguridad, las células humanas podrían manipularse de suerte tal que saltaran las señales de alerta inmunitaria en cuanto se produjera una fuga.

Las células encapsuladas, alteradas genéticamente o no, sirven a menudo para acarrear proteínas terapéuticas. 
Pero las proteínas pueden introducirse por inyección. 
¿Para qué, pues, implantar?

La terapia mediante células encapsuladas adquiere relieve cuando las inyecciones no suministran la proteína suficiente, así en tumores o allende la barrera hematoencefálica, un filtro natural que evita que muchas sustancias transportadas en la sangre lleguen al cerebro o a la médula espinal. 
Las células encapsuladas pueden prestar también óptimo servicio cuando la excesiva inestabilidad de la proteína terapéutica impida recetarla como fármaco o cuando se trate de reproducir el patrón natural de administración (en la diabetes, por ejemplo).

Es probable que las líneas celulares sometidas a manipulación genética predominen en los dispositivos biohíbridos del futuro.
Ello no obsta para que las aplicaciones donde intervienen células primarias, objeto de un estudio más dilatado, se encuentren en la fase última de los ensayos clínicos.
Nosotros lo hemos comprobado en el tratamiento del dolor crónico, mencionado en el comienzo del artículo.
Extraemos las células a implantar de glándulas suprarrenales de terneras, cuya cría se realiza en unas condiciones muy controladas.
Las células cromoafines, componentes adrenales, liberan de forma natural analgésicos.
Tras purificar cuidadosamente unos tres millones de estas células, las colocamos en una fibra hueca que sellamos en ambos extremos, atamos a un filamento (para su recuperación) e implantamos en la columna vertebral mediante un proceso mínimamente agresivo.

A mediados de los noventa, cuando los cirujanos nos confirmaron que estos implantes funcionaban durante meses, hallaron también indicios de atenuación del dolor.
Varios pacientes reconocieron que habían notado una caída sensible de su malestar y de la necesidad de morfina.
Pero estos experimentos no incluían un grupo de control que recibiese un placebo (es decir, una cápsula vacía), lo que nos resolvería si la mejora debía atribuirse al tratamiento.
Moses B. Goddarden dirige un amplio ensayo clínico con más de cien pacientes, diseñado específicamente para cuantificar el alivio del dolor. 

Sea cual sea el resultado, los datos recogidos demuestran que las células de origen animal inmunoaisladas perviven largos meses en el sistema nervioso central de pacientes, sin necesidad de administrar fármacos inmunosupresores.
En comparación, ningún órgano trasplantado de un animal al hombre ha sobrevivido sin encapsulación, por cuantiosos inmunosupresores que se le hayan administrado. 

Otra aplicación bastante perfeccionada de la terapia de inmunoaislamiento nos la ofrece el aparato de asistencia hepática. 
Fúndase en células extraídas directamente de animales.
En un hígado sano, los hepatocitos absorben toxinas y las degradan en formas inocuas.
Si el hígado deja de funcionar bien, las toxinas se acumulan y pueden alcanzar niveles letales.
Los trasplantes de hígado salvan a los pacientes, pero muchos mueren esperando la donación que no llega.
Los sistemas hepáticos biohíbridos en estudio se proponen mantener con vida al paciente mientras se espera el órgano de un donante.

Esta terapia de transición al trasplante se apoya en un aparato vascular externo.
En él, la sangre del paciente se bombea a una cámara cerrada, donde un segmento semipermeable del tubo que transporta la sangre se encuentra rodeado de una suspensión de hepatocitos de cerdo.
Los hepatocitos absorben las toxinas del flujo sanguíneo y las degradan, de modo que la sangre vuelve al cuerpo más limpia para completar su circuito.
En contraste con la implantación analgésica, donde se introducen unos cuantos miligramos de células y se espera que funcionen de manera continua durante meses o años, un aparato hepático puede abrigar entre 20 y 200 gramos de hepatocitos purificados, para sólo trabajar entre seis y 24 horas por tanda.

En un estudio inicial con 40 afectados de una enfermedad hepática terminal, el aparato sujeto a prueba funcionó de acuerdo con lo esperado.
Esta verificación, que se dio a conocer en 1998, preparó el camino para un ensayo a gran escala que ahora comienza en Estados Unidos y Europa. 
Las expectativas de éxito son buenas.
En circunstancias especiales, pensemos en una crisis hepática aguda provocada por ingestión abusiva de acetaminofén, el hígado podría regenerarse, sin necesidad de recurrir al trasplante.
Pero el recuerdo de experiencias pasadas nos aconseja no echar las campanas al vuelo.
El camino hacia los sistemas de apoyo hepático está empedrado de intervenciones que funcionaron muy bien en las pruebas iniciales pero fracasaron a la larga.

Lo mismo en las aplicaciones analgésicas que en las relacionadas con el hígado, hay que tomar en consideración la posibilidad de la presencia escondida de genes de virus animales en los cultivos celulares, que podrían inducir infecciones peligrosas en los receptores de trasplantes. (Para asegurarse de que no se ha deslizado ningún germen patógeno se procede a rastreos exhaustivos).
Las membranas de plástico deben constituir una formidable barrera contra la transmisión de virus animales.
Hasta la fecha ningún paciente ha adquirido ni siquiera una infección benigna a partir de las células donantes.

Aunque están menos avanzadas, han comenzado las pruebas en humanos de las aplicaciones de la terapia génica. 
Dos investigaciones se circunscriben a enfermedades del sistema nervioso central.
El primer ensayo con células encapsuladas alteradas genéticamente se centra en la esclerosis lateral amiotrófica (ELA), neurodegeneración caracterizada por el deterioro de los nervios espinales que controlan los músculos. 
En 1996, seis pacientes recibieron implantes que contenían una línea celular -derivada de células renales de hámster recién nacidos- en la que se había incorporado el gen de la proteína CNTF (factor neurotrófico derivado de los cilios).
Se eligió este gen porque otros estudios realizados en animales y humanos sugerían que el factor neurotrófico podría demorar el deterioro y la muerte de neuronas en los pacientes con ELA.
El protocolo recordaba bastante el seguido con el dolor crónico: implantación en la columna vertebral de un tubito repleto de células. 

El estudio examinó si las células sobrevivían y liberaban cantidades potencialmente terapéuticas de CNTF durante los tres meses que abarcó el experimento. 
Las células dieron buen resultado.
Sin embargo, el tratamiento no frenaba el progreso de la enfermedad; cierto es que la muestra era muy pequeña y la prueba demasiado corta para aportar información significativa.
Con todo, se sugería que, de encontrarse el gen o mezcla de genes adecuado para el tratamiento de la ELA, las células encapsuladas serían el medio idóneo para introducirlos en el sistema nervioso central.

Se hallan en fase de evaluación implantes de esta misma línea celular en pacientes con enfermedad de Huntington. 
Se distingue ésta por matar ciertas células cerebrales de manera gradual.
Las cápsulas se han colocado en los ventrículos cerebrales, unos espacios llenos de líquido.
Este protocolo se está ejecutando en París.
Acaba de comenzar.
Se han iniciado asimismo experimentos con animales para evaluar la eficacia del inmunoaislamiento en la terapia génica. 

Si las investigaciones sobre inmunoaislamiento progresan de manera satisfactoria en muchos frentes,
¿por qué nadie, tras 20 años largos de ensayos, ha conseguido rematar un método de encapsulación que remedie la diabetes?

Desde 1977, año en que el grupo de Chick logró la reversión de la diabetes en roedores, una docena de laboratorios ha venido reproduciendo la hazaña, con un amplio repertorio de diseños de implantes en distintos modelos múridos de diabetes.
Pero la terapia de inmunoaislamiento basada en los islotes pancreáticos no ha funcionado en perros, monos o humanos.
Aunque ha habido excepciones con resultados positivos, examinadas con atención se revela que, en su mayoría, sólo se lograron con la ayuda de agentes inmunosupresores o cierta cantidad de insulina inyectada.

En buena medida, la dificultad surge del número de islotes pancreáticos que requieren los animales de cierto tamaño y humanos: en torno a los 700.000, que abrigan unos 2000 millones de células "beta", productoras de insulina. 
Semejante cifra viene a multiplicar casi por mil el volumen celular que hoy podemos encapsular con éxito en los implantes clínicos.
La diabetes en ratones puede curarse con tan sólo unos 500 islotes, que los técnicos suelen extraer a mano del páncreas donante.
Pero recoger a mano 700.000 islotes queda fuera de nuestra capacidad, y las técnicas semiautomáticas no han avanzado lo suficiente para aislar las cantidades requeridas de islotes sanos.
Además, en el páncreas originario cada islote posee su propio suministro sanguíneo.
Los islotes se resienten en el entorno espartano de las cápsulas implantadas.
Por estas y otras razones, coincidimos con los que se muestran reticentes ante implantes de páncreas semiartificiales basados en islotes encapsulados.

Pero ese estancamiento podría romperse.
Por lo menos tres grupos de investigación están desarrollando, desde distintos enfoques, unas líneas celulares que liberan insulina en respuesta a las complejas señales que disparan la secreción de la hormona en el páncreas sano. 
Se pretende crear células que produzcan más insulina que las células beta naturales (de manera que se necesiten menos células) y sean capaces de sobrevivir en el ambiente del implante, pobre en nutrientes y desprovisto de oxígeno.

De aquí a cinco años, si no antes, esperamos obtener líneas celulares secretoras de insulina que respondan a la concentración de glucosa, en animales grandes.
Es de prever que esas líneas pasarán en seguida a los protocolos clínicos.
Algunos expertos piensan que esta predicción es demasiado pacata.
Otros retrasan el horizonte de su cumplimiento.
Todos, sin embargo, están de acuerdo en que la obtención de un páncreas artificial o de una versión biohíbrida debe seguir siendo una de las prioridades para la medicina del siglo XXI. 


Biología celular básica: estructura y función de los genes y los cromosomas

Todas las enfermedades genéticas implican defectos a nivel celular.
Por este motivo, para comprender la enfermedad genética es necesario conocer la biología celular básica.
Los errores pueden producirse en la replicación del material genético o durante la traducción de los genes en proteínas.
Estos errores suelen provocar trastornos causados por un único gen.
Además, los errores producidos durante la división celular pueden dar lugar a trastornos que afectan cromosomas enteros.
Con el fin de proporcionar las bases para el conocimiento de estos errores y de sus consecuencias, el presente capítulo se centra en los procesos de replicación de los genes y de su traducción en proteínas, así como en el proceso de división celular.

Durante el siglo XIX, los estudios microscópicos de las células suscitaron la sospecha en los científicos de que el núcleo de la célula (figura, 2-1) contenía los mecanismos importantes para la herencia.
Descubrieron que la cromatina , la sustancia que proporciona al núcleo su aspecto granular, puede observarse en los núcleos de las células cuando éstas no se dividen.

Inmediatamente antes de que una célula experimente la división, la cromatina se condensa formando unos corpúsculos oscuros separados, denominados cromosomas (del griego corpúsculos teñidos ).
Con el redescubrimiento de los experimentos de Mendel sobre la herencia, a principios de este siglo, pronto se puso de manifiesto que los cromosomas contenían genes .
Los genes se transmiten de padres a hijos y se los considera la unidad básica de la herencia.
Por la transmisión de los genes se heredan los rasgos físicos, como el color de los ojos, en las familias.
Mediante la herencia genética también pueden transmitirse enfermedades.

Los genes están formados por ácido desoxirribonucleico (DNA) .

El DNA aporta el «molde» para todas las proteínas del cuerpo.
De este modo, los genes incluyen finalmente en todos los aspectos de la estructura y la función del cuerpo.
Se calcula que el organismo humano tiene entre 50.000 y 100.000 genes estructurales (genes que codifican las proteínas).
La existencia de un error (o mutación ) en uno de estos genes origina a menudo una enfermedad genética identificable. 
Hasta la fecha, se han identificado unos 5.700 rasgos por un único gen y la mayoría corresponde a procesos patológicos (tabla 1-1).

Los genes, la unidad básica de la herencia, están contenidos en los cromosomas y están formados por DNA.

Cada célula somática humana (células distintas a los gametos , es decir, espermatozoides y óvulos) contiene 23 pares de cromosomas diferentes, con un total de 46 cromosomas.
Un miembro de cada par procede del padre y el otro de la madre. 
Uno de los pares de cromosomas está formado por los cromosomas sexuales .
Los varones normales tienen un cromosoma Y heredado del padre y un cromosoma X heredado de la madre.

En las mujeres normales se encuentran dos cromosomas X, uno heredado de cada progenitor.
Los otros 22 pares de cromosomas se denominan autosomas .
Los miembros de cada par de autosomas se consideran homólogas , puesto que su DNA es muy similar.
Los cromosomas X e Y no son homólogos entre sí. 

Las células somáticas, que tienen 2 cromosomas de cada tipo, se denominan células diploides .
Los gametos tienen el número haploide de cromosomas, 23.
El número diploide de cromosomas se mantiene constante en las generaciones sucesivas de células somáticas mediante el proceso de mitosis , mientras que el número haploide se obtiene mediante un proceso conocido por meiosis.
Ambos procesos se estudiarán con detalle en la parte final de este capítulo. 

Las células somáticas son diploides, y tienen 23 pares de cromosomas (22 autosomas un par de cromosomas sexuales).
Los gametos son haploides, con un total de 23 cromosomas.

Figura 2-1

Anatomía de la célula 

DNA, RNA Y PROTEÍNAS: HERENCIA A NIVEL MOLECULAR DNA

Composición y estructura del DNA

La molécula de DNA está formada por tres componentes básicos: desoxirribosa, un azúcar de tipo pentosa; un grupo fosfato, y cuatro tipos de bases nitrogenadas (así denominadas porque pueden combinarse con iones hidrógeno en soluciones ácidas). 
Dos de las bases, la citosina y la timina , son anillos simples de carbono-nitrógeno denominados pirimidinas .
Las otras dos bases, la adenina y la guanina , son anillos dobles de carbono-nitrógeno denominados purinas (figura, 2-2).
Se suele representar a las cuatro bases por sus primeras letras: C, T, A y G.

Una de las contribuciones de Watson y Crick fue la demostración del modo en que los tres componentes se unen físicamente para formar el DNA.
Estos investigadores propusieron el ahora famoso modelo de doble hélice , en que se puede contemplar el DNA como una escalera de mano retorcida con enlaces químicos formando sus peldaños (figura 2-3).
Los dos lados de la escalera de mano están formados por los componentes fosfato y azúcar, que se mantienen unidos por fuertes puentes fosfodiéster.
Saliendo de cada lado de la escalera, a intervalos regulares, se encuentran las bases nitrogenadas.
Las bases proyectadas a cada lado se unen entre sí por un puente de hidrógeno relativamente débil.
De este modo, Las bases nitrogenadas emparejadas forman los peldaños de la escalera.

En la figura 2-3 (pág. 10) se muestran los enlaces químicos entre las bases, observándose que los extremos de la escalera finalizan en 3' o 5.
Estos signos proceden del orden en que se numeran los cinco átomos de carbono que componen la desoxirribosa.
Cada subunidad de DNA, formada por una desoxirribosa, un grupo fosfato y una base, se denomina nucleótido .

Como veremos, secuencias diferentes de bases nucleótidas (por ejemplo xxx ) especifican proteínas diferentes.
La especificación del gran número de proteínas del organismo requiere una gran cantidad de información genética.
En realidad, cada célula humana contiene aproximadamente tres mil millones de nucleótidos, una información más que suficiente para especificar la composición de todas nuestras proteínas.

Las cuatro bases nucleótidas, la adenina, la timina, la citosina y la guanina son las constituyentes más importantes del DNA.
El DNA tiene una estructura en doble hélice.

Figura 2-2

Estructura química de las cuatro bases, mostrando los puentes de hidrógeno entre los pares de bases

Entre los pares citosina-guanina se forman tres puentes de hidrógeno y entre los pares adenina- timina se forman dos.

Replicación del DNA

Cuando las células se dividen para efectuar copias de sí mismas deben efectuarse copias idénticas del DNA que se incorporan a las nuevas células.
Este proceso es esencial para que el DNA sirva como material genético básico.
La replicación del DNA consiste, básicamente, en la rotura de los débiles puentes de hidrógeno entre las bases, dejando un único filamento (o cadena) de DNA con cada una de sus bases desemparejadas.
El emparejamiento constante de adenina con timina y de guanina con citosina, conocido como emparejamiento de bases complementarias , es la clave de la replicación exacta.
El principio de emparejamiento de bases complementarias dictamina que una base no emparejada atraerá un nucleótido libre sólo si éste posee la base complementaria apropiada.
Por lo tanto, una porción de filamento único con la secuencia de bases ATTGCT se unirá a una serie de nucleótidos libres que tengan las bases TAACGA.
Se dice que el filamento único es un molde sobre el cual se construye un filamento complementario.
Cuando finaliza la replicación se ha formado una nueva molécula de dos filamentos idéntica al original (figura 2-4, página, 10).

Varias enzimas diferentes intervienen en la replicación del DNA.
Una enzima desenrolla la doble hélice, otra mantiene separados los filamentos y otras enzimas efectúan funciones diferentes.
La DNA-polimerasa es una de las enzimas fundamentales del proceso de replicación. 
Recorre el trayecto del filamento aislado de DNA añadiendo nucleótidos libres al extremo 3' del nuevo filamento. 
Los nucleótidos sólo pueden añadirse a este extremo del filamento, de modo que la replicación sólo tiene lugar desde el extremo 5' hacia el extremo 3'.
Cuando nos referimos a la orientación de las secuencias a lo largo de un gen, la dirección 3'? 5'se denomina «contracorriente», y la dirección 5'? 3' se denomina «a favor de la corriente».

Junto a la adición de nuevos nucleótidos, la DNA-polimerasa efectúa parte de un proceso de corrección de pruebas , en el que se examina al nucleótido recién añadido para asegurar que en realidad sea complementario a la base de molde.
Si no lo es, el nucleótido es rechazado y sustituido por una base nucleótida complementaria correcta.
Este proceso mejora sustancialmente la precisión de la replicación de la molécula de DNA.
Cuando un error de replicación del DNA no se repara con éxito, se produce una mutación.
Como veremos en el capítulo 3, muchas mutaciones de este tipo provocan enfermedades genéticas.

La replicación del DNA depende fundamentalmente del principio de emparejamiento de bases complementarias.
Ello permite que un filamento único de la molécula de DNA de doble filamento forme un molde para la síntesis de un nuevo filamento complementario.

La tasa de replicación del DNA en humanos, cercana a 40-50 nucleótidos por segundo, es comparativamente lenta. 
Las bacterias tienen una velocidad muy superior, alcanzando entre 500 y 1.000 nucleótidos por segundo.
Dado que algunos cromosomas humanos poseen incluso 250 millones de nucleótidos, la replicación seria un proceso extraordinariamente lento si procediese de forma lineal de un extremo al otro del cromosoma (para un cromosoma de este tamaño, un solo proceso de replicación necesitaría casi 2 meses).
En realidad, la replicación se inicia en muchos puntos diferentes a lo largo del cromosoma, denominados orígenes de replicación .
Las múltiples separaciones resultantes de los filamentos de DNA se denominan burbujas de replicación (figura, 2-5). 
Al producirse de forma simultánea en lugares diferentes del cromosoma, el proceso de replicación puede efectuarse más rápido.

Las burbujas de replicación permiten que la replicación del DNA tenga lugar en múltiples localizaciones del cromosoma, acelerando en gran medida el proceso de replicación.

Figura 2-3

Doble hélice del DNA, con su esqueleto de azúcar-fosfato y bases nitrogenadas

Figura 2-4

Replicación del DNA

Se rompen los puentes de hidrógeno entre los dos filamentos original, permitiendo que las bases en cada filamento se sometan a un emparejamiento de bases complementarias con bases libres.
Este proceso, que se lleva a cabo de 5'? 3' en cada filamento, forma dos nuevos filamentos dobles de DNA.

De los genes a las proteínas

Mientras que el DNA se forma y replica en el interior del núcleo de la célula, la síntesis de proteínas tiene lugar en el citoplasma.
Por lo tanto, la información contenida en el DNA debe transportarse de algún modo al citoplasma y utilizarse después para dictar la composición de las proteínas.
Ello implica dos procesos, transcripción y traducción .
En resumen, el código de DNA es transcrito al RNA mensajero, que a continuación sale del núcleo para traducirse en proteínas.
Estos procesos, resumidos en la figura 2-6, se estudiarán más extensamente aquí.
Tanto la transcripción como la traducción están mediados por el ácido ribonucleico (RNA) , un tipo de ácido nucleico químicamente similar al DNA.

Como éste, el RNA consta de glúcidos, grupos fosfato y bases nitrogenadas.
Se diferencia del DNA en que su hidrato de carbono es la ribosa en lugar de la desoxirribosa y que el uracilo, y no la timina, es una de las cuatro bases. 
El uracilo es estructuralmente similar a la timina y, como ésta, puede emparejarse con la adenina.
Otra diferencia entre el RNA y el DNA es que mientras éste suele aparecer como un filamento doble, el RNA suele estar formado por un único filamento.

Las secuencias de DNA codifican las proteínas mediante los proceses de transcripción y traducción. 
En ambos interviene el ácido ribonucleico, una molécula formada por un filamento único similar al DNA, excepto en que su hidrato de carbono es la ribosa y en que contiene uracilo en vez de timina.

Figura 2-5

Se forman burbujas de replicación en múltiples puntos del filamento del DNA, lo que hace posible que la replicación se lleve a cabo con más rapidez

Figura 2-6

Resumen de las fases que conducen desde el DNA hasta las proteínas

La replicación y la transcripción se producen en el núcleo celular, después de lo cual el RNAm es transportado hasta el citoplasma, donde tiene lograr la traducción del RNAm en las secuencias de aminoácidos que componen una proteína.

Figura 2-7

Transcripción de DNA a RNAm

La RNA-polimerasa recorre el filamento de DNA en la dirección xxx , ensamblando un filamento de nucleótidos RNAm complementario al filamento molde de DNA.

Variación genética: origen y detección

El ser humano presenta un grado considerable de variación genética, observado en rasgos como la estatura, la presión sanguínea y el color de la piel.
En el espectro de la variación genética se incluyen los estados patológicos, como la fibrosis quística o la neurofibromatosis tipo 1 (v, cap, 4).
Esta faceta de la variación genética constituye el eje central de la genética médica.

Toda variación genética se origina a partir del proceso conocido como mutación , definida como una alteración de la secuencia del DNA. 

Las mutaciones pueden afectar células somáticas o germinales (células que producen gametos). 
Las mutaciones en las células somáticas (todas las que no son germinales) pueden dar lugar a cáncer y, por lo tanto, tienen un interés significativo.
Sin embargo, nuestra atención en el presente capítulo se dirigirá principalmente a la mutación de las germinales, ya que éstas pueden transmitirse de una generación a la siguiente.

Como consecuencia de las mutaciones, es posible que un gen se diferencie de un individuo a otro en términos de su secuencia del DNA.
Los genes con secuencias diferentes se denominan alelos .
La localización de un gen en un cromosoma se denomina locus (de la palabra latina que significa «lugar»).
Podría decirse que un individuo tiene un alelo determinado en el locus de la betaglobina en el cromosoma 11.
Si un individuo presenta el mismo alelo en ambos miembros de un par de cromosomas, se dice que es homocigoto .
Si los alelos difieren en la secuencia del DNA, el individuo es heterocigoto .
Los alelos presentes en un locus determinado constituyen el genotipo del individuo.
Algunos loci (plural de locus ) presentan una variabilidad considerable entre individuos.
Si un locus tiene dos o más alelos, la frecuencia de cada uno de los cuales supera el 10 en la población, se considera que se trata de un locus polimórfico («muchas formas»).
El locus polimórfico se denomina a menudo polimorfismo .

En este capitulo examinaremos la mutación como fuente de la variación genética.

Discutiremos los diferentes tipos de mutaciones, así como sus causas y consecuencias.
Además, se estudiarán las técnicas bioquímicas y moleculares empleadas en la actualidad en la detección de las variaciones genéticas en las poblaciones humanas.

MUTACIÓN: ORIGEN DE LA VARIACIÓN GENÉTICA

Tipos de mutación

Algunas mutaciones consisten en una alteración del número o de la estructura de los cromosomas en una célula. 
Estas anomalías cromosómicas principales pueden observarse por el microscopio y constituyen el tema del capitulo 6.
En el presente capitulo nos centraremos en las mutaciones que afectan un único gen y que no se pueden visualizar mediante el microscopio.
La mayor parte de nuestra discusión se centrará en las mutaciones que tienen lugar en el DNA codificador o en las secuencias reguladoras, puesto que las mutaciones que ocurren en otros segmentos del genoma no suelen provocar consecuencias clínicas.

Un tipo importante de mutación monogénica es la sustitución de un par de bases , en la que un par de bases es reemplazado por otro par [1] .
Ello puede originar un cambio en la secuencia del aminoácido, aunque, debido a la redundancia del código genético, muchas de estas mutaciones no alteran las secuencias de aminoácidos y por lo tanto no tienen consecuencias. 
Estas últimas se denominan sustituciones silenciosas o silentes .
A su vez, las sustituciones de pares de bases pueden ser de dos tipos básicos:
las mutaciones con cambio de sentido provocan un cambio en un único aminoácido, mientras que las mutaciones sin sentido producen uno de los tres codones de terminación en el RNAm: UAA , UAG o UGA (figura. 3-1A, B).
Estos codones finalizan la traducción del RNAm, y, por lo tanto, provocan la terminación prematura de la cadena polipeptídica.
A la inversa, cuando un cebón de interrupción se altera de modo que codifica un aminoácido, se produce un polipéptido anormalmente alargado.
Las alteraciones de las secuencias de aminoácidos pueden tener profundas consecuencias.
Muchas de las enfermedades genéticas graves que se estudiarán más adelante son consecuencia de las sustituciones de pares de bases.

Figura 3-1

Las mutaciones con cambio de sentido (A) producen un cambio de un único aminoácido, mientras que las mutaciones sin sentido (s) producen un codón de terminación en el mRNA

Los codones de terminación interrumpen la traducción del polipéptido.

El segundo tipo importante de mutación consiste en las deleciones o inserciones de uno o más pares de bases.
Estas mutaciones, que pueden originar la adición o la pérdida de aminoácidos en una proteína a menudo son patológicas. 
Un ejemplo de este tipo de mutación es la delación de 3 xxx que provoca la mayoría de casos de fibrosis quísticas diagnosticados en individuos de raza blanca (v, cap, 4 para más detalles).
Las delaciones e inserciones tienden a ser especialmente nocivas cuando el número de pares de bases perdidas o adicionales no es múltiplo de 3.
Puesto que los codones constan de 3 xxx , estas inserciones o deleciones pueden alterar todos los codones subsiguientes.
Por ejemplo, la inserción de una única base (una A en el segundo codón) convertiría una secuencia leída como xxx en una xxx .
Ello transformaría una secuencia de aminoácidos de Thr-Asp-Cys-Val en una de xxx .
Estas mutaciones, mostradas en la figura 3-2, se denominan mutaciones de cambio de la pauta de lectura .

Otros tipos de mutaciones alteran la regulación de la transcripción o de la traducción.
Una mutación del promotor puede reducir la afinidad de la RNA-polimerasa por un locus promotor, provocando a menudo una disminución de la producción de RNAm.
El resultado final es la reducción de la producción de una proteína determinada.
Las mutaciones del potenciador también pueden presentar efectos similares.

Figura 3-2

Las mutaciones de cambio de la pauta de lectura son consecuencia de la inserción o la deleción de un cierto número de bases (no múltiplo de 3)

Esto altera todos los codones a favor de la corriente desde el locus de inserción o deleción.

Las mutaciones también pueden interferir en el proceso de corte y unión de los intrones cuando se forma el RNAm maduro a partir del RNAm transcrito primario.

Las mutaciones en los lugares de corte y unión que se producen en los límites intrón-exón, alteran la señal necesaria para la escisión apropiada de un intrón.
Ello puede ocurrir en la secuencia GT que siempre define el locus donante 5' o en la secuencia AG que define el locus receptor 3' .
También puede ocurrir en las secuencias que se localizan cerca de los loci donante y receptor es decir, las secuencias consenso descritas en el capítulo 2.
Cuando estas mutaciones tienen lugar, la escisión se efectuará a menudo dentro del siguiente exón, en un sitio de corte y unión situado en el exón.
Estos sitios de corte y unión, cuyas secuencias de DNA presentan una ligera diferencia con las de los sitios de corte y unión normales, suelen permanecer sin utilizar y «ocultas» en el interior de los exones y por este motivo se denominan lugares crípticos para el corte y unión .
El uso de un sitio críptico para el proceso de corte y unión provoca la delación parcial del axón.
En otros casos puede ocurrir la delación de un exón completo. 
Como se muestra en la figura 3-3, las mutaciones en el sitio de corte y unión también pueden originar la inclusión anómala de la totalidad o parte de un intrón en el RNAm maduro.
También es posible que se produzca una mutación en un lugar críptico para el corte y unión normal, compartiendo así con el verdadero sitio de corte y unión normal.

Figura 3-3

En la primera mutación de lugares de corte y unión mostrada (B), la secuencia donante, GT, es sustituida por AT

Ello provoca un corte y unión incorrecto que deja parte del intrón en el transcrito RNAm maduro. 
En la segunda mutación (C), se crea un segundo sitio donante GT en el primer intrón, lo que resulta en una combinación de productos de RNAm ensamblados de forma normal y anormal.

Existen varios tipos de secuencias de DNA capaces de propagar copias de sí mismas a continuación, estas copias se insertan en otras localizaciones cromosómicas.
La inserción de estos transposones (o elementos móviles ) puede causar mutaciones de cambio de la pauta de lectura.
Hasta una fecha reciente no se había aclarado si este fenómeno que se había comprobado en experiencias con animales, como la mosca de la fruta, ocurría en el hombre.
En la actualidad, ya se ha comprobado que la inserción de elementos móviles ocasiona casos aislados de neurofibromatosis de tipo 1 y hemofilia A (un trastorno de la coagulación) en humanos.

El último tipo de mutación que consideraremos se ha descubierto recientemente y afecta secuencias de DNA repetidas en tándem (v cap. 2) que aparecen en el interior o cerca de algunos genes patológicos.
Un individuo normal tiene un número relativamente pequeño de estas repeticiones en tándem (por ejemplo de 2O a 40).
Por motivos todavía no conocidos, el número de repeticiones puede aumentar de forma espectacular durante la meiosis o, posiblemente, durante el inicio del desarrollo fetal, de modo que un recién nacido puede tener cientos o incluso miles de repeticiones.
Cuando ello ocurre en determinadas determinadas regiones del genoma, provoca una enfermedad genética Igual que otras mutaciones, estas repeticiones expendidas pueden transmitirse a la descendencia del paciente.
Hoy en día se conocen al menos siete enfermedades genéticas causadas por repeticiones expandidas.
En el capítulo 4 hay una exposición adicional.

Las mutaciones son la fuente original de la variación genética.

Algunas mutaciones provocan enfermedades genéticas mientras que otras no tienen efectos.
Los principales tipos de mutaciones incluyen las mutaciones con cambio de sentido sin sentido de cambio de la pauta de lectura, de promotor y de sitio de corte y unión.
También existen pruebas de la existencia de mutaciones causadas por la inserción al azar de elementos móviles y se conocen varias enfermedades genéticas causadas por repeticiones expandidas.

Consecuencias clínicas de la mutación:

hemoglobinopatías

Los trastornos genéticos de la hemoglobina humana son el grupo más común de enfermedades monogénicas: 
se calcula que el 5% por de la población mundial es portadora de una o más mutaciones de los genes que intervienen en la síntesis de la hemoglobina.
Puesto que todos los tipos de mutación descritos en este capítulo han sido, observados en las hemoglobinopatías, estos trastornos sirven como una importante ilustración de las consecuencias clínicas de la mutación.

La molécula de hemoglobina es un tetrámero compuesto por cuatro cadenas polipeptídicas dos denominadas alfa y dos denominadas beta.
Las cadenas beta están codificadas por un gen en el cromosoma 11, y las cadenas alfa están codificadas por dos genes en el cromosoma 16, que son similares entre sí.
Por lo tanto, un individuo normal tendría dos genes beta normales y cuatro genes alfa normales (figura 3-4).
Por lo general, la estricta regulación de estos genes asegura la producción de números aproximadamente iguales de las cadenas alfa y beta.
Cada una de estas cadenas de globina se asocia con un grupo hemo , que contiene un átomo de hierro y se puede unir con el oxígeno. 
Esta propiedad permite que la hemoglobina efectúe la función vital de transporte de oxígeno en los hematíes.

Se puede clasificar a las hemoglobinopatías en dos amplias categorías:
anomalías estructurales, en las que existe una alteración de la molécula de hemoglobina, y las talasemias, un grupo de patologías en las que la hemoglobina es estructuralmente normal pero con una reducción de su cantidad.
Otro trastorno es la persistencia hereditaria de la hemoglobina fetal (PHHF) que aparece cuando la hemoglobina fetal, codificada por los genes de alfaglobina y por dos genes similares a los de betaglobina denominados A gama y G gama(figura 3-4), continua produciéndose después del nacimiento (normalmente, la producción de cadena gama cesa y la producción de cadena beta comienza tras el nacimiento).
La PHHF no provoca ninguna enfermedad, sino que en realidad puede compensar una falta de hemoglobina adulta normal.
En este capítulo, no se proporcionarán más detalles.

Figura 3-4

Acumulación de genes de alfaglobina en el cromosoma 16 y acumulación de gen de betaglobina en el cromosoma 11

La acumulación de betaglobina incluye el gen de globina que codifica la globina embrionaria y los genes de gammaglobina que codifican la globina fetal.
Los genes fi beta I y delta no se expresan.
El grupo génico de alfaglobina incluye el gen de sigma globina, que codifica la alfaglobina embrionaria.

Anemia de células falciformes

Se ha identificado una amplia gama de hemoglobinopatías diferentes.
La discusión que sigue es una presentación muy simplificada de las principales formas de estos trastornos. 
En la tabla 3-1 se resumen los trastornos de la hemoglobina, las mutaciones que los causan y sus principales características. 

La más importante de las anomalías estructurales de la hemoglobina es la anemia de células falciformes. 
Un trastorno que afecta aproximadamente a 1 de cada 400- (100 afroamericanos.
Es incluso más frecuente en algunas partes de África, donde puede afectar a uno de cada 50 nacimientos y también se observa en algunas ocasiones en las poblaciones mediterránea y de Oriente Medio.
La anemia de células falciformes está causada por una única mutación con cambio de sentido que efectúa una sustitución de una valina por un ácido glutámico en la posición seis de la cadena de la betaglobina.
En la forma homocigota, esta sustitución de aminoácido altera la característica de la molécula de hemoglobina, de modo que los eritrocitos (glóbulos rojos) adoptan una característica forma falciforme (figura 3-5 A) bajo condiciones de una tensión de oxígeno baja.
Estas condiciones se experimentan en los capilares, pequeños vasos cuyo diámetro es inferior al de los eritrocitos. 
Los eritrocitos normales pueden comprimirse y deslizarse por los capilares, pero los eritrocitos falciformes son menos flexibles y no pueden hacer lo mismo.
La consiguiente obstrucción vascular provoca hipoxia (falta de oxigeno) localizada, «crisis» drepanocítica dolorosa, e infartos de varios tejidos, incluyendo, huesos, bazo, riñones y pulmones (un infarto consiste en la destrucción de tejidos.
La destrucción prematura de los eritrocitos falciformes reduce el número de eritrocitos circulantes y el nivel de hemoglobina, produciendo anemia .
El bazo se hipertrofia ( esplenomegalia ), si bien los infartos destruyen finalmente este órgano, provocando cierta pérdida de la función inmunitaria.
Ello contribuye a la aparición de las infecciones bacterianas recidivantes (especialmente neumonías) que se observan a menudo en los pacientes con anemia de células falciformes y que son una causa frecuente de muerte.
Se calcula que en Norteamérica cerca del 15 % de los niños con anemia de células falciformes fallecen antes de los 5 años de edad.

La enfermedad de células falciformes, que provoca anemia, infartos hísticos y múltiples infecciones, es el resultado de una mutación única con cambio de sentido que produce la sustitución de un aminoácido en la cadena de betaglobina.

Talasemia

El término talasemia procede de la palabra griega thalassa que significa «mar». 
Ello se refiere al hecho de que la talasemia se describió en primer lugar en poblaciones que vivían cerca del mar Mediterráneo, aunque también es frecuente en algunas partes de África, Oriente Medio, India y sudeste asiático. 
Se puede clasificar la talasemia en dos grupos principales. 

Talasemia alfa y talasemia beta, dependiendo de la cadena que haya disminuido en cantidad.
Cuando se reduce el número de un tipo de cadena.
La otra cadena, incapaz de participar en la formación del tetrámero normal, tiende a formar moléculas que constan de cuatro cadenas sólo de este tipo en exceso (estas moléculas se denominan homotetrámeros , en contraste con los heterotetrámeros formados normalmente por cadenas alfa y beta).
En la talasemia alfa existe un déficit de cadenas de alfaglobina, de modo que se registra un exceso de cadenas beta(o cadenas gama en el feto).
Estas cadenas forman homotetrámeros que tienen una capacidad de fijación de oxígeno muy reducida, lo que provoca anemia.
En la talasemia beta, el exceso de cadenas alfa forma tetrámeros que precipitan y lesionan las membranas celulares de los precursores de los hematíes (es decir, las células que forman los eritrocitos circulantes).
Ello conduce a la destrucción de eritrocitos prematuros y a anemia.

Tabla 3-1

Resumen de las hemoglobinopatías

Enfermedad.
Tipo de mutación.
Principales signos patológicos.


Anemia de células falciformes.
Mutación con cambio de sentido de betaglobina.
Anemia, infartos hísticos, infecciones.


Enfermedad de Hb H.
Deleción o anomalía de 3 de los 4 genes de la alfaglobina.
Anemia de gravedad moderada, esplenomegalia.


Hidropesía fetal.
Deleción o anomalía de los 4 genes de la alfaglobina.
Hipoxia grave, insuficiencia cardíaca congestiva; mortinato o muerte perinatal.


Talasemia beta 0.
En general, mutaciones sin sentido, de cambio de pauta de lectura, o de receptor o donante de puntos de corte y unión sin producción de betaglobina .
Anemia grave, esplenomegalia, anomalías esqueléticas, infecciones, a menudo fatal durante la primera década de vida si no se trata.


Talasemia beta +.
En general mutaciones de pérdida de sentido, reguladoras, o de la secuencia consenso de puntos de corte y unión o de lugares crípticos de ensamblaje; producción de una pequeña cantidad de betaglobina .
Signos similares a los de la talasemia beta 0, aunque a menudo algo más leves.


La mayoría de casos de talasemia alfa se deben a deleciones de los genes de alfaglobina.
La pérdida de uno o dos de estos genes no tiene ningún efecto clínico.
La pérdida o la anomalía de tres de los genes alfa provoca esplenomegalia y anemia moderadamente grave (enfermedad de Hb H).
La pérdida de los cuatro genes alfa, un trastorno observado principalmente entre los habitantes del sudeste asiático provoca hipoxia en el feto e hidropesía fetal (una enfermedad en que se produce una acumulación masiva de líquido.
La hipoxia grave ocasiona invariablemente un niño nacido muerto o la muerte neonatal.

Los trastornos de talasemia alfa suelen deberse a deleciones de los genes de la alfaglobina.
La pérdida de estos genes ocasiona anemia de moderada gravedad y la pérdida de los cuatro genes tiene consecuencias fatales.

Los individuos con mutación de la betaglobina en una copia del cromosoma 11 (heterocigotos) son diagnosticados de talasemia beta menor, una enfermedad que implica la existencia de escasa o nula anemia y que no suele requerir tratamiento clínico Los individuos cuyas dos copias del cromosoma tienen una mutación de la betaglobina desarrollan una talasemia beta mayor (también denominada anemia de Cooley) o el trastorno menos grave, la talasemia beta intermedia Es posible la ausencia completa de betaglobina (talasemia beta 0) o bien una disminución del 10 al 30% de la cantidad normal (talasemia, beta+).

Figura 3-5

Los eritrocitos de los pacientes con células falciformes adoptan una forma característica en determinadas condiciones de baja tensión de oxigeno(A) 

Compárese con los eritrocitos normales (B).

Puesto que el organismo no produce betaglobina hasta después del nacimiento la talasemia beta mayor no se manifiesta clínicamente hasta la edad de 2 a 6 meses.

Estos pacientes desarrollan anemia grave.
Sin tratamiento puede producirse un retraso notable del crecimiento.
La anemia Provoca la expansión de la médula ósea que a su vez origina alteraciones esqueléticas, incluyendo pómulos y maxilares superiores protuberantes y adelgazamiento de los huesos largos (aumentando su predisposición a las fracturas).
La esplenomegalia (figura 3-6) y las infecciones son usuales y los pacientes con talasemia beta principal no tratados fallecen a menudo durante la primera década de vida.
La gravedad de la talasemia Beta puede variar mucho, dependiendo de la naturaleza exacta de la mutación responsable.

En contraste con la talasemia alfa, las deleciones génicas son relativamente raras en la talasemia Beta.
En realidad la mayoría de casos se deben a mutaciones de una única base.
Las mutaciones sin sentido, que provocan la terminación prematura de la traducción de la cadena beta suelen producir talasemia beta.
Las mutaciones de cambio de la pauta de lectura también generan de forma característica la forma beta 0.
Además de las mutaciones en el gen de la betaglobina, también se observan alteraciones en las secuencias reguladoras.
La transcripción de la betaglobina es regulada por un promotor, dos potenciadores y una región contracorriente conocida como región de control del locus ( RCL del inglés locus control region ), (figura 3-4).
Las mutaciones producidas en estas regiones reguladoras suelen originar reducción de la síntesis de RNAm y disminución, aunque no ausencia completa, de betaglobina (talasemia beta+).
También se han observado varios tipos de mutaciones en el sitio de corte y unión.
Cuando ocurre una mutación puntual en los sitios donante o receptor, el corte y unión normal se destruye por completo, originando talasemia beta 0.
Las mutaciones en las secuencias consenso circundantes suelen provocar beta+-talasemia.
También se producen mutaciones en los sitios de corte y unión crípticos, localizados en los intrones o los exones del gen de la betaglobina, lo que provoca que estos sitios estén disponibles para el mecanismo de corte y unión.
Estos sitios de corte y unión adicionales compiten después con los sitios de corte, y unión normales, produciendo algunas cadenas de betaglobina normales y otras anormales.
El resultado suele ser la aparición de una talasemia beta+.

Las talasemias beta pueden estar causadas por muchos tipos diferentes de mutaciones.
Las mutaciones sin sentido, de cambio de la pauta de lectura, y de los sitios de corte y unión donante y receptor tienden a provocar enfermedades más graves.
Las mutaciones de los mecanismos de regulación y las que implican las secuencias consenso de los sitios de corte y unión y a los sitios de corte y unión crípticos tienden a originar enfermedades menos graves.

Se han descrito más de 100 mutaciones diferentes de la betaglobina.
En consecuencia, la mayoría de pacientes con talasemia beta no son «homocigotos» en sentido estricto; suelen poseer una mutación diferente de la betaglobina en cada copia del cromosoma 11 y se les denomina heterocigotos compuestos .
Aun cuando las mutaciones difieran, los dos genes de la betaglobina están alterados, produciendo un estado patológico.
Es común aplicar el término «homocigoto» a los heterocigotos compuestos.

[1] En genética molecular, las sustituciones de un par de bases también se denominan mutaciones puntuales .
Sin embargo, este último término se utilizó en genética clásica para indicar toda mutación lo bastante pequeña para no ser visualizable por el microscopio.

Mapeo genético

El trazado del mapa o mapeo de los genes en sus localizaciones cromosómicas específicas constituye uno de los ejes centrales de la genética médica.
Los espectaculares avances registrados en las técnicas de genética médica, junto con el notable desarrollo del análisis estadístico de los datos genéticos, han acelerado en gran medida el trazado del mapa de los genes (figura 7-1).
En la actualidad se está trazando el mapa de genes codificadores de proteínas a una velocidad cercana a uno al día y casi 3.000 de estos genes ya se han asignado a sus localizaciones cromosómicas.
Si bien se trata de un progreso de admirable velocidad, este número sólo representa una peña fracción del total estimulado de 50.000 a 100.000 genes que se calcula que componen el genoma humano.
Es evidente que resta por llevar a cabo una ingente tarea.
Uno de los objetivos del Proyecto del Genoma Humano ( Human Genome Project ), tratado más adelante, es trazar el mapa de todos nuestros genes en sus posiciones cromosómicas específicas.
Con toda seguridad a medida que progrese esta investigación, también aumentará nuestro conocimiento acerca de la base biológica de la patología genética.

El trazado del mapa genético es una etapa fundamental para el conocimiento, el diagnóstico y el eventual tratamiento de una enfermedad genética.
Cuando se identifica con precisión la localización de un gen patológico a menudo se puede establecer un pronóstico más exacto para los individuos con riesgo de padecer una enfermedad genética.
La localización de un gen patológico a menudo también es necesaria para que se pueda realizar la clonación del gen.
Una vez clonado un gen es posible estudiar su secuencia de DNA y su producto proteico, y ello puede contribuir a nuestro conocimiento de la causa real de la enfermedad.

Además, abre el camino para la síntesis artificial de un producto genético normal mediante las técnicas de DNA recombinante, permitiendo un tratamiento más efectivo de muchas enfermedades genéticas.
La terapia génica, O inserción de genes normales en los cuerpos de los individuos afectados con una enfermedad genética, también se convierte en una posibilidad.
Por lo tanto, el trazado del mapa genético contribuye de modo directo a alcanzar muchos de los principales objetivos de la genética médica. 

En este capítulo estudiaremos los métodos utilizados de forma habitual en el trazado del mapa de los genes. 
Pueden diferenciarse dos tipos principales de mapeo del gen.
En el mapeo genético , se utiliza la frecuencia de los entrecruzamientos meióticos entre loci para determinar las distancias entre loci .
El mapeo físico implica el uso de técnicas citogenéticas y moleculares para determinar la localización física real de los genes en los cromosomas.
Además de discutir las técnicas de trazado del mapa, estudiaremos la forma en que estas técnicas posibilitan una predicción más precisa del riesgo patológico en las familias y el modo en que conducen al aislamiento y clonación de los genes patológicos.

MAPEO GENÉTICO

Análisis del ligamiento

Una de las leyes de Gregor Mendel, el principio de distribución independiente, establece que los genes de un individuo se transmitirán a la siguiente generación de modo independiente entre sí (cap. 4).
Mendel no sabía que los genes se localizan en cromosomas ni que los genes localizados uno cerca de otro en el mismo cromosoma se transmiten juntos, no de forma independiente.
Así, el principio de Mendel de la distribución independiente continúa siendo cierto para la mayoría de pares de loci , pero no para aquellos que ocupan la misma región de un cromosoma. 
A estos loci se les denomina ligados .

Figura 7-1

Número de genes codificantes mapeados en localizaciones cromosómicas específicas 

Los loci A y B están ligados en el mismo cromosoma, por lo que los alelos A1, y B1 suelen heredarse juntos.
El locus C se encuentra en un cromosoma diferente, de modo que no está ligado a A ni a B y sus alelos se transmiten de forma independiente de los alelos A y B.

La figura 7-2 muestra dos loci , A y B, localizados muy cerca entre sí en el mismo cromosoma. 
Un tercer locus , C, se localiza en otro cromosoma.
En el individuo de nuestro ejemplo, cada uno de estos loci tiene dos alelos, denominados 1 y 2.
A y B están ligados, de modo que Al y B, se heredan juntos.
Puesto que A y C se encuentran en cromosomas distintos y, por lo tanto, no están ligados, sus alelos no siguen el principio de distribución independiente. 
En este caso, si el proceso de meiosis coloca a A, en un gameto, la probabilidad de que se encuentre a C, en el mismo gameto es del 50 %.
Recuerde que en el capitulo 2 se mencionaba que los cromosomas homólogos a veces intercambien segmentos de su DNA durante la profase I (cruzamiento).
El cromosoma medio experimentará entre uno y tres entrecruzamientos durante la meiosis.
Como resultado del entrecruzamiento, pueden formarse nuevas combinaciones de alelos en un cromosoma. 
Consideremos los loci ligados, A y B, antes estudiados.
Los alelos A1, y Bl se localizan juntos en un cromosoma y los alelos A2 y B2 se localizan en el cromosoma homólogo.
La combinación de alelos en cada cromosoma se denomina haplotipo (de «genotipo haploide»).
Los dos haplotipos de este individuo se designan como A1B1/A2B2 .
Como se muestra en la figura 7-3 A, A,B, se encuentra en un gameto, y A2B2 se encuentra en el otro gameto en ausencia de un entrecruzamiento. 
Cuando existe un entrecruzamiento, se encuentran nuevas combinaciones de alelos, A, B2 y A2B" en cada gameto (figura 7-3, B).

Figura 7-3

Resultados genéticos del entrecruzamiento

A, falta de entrecruzamiento: Al y B1 permanecen juntos tras la meiosis;
B, el cruzamiento entre A y B resulta en una recombinación: Al y B2 se heredan juntos en un cromosoma, y 4 y B1 se heredan juntos en otro cromosoma; 
C, un entrecruzamiento doble entre A y B no produce ninguna recombinación de alelos.

El proceso de formación de estas nuevas distribuciones de alelos se denomina recombinación .
Sin embargo, el entrecruzamiento no origina necesariamente una recombinación, ya que puede producirse un entrecruzamiento doble entre los dos loci , teniendo como resultado final una falta de recombinación (figura 7-3, C).

Como se muestra en la figura 7-4, es más probable que los entrecruzamientos ocurran entre loci alejados en un cromosoma que entre loci situados en posiciones muy próximas.
Por lo tanto, puede inferirse la distancia entre dos loci calculando la frecuencia con que se producen recombinaciones en las familias (esto se denomina frecuencia de recombinación ).
Si en una serie grande de meiosis estudiadas en familias los alelos de A y B sufren recombinaciones el 5 % del tiempo, la frecuencia de recombinación de A y B sería del 5 %.

La distancia genética entre dos loci se mide en centiMorgan ( cM ) , en honor de T. H. Morgan, quien descubrió el proceso de entrecruzamiento en 1910.
1 cM es aproximadamente igual a una frecuencia de recombinación del 1 %.
La relación entre frecuencia de recombinación y centiMorgan es aproximada, ya que los entrecruzamientos dobles no producen ninguna recombinación.
Por lo tanto, la frecuencia de recombinación infraestima la distancia en el mapa, especialmente cuando la frecuencia de recombinación aumenta por encima del 5-10 %.
Se han disonado funciones de mapeo para corregir esta infraestimación.

Los loci que se encuentran en el mismo cromosoma se denominan sinténicos («mismo filamento»).
Si dos loci sinténicos se encuentran a 50 cM de distancia, se considera que no están ligados, ya que su frecuencia de recombinación es tan grande como si se localizaran en cromosomas diferentes.

Los entrecruzamientos entre loci en un mismo cromosoma pueden producir recombinaciones.
Los loci en un mismo cromosoma que experimentan recombinación menos del 50 % del tiempo se denominan ligados.
La distancia entre loci puede expresarse en centiMorgan; I cM representa una frecuencia de recombinación aproximadamente del I %.

Las frecuencias de recombinación pueden calcularse observando la transmisión de genes en árboles genealógicos. 
La figura 7-5, A, es un ejemplo de árbol genealógico en que se transmite la neurofibromatosis de tipo 1 ( xxx ).
También se ha efectuado la tipificación de los miembros de este árbol genealógico para un RFLP de dos alelos denominado xxx que, como el gen xxx , se localiza en el cromosoma 17.
Los genotipos xxx se presentan debajo de cada individuo en el árbol genealógico.
El examen de las generaciones 1 y 2 nos permite determinar que el gen xxx debe de estar en el mismo cromosoma que el alelo 1 del sistema xxx en esta familia, ya que el individuo I-2, homocigoto para el alelo 2, no está afectado por la enfermedad.
La distribución de estos alelos en cada cromosoma se denomina fase de ligamiento .
Conociendo la fase de ligamiento, los haplotipos del individuo 11-2 serían entonces Nl/n2 , donde N indica el alelo causante de IVF1, n indica el alelo normal, y 1 y 2 son los dos alelos de xxx .
El marido de esta mujer (individuo II-1) no está afectado por la enfermedad y es homocigoto para el alelo 2 en xxx .
Debe de tener los haplotipos xxx .
Puesto que los loci. xxx y xxx están ligados, los hijos de esta unión que resulten afectados con xxx tendrán el alelo 1 de xxx , mientras que los no afectados presentarán el alelo 2.
En siete de ocho niños en la generación 3,se comprueba que esto es cierto.
En un caso se produjo una recombinación (individuo III-6).
Ello da una frecuencia de recombinación de 1/8 = 12,5 % .

Figura 7-4

El entrecruzamiento es más probable entre loci alejados en el cromosoma (izquierda)que entre loci cercanos (derecha)

En la práctica real se utilizaría una muestra mucho más amplia de familias con el fin de asegurar la precisión estadística de este resultado (si se efectuase, se demostraría que xxx y A7F1 están, en realidad, mucho más ligados de lo que indica este ejemplo, con una frecuencia de recombinación inferior al 1%).

Las estimaciones de las frecuencias de recombinación se obtienen observando la transmisión de los alelos en las familias.
La determinación de la fase de ligamiento (es decir, el cromosoma en que se localiza cada alelo) es una parte importante de este procedimiento.

Los polimorfismos que, como el xxx , pueden utilizarse para efectuar el seguimiento de un gen patológico en una familia, se denominan marcadores .
Puesto que puede efectuarse la tipificación de los marcadores ligados en un individuo de cualquier edad (incluso en un feto), son útiles para el diagnóstico precoz de la enfermedad genética (v. cap. 11).

En general, 1 cM corresponde aproximadamente a 1 millón de xxx (1 Mb) de DNA.
Sin embargo, sólo se trata de una relación aproximada ya que se conocen varios factores que influyen en las tasas de entrecruzamiento.
En primer lugar, son casi 1,5 veces más frecuentes en la meiosis femenina (oogénesis) que en la meiosis masculina (espermatogénesis). 
Además, los entrecruzamientos tienden a ser más frecuentes cerca de los telómeros de los cromosomas que junto a los centrómeros.
Por último, algunas regiones cromosómicas exhiben tasas de entrecruzamiento muy superiores a otras regiones.
Estas regiones se denominan zonas calientes de recombinación .
Todavía no se conocen las causas de las zonas calientes de recombinación en humanos, aunque se cree que algunas secuencias de DNA específicas, que posiblemente incluyen secuencias Alu (v. cap. 2), cuentan con una predisposición especial al entrecruzamiento.

Aunque existe una correlación entre los centiMorgan y las distancias físicas reales entre loci, esta relación resulta complicada por la diferencia de género sexual en la recombinación, unas frecuencias de recombinación superiores cerca de los telómeros y la existencia de puntos calientes de recombinación.

Figura 7-5

Un árbol genealógico en que se ha efectuado la tipificación de cada miembro para el polimorfismo xxx (A)

Los genotipos de este locus marcador de dos alelos se presentan debajo de cada individuo en el árbol genealógico. 
Los miembros afectados del árbol genealógico se señalan con un símbolo sombreado.
Autorradiografía del polimorfismo xxx en esta familia (B).

Puntuaciones LOD: determinación de la significancia de los resultados de ligamiento

Como en cualquier estudio estadístico, se ha de comprobar con cuidado que los resultados obtenidos en un estudio de ligamiento no se deban simplemente al azar.
Por ejemplo, consideremos que, en un árbol genealógico se ha tipificado un locus marcador de dos alelos. 
Es posible que, por azar , todos los descendientes afectados hereden un alelo y que todos los no afectados hereden el otro alelo, incluso aunque el marcador no esté ligado al gen patológico.
Este resultado engañoso es menos probable que ocurra cuando aumentamos el número de individuos en nuestro estudio de ligamiento (igual que la probabilidad de una fuerte desviación de la proporción 50-50 entre caras y cruces disminuye cuando efectuamos un número superior de lanzamientos de moneda).

¿Cómo averiguamos si un resultado de ligamiento es probable que se deba únicamente al azar? En los análisis de ligamiento se utiliza un método estándar.

Comenzamos comparando la probabilidad de que dos loci estén ligados con una frecuencia de recombinación determinada (señalada como teta) contra la probabilidad de que ambos loci no estén ligados (frecuencia de recombinación = 50 % o teta = 0,5).
Supongamos que se desea probar la hipótesis de que los dos loci están ligados con una frecuencia de recombinación de teta= 0,1, en comparación con la hipótesis de que no están ligados.
Utilizaremos los datos del árbol genealógico para formar una relación de probabilidad : probabilidad de observar datos del árbol genealógico si teta = 0,1 probabilidad de observar datos del árbol genealógico si teta = 0,5 .

Si nuestros datos del árbol genealógico indican que es más probable que teta sea 0,1 en vez de 0,5, entonces la relación de probabilidad será mayor a 1.
Sin embargo si los datos del árbol genealógico argumentan en contra del ligamiento de los dos loci , entonces el denominador será mayor que el numerador, y por lo tanto la relación será inferior a 1,0.
Por razones de conveniencia, suele adoptarse el logaritmo [2] decimal de la relación: este «logaritmo de la relación», se denomina puntuación LOD ( logarithm of the odds ).
Por convenio, una puntuación LOD de 3,0 se acepta como prueba de ligamiento: una puntuación LOD de 3,0 indica que la probabilidad en favor del ligamiento es 1.000 veces superior que la probabilidad contra el ligamiento. 
A la inversa, una puntuación LOD inferior a 2 (una probabilidad de 100 a 1 contra el ligamiento ) se considera una prueba de que ambos loci no están ligados.

Un ejemplo sencillo nos ayudará a ilustrar los conceptos de relaciones de probabilidad y de puntuaciones LOD.
Consideremos el árbol genealógico de la figura 7-6, que presenta otra familia en la que se transmite la xxx .
Se ha efectuado la tipificación del marcador xxx en la familia, igual que en la figura 7-5.
El hombre de la generación 2 ha recibido el alelo xxx de su madre ya que ésta únicamente puede transmitir este alelo marcador.
Por lo tanto, su alelo xxx sólo puede provenir de su padre, en el mismo cromosoma que el gen patológico xxx .
Ello nos permite establecer la fase de ligamiento en este árbol genealógico: el hombre afectado en la generación 2 debe tener los haplotipos xxx .
Este hombre se casa con una mujer homocigota para el alelo xxx Así, la hipótesis de ligamiento próximo (9 = 0,0) predice que cada descendiente en la generación 3 que reciba el alelo 2 de su padre o de su madre también debe recibir el alelo patológico xxx .
Bajo la hipótesis de ligamiento, el padre puede transmitir únicamente dos combinaciones posibles: el cromosoma con el gen patológico y con xxx , o el otro cromosoma, con el gen normal y con xxx .
La probabilidad de cada uno de estos sucesos es de 1/2.
Por lo tanto, si teta = 0,0 la probabilidad de observar cuatro hijos con los genotipos mostrados en la figura 7-6 es de (1/2)4 o de 1/16.
Éste es el numerador de la relación de probabilidad.

Consideremos ahora la probabilidad de observar estos genotipos si xxx y xxx no estuviesen ligados(teta = 0,5) Bajo esta hipótesis, existe la distribución independiente de alelos en xxx y xxx .
Por lo tanto, el padre puede transmitir cualquiera de las cuatro combinaciones, xxx con idéntica probabilidad (1/4).
Entonces, la probabilidad de que se observen cuatro niños con los genotipos observados es ( l/4)4 = 1/256 .
Esta probabilidad es el denominador de la relación de probabilidad. 
Entonces, la relación de probabilidad es de 1/16:1/256 = 16 .
Por lo tanto, los datos de este árbol genealógico nos dicen que el ligamiento , para teta = 0,0 es 16 veces más probable que el no ligamiento .
Si tomamos el logaritmo común de 16, encontramos que la puntuación LOD es 1,2 todavía demasiado pequeña respecto al valor de 3,0 que suele aceptarse como prueba de ligamiento.
Para probar la existencia de ligamiento, necesitamos examinar datos de familias adicionales.
Las puntuaciones LOD de familias individuales pueden añadirse para obtener una puntuación global.
Supongamos que se ha producido una recombinación en la meiosis que genera uno de los hilos en la generación 3.
Este suceso es imposible bajo la hipótesis de que teta = 0,0, por lo que el numerador de la relación de probabilidad se convierte en 0, y la puntuación LDR es -8 .

Figura 7-6

Árbol genealógico en que se ha efectuado la tipificación de cada miembro para el polimorfismo xxx .

De nuevo, se muestran los genotipos marcadores bajo cada individuo.

Al efectuar un análisis de ligamiento se suele probar una serie de valores de teta, comparando la probabilidad de cada valor de teta con la probabilidad de teta = 0,5.
Ello produce una puntuación LOD para cada valor de teta probado. 
A continuación se realiza una gráfica comparando las puntuaciones LOD contra los valores teta, como se muestra en la figura 7-7.
La máxima puntuación LOD en la gráfica es la máxima estimación de probabilidad de 9.
Es decir, es la distancia más probable entre los loci analizados. 
En la práctica, el análisis de los datos de ligamiento humano no es tan sencillo como en el ejemplo anterior. 
A menudo se desconoce la fase de ligamiento, la penetrancia del gen de la enfermedad puede ser incompleta y el modo de herencia puede ser dudoso.
Por estos motivos, se analizan los datos de ligamiento utilizando alguno de los diversos programas informáticos disponibles.
Estos programas informáticos también nos permiten efectuar un mapeo multipuntual , un método de cálculo simultáneo de las localizaciones en el mapa de varios marcadores.

Las probabilidades estadísticas de que dos loci se encuentren separados por un número determinado de centiMorgan pueden calcularse midiendo la relación de dos probabilidades: la probabilidad de ligamiento a una distancia determinada dividida por la probabilidad de no ligamiento.
El logaritmo de esta relación de probabilidades es una puntuación LOD.
Las puntuaciones LOD superiores a 3,0 se toman como evidencia de ligamiento, mientras que las puntuaciones inferiores a 2,0 se toman como evidencia de que dos loci no están ligados.

Análisis de ligamiento y mapa génico humano

Supongamos que estamos estudiando un gen patológico en una serie de árboles genealógicos y que deseamos trazar su mapa en una localización cromosómica especifica.
De forma típica, efectuaríamos la tipificación de los miembros de nuestro árbol genealógico para los loci marcadores distribuidos a lo largo de cada cromosoma, analizando el ligamiento entre el gen patológico y cada marcador.
La mayoría de estas pruebas registrarán puntuaciones LOD negativas, indicando la ausencia de ligamiento entre el marcador y el gen de la enfermedad.
Por último, este ejercicio revelará el ligamiento entre un marcador y el gen de la enfermedad.
Debido al gran tamaño del genoma humano, es posible que se deba efectuar la tipificación de docenas, o incluso cientos, de marcadores antes de encontrar un ligamiento.
Utilizando este método se han localizado enfermedades importantes, incluyendo la fibrosis quística, la enfermedad de Huntington, el síndrome de Marfan y la xxx .

Figura 7-7

La puntuación LOD (logaritmo de las relaciones, eje y) se traza contra la frecuencia de recombinación (eje x) para determinar la frecuencia de recombinación más probable de un par de loci

Figura 7-8

Número de loci marcadores polimórficos conocidos; durante la última década se observa una rápida tasa de aumento

Hasta hace poco más de un década, los análisis de ligamiento tenían escasa posibilidad de éxito ya que sólo se disponía de unas pocas docenas de marcadores polimórficos en todo el genoma humano.
Por lo tanto, era improbable que un gen patológico estuviera localizado lo bastante cerca de un marcador para obtener un resultado de ligamiento significativo.
Esta situación ha cambiado de forma espectacular durante los últimos años, cuando se han generado miles de nuevos marcadores polimórficos (RFLP, VNTR y polimorfismos de microsatélites: v., cap., 3) (figura, 7-8).
En la actualidad es frecuente trazar el mapa de un gen patológico en tan sólo unos meses de análisis de laboratorio y estadístico. 

Para que los loci marcadores sean útiles para el mapeo génico deben tener varias propiedades.
En primer lugar, deben ser codominantes (es decir, los homocigotos deben ser diferenciables de los heterocigotos), lo que ayuda a determinar la fase de ligamiento.
Los RFLP los VNTR y los polimorfismos de microsatélites satisfacen este criterio, no ocurrido igual con algunos de los tipos más antiguos de marcadores, como los grupos sanguíneos ABO y Rh.

Figura 7-9

Mapa genético del cromosoma 9, que muestra las localizaciones de un gran número de marcadores polimórficos

Puesto que las tasas de recombinación suelen ser superiores en la meiosis femenina, las distancias entre marcadores (en cM ) son más grandes en las mujeres que en los hombres.

[2] Recuerde que el logaritmo decimal de un número; es la potencia a la que se eleva 10 para obtener dicho número.
Así, el logaritmo de 100 es F, el logaritmo de 1.000 es 3, y así sucesivamente. 

Genética clínica y consejo genético

La genética médica ha surgido recientemente como una auténtica especialidad médica.
En los años sesenta se desarrollaron los campos de la genética médica, la citogenética médica y la dismorfología (el estudio del desarrollo físico anormal).
Los años setenta fueron testigos del establecimiento de las técnicas necesarias para el diagnóstico prenatal de los trastornos genéticos. 
A finales de la década de los setenta surgió la discusión acerca de la formación de un American Board of Medical Genetics y en 1981 se concedió el primer examen de certificación.
Diez años más tarde, el American Board of Medical Specialties reconoció esta nueva especialidad. 

Mientras que la genética médica es el estudio de la naturaleza hereditaria de la enfermedad humana, la genética clínica trata del cuidado clínico directo de las personas con enfermedades genéticas.
Los temas relativos a diagnóstico, consejo y tratamiento que rodean a la enfermedad genética constituyen los ejes centrales de la genética clínica.

En este capítulo se resumen los principios de la genética clínica y el proceso de consejo genético. 
Además, se proporciona un resumen de la dismorfología, ya que el crecimiento de este campo ha influido y se ha desarrollado de forma paralela a la genética clínica.

PRINCIPIOS Y PRÁCTICA DE LA GENÉTICA CLÍNICA

Como se ha mencionado en el capítulo 1, las enfermedades genéticas forman un grupo común y son una causa significativa de la mortalidad y morbididad humanas.

De forma característica.
Los trastornos genéticos son complejos, multiorgánicos y sistémicos.
En el cuidado de las personas que padecen estos trastornos intervienen a menudo múltiples especialidades.
Por esta razón, los trastornos genéticos forman parte del diagnóstico diferencial de la mayoría de presentaciones sintomáticos y clínicas.
Por ejemplo, cuando se valora a un lactante con enfermedad cutánea vesiculosa, dentro de las cualidades del médico deben figurar la capacidad para distinguir entre alguna de las muchas formas de la epidermólisis ampollosa (un trastorno hereditario de los queratinocitos en que se desarrollan vesículas cutáneas después de un traumatismo leve) y la enfermedad cutánea estafilocócica. 

Debido a la complejidad y al número de enfermedades genéticas humanas, su diagnóstico clínico y tratamiento pueden parecer abrumadores.
Para facilitar el manejo de esta información, proporcionamos un resumen de los conceptos más importantes.
Entre ellos se incluyen la importancia de un diagnóstico preciso, la aplicación de los principios de la genética médica a la práctica médica y la función que tiene el consejo genético en el cuidado de las personas con enfermedad genética.

Diagnóstico preciso

Nunca se insistirá bastante en la importancia de este principio médico básico.

El proceso de consejo genético, uno de los servicios fundamentales de la genética médica comienza con un diagnóstico correcto.
Todas las discusiones sobre historia natural, pronóstico, tratamiento, determinación del riesgo, opciones de diagnóstico prenatal y consulta con grupos de apoyo dependen de un diagnóstico preciso. 
Por ejemplo, el consejo genético a una familia que ha tenido un hijo con retraso mental suele tratar de cuestiones relativas al riesgo de este trastorno en la futura descendencia. 
La respuesta precisa requiere que el médico identifique un trastorno de etiología conocida.
Si se establece un diagnóstico específico (p. ej., síndrome del cromosoma X frágil), puede comenzar el resto del proceso de consejo genético: puede comunicarse la información actual e iniciarse el tratamiento (v. comentario clínico xxx para una discusión adicional de este punto).

En genética clínica, como en todos los campos de la medicina, el diagnóstico preciso es la primera etapa más importante en el cuidado del paciente. 

El proceso diagnóstico de un trastorno genético es una secuencia compleja de episodios.
Depende de la toma de decisiones diagnósticas y de las pruebas bioquímicas, de la dismorfología, diagnóstico de laboratorio y de los principios básicos de la genética médica. 
En las enfermedades que tienen criterios diagnósticos establecidos, el médico dispone de unas normas para emitir el diagnóstico.
Un ejemplo de estos criterios serían los recomendados por el National Institute of Health Consensus Development Conference para el diagnóstico de la neurofibromatosis de tipo 1 (NF1, v. cap.

4).
El procedimiento diagnóstico de los trastornos definidos por un marcador especifico de laboratorio, como un cariotipo o un análisis bioquímico anormal, es en general directo.
Sin embargo, gran número de enfermedades genéticas no tienen criterios establecidos y su definición y delimitación no es concluyente. 

Los síndromes dismórficos requieren el conocimiento y la habilidad necesarios para la identificación de las malformaciones leves, anomalías menores y variaciones fenotípicas.
El diagnóstico de otras enfermedades genéticas puede requerir experiencia en una diversidad de disciplinas.
Por ejemplo, el diagnóstico de alguna de las formas de retinitis pigmentaria (v, cap, 7) requiere la intervención de un oftalmólogo familiarizado con este grupo de trastornos degenerativos de la retina.
El proceso diagnóstico se complica aún más por la expresión variable, la penetrancia incompleta y la heterogeneidad de muchas enfermedades genéticas.
Estos conceptos se estudiaron en el capítulo 4.

Comentario clínico l2.l. Razones para establecer el diagnóstico de un síndrome 

La larga lista de síndromes asociados con malformaciones congénitas es abrumadora para el médico.
En los Smith's Recognizable Patterns of Human Malformations se enumeran más de 1.000 trastornos y más de 1.000 son accesibles mediante de la base de datos informatizada POSSUM.
Este número da la sensación de que el diagnóstico de un síndrome de malformación se sitúa en el marco de «trivialidades académicas».
Sin embargo, esta sensación no es correcta.

Por ejemplo consideremos un niño grande para su edad gestacional y con cierto número de anomalías físicas: onfalocele (protrusión intestinal en el ombligo), lengua grande, hemangioma facial, masa en el flanco y longitud asimétrica de las extremidades (figura 12-1). 
Su familia plantea interrogantes como ¿qué enfermedad tiene?,
¿qué le pasará?, 
¿tendrá un aspecto diferente?,
¿tendrá retraso mental?
Y ¿cuál es la posibilidad de que este trastorno ocurra de nuevo?

Juntando todas estas manifestaciones y efectuando el diagnóstico, por identificación del cuadro clínico, de síndrome de Wiedemann-Beckwith, el médico es capaz de dar una respuesta bastante precisa a todas las preguntas sobre el paciente.
El síndrome de Wiedemann-Beckwith es un trastorno autosómico recesivo con expresión variable y penetrancia incompleta. 
Además, se cree que el gen exhibe la huella genética. 
Sin embargo, en caso de ausencia de antecedentes familiares, el riesgo de recurrencia en hermanos es inferior al 5%.
En caso de antecedentes familiares el riesgo de recurrencia es muy superior y el análisis de ligamiento puede proporcionar un cálculo del riesgo más preciso.
En los embarazos posteriores se puede llevar a cabo el diagnóstico prenatal con ecografía para descartar onfalocele en el segundo trimestre.
Así como el tamaño grande para la edad gestacional, exceso de líquido amniótico (polihidramnios) y lengua grande.
Si se cree que un feto tiene el síndrome de Wiedemann-Beckwith se debe modificar la planificación del parto y procurar que el nacimiento del bebé se produzca en un centro de asistencia terciario. 

Los niños con síndrome de Wiedemann-Beckwith no suelen presentar retraso mental.

Aunque la lengua grande puede provocar problemas de ortodoncia. 
Dificultades del lenguaje y, en algunas ocasiones, problemas de las vías respiratorias superiores estos trastornos suelen mejorar con la edad.
El aspecto facial no es tan sorprendentemente anormal a finales de la infancia.

Es probable que se efectúe un estudio cromosómico, aunque la mayoría de los pacientes con síndrome de Wiedemann-Beckwith no presentan la anomalía del cromosoma 11 descrita en un pequeño número de casos.

Asimismo, el plan de cuidados médicos incluye de modo prioritario ecografías periódicas para descartar la presencia de neoplasias malignas intraabdominales, especialmente el tumor de Wilms y hepatoblastomas.
Los niños con síndrome de Wiedemann-Beckwith tienen un riesgo del 5 al 10 % de desarrollar estos tumores y ambos tipos de neoplasias tienen tratamiento si se detectan precozmente.

En este ejemplo resultó fundamental la obtención del diagnóstico del síndrome de Wiedemann-Beckwith. 
El diagnóstico correcto proporcionó la información precisa para el consejo genético la predicción de la historia natural (incluyendo tranquilización), la organización de los estudios apropiados de laboratorio, el plan de mantenimiento de la salud y para la consulta con un grupo de apoyo.
El diagnóstico fue útil para los padres, para el médico de familia y para el niño. 

Figura 12.1

Niño con síndrome de Wiedermann-Beckwith

Obsérvense los ojos prominentes y la lengua grande y prominente.

Aplicación de los principios de la genética médica

El desarrollo de un enfoque genético de la enfermedad humana en el marco clínico requiere la aplicación de todos los principios básicos de la genética médica descritos en este libro.
Por ejemplo, para establecer o descartar el diagnóstico de una N'F1 es necesario conocer la variabilidad clínica y la edad de presentación de ciertos signos de la enfermedad (para un estudio adicional, véase el comentario clínico 12-2).
También es importante la identificación de las diversas formas de la neurofibromatosis (es decir, la identificación de la heterogeneidad).

El conocimiento de los otros principios formales de la genética médica también es necesario en el cuidado de las personas con trastornos genéticos.
La acumulación de datos sobre el historial de la familia y la interpretación de la información que proporciona el árbol genealógico son importantes para responder a las cuestiones relativas al riesgo de la familia.
La comprensión de los diferentes modos de herencia es importante en cualquier explicación del riesgo de recurrencia.
La discusión de los conceptos de nueva mutación y pleiotropía es común al revisar con una familia la etiología y la patogénesis de una enfermedad genética.
Incluso una comprensión de la meiosis es necesaria para las discusiones de la etiología con la familia de un recién nacido con síndrome de Down.

Consejo genético

Definición y principios

El consejo genético representa uno de los ejes centrales de la genética médica. 
A primera vista, el uso del término consejo implica que este servicio se sitúe en el terreno de la salud mental, asistencia social o psicoterapia. 
De hecho, el consejo genético se fundamenta en el modelo médico convencional ya que depende de forma significativa de un diagnóstico preciso y del conocimiento de la genética médica.
Tradicionalmente, el consejo genético se ha desarrollado dentro del campo de la genética humana, en vez de formar parte de las ciencias de la conducta, a diferencia de otras disciplinas de consejo. 

En 1975, la American Society of Human Genetics formuló una definición del consejo genético que ha resistido el paso del tiempo:

El consejo genético es un proceso de comunicación que trata de los problemas humanos asociados con la ocurrencia, o riesgo de ocurrencia, de un trastorno genético en una familia.
Este proceso requiere que una o más personas adiestradas de forma apropiada ayuden al individuo o familia a:

1 ) comprender los hechos médicos, incluyendo el diagnóstico, la evolución probable del trastorno y el tratamiento disponible,
2) apreciar el modo en que la herencia contribuye al trastorno y al riesgo de recurrencia en parientes especificados;
3)comprender las alternativas para tratar con el riesgo de recurrencia;
4)escoger un curso de acción que les parezca apropiado a la vista de su riesgo, sus objetivos familiares y sus normas éticas y religiosas, y actuar de acuerdo con la decisión tomada, y
5) procurar la mejor adaptación posible a la enfermedad en un miembro afectado de la familia y/o al riesgo de esta enfermedad.

Esta definición ilustra las complejas tareas que afronta el médico.
La primera tarea implica el establecimiento del diagnóstico y la discusión de la historia natural y el tratamiento del trastorno.
A este respecto, el tratamiento de una enfermedad genética no difiere del tratamiento de cualquier otro tipo de enfermedad. 

La segunda tarea requiere una comprensión de los principios básicos de la genética médica, en especial de la determinación del riesgo.
Para estimar la recurrencia de los trastornos cromosómicos y multifactoriales se utilizan los riesgos empíricos. 
Los patrones de segregación se emplean para predecir el riesgo de recurrencia de los trastornos mendelianos.
Sin embargo, a menudo los temas médicos se complican a causa de una penetrancia reducida, una expresión variable, una edad tardía de presentación y la heterogeneidad alélica y de loci .
En ciertos casos, la incorporación de información adicional utilizando el método de probabilidad bayesiana puede modificar de forma significativa las estimaciones del riesgo (comentario clínico 12-3., página 237.

237).

Los objetivos tercero y cuarto del proceso de consejo genético subrayan las principales diferencias que existen entre el modelo genético y el método biomédico tradicional. 
Estas tareas exigen la discusión de las opciones de reproducción y la facilitación de la toma de decisiones.
Implícita en la cuarta parte de la definición se encuentra la noción de respeto por la autonomía de la familia y por sus percepciones acerca del riesgo y de la enfermedad.
Este enfoque se ha denominado no directivo : el consejero deja que la familia tome todas las decisiones acerca de una futura reproducción. 
Esta aproximación difiere en alguna medida del enfoque médico más tradicional en las que se efectuaban recomendaciones relativas al tratamiento o la intervención.

Se trata de un tema importante puesto que este método no directivo entra, en algunas ocasiones, en conflicto con la visión más amplia de la medicina preventiva, que sugiere que el principal objetivo del consejo genético sería la reducción de la incidencia de las enfermedades genéticas.
Si la prevención o la reducción de la ocurrencia es el principal objetivo, entonces el enfoque debería ser más directivo.
Sin embargo, el principal objetivo del consejo genético es ayudar a las familias a comprender y afrontar la enfermedad genética, no a reducir la incidencia de la enfermedad genética.

Aunque la mayoría de genetistas suscriben los principios de autonomía y método no directivo, en algunas ocasiones puede ser imposible comportarse de una forma no directiva por completo, simplemente debido a las opciones limitadas que el consejero tiene en una breve sesión.
Además, la información puede presentarse de modo distinto en contextos diferentes.
Por ejemplo, la información sobre el síndrome de Down puede comunicarse de una forma si el diagnóstico se realiza prenatalmente o de otra forma si el diagnóstico se establece después del nacimiento de un neonato afectado(comentario clínico 12-4, pág. 239).

La mayoría de los genetistas clínicos suscriben el principio del método no directivo:
la información acerca de riesgos, historia natural, tratamiento y pronóstico se presentan de una manera equilibrada y neutral, pero las decisiones acerca de la reproducción se dejan a la familia.

La facilitación de la discusión sobre la toma de decisiones sobre la reproducción es fundamental para el consejo genético.
Varios factores intervienen en la decisión de la familia acerca de futuros embarazos cuando existe un aumento del riesgo.
Los más evidentes son la magnitud de La cifra del riesgo y la carga o el impacto de la enfermedad.
Sin embargo, estos temas no son los únicos importantes.
La percepción que una familia tiene del impacto del trastorno es probablemente más importante en su toma de decisión que la percepción que tenga el profesional.
El significado de los hilos para una familia, según sus propias preferencias religiosas, y personales, pesa mucho en el proceso de toma de decisión sobre la reproducción.
Además, a menudo las familias se plantean tener que afrontar la recurrencia del trastorno en otro hijo.
La identificación de estos temas ayuda a la familia.
A menudo a estimular sus propias discusiones.
Algunas familias perciben el riesgo en forma cualitativa en vez de cuantitativa: se consideran con riesgo o no, siendo la estimación real del riesgo una consideración secundaria.
El hecho de que exista una variación tan grande en la importancia que las personas asignan a cada uno de estos factores (percepción del riesgo, percepción del impacto, significado de los hijos y posibilidad de recurrencia) subraya de nuevo que el profesional debe facilitar la decisión y no ser quien la tome.

Comentario clínico 12-2., Historial familiar negativo

Una de las discusiones más frecuentes en las visitas hospitalarias es la anotación de que los antecedentes familiares de una persona son «negativos, o no «cooperantes». 
A menudo, ello basta para descartar un trastorno genético. 

Sin embargo, la mayoría de individuos con una enfermedad genética no tienen un historial familiar «positivo».
Una rápida revisión de los mecanismos de herencia mendelianos, cromosómicos y multifactoriales muestra que la ausencia de otras personas afectadas en la familia es frecuente y no descarta en modo alguno la presencia de enfermedad genética.
Por ejemplo, el riesgo de recurrencia en hermanos es del 25 % para las enfermedades con herencia autosómica recesiva.
Por lo tanto, un número significativo de familias con múltiples descendientes sólo tendrán un niño afectado y sin antecedentes familiares.
Incluso algunos trastornos autosómicos dominantes bien establecidos pueden presentar a menudo un historial familiar negativo a causa de las elevadas proporciones de nuevas mutaciones [entre los ejemplos se incluyen el síndrome de Marfan, la neurofibromatosis de tipo 1 (NF1) y la acondroplasia, en los que las proporciones de casos provocados por nuevas mutaciones son del 30, 50, y 80 %, respectivamente].
Los síndromes cromosómicos suelen tener bajo riesgo de recurrencia. 
Aun cuando un progenitor sea portador de una redistribución cromosómica equilibrada, el riesgo de recurrencia entre la descendencia suele ser inferior al 15 %.
Los riesgos de recurrencia en hermanos, en el caso de trastornos multifactoriales, suele ser del 5 % o menos.

Caso

Una familia acude con un niño de 6 años de edad que presenta 10 manchas café-au-lait de diámetro superior a 0.5 cm y un glioma óptico (fig. 12-2).
La familia formula preguntas acerca del diagnóstico y el riesgo de recurrencia en embarazos futuros.
Mediante un Contacto telefónico inicial se averigua que no existen antecedentes familiares de manifestaciones similares.

Existen varias explicaciones posibles a este hallazgo.

Su exploración subraya las implicaciones de un historial familiar negativo:

1. Nueva mutación del gen de NF1.
Debido a la proporción relativamente elevada de nuevas mutaciones de este trastorno, esta explicación es la más probable.

2. Expresión variable.
También es posible que uno de los progenitores sea portador del gen con expresión leve.
De forma ocasional, un progenitor tiene múltiples manchas caféau-lait y algunos neurofibromas, pero nunca ha sido diagnosticado de NF1.

3. Penetrancia incompleta.
Se trata de una posibilidad; sin embargo, no es probable en el caso de la NF1, cuya penetrancia se aproxima al 100 %.
Si una familia tiene dos hijos con NF1 y ninguno de los progenitores tiene el gen, la existencia de un mosaicismo de la línea germinal seria la explicación más probable.

4. Diagnostico incorrecto.
Uno de los supuestos y principios básicos de la genética médica es un diagnóstico preciso.
Este paciente cumple los criterios NIH establecidos para la NF1 (v. cap. 4).
Sin embargo, si este individuo presentara sólo manchas café-au-lait , el diagnóstico sería objeto de discusión.
Sería necesario conocer el diagnóstico diferencial de las manchas café-au-lait múltiples.

5. Falsa paternidad.
Aunque relativamente poco frecuente debe tenerse en cuenta esta posibilidad. 


Comenzamos con un individuo que presenta un trastorno autosómico dominante clásico sin antecedentes familiares. 
Esta ausencia de historial familiar puede tener diversas explicaciones.
La declaración de que existe un «historial familiar negativo» no debe considerarse como prueba concluyente contra la presencia de un trastorno hereditario. 

Figura 12-2

Niño de 6 años de edad con múltiples manchas café-au-lait y pecosidad axilar

Comentario clínico 12-2.
Historial familiar negativo (Cont.,).

Comentario clínico 12-3., Riesgos de recurrencia y teorema de Bayes

El cálculo de los riesgos de recurrencias se describió en los capítulos 4 y 5.
Un ejemplo típico de estimación del riesgo de recurrencia seria el caso de un hombre afectado con hemofilia A, un trastorno recesivo ligado al sexo, que tiene una hila (individuo II-1 en el árbol genealógico de la derecha).
Puesto que el hombre sólo puede transmitir el cromosoma X portador de la mutación de hemofilia A a su hija, ésta será una portadora.
La hija de la portadora, individuo III-6, tiene una probabilidad del 50% de recibir el cromosoma X con la mutación y, por lo tanto, de ser portadora ella misma. 
Aun cuando la hija de la generación III tenga cinco hermanos normales, su riesgo continúa siendo del 50% ya que sabemos que la madre de la generación 11 es una portadora.

Supongamos ahora que la mujer de la generación III tiene tres hijos (generación IV) y ninguno de ellos con hemofilia A.
De forma intuitiva, podríamos sospechar que, después de todo, no es una portadora.
¿Cómo podemos incorporar esta nueva información en nuestra estimación del riesgo de recurrencia?

Un principio estadístico que nos permite utilizar esta información es el denominado teorema de Bayes (la aplicación del teorema de Bayes es denominada a menudo como análisis bayesiano o inferencia bayesiana). 
La tabla resume las etapas básicas que intervienen en el análisis bayesiano.

Comenzamos con la probabilidad previa de que la mujer de la generación III sea una portadora. 

Como su nombre sugiere, la probabilidad previa indica la probabilidad de que sea una portadora antes de que expliquemos que ha tenido tres hijos varones normales. 
Puesto a que sabemos que su madre es una portadora.
Su probabilidad previa debe ser de 1/2.
Entonces, su probabilidad previa de que no sea una portadora también es de 1/2.

Tabla 3-2

Es portadora.
No es portadora .


Probabilidad previa.


Probabilidad condicional.


Probabilidad conjunta.


Probabilidad posterior.


A continuación consideramos a sus tres hijos normales, calculando la probabilidad de que los tres sean normales dado que ella es una portadora.
Ya que esta probabilidad está condicionada por su estado de portadora,se denomina probabilidad condicional .
Si es una portadora, la probabilidad condicional de que sus tres hijos sean normales es de (1/2)3 , o de 1/8 .
También calculamos la probabilidad de que sus tres hilos fueran normales si ella no fuese portadora.

Naturalmente, su probabilidad condicional es muy próxima a 1.

A continuación queremos averiguar la probabilidad de que la mujer sea una portadora y de que sea una portadora con tres hijos normales.
Para obtener la probabilidad de la coocurrencia de estos dos sucesos, multiplicamos la probabilidad previa por la probabilidad condicional para obtener la probabilidad conjunta (es decir, la probabilidad de que ambos sucesos ocurran juntos, un concepto descrito en el capítulo 4).
La probabilidad conjunta de que sea una portadora es de 1/2 x 1/8 = 1/16 .
De modo similar, la probabilidad conjunta de que no sea una portadora es de 1/2 x 1 = 1/2 .
Estas probabilidades conjuntas indican que la mujer tiene una probabilidad de no ser portadora 8 veces superior a la probabilidad de ser portadora.

La etapa final consiste en estandarizar las probabilidades conjuntas de modo que ambas probabilidades estudiadas (es decir, ser portadora en comparación con no serlo) sumen 1.
Para lograrlo, simplemente se divide la probabilidad conjunta de que la mujer sea portadora (1/16) por la suma de ambas probabilidades conjuntas (1/16 + 1/2) .
De este modo se obtiene una probabilidad posterior de 1/9 de que sea una portadora y de 8/9 de que no lo sea.
Obsérvese que este proceso de estandarización nos permite proporcionar una estimación del riesgo (1/9 u 11 %) mientras conserva la relación entre estado de portador y no portador indicado por las probabilidades conjuntas.

Una vez efectuado el análisis bayesiano, observamos que nuestra intuición se confirma: el hecho de que la mujer en cuestión tuviera tres hilos normales reduce de forma notable su riesgo de ser portadora, desde una estimación inicial del 50 % hasta una probabilidad final de sólo el 11%.

Antes de la llegada del diagnóstico genético mediante marcadores ligados o detección de mutaciones, el análisis bayesiano era a menudo el único modo de obtener una estimación del riesgo en una situación como la descrita.

En la actualidad, se intenta identificar directamente la mutación del factor VIII que provoca la hemofilia A en esta familia 0 si esto falla, se utilizan marcadores ligados. 
Se trata de un método mucho más directo y preciso para determinar el estado de portador.
Sin embargo, como se describe en el capitulo 11, no siempre es posible identificar la mutación responsable, sobre todo cuando el trastorno puede estar causado por gran número de mutaciones (como en el caso de la hemofilia A).
Además, el análisis de ligamiento no siempre es informativo.

Por lo tanto, el análisis bayesiano todavía es en algunas ocasiones un instrumento útil para precisar las estimaciones del riesgo.

La información adicional incorporada en el análisis bayesiano no se limita a la valoración del estado de salud de los parientes como se muestra en este ejemplo.

Otro tipo de información es un análisis bioquímico, como el nivel de actividad del factor V111, que podría contribuir a indicar el estado de portador.
Dado que en estas pruebas suele existir una solapación entre portadores y homocigotos normales, el análisis no puede determinar con certeza el estado de portador, pero nos proporciona una estimación de probabilidad para su incorporación al análisis bayesiano.
En enfermedades con edad tardía de presentación, como la nefropatía poliquística del adulto, la probabilidad de estar afectado a una edad determinada puede utilizarse en un análisis bayesiano.

En este caso se considera que la probabilidad de que el individuo con riesgo tenga el gen patológico disminuye cada vez más si permanece normal a una edad determinada. 

La tarea final del consejo genético consiste en ayudar a la familia afrontar la presencia del trastorno y/o de su riesgo de recurrencia.
Esta tarea es similar a la que presta el médico de familia al tratar con una enfermedad crónica y/o discapacidad.
Posiblemente lo único excepcional sea la percepción que la familia tiene sobre el significado de un trastorno genético. 
En muchos procesos adquiridos, como infecciones o accidentes. 
Se exterioriza el significado último de la enfermedad. 
En los trastornos genéticos, la enfermedad es más intrínseca para el individuo y su familia: por lo tanto provocan a menudo un completo dilema personal.
La confirmación del compromiso de las familias es vital, y probablemente es más efectivo que los intentos simplistas de quitar la culpa.
Los sentimientos de culpabilidad y vergüenza son naturales en esta situación y también es necesario reconocerlos.

El médico de familia desempeña un papel vital en el apoyo continuado a las familias que tienen uno de sus miembros afectado por una enfermedad genética.

Entre las estrategias de apoyo adicionales se incluyen la consulta de la familia con un grupo de apoyo genético, la distribución de información escrita sobre el trastorno, la consulta con especialistas en salud mental para consejo continuado y las frecuentes visitas de control que dispongan del tiempo necesario para la exposición de los sentimientos y los pensamientos.

El consejo genético incluye cinco temas: tratamiento médico, determinación del riesgo, opciones de riesgo, toma de decisiones sobre la reproducción y servicios de apoyo.

Numerosos estudios realizados en los últimos 20 años han intentado valorar la eficacia del consejo genético. 
La metodología de estos estudios es complicada y la valoración de los resultados depende de la interpretación que se haga del objetivo del consejo genético.
Sin embargo, pueden establecerse algunos puntos generales: Las familias tienden a recordar los riesgos de recurrencia relativamente bien.
Una carta enviada después de la visita mejora este recuerdo.

Las familias que perciben el estado de su descendencia como grave e impactante recuerdan mejor las cifras de riesgo. 
La mayoría de los estudios sugieren que el consejo genético es relativamente eficaz en proporcionar información acerca de los aspectos médicos y los riesgos genéticos de la enfermedad.
Es necesaria una investigación adicional sobre los temas que rodean la toma de decisión y el apoyo psicosocial.

Transmisión del consejo genético: consejeros genéticos y grupos de apoyo

Desde que el consejo genético apareció como disciplina independiente en la década de los setenta, se puso de manifiesto que la entrega de este servicio es compleja y requiere mucho tiempo.
No sólo se requiere que los genetistas tengan conocimientos suficientes de la mayoría de especialidades de la medicina, sino que también es necesario que faciliten la toma de decisiones y proporcionen apoyo psicológico.
Cuando se hizo evidente la necesidad de profesionales en genética que no sólo fueran médicos, se iniciaron programas de adiestramiento en consejo genético.
En la actualidad existen más de una docena de cursos con grado de diplomado en consejo genético. 
Los consejeros genéticos han pasado a formar parte integral de los servicios de genética médica.
Por todas estas razones se fundó una sociedad profesional, la National Society of Genetics Counselors y, más recientemente, un tribunal de diplomación y acreditación, la American Board of Genetic Counseling .
Aunque la gama de habilidades es amplia y las descripciones del trabajo varían en los diferentes centros médicos, los consejeros genéticos se han establecido como expertos en la determinación del riesgo, en la toma de decisiones sobre la reproducción y en apoyo psicosocial.

Cada vez resulta más evidente la importancia que tienen los grupos de apoyo para la asistencia a las familias con uno de sus miembros afectado por un trastorno genético. 
Estos grupos de voluntarios proporcionan a la familia el sentimiento de «compañero de viaje».


La cadencia del reloj biológico 

Los organismos se ajustan en su mayoría a ritmos cronométricos de 24 horas.
La genética nos ha revelado parentescos entre los relojes moleculares de la mosca del vinagre, el ratón y el hombre.

Hay que hacer un auténtico esfuerzo para no quedarse dormido a las siete de la tarde.
El apetito sentido a las quince desaparece a la hora de la cena.
Nos despertamos a las cuatro de la madrugada, sin posibilidad de seguir dormidos. 

Son situaciones que conocen muy bien quienes viajan de Nueva York a San Francisco.
En general, con motivo de unas vacaciones de una semana o por traslados de negocios, cuando el organismo se ha acostumbrado ya al nuevo horario es el momento de volver a casa y retomar la rutina.

En mi laboratorio, un grupo de moscas Drosophila viaja también de Nueva York a San Francisco, o al revés, aunque en vuelo simulado.
En vez de terminales de aeropuerto hay sendas incubadoras, del tamaño de un frigorífico, donde se lee, en una, «Nueva York» y, en la otra, «San Francisco».
Las luces de las incubadoras se encienden y se apagan en el momento en que sale o se pone el sol en esas ciudades. (Por razones de coherencia, hemos puesto el orto a las seis de la mañana y el ocaso a las seis de la tarde, en ambas urbes).
Y hemos fijado la temperatura de las incubadoras en 22°C.

Las moscas emprenden su viaje simulado en el interior de pequeños tubos de vidrio, ajustados dentro de bandejas que controlan sus movimientos con un haz de luz infrarroja.
Cada vez que una mosca atraviesa el haz, proyecta una sombra en un fototransmisor, conectado a su vez a un ordenador que registra la actividad.

El ir de Nueva York a San Francisco no implica ningún vuelo real de cinco horas para los insectos.
Nos limitamos a desconectar la bandeja de una incubadora, la trasladamos a la otra y conectamos.

Nos servimos del argot transcontinental para identificar y estudiar las funciones de varios genes que parecen ser los engranajes del reloj biológico que controla los ciclos de día y noche en una gama amplia de organismos.
Incluimos en éstos la mosca del vinagre, el ratón y el hombre.
La identificación de tales genes nos permite determinar las proteínas que cifran, proteínas que podrían convertirse en diana del tratamiento de diversas afecciones, desde trastornos del sueño hasta depresiones estacionales.

El engranaje fundamental del reloj biológico se halla en el núcleo supraquiasmático ( NSQ ), un grupo de células nerviosas del hipotálamo. 
Cuando la luz estimula al amanecer la retina del ojo, ciertos nervios especializados envían señales al núcleo supraquiasmático, que a su vez controla el ciclo de producción de numerosas sustancias dotadas de actividad biológica.
El NSQ estimula la glándula pineal, una región cerebral vecina.
Según las instrucciones del NSQ , esta glándula produce con determinada cadencia melatonina, la hormona del sueño.

Con el avance del día, hasta el atardecer, la glándula pineal incrementa de forma gradual la síntesis de melatonina. 
Al subir los niveles sanguíneos de la hormona, se asiste a un descenso ligero de la temperatura corporal y a un aumento de la tendencia al sueño.

Figura 1

EL SOMETIMIENTO de moscas del vinagre a un salto simulado de horario ayuda a entender las bases moleculares de los relojes biológicos de los organismos, incluido el hombre

Las moscas se mantienen en tubos de vidrio pequeños ( fotografía a la izquierda )dispuestos en bandejas equipadas con sensores que registran la actividad de los insectos. (En la fotografía, Michael W., Young nos muestra una de éstas repleta de tubos).
Cuando una bandeja de una incubadora con la hora de Nueva York, donde es de noche a la 7:30 de la tarde, se traslada a otra incubadora que simula la hora de San Francisco, donde son las 4:30 y hay luz todavía, se registra una caída en picado de los niveles de proteínas clave del cerebro de los insectos. 

Figura 2

LA LUZ que incide en el ojo frena la síntesis de melatonina en la glándula pineal

Según parece, esa hormona ( recuadros ) interviene en la inducción del sueño. 
La señal para reducir la secreción de melatonina se transmite desde la retina, vía nervio óptico, al núcleo supraquiasmático( NSQ ).
La conexión del NSQ con la glándula pineal es indirecta.

EL RELOJ BIOLÓGICO CUESTIONES BÁSICAS

¿Dónde reside el reloj biológico? 
En los mamíferos el reloj principal que dicta la actividad del ciclo de día y noche, el ritmo circadiano, se halla en el núcleo supraquiasmático del cerebro. 
Pero hay células por todo el organismo que también presentan este tipo de actividad rítmica.

¿Qué es lo que dirige el reloj? 
En el interior de las células del núcleo supraquiasmático, hay genes rítmicos especializados que se activan o inactivan por las propias proteínas que determinan, en un bucle de realimentación que tiene una cadencia de 24 horas.

Por lo que concierne al reloj biológico del ciclo normal de 24 horas,¿depende de la luz y la oscuridad? 
No.
Los ritmos moleculares de la actividad de estos genes cronometradores son innatos y automantenidos. 
Persisten en ausencia de ciclos ambientales de día y noche.

¿Qué papel desempeña la luz en la regulación y reajuste del reloj biológico? 
La luz brillante que absorbe la retina durante el día ayuda a sincronizar los ritmos de actividad de los genes cronometradores con el ciclo ambiental vigente.
La exposición a una luz brillante durante la noche reajusta los ritmos circadianos cambiando rápidamente la cantidad de algunos productos de los genes vinculados con la función de reloj.

¿Cómo regula un reloj molecular la actividad día - noche de un individuo? 
Las proteínas fluctuantes sintetizadas por los genes cronometradores controlan vías genéticas adicionales que sincronizan el reloj molecular con cambios pautados de la fisiología y comportamiento.

El reloj humano

La luz, así parece, pone en hora cada día el reloj biológico.
No obstante, el ritmo circadiano, el cambio de día y noche, sigue funcionando incluso en ausencia de luz, prueba de que la actividad del NSQ es innata.
A comienzos de los años sesenta, el grupo de Jürgen Aschoff, del Instituto Max Planck de Fisiología del Comportamiento en Seewiesen, demostró que los voluntarios encerrados en un bunker experimental - sin luz natural, relojes ni otras pistas acerca del tiempo - mantenían un ciclo casi normal de sueño y vigilia de unas 25 horas.

Más recientemente, los grupos encabezados por Charles Czeisler y Richard E., Kronauer, de la Universidad de Harvard, han determinado que el ritmo circadiano humano es de 24,18 horas. 
Investigaron el comportamiento de 24 hombres y mujeres (11 de ellos tenían veintitantos años y 13 tenían sesenta años o algo más) que vivieron durante más de tres semanas en un recinto sin más clave horaria que un ciclo débil de luz y oscuridad artificialmente ajustado a 28 horas y que daba a estos individuos la señal para dormir.

Midieron la temperatura corporal de los participantes, que en condiciones normales desciende por la noche, así como la concentración sanguínea de melatonina y de cortisol, una hormona del estrés que mengua su concentración al atardecer.
Observaron que, aun cuando se les había prolongado el día en cuatro horas, la temperatura corporal y los niveles de melatonina y cortisol seguían funcionando de acuerdo con su propio reloj circadiano interno de 24 horas. 

Además, no parecía que la edad influyera en la cadencia del reloj.
Aunque los resultados obtenidos en estudios anteriores sugerían que la edad alteraba los ritmos circadiano, en la investigación de Harvard las fluctuaciones hormonales y de temperatura corporal de los individuos de más edad seguían siendo tan regulares como las del grupo de sujetos más jóvenes.

Por muy fructíferos que fueran los estudios del bunker , hubo que recurrir a las moscas del vinagre para investigar los genes responsables del reloj biológico. 

Las moscas son ideales para los estudios genéticos gracias a su vida corta y tamaño manejable, factores que posibilitan realizar cultivos e intercambiar miles de ellos en el laboratorio hasta que empiecen a aparecer en número creciente las mutaciones de interés.
Para acelerar el proceso de mutación, se exponen los insectos a agentes químicos inductores de la misma.

A principios de los setenta Ron Konopka y Seymour Benzer, del Instituto de Tecnología de California, daban cuenta de las primeras moscas mutantes que presentaban ritmos circadianos alterados.
Les administraron un agente mutágeno; registraron, luego, el movimiento de 2000 individuos de la progenie, sirviéndose en parte de un aparato similar al que empleamos ahora nosotros en los vuelos simulados.
La mayoría de las moscas tenían un ritmo circadiano normal:
persistían activas unas 12 horas del día, para descansar las doce siguientes.

Pero en tres de las moscas había mutaciones que alteraron esa pauta.
Una seguía un ciclo de 19 horas, otra un ciclo de 28 horas y la tercera carecía de ritmo circadiano, descansando y entrando en actividad de forma errática.

En 1986 mi grupo de trabajo de la Universidad de Rockefeller y el que dirige Jeffrey Hall en la Universidad de Brandeis, y Michael Rosbash en el Instituto Howard Hughes, también en Brandeis, hallaron que las tres moscas mutantes mostraban tres alteraciones diferentes en el gen period , o per ; un gen que habíamos aislado, cada grupo por separado, dos años antes.
Ante el fenómeno de que mutaciones diferentes en un mismo gen causaran tres comportamientos, pensamos que per se hallaba involucrado en la producción del ritmo circadiano y en la fijación de la cadencia de ese ritmo. 

Aislado el gen, cabía preguntarse si controlaba en solitario el ciclo de día y noche.
Para averiguarlo, Amita Sehgal y Jeffrey Price, becarios de mi laboratorio, examinaron más de 7000 moscas; buscaron si había más mutantes con el ritmo alterado.
Por fin dieron con una mosca que, al igual que una de las mutantes per , carecía de ritmo circadiano manifiesto.
La nueva mutante presentaba interesado el cromosoma 2, mientras que el per pertenecía al cromosoma X. 
Supimos entonces que teníamos un gen nuevo; lo llamamos timeless, o tim .

¿Qué relación guardaba el nuevo gen con per ?
Los genes constan de ADN , que porta las instrucciones de la síntesis de proteínas.
El ADN nunca abandona el núcleo de la célula; sus órdenes moleculares se leen ARNm mediante, que sí sale del núcleo y penetra en el citoplasma, donde se fabrican las proteínas.
Con los genes tim y per acometimos la síntesis in vitro de las proteínas PER y TIM .
En colaboración con Charles Weitz, de la Facultad de Medicina de Harvard, observamos que, si mezclábamos las dos proteínas, éstas se adherían entre sí; podrían, pues, interaccionar en el interior de la célula.

Tras una cohorte de experimentos, comprobamos que la producción de las proteínas PER y TIM implicaba un bucle de realimentación de tipo reloj.
Los genes per y tim permanecían activos hasta que las concentraciones de sus proteínas respectivas alcanzaban un nivel de concentración suficiente para que éstas se juntaran.
Formaban entonces complejos que penetraban en el núcleo y bloqueaban los genes correspondientes.
Pasadas unas horas, las enzimas degradaban los complejos, los genes tornaban a activarse y se iteraba de nuevo el ciclo.

RELOJES POR TODAS PARTES NO ESTÁN SÓLO EN EL CEREBRO

En su mayor parte, la investigación sobre los relojes biológicos se ha centrado en el cerebro.
Pero no es éste el único órgano que observa un ritmo de día y noche.

Jadwiga Giebultowicz, de la Universidad estatal de Oregón, ha identificado proteínas PER y TIM - componentes clave de los relojes biológicos - en los túbulos malpighianos de la mosca del vinagre, semejantes a los que vemos en el riñón.
También ha observado que las proteínas se sintetizan según un ciclo circadiano; aumentan de noche y decrecen de día.
El ciclo se mantiene incluso en moscas decapitadas, lo que demuestra que las células malpighianas no responden sólo a señales procedentes del cerebro del insecto.

El grupo de investigación de Steve Kay, del Instituto Scripps de Investigación en La Jolla, ha obtenido, además, pruebas de la existencia de relojes biológicos en las alas, patas, partes bucales y antenas de la mosca del vinagre. 
Al transferir genes que regulan la síntesis de proteínas PER fluorescentes a moscas vivas, el grupo de Kay ha puesto de manifiesto que cada tejido lleva un reloj fotorreceptor, independiente.
Los relojes siguen funcionando y responden a la luz cuando se extraen los tejidos del insecto para su análisis.

Los relojes biológicos extracraneales no son algo privativo de las moscas del vinagre.
Ueli Schibler, de la Universidad de Ginebra, demostró en 1998 que operan también según un ciclo circadiano los genes per de fibroblastos de rata, células del tejido conjuntivo. 

Ante la diversidad de tipos celulares que presentan actividad de reloj circadiano, cabe inferir que, en muchos tejidos, la sincronización reviste interés suficiente para garantizar su seguimiento local.
Estos hallazgos podrían aportar un significado nuevo a la expresión «reloj biológico».

Figura 3

EN LA CABEZA de la mosca del vinagre hay varios relojes biológicos

Las células extraídas de las partes bucales y antenas reflejan la misma respuesta ante ciclos de luz y oscuridad que otras células pertenecientes al cerebro.

UN DÍA EN EL RELOJ BIOLÓGICO DE UNA MOSCA EN SUS CÉLULAS CEREBRALES HALLAMOS RELOJES MUSCULARES EMPARENTADOS CON LOS DEL HOMBRE

Si exponemos a la luz una mosca, comienzan a degradarse los complejos moleculares constituidos por proteínas PER y TIM en las células del cerebro. 
Los completos PER / TIM forman parte de un bucle de realimentación que controla la actividad de per y tim, genes que contienen las instrucciones para fabricar las proteínas PER y TIM.

Hacia mediodía, se han degradado ya todas las proteínas PER y TIM.
Otras dos, las proteínas CYCLE y CLOCK, se adhieren entre sí y forman completos que se unen a los genes per y tim para activarlos.
Cuando estos genes entran en funcionamiento, se transcriben en ARN mensajeros, intermediarios genéticos que pasan al citoplasma.

Una vez en el citoplasma, los ARN mensajeros de per y tim se adosan a los ribosomas.
Estos orgánulos empiezan a «leer», es decir, a traducir la información de los ARN mensajeros para crear las cadenas correspondientes de aminoácidos.
Tales cadenas se pliegan luego en proteínas PER y TIM ya maduras que se unen entre sí y producen los complejos PER /TIM al anochecer. 

Durante la noche, los nuevos complejos PER / TIM se dirigen al núcleo; aquí bloquean la actividad de CYCLE y CLOCK , silenciando su producción.
Cuando amanece al día siguiente, los complejos PER / TIM se degradan.
Vuelve a reanudarse el ciclo.

El movimiento de las manecillas del reloj

Una vez que hubimos encontrado dos genes que operaban concertados para crear un reloj molecular, nos planteamos conocer de que modo arrancaba y se ponía en hora.
Después de todo, nuestros ciclos de sueño y vigilia se adaptan muy bien a los viajes entre distintos husos horarios, aunque el reajuste quizá requiera un par de días o una semana.

Eso sucede cuando comenzamos a trasladar las bandejas de moscas de «Nueva York» a «San Francisco». 
Entre las primeras cosas que observamos fue que, si pasábamos una mosca de una incubadora oscura a otra brillantemente iluminada para imitar la luz del día, las proteínas TIM desaparecían del cerebro de la mosca en cuestión de minutos.

Y lo que revestía mayor interés:
la dirección en que «viajaban» las moscas afectaba a los niveles de sus proteínas TIM.
Si sacábamos a las moscas de «Nueva York» a las 8 de la tarde, cuando ya había oscurecido, y las colocábamos en «San Francisco», donde todavía había luz a las 5 de la tarde, los niveles de TIM se hundían.
Pero una hora más tarde, cuando las luces se apagaban en «San Francisco», la proteína TIM empezaba a acumularse de nuevo.

Era evidente que, con la transferencia, los relojes moleculares de las moscas se paraban al principio, para reanudar, tras una demora, la cadencia de acuerdo con el nuevo huso horario.

Por contra, las moscas trasladadas a las 4 de la mañana desde «San Francisco» experimentaban una prematura salida del sol en «Nueva York», donde eran las 7 de la mañana.
Este viaje provocaba también la caída de los niveles de TIM , pero ahora la proteína no comenzaba a acumularse porque se había adelantado el reloj molecular con el cambio de huso horario.

El examen de la evolución de la síntesis del ARN correspondiente al gen tim nos enseñó nuevas cosas sobre el mecanismo subyacente bajo las diferentes respuestas moleculares.
Los niveles de este ácido nucleico son más altos a las 8 de la tarde y más bajos entre las 6 y las 8 de la mañana.
Una mosca trasladada a las 8 de la tarde de «Nueva York» a «San Francisco» produce niveles máximos de ARN de tim ; por eso, la proteína perdida por la exposición a la luz en «San Francisco» se reemplaza fácilmente tras el ocaso en su nuevo destino.
Pero la mosca que viaje a las 4 de la mañana de «San Francisco» a «Nueva York», fabricaría muy poco ARN de tim antes de la partida.
Lo que la mosca experimenta como una temprana salida del sol provoca la eliminación de TIM y permite que el ciclo siguiente de producción empiece con un horario adelantado.

CAMBIOS EN EL ORGANISMO EN EL PERIODO DE 24 HORAS

- Es la hora más probable de que las embarazadas alumbren
- Alcanzan su apogeo los linfocitos T coadyuvantes, células del sistema inmunitario

- Ascienden a su cota máxima los niveles de hormona del crecimiento

- Es el momento proclive para los ataques de asma

- Momento más probable del inicio de la menstruación
- Los niveles sanguíneos de insulina están en su mínimo
- Comienza la subida de la presión sanguínea y se aviva el ritmo cardíaco
- Aumentan los niveles de cortisol, una hormona del estrés 
- Empiezan a descender los niveles de melatonina

- El peor momento de los síntomas de la fiebre del heno

- Riesgo máximo de un ataque al corazón o un derrame cerebral
- El peor momento para la artritis reumatoide
- Los linfocitos T coadyuvantes están en su nivel más bajo del día

- Alcanza su máximo el nivel de hemoglobina en sangre

- Apogeo de la fuerza prensil, el ritmo respiratorio y la sensibilidad de los reflejos

- Punto máximo de la temperatura corporal, ritmo cardíaco y presión sanguínea 

- El flujo urinario está en su máximo

- El umbral del dolor es mínimo 

- Probabilidad máxima de la respuesta alérgica


REGISTRO DE LOS RITMOS CIRCADIANOS DE LAS MOSCAS MONTAJE EXPERIMENTAL PARA SEGUIR LA ACTIVIDAD VOLADORA DE LA MOSCA

Para descubrir qué genes intervenían en el ciclo de actividad noche - día de las moscas del vinagre, se las expuso a substancias inductoras de mutaciones génicas, que se reflejarían en su progenie.
Cuando los descendientes llegan a la madurez, se les encierra en tubos de vidrio.
Los tubos, individuales, tienen en un extremo el alimento para las moscas y, en el otro, un algodón que permite el paso del aire.

Se colocan los tubos en bandejas especiales, equipadas con luces de rayos infrarrojos y detectores.
La mosca del vinagre, por norma, descansa durante la noche y está activa durante el día.
Las bandejas, conectadas a un ordenador, registran cada movimiento de la mosca al señalar cuántas veces atraviesa un haz de rayos infrarrojos.
El análisis de miles de moscas mutantes con este sistema nos descubre que algunas presentan anomalías en los ritmos circadianos.

No sólo los insectos

El estudio de estos saltos horarios en las moscas nos ha ayudado a entender los ritmos circadianos de los mamíferos, hombre incluido.
En 1997, Cheng Chi Lee, de la Facultad de Medicina de Baylor, por un lado, y, por otro, el grupo encabezado por Hajime Tei, de la Universidad de Tokio, e Hitoshi Okamura, de la Universidad de Kobe, aislaron en el ratón y en el hombre equivalentes del gen per .
La investigación subsiguiente, que contó con la implicación de numerosos laboratorios, condujo al aislamiento en 1998 de genes tim humanos y de ratón.

Los genes demostraron su actividad en el núcleo supraquiasmático.

Los estudios con ratones contribuyeron también a la solución de una cuestión clave.
¿Qué es lo que insta, en primera instancia, la actividad de los genes per y tim?
En 1997 el grupo de Joseph Tatahashi, del Instituto Howard Hughes de la Universidad Northwestern, aisló el gen Clock , así lo llamaron, cuya versión mutada producía ratones sin un ritmo circadiano definido.
El gen cifra un factor de transcripción, una proteína que en este caso se unía al ADN y permitía que se leyera transcrito en ARN mensajero.

No tardó en aislarse, en la mosca, una versión del gen Clock del ratón.
Varios equipos empezaron a introducir combinaciones de genes per , tim y Clock en células de mamíferos y de moscas del vinagre.
Tales experimentos revelaron que la proteína CLOCK tenía por diana el gen per en el ratón; y los genes per y tim, en la mosca.
El sistema cerraba un círculo completo:
en las moscas, cuyos relojes son los mejor conocidos, la proteína CLOCK - en combinación con la proteína codificada por el gen cycle - se une a los genes per y tim para activarlos, pero sólo si no están presentes en el núcleo las proteínas PER y TIM.
Los cuatro genes con sus proteínas constituyen el corazón del reloj biológico en las moscas.
Con algunas modificaciones forman el mecanismo responsable de los ritmos circadianos en el reino animal, de los peces a las ranas, de los ratones al hombre.

El grupo de Steve Reppert, en Harvard, y Justin Blau, en mi laboratorio, acaban de abordar las señales específicas que sincronizan los relojes biológicos con los momentos de diversas conductas, con las fluctuaciones hormonales y con otras funciones.
Todo indica que algunos genes de interés se ponen en marcha mediante la interacción directa con la proteína CLOCK .
Por su parte, PER y TIM atenazan la capacidad de CLOCK de activar dichos genes, al propio tiempo que producen las oscilaciones del bucle central de realimentación, instaurando pautas generales de actividad génica cíclica. 

Se nos avecina un futuro en el que se obtendrá, en la mosca del vinagre y el ratón, un sistema entero de genes regulados por su reloj biológico.
Con toda verosimilitud, se descubrirán, en el seno de esas redes, productos sin caracterizar todavía que ejercen efectos llamativos sobre el comportamiento.
Tal vez uno de esos productos, si no un componente del propio reloj molecular, sea un blanco idóneo para los fármacos que alivien los trastornos provocados por los cambios horarios, los efectos secundarios de los turnos de trabajo, las alteraciones del sueño y las depresiones que les acompañan.


Vacunas contra el sida

¿Dispondremos pronto de alguna que sea eficaz contra el virus? Nadie se imaginaba que las cosas iban a ser tan difíciles. 
Cuando el VIH, virus responsable del sida, se identificó en 1984, Margaret M. Heckler se hallaba al frente del Departamento de Salud y Asistencia Social de los Estados Unidos.
En su ingenuidad, predijo que habría una vacuna en el plazo de dos años.

Han pasado veinte años desde que irrumpió la pandemia.
Suman ya, repartidos por todo el planeta, 40 millones los afectados.
A tres millones de fallecidos se elevó el recuento del año pasado.
Si bien se dispone ya de varias vacunas potenciales contra el sida en diferentes etapas de ensayo clínico, por el momento ninguna parece responder a la promesa primera.
Una y otra vez, los investigadores han venido cosechando resultados que se creían esperanzadores, para terminar dándose de bruces con el muro infranqueable de la realidad.
Dos años atrás, era todavía frecuente oírles confesar en privado que ellos no verían el día en que se lograra una vacuna, siquiera aportase una protección parcial.

Cierto es que en los meses transcurridos no se ha conseguido un avance decisivo.
Pero han aparecido algunos datos que permiten mantener viva la esperanza de que pronto se abra un camino a los empeñados en hallar la vacuna contra el sida. 
Nos hallamos ante un momento interesante para la investigación, si nos hacemos eco de la declaración de Gregg Gonsalves, director de Tratamiento y Prevención de la Sociedad Crisis de Salud de los Homosexuales de la Ciudad de Nueva York.
En su lenguaje metafórico, extraído del mundo teatral, nos encontraríamos en el segundo acto.

Los dramaturgos presentan, en el primer acto, los personajes y el cuadro espacio-temporal.
Dedican el segundo a profundizar en el núcleo de la situación teatralizada.
En el primer acto de la investigación sobre la vacuna contra el sida debutó el VIH, uno de los primeros retrovirus causantes de una enfermedad en humanos.
A diferencia de lo que ocurre con otros virus, los retrovirus introducen su material genético en el de las células del organismo que invaden; los genes víricos constituyen, desde entonces, parte permanente de las células infectadas y de la descendencia de las mismas.
Los retrovirus se reproducen con celeridad y astucia asombrosas, algo que les confiere una capacidad singular para experimentar mutaciones que permiten al VIH cambiar de identidad, burlando los fármacos antirretrovíricos y al sistema inmunitario.

El primer acto reveló también la respuesta inmunitaria del organismo ante el VIH.
Integran dicha respuesta anticuerpos (moléculas con forma de Y que se unen a los invasores, aquí víricos, y los marcan para su ulterior destrucción) y células T citotóxicas, o asesinas (leucocitos encargados de la destrucción de células infectadas de virus).

Tras la infección, el sistema inmunitario libra una lucha tenaz, durante años, contra el VIH.
Produce millones de nuevas células T citotóxicas contra los miles de millones de partículas víricas que se generan cada día en las células infectadas. 
Además, el sistema inmunitario despliega ejércitos de anticuerpos dirigidos contra el VIH, al menos en los primeros momentos de la infección.
Los anticuerpos, sin embargo, demuestran ser bastante ineficaces contra el enemigo. 

Se levanta el telón para el segundo acto.
El VIH sigue en escena.
Para finales del año en curso, deberían estar disponibles los resultados del primer ensayo a gran escala de una vacuna contra el sida.
Pero son muy pocos los expertos que se sienten optimistas; de un primer análisis somero se desprende la escasa eficacia de la prueba en cuestión.
Añádase a ello la controversia que rodea a otro ensayo gigantesco apoyada por el gobierno de los Estados Unidos; en él se someterá a prueba una vacuna potencial a principios de septiembre.
Se llevará a cabo en Tailandia.

Planean en vuelo otros enfoques que mantienen atenta a la comunidad científica.
Las estrategias seguidas reabren el debate sobre si una vacuna, para que sea útil, debe instar respuestas inmunitarias que eviten por completo que el VIH colonice las células de una persona o si una vacuna pudiera reputarse aceptable aun cuando no lograra ese objetivo en toda su plenitud.
Algunos le reconocen un valor potencial a las vacunas que despiertan los tipos de respuesta inmunitaria que operan en los inicios de la instauración celular del virus.
Al restringir la replicación vírica de una manera más eficaz que las respuestas naturales del organismo, tales vacunas, aducen, podrían al menos contribuir a prolongar la vida de las personas infectadas y retrasar la aparición de la fase sintomática del sida.

A principios del decenio de los noventa, creíase que la mejor estrategia para fabricar una vacuna contra el sida habría de partir del estudio de los individuos que albergaron el VIH durante un decenio, por lo menos, sin haber caído enfermos de sida.
Por desgracia, muchas de estas personas también sucumbieron a sus estragos al cabo de un tiempo. 
John P. Moore, de la Universidad de Cornell, atribuye esa longevidad relativa a la conjunción de un virus debilitado y a un sistema inmunitario potente.
En otras palabras, se habrían encontrado con una forma de VIH de crecimiento lento en un momento en que su organismo disponía de la munición suficiente para mantener las cosas bajo control. 

¿Sin rastro en la naturaleza? Durante años se han venido buscando los correlatos de la inmunidad para el VIH, vale decir, la combinación de respuestas inmunitarias que, una vez inducidas por una vacuna, protegerían al individuo contra la infección.
Max Essex, de la Universidad de Harvard, admite que tal empeño carece de fundamento en la naturaleza.
Por eso mismo, la búsqueda de una vacuna contra el sida debe avanzar a ciegas.

Para que sea útil una vacuna contra el sida tiene que superar con éxito tres etapas de ensayo en humanos. 
En la fase I, se administra la vacuna a decenas de personas con el fin de valorar su seguridad y establecer la dosis apropiada. 
En la fase II del ensayo la muestra abarca ya centenares de personas; presta especial atención a la inmunogenicidad, su capacidad para desencadenar una respuesta inmunitaria.
En la fase III, la vacuna potencial se aplica a millares de voluntarios que se someten a observación durante un largo período para comprobar si les protege de la infección. 
Las pruebas de la fase III para cualquier fármaco suelen ser muy costosas y difíciles de administrar; en el caso del sida, los ensayos presentan un reto especial por la exigencia de una condición paradójica: a los sujetos que reciben la vacuna se les insiste en que deben reducir las posibilidades de infección con el uso de preservativos o, si son drogadictos, de agujas esterilizadas, porque el VIH se transmite por contacto sexual o por vía sanguínea. 
Pero el estudio aportará resultados de interés sólo si algunas personas no siguieron el consejo y se expusieron a la enfermedad.

La primera vacuna potencial que ha llegado a la fase III contiene una proteína, la xxx , que emerge de la envoltura externa del VIH.
De ella se vale el virus para asirse a las células e infectarlas.
Por lo menos en teoría, la presencia de xxx en la circulación debería activar el sistema inmunitario del receptor, provocando un ataque inmediato contra la xxx en cuanto el virus VIH penetre en el organismo.

Esta vacuna, fabricada por VaxGen, división de Genentech, se halla en fase de pruebas.
La muestra incluye más de 5400 personas (en su mayoría varones homosexuales) de Estados Unidos y Europa y unos 2500 drogadictos del sudeste asiático.
Para finales del año en curso se espera conocer los resultados de esta prueba europea/estadounidense, que comenzó en 1998.

Muchos estudiosos del sida no esconden su escepticismo ante el enfoque de VaxGen.
Por razones de peso.
De suyo, la xxx se presenta en agrupaciones de tres sobre la superficie del virus, en tanto que la vacuna en cuestión emplea la forma monomérica de la molécula.
Además, las vacunas constituidas exclusivamente por proteína promueven la acción de sólo un anticuerpo, una respuesta humoral, sin que se estimule el brazo celular del sistema inmunitario, la rama de este sistema que comprende la actividad de las células T citotóxicas.
Abunda la sospecha de que no basta la respuesta exclusiva de los anticuerpos; para prevenir el sida, debe contarse con una respuesta celular. 

Los primeros resultados obtenidos no parecen reforzar la esperanza.
El pasado octubre, un panel independiente de control del seguimiento ofreció un análisis preliminar de los resultados del programa europeo/estadounidense.
Aunque el panel puso el énfasis en la seguridad de que los voluntarios no registraran efectos secundarios peligrosos asociados a la vacuna, se hallaba facultado también para recomendar que se abreviara el ensayo si parecía que la vacuna funcionaba.
No ha habido tal recomendación. 

Por su parte, VaxGen afirma que solicitará la aprobación de la administración de los Estados Unidos para vender la vacuna aun cuando las pruebas de la fase III demostraran una eficacia de sólo el 30%.
Donald P. Francis, presidente de la compañía y su cofundador, recuerda que la eficacia de la primera vacuna contra la poliomielitis, desarrollada por Jonas Salk en 1954, no superaba el 60%, pese a lo cual acabó con la incidencia de la enfermedad en Estados Unidos drástica y rápidamente.

Pero la estrategia podría resultar contraproducente, si las personas que reciben una vacuna de eficacia parcial contra el sida supusieran que se hallan protegidas contra la infección y se entregaran a conductas de riesgo.
Karen M. Kuntz y Elizabeth Bogard, de la Universidad de Harvard, han pergeñado un modelo de ordenador que simula los efectos de esa posible vacuna en un grupo de drogadictos de Tailandia.
De acuerdo con ese modelo, una vacuna de una eficacia del 30% no frenaría la diseminación del sida en una comunidad si el 90 por ciento de las personas que la recibieron volvió a utilizar jeringuillas contaminadas.
Pero este retorno a las conductas de riesgo no eliminaría el beneficio público si la vacuna alcanzara una cota mínima del 75 por cierto en punto a eficacia.

El estudio controvertido que ha de comenzar en Tailandia constituye otro ensayo de fase III a gran escala; participarán 16.000 personas.
Combina la vacuna VaxGen con un virus del tipo de varicela, en el que se han insertado genes determinantes de la xxx y de otras dos proteínas, una que interviene en la fabricación del núcleo del VIH y otra que permite la multiplicación del virus.
Puesto que el virus genéticamente modificado (producido por Aventis Pasteur) entra en las células obligándolas a desplegar en su superficie fragmentos de VIH, estimula la rama celular del sistema inmunitario.

La política se ha interpuesto a menudo en las investigaciones y desarrollo de los ensayos de la vacuna xxx de la varicela. 
Así, en un comienzo, el Instituto Nacional de Alergia y Enfermedades Infecciosas y el Departamento de Defensa de los Estados Unidos (NIAID) tenían previsto realizar un doble ensayo.
Pero el NIAID renunció a ese propósito tras examinar los datos de un estudio de fase II que demostró que no llegaban al 30 por ciento los voluntarios que habían generado células T citotóxicas contra el VIH.

Protección parcial

En la escena aparecen los laboratorios Merck.
Están completando pruebas de fase I de dos posibles vacunas, que son susceptibles de administrarse juntas.
En febrero Emilio Emini, alto directivo de la empresa, informó a los asistentes a la Novena Conferencia sobre Retrovirus e Infecciones Oportunistas en Seattle sobre los primeros datos recogidos en ambos ensayos. 

El primer ensayo investiga una vacuna potencial compuesta sólo por el gen gag del VIH, que codifica la proteína del núcleo del virus.
Se administra en forma de vacuna de ADN desnudo; así se llama la que consta exclusivamente de ADN.
Las células incorporan el gen y lo utilizan para marcar la proteína vírica, que a su vez estimula una respuesta humoral suave (y probablemente inútil) y una respuesta celular potente.
De acuerdo con las cifras aportadas por Emini, el 42 por ciento de los voluntarios que habían recibido la dosis mayor de la vacuna de ADN desnudo elevaron el nivel de células T citotóxicas, apropiadas para atacar las células infectadas por VIH. 

El segundo ensayo emplea el gen gag del VIH introducido en un adenovirus alterado. (Los adenovirus son los agentes responsables de muchos resfriados).
Este adenovirus alterado transporta el gen gag hasta las células, que, entonces, fabrican la proteína del núcleo del VIH y despiertan una respuesta inmunitaria contra dicho polipéptido. 
Entre el 44 y el 67 por ciento de las personas que habían recibido inyecciones de la vacuna fundada en el adenovirus generaron, informó Emini, una respuesta inmunitaria celular cuya intensidad variaba de acuerdo con la dosis administrada y el tiempo que había transcurrido desde su aplicación. 

Los laboratorios Merck han comenzado a ensayar una combinación de los enfoques con el ADN y el adenovirus.
Sostiene Emini que las vacunas actuarán mejor cuando se administren en el marco de un mismo régimen.
En su opinión, no se trata de que la vacuna del ADN sea una buena vacuna por sí misma, sino de que puede actuar como un cebador del sistema inmunitario al que ha de seguir meses después una inoculación de vacuna con el adenovirus.
Pudiera objetarse con razón que la mayoría de las personas han padecido resfriados por adenovirus; en cuyo caso, el sistema inmunitario de esos individuos poseería ya un arsenal dispuesto a barrer la vacuna de adenovirus antes de poder administrar su cargamento de genes VIH y estimular la inmunidad contra el sida.
Pero quizás un aumento de la dosis de la vacuna de adenovirus superaría dicho obstáculo. 

El grupo de Emini resalta la importancia de la inmunidad celular, cuya omisión podría explicar en parte los resultados desalentadores registrados hasta la fecha con vacunas diseñadas para generar respuestas humorales.
Por repetir sus palabras: "Hay un buen grupo de anticuerpos razonablemente potentes aislados de personas infectadas con el VIH, pero no sabemos todavía cómo promover la aparición de esos anticuerpos con una vacuna".
Opinión en la que abunda Lawrence Corey, del Centro Fred Hutchinson de Investigaciones Oncológicas de Seattle:
"A uno le gustaría desencadenar ambas respuestas [celular y de anticuerpos], pero el progreso más claro se ha conseguido al despertar una respuesta celular".

Los anticuerpos constituyen la primera línea defensiva del sistema inmunitario.
Se le supone un papel clave a la hora de evitar que los virus entren en contacto con las células que infectan.
Aunque las vacunas diseñadas para poner en marcha la inmunidad celular (como las de Merck) no logren impedir la infección, sí permitirían un buen punto de partida para combatir el virus en el sujeto infectado. 
Con todo, las vacunas que se limitan a frenar la progresión de la enfermedad no detienen la pandemia;
los sidosos podrían seguir diseminando la infección, pese a tener menos virus en la sangre.

No ha sido fácil hallar una vía para inducir la producción de anticuerpos capaces de neutralizar el VIH.
Por varias razones.
En primer lugar, la extraordinaria versatilidad del virus; con su rápido cambio de forma, siempre va un paso por delante de la respuesta inmunitaria.
Distingue al VIH, frente a los otros virus humanos, su notable capacidad de mutación.
Cuando se consigue un anticuerpo neutralizante, no se trata ya del VIH operativo, sino del que había en el organismo un mes atrás.

Las vacunas que incorporan una molécula lógica, el xxx -la proteína que el virus utiliza para invadir las células inmunitarias, como ya se ha señalado-, han fracasado quizá porque los anticuerpos que producen se unen a la zona equivocada de la molécula.
El xxx protege el sitio de enlace preciso que utiliza para engancharse al CD4, lugar de anclaje en las células inmunitarias, hasta el último nanosegundo, en que se abre de repente como una navaja.
Para obviar ese problema, Jack H. Nunberg y su equipo, de la Universidad de Montana, sugirieron, hace tres años, producir vacunas con moléculas de xxx expuestas previamente a CD4 y, por tanto, abiertas ya. 
Al no poderse avanzar en esa línea, ha cundido el escepticismo en torno a la misma.

Otro obstáculo posible para conseguir una vacuna contra el sida que desencadene la síntesis de anticuerpos VIH reside en la variedad de los subtipos de VIH.
También llamados clados, afectan a distintas zonas del mundo.
Hay cinco clados o subtipos principales, del A al E. Si el subtipo B es la cepa predominante en los Estados Unidos y Europa, en la mayoría de los países del Africa subsahariana -la región más castigada del planeta- señorea el clado C.
Los subtipos fundamentalmente responsables del sida en el sur y sudeste de Asia -la segunda zona con mayor incidencia de sida en el mundo- son B, C y E. De acuerdo con diversas investigaciones, los anticuerpos que reconocen el virus del sida de un subtipo quizá no identifiquen virus de otros subtipos.
De lo que se desprende que una vacuna sintetizada a partir de la cepa que se encuentra en los Estados Unidos podría no proteger a personas de Sudáfrica, por ejemplo.
Pero no existe unanimidad en torno al significado de las diferencias de clado; se discrepa también sobre la exclusividad de lugar de los tests de cepas correspondientes al subtipo prevalente en la zona en cuestión.
Essex prepara ensayos de fase I de una vacuna fundada en un subtipo C en Botswana para finales de este año; mientras no exista seguridad, alega, de que una vacuna diseñada contra un subtipo pueda tener una reacción cruzada con virus de otra, hay que abordar los ensayos de vacunas con el subtipo prevalente en las poblaciones estudiadas.
La reactividad cruzada podría ocurrir en circunstancias ideales.

El uso del clado correspondiente evita que los países del Tercer Mundo se conviertan en conejillos de Indias para ensayar una vacuna destinada a los Estados Unidos y Europa.
Las pruebas de VaxGen en Tailandia se basan en una combinación de los subtipos B y E; en abril la Iniciativa Internacional de Vacunas contra el Sida expandió sus pruebas del subtipo A en Kenia, donde se encuentra el subtipo de clase A.
Pero en enero Malegapuru William Makgoba y Nandipha Solomon, del Consejo de Investigaciones Médicas de Sudáfrica, junto con Timothy Johan Paul Tucker, de la Iniciativa Sudafricana de Vacunas contra el Sida, escribían en el British Medical Journal que la pertinencia de los subtipos de VIH estaba todavía por resolverse.
Al haber los subtipos adquirido una importancia política, los gobiernos podrían entrometerse en el camino de los ensayos de alcance internacional sobre su eficacia. 

Si nos guiamos por los resultados preliminares de los ensayos realizados con la vacuna Merck, habrá que aceptar que las diferencias de clado se difuminan cuando entra en acción la inmunidad celular.
En la reunión sobre retrovirus de febrero, Emini señaló que las células citotóxicas, 10 de cada 13 personas que habían recibido una vacuna basada en el subtipo B, reaccionaban también en las pruebas de laboratorio ante proteínas víricas de los subtipos A o C. "Hay una respuesta potencial para tipos cruzados en la inmunidad celular", fueron sus palabras.
Aunque admitió que eso no va a ser así para los anticuerpos.
Es probable que la variación de subtipos desempeñe un papel mucho menor para las células asesinas que para los anticuerpos, porque la mayoría de las células T citotóxicas reconocen zonas del VIH que comparten los distintos clados.

Así las cosas, Johnston, del NIAID, propone utilizar los cinco anticuerpos en cada vacuna.
Línea en que trabajan ya los laboratorios farmacéuticos Chiron con su vacuna multiclado; acaban de comenzar los ensayos clínicos de la misma.
Pudiera ocurrir, no obstante, que su eficacia fuera limitada: que se reconozca sólo un determinado clado, con omisión del resto.

Cualquiera que sea la salida de la cuestión sobre los subtipos, no se pierde la esperanza de diseñar una vacuna contra el sida que promueva la puesta en marcha de las células asesinas y de los anticuerpos.
Una meta tan prometedora cuan difícil.


Desarrollo de resistencia contra los antibióticos 

El estudio de los mecanismos que intervienen en la adquisición bacteriana de resistencia contra los fármacos nos enseña a diseñar medicinas más eficaces.

Los responsables de la administración sanitaria y los médicos contemplan con temor fundado la creciente ineficacia de la farmacopea antibiótica.
Uno tras otro, el doble centenar de antibióticos va quedando fuera de servicio, por inútil.
Las bacterias que sobreviven se hacen más fuertes.
Y se propagan.
Cada vez hay más cepas resistentes a los antibióticos. 
La tuberculosis, la meningitis o la neumonía, infecciones que se combatían con antibiótico, no se curan con la facilidad de antaño.
Aumenta el número de infecciones bacterianas de pronóstico quizá letal.

Las bacterias son agresores astutos.
Además, les hemos dado, y les seguimos concediendo, lo que necesitan para su éxito asombroso.
Con el uso inadecuado o abusivo de los antibióticos hemos fomentado la evolución de cepas superiores de bacterias.
Así ocurre cuando no completamos una tanda de antibióticos, los usamos para una infección vírica o lo aplicamos a un mal inadecuado. 
Se calcula que entre un tercio y la mitad de los antibióticos recetados no eran necesarios.

Un 70 por ciento de los antibióticos que se producen cada año en los Estados Unidos se administra al ganado. 
Agregamos antibióticos al líquido de las lavadoras y al jabón de manos.
Con todo ello, lo único que conseguimos es que la bacteria débil muera y la fuerte se torne más vigorosa.

Al margen de ese mal uso de la sociedad y de su abuso en clínica, el destino inevitable de los antibióticos es su presto envejecimiento.
Las bacterias -que se multiplican a través de muchas divisiones celulares a lo largo del día- siempre aprenden algo nuevo;
algunas de las más fuertes sobrevivirán y prosperarán. 
Buena razón ésta para ganarles en astucia. 

En los últimos diez años, hemos salido por fin de la situación de complacencia en que nos hallábamos sobre el dominio de las infecciones.
Laboratorios de titularidad pública y empresas farmacéuticas se han volcado en la investigación antibacteriana.
Se ensayan todos los procedimientos imaginables para atacar a las bacterias y se multiplican los antibióticos que se preparan con la información obtenida en el estudio del genoma y de las proteínas.

Pero ni la investigación apasionante ni el desarrollo de fármacos constituyen ninguna panacea.
Ahora bien, si se combinan con un uso razonable de los antibióticos pueden prestarnos ayuda.
La oficina norteamericana sobre control de alimentación y fármacos (FDA) aprobó en abril del año 2000 el primer tipo nuevo de antibiótico clínico en 35 años: el linezolid.
En lista de espera, o en fases previas, hay varios agentes más. 

El desmantelamiento de la pared bacteriana 

Casi todos los antibióticos que se han desarrollado hasta la fecha proceden de la naturaleza.
Los científicos los han identificado y los han refinado, pero no los han creado. 
Desde el comienzo de la vida en nuestro planeta, los organismos han luchado por los limitados recursos que tenían a su disposición.
De esa pugna surgió la evolución de los antibióticos.
La capacidad de producir tales compuestos poderosos confiere a un organismo -hongo, planta u otra especie bacteriana- una ventaja sobre las restantes bacterias sensibles al antibiótico.

En esa presión de selección se esconde el motor natural del desarrollo de los antibióticos.

Nos integramos en semejante carrera armamentística de los organismos con el descubrimiento de la penicilina en 1928. 
Alexander Fleming, del hospital clínico Santa María de la Universidad de Londres, advirtió que el moho Penicillium notatum mataba las bacterias Staphylococcus que crecían cercanas en agar en una placa de petri.
Se había inaugurado el campo de los antibióticos.
El análisis al azar de otros compuestos, procedentes o no de mohos, para averiguar si destruían bacterias o retardaban su desarrollo, llevó a la identificación de una amplia gama de antibióticos.

Entre los que han conocido mayor éxito se cuenta la vancomicina, identificada por los laboratorios Eli Lilly en 1956.
El comprender su mecanismo de operación -una proeza que ha llevado más de tres decenios culminar- nos ha permitido adentrarnos en el mecanismo de acción de los antibióticos glicopéptidos, una de las siete clases principales.
Se trata de un avance importante, por cuanto la vancomicina se ha convertido en el último recurso, el único fármaco eficaz que nos queda frente a la infección más letal que puede contraerse en el hospital: la del Staphylococcus aureus , resistente a la meticilina.
Pero el poder de la vancomicina se encuentra en peligro.

La vancomicina ataca la pared bacteriana; ciñe ésta a la célula y su membrana, confiriéndole estructura y sostén.
Ni la vancomicina ni otros fármacos afines dañan las células de humanos y mamíferos, que carecen de tal pared (poseen en cambio un citoesqueleto, una estructura interna que les da consistencia).
La pared bacteriana consta fundamentalmente de peptidoglicanos, un material que, de acuerdo con su nombre, contiene péptidos y azúcares. 
A medida que la célula organiza este material -un proceso constante, porque todo peptidoglicano necesita reemplazarse cuando se degrada-, las unidades glucídicas se unen entre sí mediante la acción de la enzima transglucosidasa y forman una suerte de malla.
En esta estructura, una de cada dos unidades de azúcar porta enlazada una cadena peptídica corta.
Cada cadena peptídica posee cinco aminoácidos, de los cuales los últimos son una L-lisina y dos D-alaninas.
Se encarga la enzima transpeptidasa de reunir las cadenas peptídicas, eliminando la D-alanina final y uniendo la D-alanina penúltima a una L-lisina de una cadena de azúcares diferente.
En razón de ello, las cadenas de glúcidos quedan amarradas mediante cadenas peptídicas.
Todos estos enlaces cruzados tejen un material muy trenzado, esencial para la supervivencia de la célula;
sin él, la célula estallaría por su propia presión interna.

La vancomicina se interpone en la formación de ese material decisivo.
El antibiótico se halla cabalmente preparado para unirse a las cadenas peptídicas, antes de que éstas lo hagan entre sí por intervención de la transpeptidasa.
El fármaco, al engarzarse en las D-alaninas terminales, evita que la enzima lleve a cabo su tarea.
Sin la espesura de conexiones entrelazadas, el peptidoglicano se degrada, cual paño mal urdido.
La célula se desgarra y muere.

Minando la resistencia

El encaje perfecto de la vancomicina en el extremo de la cadena peptídica resulta clave para su eficacia antibiótica. 
Por desgracia, su conexión peptídica es también decisiva para la resistencia bacteriana.
En 1988 apareció un S. aureus resistente a la vancomicina en tres lugares distintos. 
Hay motivo para preocuparse de la posibilidad de expansión de las cepas, que dejarían sin tratamiento las infecciones letales de estafilococos.

Si conocemos el mecanismo de resistencia, podremos derrotarla. 
La investigación concentra ahora su atención en otra bacteria que, desde finales de los años ochenta, se sabe que es resistente a este fármaco poderoso: el enterococo resistente a la vancomicina (VRE).
En la mayoría de las bacterias enterocócicas, la vancomicina cumple con su misión de unirse a las dos D- alaninas terminales. 
En el plano molecular, tal unión comporta la formación de cinco enlaces o puentes de hidrógeno, a la manera de cinco dedos que aprieten una pelota.
Pero en la VRE la cadena peptídica difiere ligeramente.
Aquí, la D- alanina final está alterada por una sustitución simple:
un oxígeno reemplaza al par de átomos constituido por un nitrógeno unido a un hidrógeno. 
En términos moleculares, esta sustitución determina que la vancomicina se una a la cadena peptídica con sólo cuatro enlaces de hidrógeno.
La pérdida de uno de estos enlaces genera la diferencia. 
Si son sólo cuatro los dedos que aprietan la pelota, el fármaco no puede aferrarse bien;
las enzimas consiguen entrometerse y posibilitar que las cadenas peptídicas se unan de nuevo.
Una mera sustitución atómica reduce en un factor de 1000 la actividad del medicamento.

Se han estudiado también otros antibióticos glicopeptídicos con la esperanza de observar si los hay con una estrategia que la vancomicina pudiera adoptar frente a los VRE.
Se da la circunstancia de que algunos miembros de ese grupo de antibióticos poseen largas cadenas hidrofóbicas, muy útiles.
Estas cadenas prefieren rodearse de otras moléculas hidrofóbicas, como las que constituyen la membrana celular, oculta tras el escudo peptidoglicano protector. 
Los investigadores de Eli Lilly, trabajando sobre esa pauta, han unido cadenas hidrofóbicas a la vancomicina y creado el análogo LY333328.
El fármaco se adhiere a la membrana celular en concentraciones elevadas, lo que permite un agarre más firme y, en consecuencia, un poder mayor contra el peptidoglicano.

Este análogo es eficaz contra los VRE y se halla en fase de ensayo clínico.

Otros antibióticos glicopeptídicos emplean una estrategia diferente.
Se trata de la dimerización, proceso en cuya virtud dos moléculas se unen entre sí para formar un complejo unitario.
Al crear parejas, o dímeros de vancomicina, podemos reforzar la fuerza del fármaco.
Una molécula de vancomicina se une al peptidoglicano y arrastra la aproximación de la otra mitad del par, la otra molécula de vancomicina.
Aumenta así la eficacia del fármaco con su presencia mayor.
En nuestro laboratorio nos proponemos facilitar el emparejamiento de la vancomicina;
hemos logrado ya varias moléculas de vancomicina diméricas con una actividad excepcional frente a VRE.

Pese a todo, podríamos fracasar.
Se acaba de descubrir un segundo mecanismo en virtud del cual la VRE engaña a la vancomicina.
En vez de sustituir un átomo en la D-alanina terminal, la bacteria agrega un aminoácido mayor que la D-alanina en el extremo de la cadena peptídica; 
de ese modo, el aminoácido evita que la vancomicina llegue a su destino.

Comenzamos a desentrañar también el método que sigue el letal S. aureus para adquirir resistencia.
La bacteria apelmaza la capa de peptidoglicano y relaja, a la vez, la unión entre los segmentos peptídicos.
No importa, pues, que la vancomicina se engarce en la D- alanina;
el espesor ha sustituido a la interconexión como fundamento de la fuerza del peptidoglicano.
La inserción de la vancomicina carece de eficacia.

El filo de la navaja

Nos enseña la historia de la vancomicina que bastan alteraciones moleculares muy pequeñas para engendrar profundas diferencias.
Y las bacterias encuentran múltiples estrategias para engañar a los fármacos, lo que obliga a la búsqueda de medicamentos, nuevos o regenerados. 
Tradicionalmente, el proceso de identificación de candidatos consistía en un muestreo con células íntegras:
las moléculas de interés se aplicaban a células bacterianas vivas.
Se trata de un método de probada utilidad, corroborado con el descubrimiento de muchos fármacos, la vancomicina incluida.
Suma a su sencillez el rastreo de toda diana posible del fármaco en la célula.
Pero la criba de numerosas dianas presenta también inconvenientes.
El hombre y las bacterias comparten diversas dianas;
los compuestos que actúan contra éstas resultan tóxicos para las personas.
Con tal barrido no se saca ninguna información acerca del mecanismo de acción:
se sabe que un agente actuó, pero no cómo.
Sin ese conocimiento imprescindible, es casi imposible que un nuevo fármaco llegue al dispensario.

Los ensayos que se realizan en dominios moleculares ofrecen una alternativa poderosa.
A través de ese muestreo se identifican sólo los compuestos que tienen una mecanismo de acción especificado.
Pensemos, por ejemplo, en la búsqueda específica de inhibidores de la transpeptidasa.
Pese a la dificultad que entraña el diseño de tales ensayos, descubre fármacos potenciales con modos de acción conocidos.
El problema es que sólo se investiga una enzima cada vez.
Se daría un gran paso si pudiéramos perseguir simultáneamente más de un objetivo (como ocurre en el proceso en que participan células íntegras) sin merma del conocimiento implícito del mecanismo de operación del fármaco.
El gran paso se ha dado.
Se ha reconstruido en el tubo de ensayo la vía multienzimática de una bacteria. 
Con este sistema se pueden identificar moléculas que degradan profundamente una de las enzimas o alteran de un modo sutil varias de ellas.

Con la automatización y la miniaturización se ha multiplicado la celeridad en el cribado de compuestos. 
La robótica permite estudiar los compuestos a millares.

Al mismo tiempo, la miniaturización ha reducido el costo del proceso utilizando cantidades mínimas de reactivos.
Con sistemas de cribado ultrarrápidos, podemos investigar cientos de miles de compuestos en un día. 
Merced a los nuevos métodos de la química combinatoria podemos diseñar cantidades inmensas de compuestos. 
En el futuro, algunas de estas moléculas nuevas procederán de las mismas moléculas.

Una vez que comprendamos la vía por la que estos organismos producen antibióticos, la ingeniería genética facilitará la síntesis de nuevas moléculas relacionadas con ellos.

La ventaja de la genómica El diseño de fármacos y su muestreo se han beneficiado enormemente del desarrollo reciente de la genómica.
Con el conocimiento de los genes y de la síntesis de las proteínas por ellos cifradas, la ciencia se ha adentrado en las propias entrañas moleculares del organismo.
A la manera de un servicio de contraespionaje microbiano, importa ahora atentar contra genes de importancia capital, bloquear la síntesis de una proteína específica o alterar la capacidad de un organismo para infectar o desarrollar resistencia.

Muchos de los objetivos contra los que van dirigidos los antibióticos son genes esenciales, genes que provocan la muerte celular si dejan de funcionar.
Apenas si cuesta la pronta identificación de dichos genes.
Pensemos en un análisis sistemático de los 6000 genes de la levadura Saccharomyces cerevisiae .
Podemos alterar experimentalmente cada uno de estos genes y determinar el efecto en la levadura.

Las proteínas codificadas por los genes esenciales no son los únicos objetivos moleculares en la búsqueda de antibióticos.
Importan también los genes que codifican factores de virulencia;
eluden éstos la respuesta inmunitaria del huésped y de ese modo preparan el terreno para la colonización de la bacteria.
Resultaba antaño harto difícil identificar tales genes porque se "activan" o transcriben por sucesos en el tejido del huésped cuya reproducción in vitro se hacía muy compleja.
Mas con la técnica actual de expresión in vivo (IVET) se puede insertar una secuencia singular de ADN, una forma de etiqueta que desactiva un gen, en cada gen bacteriano.
Se aplican bacterias etiquetadas para infectar un organismo, se recuperan luego y se identifican las etiquetas. 
La desaparición de cualquier etiqueta significa que los genes a los que estaban unidas eran esenciales para la supervivencia de la bacteria, tan esenciales que la bacteria no podría sin ellos vivir en el huésped.

Desde hace tiempo se esperaba que la identificación e inhibición de los factores de virulencia permitieran al sistema inmunitario del organismo combatir las bacterias patogénicas antes de su instalación.
Algo se ha avanzado.
En un estudio reciente, una molécula experimental que inhibe el factor de virulencia de S. aureus ha servido para que los ratones de prueba resistieran la infección.

Además de investigar los genes esenciales y los factores de virulencia, los laboratorios empiezan a descubrir qué genes confieren resistencia a los antibióticos. 
La polarización en ellos del ataque nos ayudará a rejuvenecer antibióticos desechados por ineficaces. 
En esa línea laboran los antibióticos b-lactámicos como la penicilina.
El mecanismo habitual de resistencia contra los antibióticos b-lactámicos estriba en la producción bacteriana de b-lactamasa, enzima que corta uno de los enlaces del antibiótico, cambiando su estructura y evitando la inhibición de la transpeptidasa. 
Si se silencia la b-lactamasa, el antibiótico permanece eficaz.
Eso es justamente lo que hace el ácido clavulánico, un inhibidor de la b-lactamasa:
se mezcla con la amoxicilina para crear el antibiótico Augmentine. 

Cuando en un futuro próximo dominemos mejor la transcripción del ADN, será práctica rutinaria la identificación de determinantes de resistencia, como la b-lactamasa, y factores de virulencia.
Habrá llegado entonces el momento de poder identificar los genes que intervienen en diferentes condiciones de desarrollo celular.
Mediante la identificación de los genes bacterianos cuya expresión aumenta al infectar un huésped determinaremos los genes de virulencia.

Estableceremos los genes de resistencia a los antibióticos mediante la comparación entre niveles de expresión en bacterias tratadas con antibióticos y los manifestados en bacterias sin tratar.
Aunque en su infancia, esta técnica ha detectado cambios minúsculos en el número de episodios de transcripción.
Con el dominio del perfil de transcripción del ADN, podrá establecerse si determinados fármacos aplican mecanismos de acción totalmente nuevos o tienen dianas celulares inéditas que podrían señalar caminos a la investigación antibiótica no transitados.

Sacrificio del mensajero

En otra línea interesante de la investigación genómica se busca la inoperancia del ARN bacteriano.
En su mayor proporción, el ARN es ribosómico (ARNr), componente estructural principal de los ribosomas.
Son éstos fábricas de ensamblaje de las proteínas. 
La vulnerabilidad del ARN ribosómico yace en la diversidad de lugares donde pueden anclarse los fármacos. 
Carece, además, de capacidad para autorrepararse. 
En 1987 se comprobó que los antibióticos aminoglicósidos -la estreptomicina, entre ellos- se unían al ARNr, lo que comportaba que el ribosoma errase en la lectura del código atinente al ensamblaje de la proteína. 
Muchos de estos antibióticos, sin embargo, amén de ser tóxicos, presentan una utilidad limitada.
En el Instituto Scripps de Investigaciones de La Jolla se acaba de observar un nuevo dímero de aminoglicósido sintético que muestra menor toxicidad.

Podemos entorpecer la acción del ARN mensajero (ARNm), que dirige la síntesis de proteínas y cursa entre el código genético y el ribosoma.
El ARN mensajero se crea mediante la lectura de una de las hebras del ADN, aplicando las mismas interacciones entre ácidos nucleicos, o pares de bases, que mantienen unida la doble hélice. 
La molécula de ARNm porta entonces el mensaje al ribosoma, donde se sintetiza una proteína a través del proceso de traducción.
Puesto que cada ARNm codifica una proteína específica y difiere de otros ARNm, contamos con la posibilidad de forjar otras interacciones entre pequeñas moléculas orgánicas - esto es, no proteínas- y ARNm específicos.
Es ni más ni menos lo que se les ha ocurrido a químicos de Parke-Davis para combatir la infección de VIH.
Tras identificar moléculas que se unen a una parte de una secuencia de ARNm, evitan que ésta interaccione con una proteína activadora necesaria, inhibiendo así la replicación del VIH.
Semejante logro experimental debería incitar nuevos estudios sobre el ARNm en la búsqueda de fármacos.

Se trabaja con similar empeño en el dominio de la terapia antisentido.
Al generar secuencias de nucleótidos con una secuencia específica de ARNm, encorsetan dicho ácido nucleico.
Así aherrojado, el ARNm no puede liberarse del fármaco;
o se destruye o queda inactivo.
Aunque la FDA ha aprobado ya el primer fármaco antisentido para infecciones del citomegalovirus en el hombre, este tipo de medicamentos no han tenido éxito en las infecciones bacterianas, por diversas razones, entre ellas la toxicidad y la dificultad de que el fármaco alcance una concentración adecuada en el sitio de la infección. 
Pese a todo, se trata de un campo esperanzador.

De lo que no cabe duda es de que todos estos planteamientos genómicos facilitan la identificación y evaluación de una gama de objetivos biológicos contra los que pueden dirigirse moléculas eficaces.
Quedan descartados, por nocivos, numerosos antibióticos desarrollados en el siglo XX.
Pero al comparar la secuencia génica del objetivo potencial con los genes que se encuentran en el hombre, podemos identificar genes exclusivos de las bacterias; 
una vez acotados, podemos concentrar la atención sobre ellos.
Igualmente, al comparar la secuencia génica de un objetivo con las de otras bacterias, podemos evaluar la selectividad de un fármaco.
Una secuencia objetivo que aparece en todas las bacterias habría de generar, con probabilidad, un antibiótico activo contra muchas bacterias diferentes; vale decir, un antibiótico de amplio espectro.
Por el contrario, una secuencia objetivo que aparece sólo en el genoma de algunas bacterias generaría un antibiótico de margen estrecho.

Si los médicos descubren en una fase temprana la cepa bacteriana causante de la infección, pueden afinar y recetar un antibiótico de espectro limitado.
Puesto que el medicamento afectará sólo a un subgrupo de la población bacteriana, la presión de selección para desarrollar resistencia será escasa.
Los avances en replicación rápida del ADN y en la obtención de perfiles de transcripción podrán hacer que pronto la identificación de cepas bacterianas constituya un protocolo rutinario.

Aunque el cuadro parezca ahora más nítido que años atrás, conviene no perder de vista que, en biología, la carrera armamentística no es cosa nueva.
A cada contraataque del hombre le seguirá una respuesta de la bacteria, del tipo que sea;
a veces le basta con cambiar un átomo de un aminoácido. 


Genes que oponen resistencia al sida

Se conoce ya un carácter genético que protege del sida.
Muy pronto podrían aparecer más, abriendo con ello nuevas posibilidades de tratamientos preventivos A todos nos son familiares los terribles efectos del virus de la inmunodeficiencia humana (VIH), que se transmite por contacto sanguíneo.
Aunque puede pasar inadvertido durante mucho tiempo, lo normal es que al cabo de 10 o 15 años haya destruido células esenciales del sistema inmunitario y produzca el sida (síndrome de inmunodeficiencia adquirida). 
La pérdida de inmunidad franquea el camino a la proliferación de microorganismos que, de no ocurrir tal percance, estarían a buen recaudo y facilita el desarrollo de ciertos tumores.
Sólo en Estados Unidos, el sida ha matado a más de 350.000 personas y se ha convertido en la causa principal de muerte entre los 24 y 44 años. 
Hay en ese país otras 750.000 que son portadoras y en todo el mundo puede haber unos 30 millones de afectados. 

En los últimos años, se han desarrollado fármacos que ayudan a prolongar la vida de los pacientes. 
Las combinaciones sutiles de tales medicamentos provocan una disminución de los niveles víricos en el cuerpo y restablecen la función inmunitaria.
Esos logros son ya de dominio público, pero hay otros descubrimientos, menos conocidos, que han causado también una indudable conmoción en la comunidad de investigadores del sida. 

Desde hace tiempo la ciencia se viene preguntando por qué algunos sujetos se escapan de la acción del VIH, pese a pertenecer a grupos de alto riesgo, y por qué en otros infectados el sida avanza a un paso lentísimo.
Entre 1978 y 1984, antes de que fuera obligatorio el análisis rutinario de la sangre procedente de donaciones, se infectaron casi 12.000 hemofílicos que recibieron transfusión sanguínea;
sin embargo, entre el 10 y el 25 por ciento de esas personas se libraron del virus.
Y aproximadamente el 1 por ciento de los portadores del VIH permanecieron bastante sanos durante períodos de tiempo insólitamente largos, de 15 años o más, sin síntomas apenas y con un sistema inmunitario que funcionaba de forma adecuada. 

A tenor de ciertos descubrimientos recientes, hay personas que son parcial o totalmente resistentes a las infecciones del VIH gracias a sus genes;
para ser más exactos, deben esa buena estrella a cierta variante de un gen implicado en la función inmunitaria, de la que son portadoras.
Este hallazgo ha desencadenado un alud de esfuerzos para traducir los nuevos conocimientos genéticos en estrategias innovadoras encaminadas a prevenir y controlar las infecciones del VIH. (Utilizamos el término "VIH" para referirnos al VIH-1, el virus responsable de la mayoría de los casos de sida en todo el mundo.
Otra forma del virus, la VIH-2, produce el síndrome con mayor parsimonia y está restringido a determinadas zonas de África.
No se ha estudiado todavía la resistencia genética al VIH-2.)
La historia del hallazgo del primer gen de resistencia al VIH es el relato de un período de desesperante lentitud en las investigaciones, en el que de pronto empiezan a producirse descubrimientos extraordinarios. 
Junto con nuestros colegas del Instituto Nacional del Cáncer (INC), iniciamos la búsqueda de tales genes en 1984, un año después de que se confirmase que el VIH era el agente causal del sida y tres desde que se identificara la enfermedad.

En aquel entonces nuestro proyecto parecía una empresa revolucionaria.
En los años ochenta, para explicar el fenómeno de la distinta respuesta al VIH mostrada por los componentes de un mismo grupo, la mayoría de los investigadores aducía razones del tenor siguiente: características genéticas del virus (estirpes diferentes tendrían virulencia distinta) o "cofactores" no genéticos que pudiesen influir en su virulencia (infección concomitante del paciente por otro microorganismo).
Carecíamos, además, de pruebas convincentes de la existencia de una protección genética contra el sida en humanos.
De hecho, había compañeros nuestros que llegaron a mirar con escepticismo la posibilidad de hallar algo vinculado con causas genéticas, tras invertir tantos recursos y tanto tiempo.

Pese a todo, no nos lanzamos a ciegas en esa aventura. 
Las investigaciones con animales habían establecido sin ambages que, a menudo, los genes influyen en la producción y desarrollo de infecciones, sobre todo en las causadas por retrovirus, familia a la que pertenece el VIH.
La mayoría de los genes cifran información para sintetizar proteínas, que son las moléculas que acometen la mayoría de las funciones celulares.
Cuando se activa un gen de una proteína, su secuencia de nucleótidos de ADN sirve de guía para ensamblar la secuencia de aminoácidos que define dicha proteína.
Si el gen es polimórfico -es decir, si está presente en más de una forma en la población- sus variantes, o alelos, pueden determinar proteínas que operen de manera distinta.
En ratones, se conocen alelos específicos de más de 30 genes que confieren resistencia a retrovirus.

Otras investigaciones con animales habían demostrado también la presencia de un componente genético en las enfermedades infecciosas.
Los ratones, ratas y ganado con alto grado de consanguinidad (endogamia) se muestran especialmente proclives a las enfermedades contagiosas.
El motivo principal reside en la consanguinidad, que limita bastante el repertorio de alelos de resistencia a enfermedades. 
En los grupos donde no se da consanguinidad (exogamia) una parte de la población podría portar algún alelo que protegiera contra un determinado patógeno.
Ese alelo permitiría a sus portadores sobrevivir ante una eventual epidemia y aseguraría la pervivencia del grupo.
Considerada la diversidad genética de las poblaciones humanas, sospechábamos que, lo mismo que ocurre con otras especies exógamas, tendrían alelos capaces de conferir resistencia a enfermedades y, quizás, al virus VIH.
Simplemente, había que descubrirlos.

Además, aunque en humanos habían aparecido muy pocos alelos de resistencia ante agentes patógenos, no faltaban estudios epidemiológicos que abogaban por una enérgica influencia genética en la proclividad a la patología.
En ese sentido, uno de los análisis demostraba que, si un progenitor biológico de un individuo adoptado moría de una enfermedad infecciosa antes de cumplir los 50 años, el adoptado tenía un riesgo notablemente alto de morir también de una infección.

Para nuestro infortunio, la ciencia no ofrecía medios sencillos que nos facilitaran la búsqueda de alelos de resistencia al VIH en humanos.
Nos vimos, pues, forzados a recurrir a la combinación de conocimientos y técnicas de tres disciplinas dispares: la epidemiología del sida, la genética molecular humana y la genética de poblaciones. 

De entrada, necesitábamos un reservorio de genes de las poblaciones que nos interesaban.
Habíamos de trabajar con individuos que corriesen un riesgo alto de resultar infectados por VIH y que terminaran o por caer infectados tras quedar expuestos al virus o por librarse.
Si los dos grupos diferían genéticamente - esto es, en sus alelos para genes específicos- podíamos sospechar que los genes que presentaban variación influían en la propensión a las infecciones por el VIH.

Para obtener el ADN humano que se necesitaba en nuestra investigación, solicitamos la colaboración de los epidemiólogos que intentaban aclarar el comportamiento de la, por entonces, nueva pandemia.
Esos profesionales controlaban grupos de varios cientos de individuos con alto riesgo de ser infectados por el VIH: homosexuales, consumidores de droga por vía intravenosa y hemofílicos que habían recibido sangre contaminada.
Esos grupos debían ser sometidos a un seguimiento durante años por médicos que (con el permiso de los pacientes) suministrarían muestras de sangre y tejidos y nuevos casos a los investigadores.
Conforme se iban recogiendo las muestras de sangre, nuestro equipo de biología celular, dirigido por Cheryl Winkler, se encargaba de producir líneas inmortales de células cultivadas, que serían una fuente ilimitada de ADN para los ensayos genéticos.

Para determinar qué genes debíamos comparar, apelamos a los métodos que en su avance aportaba la cartografía genética, un conjunto de procedimientos que indica la localización de genes en los cromosomas y determina sus secuencias de nucleótidos.
Hay ya cartografiados más de 6000 de los aproximadamente 50.000 o 100.000 genes implicados en los cromosomas humanos.
Pero en 1984 no llegaban al millar los cartografiados.
Pese a ello, hasta el manejo de 1000 genes en nuestros grupos de afectados de sida constituía una tarea imposible.

Limitamos algo la selección.
Nos lo permitían los conocimientos ya establecidos sobre el comportamiento de los retrovirus en sus hospedadores.
El hospedador es siempre un colaborador de conducta imprevisible ante la infección y la propagación de los patógenos por los tejidos. 
Para entrar en las células, los virus deben reconocer ciertas proteínas, cifradas por genes del hospedador y desplegadas por la superficie celular.
En condiciones normales esas proteínas actúan como receptores de otras moléculas del hospedador, pero los virus pueden servirse de tales receptores para introducirse en las células. 

Una vez en el interior celular, los retrovirus insertan sus genes en los cromosomas del hospedador.
Se aseguran así que sus genes, y con ellos una fuente ilimitada de partículas víricas, se transmitan a cada nueva generación de células, en cada tanda de replicación de la célula hospedadora.
Aquí, de nuevo, los virus requieren la ayuda del hospedador.
Deben reclutar enzimas celulares para integrar los genes víricos en los cromosomas, para producir nuevas partículas víricas e incluso para zafarse de las defensas inmunitarias del hospedador. 

Conocidos tales presupuestos, decidimos concentrarnos de momento en unos 50 genes cuyas proteínas podían en principio condicionar el ciclo de vida del VIH.

Examinamos también 250 segmentos de ADN polimórficos (variables), identificados de antemano en regiones intergénicas. 
Si los individuos que estudiábamos diferían sistemáticamente en esos segmentos, las discrepancias indicarían, a su vez, la existencia de diferencias en alelos de genes cercanos.
Podríamos, entonces, centrarnos en esos genes y tratar de determinar su función en las células y su papel en la infección del VIH. 

Finalmente, para identificar caracteres genéticos capaces de conferir resistencia al VIH, adoptamos estrategias propias de la genética de poblaciones humanas.
Dividimos cada grupo en dos, de acuerdo con aspectos relacionados con su salud -infectados con VIH frente a los que permanecían libres de infección tras una exposición prolongada; pacientes infectados donde progresaba el sida con rapidez frente a los que experimentaban un avance lento del mismo o no lo desarrollaban; pacientes infectados que contraían una enfermedad específica relacionada con el sida (neumonía debida a Pneumocystis carinii o sarcoma de Kaposi) frente a los que no la sufrían.

Una vez realizadas estas particiones, comparamos la frecuencia con que cada alelo o segmento polimórfico conocidos aparecían en cada grupo.
También comparamos los genotipos. 
Por herencia recibimos dos copias de todos los genes (una copia de la madre y otra del padre), excepto de los que están en los cromosomas sexuales;
el par de alelos de un locus cromosómico particular, o localización génica, constituye el genotipo.
Quienes heredan dos alelos idénticos de un determinado gen se dice que son homocigóticos, y los que heredan dos alelos distintos, heterocigóticos.

En nuestros análisis anotábamos los porcentajes de pacientes de cada grupo que eran homocigóticos para un alelo y el porcentaje que era heterocigótico.
La existencia de diferencias significativas en frecuencias alélicas, genotípicas o ambas, entre dos grupos de sujetos, denunciaría que el gen estudiado tendría quizás alguna relación con los destinos divergentes de los individuos de esos grupos. 

Año tras año fue aumentando el número de pacientes, genes, segmentos polimórficos y programas informáticos que utilizábamos para el análisis de los datos.
Periódicamente creíamos observar alguna diferencia genética, pero casi siempre resultaba falsa cuando se analizaba con mayor detalle.
Mientras tanto, procurábamos estar al día de los avances que se apuntaba la inmunología humana y conocer cuánto se adelantaba en el comportamiento del VIH en el cuerpo.

Procurábamos estar alerta ante posibles pistas que nos indicasen cómo identificar otros posibles genes candidatos. 
A finales de 1995 y principios de 1996, tras un decenio largo de trabajo tedioso y absorbente, el panorama empezó a cambiar.

El nuevo cuadro lo pergeñaron otros equipos investigadores, que resolvieron dos viejos misterios relacionados con la interacción molecular entre el VIH y las células hospedadoras.
Su esclarecimiento despejó el camino hacia los genes implicados en la resistencia al VIH.

A mediados de los años noventa era ya obvio que la deficiencia inmunitaria se debía al VIH, que mermaba la población de linfocitos T .
Estos leucocitos presentan una proteína denominada CD4 en su superficie.
Las células T suelen dirigir muchos aspectos de la respuesta inmunitaria ante la agresión vírica.
También se sabía que el VIH infectaba y permanecía durante años en otra clase de células inmunitarias portadoras de moléculas CD4, los macrófagos.
El VIH no destruye a los macrófagos; se encuentra a salvo en su interior.

En condiciones normales, las moléculas CD4 de los linfocitos T y de los macrófagos participan en los sistemas de señalización entre células inmunitarias.
Pero cuando el VIH entra en acción, las moléculas CD4 se unen a una proteína azucarada ( gp120 ) que sobresale de la cubierta exterior del VIH; 
esa trabazón facilita la entrada del virus en la célula.
Algunos experimentos habían demostrado que CD4, aunque necesaria para la entrada del VIH en las células, no era suficiente.
Las células debían presentar al menos otra proteína a la que se agarrara el virus. 
Diez años después del descubrimiento del VIH, sin embargo, seguía ignorándose la naturaleza exacta de ese segundo receptor.

El otro misterio tenía que ver con un descubrimiento del que había informado, en 1986, Jay A. Levy, de la Universidad de California en San Francisco.
Observó que un tipo de linfocito T que portaba una proteína diferente, CD8, secretaba factores supresores;
estas moléculas impedían que el VIH, en cultivo, invadiese células que, de suyo, eran proclives a ello.
Este tipo de factores supresores que limitaban la infección vírica habían aparecido también en monos africanos portadores del VIS (la forma simia del VIH), que no desarrollaban sida, así como en personas que sobrevivían a la infección del VIH durante períodos de longitud inusitada.
Pero se desconocían los factores supresores en cuestión. 

En diciembre de 1995, un grupo de investigadores del norteamericano Instituto Nacional del Cáncer, entre los que se hallaba Robert Gallo, anunciaron que habían identificado tres factores supresores emparentados;
bloqueaban, decían, la infección de las variantes del VIH que mostraban preferencia por colonizar los macrófagos (las estirpes M-trópicas). 
Los tres factores resultaron ser quimiocinas conocidas. 
Estas moléculas constan de cadenas cortas de aminoácidos encargadas de atraer a las células inmunitarias hacia los tejidos dañados o enfermos.

Los comprometidos en la resolución del primer misterio -la búsqueda del segundo receptor del VIH- advirtieron que las quimiocinas actuaban sobre las células defensivas trabándose con proteínas alojadas en la superficie de éstas.
No era descabellado pensar, por tanto, que las quimiocinas aisladas por el grupo de Gallo -denominadas RANTES, MIP - xxx y MIP - xxx - impidiesen la entrada del VIH en las células inmunitarias al unirse y bloquear algunas de las proteínas de la superficie celular que el VIH necesitaba para acceder al interior.
En otras palabras, el receptor (o receptores) de superficie celular para las quimiocinas de Gallo podía muy bien llevar una doble vida como segundo receptor para el VIH en macrófagos y quizás en otras células portadoras de CD4.

La idea se resistía a su comprobación inmediata, pues no se habían aislado todavía los receptores celulares para RANTES y sus parientes.
Pero ciertos descubrimientos de los que se tuvo noticia a comienzos de 1996 permitieron llevar a cabo tales ensayos y nos proporcionaron nuevos genes para analizar como posibles factores de resistencia.

En efecto, el grupo encabezado por Edward A. Berger, del Instituto Nacional de la Alergia y Enfermedades Infecciosas, aislaron el segundo receptor para las variantes del VIH que mostraban preferencia por la colonización de los linfocitos T (estirpe T -trópicas).

Resultó ser un receptor de quimiocina (denominado CXCR4), si bien la quimiocina que se une a éste difiere de las RANTES, MIP - xxx y MIP - xxx .
Si los descubrimientos de Gallo no convencieron del todo a los investigadores de que los receptores de quimiocinas desempeñaban un papel en la infectividad del VIH, los resultados de Berger despejaron cualquier duda.

Casi simultáneamente, Michael Samson y Marc Parmentier, de la Universidad Libre de Bruselas, aislaron el gen para un receptor reconocido por RANTES, MIP - xxx y MIP - xxx , y necesario para que estos factores dirijan las células defensivas hacia el tejido dañado.
En dos meses, cinco grupos demostraron, por separado, que la proteína cifrada por dicho gen, la actual CCR5, era también el escurridizo segundo receptor para las estirpes M-trópicas del VIH.

Sumados a otros trabajos, estos nuevos descubrimientos sobre los receptores de quimiocinas aclaraban bastante el mecanismo de infección del VIH.
El virus inicia la infección estableciendo de entrada su residencia en los macrófagos. 

Penetra en esas células sirviéndose de su proteína gp120 para unirse a dos receptores de los macrófagos: CD4 y CCR5.
Una vez en el interior de los macrófagos, el VIH sintetiza grandes cantidades de virus y desafía al sistema inmunitario hasta sus límites. 

Años después, el virus, que no cesa de mutar, puede alterar el gen de la gp120 de tal forma que haga que la proteína gp120 cambie su fidelidad por el segundo receptor.
El cambio genético origina que la región que reconoce a CCR5 se una con mayor eficacia a CXCR4 en los linfocitos T .
A partir de entonces, en la población de VIH dominan las variantes T -trópicas, las que prefieren infectar células T .

Este cambio de afinidad resulta mortal, pues los virus T - trópicos matan a las células que infectan.
No es ninguna sorpresa que a ese cambio le siga una rápida caída global de la concentración de células T CD4 en los pacientes y, simultáneamente, la aparición de las infecciones oportunísticas y cánceres que durante muchos años han definido al sida.
En la actualidad, los Centros de Control y Prevención de Enfermedades definen formalmente al sida por la presencia de afecciones propias del síndrome o por una disminución en la concentración de células T CD4 hasta menos de 200 células por milímetro cúbico de sangre;
los niveles normales se cifran en 1000 por milímetro cúbico. 

En cuanto supimos que CCR5 y CXCR4 eran co-receptores del VIH, decidimos comprobar si los genes para esas proteínas afectaban a la resistencia al VIH en nuestros grupos.
A ese respecto, debíamos empezar por determinar si los genes CCR5 y CXCR4 eran polimórficos. 
Si todos los individuos tenían versiones idénticas de esos genes, significaría que tales genes no eran los responsables de las diferencias en la susceptibilidad al VIH. 

Todas las copias del gen CXCR4 examinadas se repetían.
Pero en julio de 1996, Mary Carrington, de nuestro grupo, descubrió que uno de cada cinco individuos presentaba una variante del gen CCR5 .
La comparación de las secuencias de nucleótidos de los dos alelos CCR5 reveló que la más infrecuente tenía 32 nucleótidos menos. 
Debido a las peculiaridades del código genético, sospechamos que dicha merma podría originar la aparición prematura de una señal de "stop" en el gen, provocando que las células fabricasen versiones más cortas y anormales de la proteína CCR5.

Cuando dividimos al doble millar de pacientes de alto riesgo entre infectados y no infectados, y comparamos sus genotipos CCR5 , encontramos diferencias espectaculares. 
El 3 por ciento de los no infectados portaba en sus células sólo el alelo mutante delecionado de CCR5 (eran homocigóticos).
Por el contrario, ninguno de los 1343 del grupo de infectados era homocigótico para el mutante de deleción.
La diferencia, que indicaba que la homocigosis para la deleción protegía contra el VIH, era muy significativa estadísticamente, no una casualidad.

Además, la aparente protección que proporcionaba el tener sólo alelos CCR5 mutantes no dependía de la vía de infección, ya que entre los homocigóticos que no resultaban infectados había homosexuales, hemofílicos y drogadictos.
Sospechábamos que la homocigosis para la deleción protegía a los pacientes porque determinaba la producción de proteínas CCR5 truncadas, incapaces de alcanzar la superficie celular o tan deformadas, que no reconocían al VIH.

A las pocas semanas de remitir a Science un artículo con estos resultados, nos enteramos de que había otros grupos buscando polimorfismos en los receptores para quimiocinas.
Nathaniel R. Landau y Richard A. Koup, del neoyorquino Centro Aaron Diamond de Investigaciones sobre el Sida, habían descubierto de forma independiente el mismo alelo con la deleción de 32 bases.
Habían estudiado un grupo de homosexuales con un riesgo alto de exposición al VIH y que no habían resultado infectados.
El examen de los leucocitos de dos de esos individuos reveló la ausencia de la proteína CCR5 de la superficie celular.
Un examen de la secuencia de nucleótidos de los genes CCR5 puso de manifiesto que los dos individuos eran homocigóticos para la deleción.
Además, en otro trabajo, Samson y el equipo de Parmentier habían buscado sin éxito homocigóticos para la deleción en un grupo de 743 infectados con VIH. (Esos dos artículos salieron publicados en agosto de 1996 y el nuestro en septiembre.)
Otros estudios resaltaban la ausencia de homocigóticos entre africanos, asiáticos o afroamericanos.
Señalaban que entre el 1 y el 2 por ciento de los caucasianos americanos (descendientes de europeos o de asiáticos occidentales) eran homocigóticos para la mutación.
Además, cuando analizábamos los genotipos de personas sin infectar que habían corrido un riesgo extremado de infección vírica (por reiterada actividad sexual sin protección o por haber recibido dosis elevadas de factores de coagulación contaminados con VIH durante el tratamiento de hemofilia), observamos que hasta el 20 por ciento de esos individuos eran homocigóticos para la deleción.
La causa de la resistencia a la infección en el 80 por ciento restante debía tener algún origen genético distinto o era de origen no genético. 

Cabía, pues, pensar que, si dos genes CCR5 mutantes proporcionaban una protección completa contra el VIH, la presencia de un alelo mutante y otro normal proporcionaría una protección parcial, al reducir a la mitad el número de proteínas CCR5 funcionales sintetizadas por una célula.

Cuando analizamos el tiempo transcurrido entre infección y aparición de los síntomas propios del sida, comprobamos que la manifestación de síntomas claros del sida se retrasaba entre dos y tres años en los individuos infectados que portaban un alelo mutante.
Este retraso era apreciable tanto en los varones homosexuales como en los hemofílicos. 
El genotipo heterocigótico (que presentaba el 20 por ciento de los norteamericanos caucasianos, aproximadamente) también retrasaba el momento en el que los niveles de células T CD4 bajaban de 200 por milímetro cúbico de sangre.

La emoción era grande.
La deleción, cuando se heredaba de ambos progenitores, ofrecía una vigorosa protección genética contra el VIH, incluso después de repetidas exposiciones al virus.
Y la herencia de una variedad mutante por deleción podía retrasar el desarrollo del sida en los individuos infectados. 

Estos resultados venían a decir que un tratamiento capaz de bloquear la interacción del VIH con la proteína CCR5 normal podría proteger a los individuos sanos frente a la infección del VIH, o retrasar el avance del sida en las personas que hubiesen sido ya infectadas con el virus. 

Durante años, los laboratorios farmacéuticos habían polarizado sus investigaciones sobre terapias anti-sida en el virus, sin prestar la debida atención a la participación de la maquinaria celular del hospedador en el establecimiento crónico de la enfermedad.
Las drogas utilizadas en terapia combinatoria, por ejemplo, salen directamente al paso del desenvolvimiento del propio VIH, impidiendo la operación de algunas de sus enzimas.
A tenor de los resultados genéticos, convenía profundizar en la intervención del hospedador en el desarrollo del sida;
esos nuevos planteamientos se ordenarían a controlar la replicación del VIH en pacientes infectados o a impedir la propia infección. 

Muchos expertos se afanaron en la búsqueda de vías para impedir la interacción entre el VIH y las proteínas CCR5.
En teoría, ese tipo de estrategias podría fundarse en la utilización de sustancias capaces de unirse a gp120 y enmascararla.
En la práctica, sin embargo, la mayoría de las investigaciones se propuso dar con procesos que taponaran el sitio de CCR5 que reconoce el VIH.

Un primer asunto a tener en cuenta era que el bloqueo de CCR5 podría resultar arriesgado, ya que si los macrófagos se vuelven sordos a las llamadas de RANTES y otras quimiocinas relacionadas podría resentirse la inmunidad.
Pero este temor se disipó en seguida.
Los individuos que poseen dos alelos mutantes no presentan problemas inmunitarios ni ninguna patología hística;
muestran una salud envidiable.
Es evidente que otros receptores de quimiocinas son capaces de compensar la falta de CCR5.
Dos de ellos (CCR2B y CCR3) pueden también servir de co-receptores del VIH, aunque en condiciones normales no desempeñan ese trabajo con la eficacia que caracteriza a CCR5.

Entre las estrategias terapéuticas sometidas a consideración se baraja la introducción directa en el organismo de moléculas capaces de hacer inaccesible al VIH el sitio por el que se une a CCR5.
Se citan, entre esas moléculas, las quimiocinas o sus derivados sintéticos.
En ese orden, un equipo internacional ha desarrollado un derivado químicamente modificado de RANTES, muy prometedor en el tubo de ensayo.
Revisten parejo interés los antibióticos sintéticos, moléculas inmunitarias de mayor tamaño que podrían dirigirse específicamente al CCR5 para impedir la unión del VIH.

Se habla, asimismo, de la vacunación con fragmentos de CCR5.
Con esa práctica, el sistema inmunitario del receptor se vería inducido a fabricar sus propios anticuerpos contra CCR5.
Y se ensaya con el recurso a la ingeniería genética para producir macrófagos con nuevos genes cuyos productos impidan la síntesis de CCR5 o que lo inutilicen como sitio de unión para el VIH. 

Para quienes se enfrentan a una muerte inminente -con sida terminal y linfomas-, nuestro grupo está considerando modificar un tratamiento radical, cada vez más utilizado en el tratamiento de casos avanzados de leucemias o cánceres de mama.

Cuando se trata de abordar esos cánceres, los pacientes se someten a dosis altas de quimioterapia o radioterapia para las células tumorales.
Puesto que la terapia destruye las células hematopoyéticas de la médula ósea (entre ellas, las que originan el sistema inmunitario), los médicos vuelven a reconstituir el sistema inmunitario del paciente introduciéndoles médula sana compatible. 

¿Y en el caso del sida?
Nosotros nos proponemos destruir todas las células sanguíneas infectadas por el virus y reintroducirle al paciente médula ósea de donantes homocigóticos para la deleción del gen CCR5 .

Este último paso coadyuvaría a proteger al paciente contra nuevas infecciones del VIH y a impedir también la propagación célula-célula de cualquier partícula de VIH que hubiese sobrevivido al tratamiento antivirus.
Eso pensamos.

Aunque la idea de simultanear curación y protección contra VIH residuales o secuestrados resulta muy atractiva, hay que andar con tiento antes de emprender una terapia de médula ósea.
Por varias razones importantes.
En primer lugar, los transplantes de médula ósea encierran en sí mismos un riesgo:
las diferencias inmunitarias entre donante y receptor pueden originar un rechazo del transplante o, lo que es peor, pueden hacer que las células inmunitarias de la médula donante ataquen a los tejidos del receptor y maten al paciente.

Además, en los últimos meses, se han descubierto individuos que son homocigóticos para la deleción y, pese a ello, se han infectado con el VIH.

Ignoramos el motivo y el mecanismo de ello, si bien ciertos indicios inducen a sospechar que el agente era una estirpe T -trópica insólita, poderosamente virulenta, que suele aparecer sólo en los últimos estadios de la infección del VIH.

Creíase hasta ahora que los virus T -trópicos no se propagaban de una persona a otra, porque el sistema inmunitario sano de los individuos que se exponían al virus era capaz de reconocerlos y destruirlos.

Sólo los virus M-trópicos serían capaces de reinfectar con éxito, al multiplicarse en los macrófagos sin que su presencia provoque la destrucción de esas células. 
Algunos indicios sugieren que los pacientes que han resultado infectados, a pesar de ser homocigóticos para la deleción, lo deben simplemente a la mala fortuna de encontrar una estirpe T -trópica rara, capaz de sortear las defensas inmunitarias y provocar la infección, sin necesidad de contar con las estirpes M-trópicas para hacer el trabajo preliminar.
Cabe, además, la posibilidad de que la resistencia innata de los pacientes a las estirpes M-trópicas haya acelerado la transición de las estirpe M-trópicas a tipos T -trópicos capaces de establecer la infección ellos mismos.

Si la resistencia a las estirpes M-trópicas mediada por CCR5 favorece ese cambio de las estirpes VIH, ello significa que los transplantes de médula ósea, y de hecho cualquier tratamiento preventivo o terapéutico pensado para bloquear el acceso del VIH a CCR5, podría resultar perjudicial al estimular, en vez de impedir, la infección y el desarrollo del sida.
Pero no deja de ser tranquilizador el que la mayoría de la gente que es homocigótica para la deleción se libre de la infección, en vez de sucumbir ante los virus T -trópicos. 
No obstante, antes de que los médicos puedan recetar, de forma rutinaria, antagonistas de CCR5, los investigadores deben demostrar que tales fármacos mejoran, y no disminuyen, las probabilidades de supervivencia.

Además de investigar formas eficaces y seguras de sacar provecho de los logros de la genética, los expertos siguen buscando en el genoma otros factores que sugieran nuevas estrategias para proteger al hombre contra la agresión del sida.

Es el caso, por ejemplo, de nuestro propio grupo.
Hemos identificado una variante del gen CCR2B , que, incluso cuando se presenta en una sola copia, retrasa la aparición del sida dos o tres años, igual que ocurre en los heterocigotos para CCR5 .
A principios del año en curso, Jianglin He, del Instituto Dana-Faber de Investigaciones Oncológicas, ha dado a conocer que la proteína CCR3 facilita la entrada del VIH en la microglía (células inmunitarias del cerebro) y que el bloqueo de los receptores impide, en el laboratorio, la infección de dichas células por el VIH.

Tras más de una década de búsqueda de rasgos genéticos implicados en la resistencia al sida, nos sentimos satisfechos de la marcha de la investigación. 

No olvidamos, sin embargo, que el objetivo final es la transformación de los avances genéticos en nuevos procedimientos que permitan evitar o luchar contra el VIH, un virus suficientemente listo como para acabar con todas las células destinadas a eliminarlo.
Aunque las aplicaciones terapéuticas se mueven todavía en el reino de la especulación, confiamos en que el concurso de investigadores capaces procedentes de campos muy dispares terminará por encontrar una receta que invierta el avance implacable de la epidemia de sida.


Propuesta de Decisión del Consejo por la que se adopta un programa específico de investigación, desarrollo tecnológico y demostración en el campo de la biomedicina y la salud (1994- 1998)(94/C 228/09) (Texto pertinente a los fines del CEE) 

COM(94) 68 final 94/0087(CNS) (Presentada por la Comisión el 30 de marzo de 1994) 

 

EL CONSEJO DE LA UNIÓN EUROPEA, Visto el Tratado constitutivo de la Comunidad Europea y, en particular, el apartado 4 de su artículo 130 I, Vista la propuesta de la Comisión, Visto el dictamen del Parlamento Europeo, Visto el dictamen del Comité Económico y Social.

Considerando que el Consejo y el Parlamento Europeo, mediante la Decisión .../.../CE, adoptaron el Cuarto Programa Marco de acciones comunitarias de investigación, desarrollo tecnológico y demostración (en lo sucesivo IDT ) para el período comprendido entre 1994 y 1998, que establece, en particular, las actividades que deben llevarse a cabo en el campo de la investigación en biomedicina y salud; que la presente Decisión se ha adoptado habida cuenta de los motivos expuestos en el preámbulo de dicha Decisión.

Considerando que, según el apartado 3 del artículo 130 I, el programa marco se ejecutará mediante programas específicos desarrollados dentro de cada una de las acciones y que cada programa específico precisará las modalidades de su realización, fijará su duración y preverá el Importe que se estime necesario;

Considerando que el presente programa debe realizarse principalmente por medio de acciones de gastos compartidos, acciones concertadas, medidas específicas, y medidas de preparación, acompañamiento y apoyo.

Considerando que, en virtud del apartado 3 del artículo 1301, deben calcularse los recursos financieros necesarios para la realización del presente programa específico; que la Autoridad Presupuestaria adoptará los importes definitivos de acuerdo con la prioridad relativa establecida con respecto al campo objeto del presente programa en la Primera Acción del Cuarto Programa Marco.

Considerando que la Decisión .../...ICE (Cuarto Programa Marco) establece que el importe global máximo del Cuarto Programa Marco volverá a examinarse como muy tarde el 30 de junio de 1996 con vistas a un posible incremento; que como consecuencia de ese nuevo examen podría aumentar el importe estimado necesario para la realización del presente programa.

Considerando que deben fomentarse las actividades de investigación y desarrollo tecnológicos en biomedicina y salud con el fin de mejorar la eficacia de la política de salud en la Comunidad.

Considerando que el contenido del Cuarto Programa Marco se ha elaborado de acuerdo con el principio de subsidiariedad; que le presente programa específico establece el contenido de las actividades que deben realizarse de conformidad con el principio de subsidiariedad en el campo de la investigación en biomedicina y salud.

Considerando que la Decisión .../.../CE (Cuarto Programa Marco) justifica la realización de una acción comunitaria si, entre otras cosas, la investigación contribuye a aumentar la cohesión económica y social de la Comunidad, favorece el desarrollo global armonioso en su seno y cumple, al mismo tiempo, con el objetivo de calidad científica y técnica; que el presente programa pretende contribuir a la consecución de tales objetivos.

Considerando que el presente programa y su ejecución contribuyen a consolidar las sinergias entre las actividades de IDT que, en el campo de la investigación en biomedicina y salud, realizan centro de investigación, universidades y empresas, en particular las pequeñas y medianas, establecidas en los Estados miembros, así como entre éstas las correspondientes actividades comunitarias de IDT .

Considerando que en el presente programa específico se aplican las normas para la participación de empresas, centros de investigación (incluido el CCI) y universidades, y las normas aplicables a la difusión de los resultados de la investigación que se especifican en las medidas a que se refiere el artículo 130 J.

Considerando que en la ejecución del presente programa, además de la asociación de los países del Acuerdo sobre el Espacio Económico Europeo (EEE), puede ser conveniente realizar, de acuerdo con el artículo 130 M, actividades de cooperación internacional con otros terceros países y organizaciones internacionales.

Considerando que la ejecución del presente programa incluye también actividades de difusión y explotación con los resultados de la IDT , en particular con respecto a las pequeñas y medianas empresas, así como actividades de fomento de la movilidad y formación de investigadores que se realizarán dentro del presente programa en medida necesaria para su correcta ejecución.

Considerando que en la ejecución del presente programa deben establecerse medidas para favorecer la partición de las PYME, en particular medidas de estímulo tecnológico.

Considerando que debe fomentarse el que la investigación- fundamental en el campo de la biomedicina a salud refuerce las bases científicas y tecnológicas de la industria europea de la salud.

Considerando que deben evaluarse la repercusión económica y social y el impacto de los posibles riesgos tecnológicos de las actividades que se realicen en aplicación presente programa.

Considerando que conviene, por un lado, examinar forma permanente y sistemática el estado de realización del presente programa para adoptarlo, cuando sea necesario, a la evolución científica y tecnológica en e campo y, por otro, proceder en el momento oportuno una evaluación independiente del estado de las actividades del programa que proporcione todos los elementos necesarios para establecer los objetivos del Quinto Programa Marco de IDT ; que, por último, al finalizar e programa, conviene realizar una evaluación final de resultados obtenidos con respecto a los objetivos establecidos en la presente Decisión.

Considerando que el CCI puede participar en las acciones indirectas reguladas por el presente programa.

Considerando que se ha consultado al Comité de Investigación Científica y Técnica (CREST), HA ADOPTADO LA PRESENTE DECISIÓN.

 

 

Se adopta un programa específico de investigación, desarrollo tecnológico y demostración en el campo de biomedicina y la salud en la forma descrita en el Anexo I para el período comprendido entre (fecha de adopción del presente programa) y el 31 de diciembre de 1998.

 

El importe estimado necesario para la ejecución del programa asciende a 336 millones de ecus, incluido un 8,5% para los gastos de personal y funcionamiento.

En el Anexo II se ofrece la distribución indicativa de estos recursos.

El importe estimado necesario para la ejecución del programa, ya citado anteriormente, podrá aumentar como consecuencia y en virtud de la Decisión mencionada en el apartado 3 del artículo 1 de la Decisión ../../CE.

La Autoridad Presupuestaria determinará los créditos disponibles en cada ejercicio habida cuenta de las prioridades científicas y tecnológicas establecidas en el Cuarto Programa Marco.

 

En el Anexo III se describen las modalidades de realización del presente programa que no menciona el artículo 5.

 

La Comisión examinará con carácter permanente y Sistemático el estado de realización del presente programa con respecto a los objetivos enumerados en el Anexo 1, contando para ello con la asistencia apropiada de expertos externos independientes.

Comprobará, en particular, si los objetivos, prioridades y recursos financieros siguen estando adaptados a la evolución de la situación.
Llegado el caso presentará propuesta para adaptar o completar este programa en función de los resultados de tal examen.

Como parte de la evaluación global de las actividades comunitarias a que se refiere el apartado 2 del artículo 4 de la Decisión por la que se adopta el Cuarto Programa Marco, la Comisión encargará en su momento a expertos independientes la evaluación de las actividades realizadas en el área o áreas directamente reguladas por el presente programa, así como de su gestión a lo largo de los cinco años que precedan a la evaluación.

Cuando finalice el presente programa, la Comisión encargará a expertos independientes la evaluación final de los resultados obtenidos con respecto a los objetivos enumerados en el Anexo III del Cuarto Programa Marco Y en el Anexo I de la presente Decisión.
El informe de evaluación final se presentará al Consejo, al Parlamento Europeo y al Comité Económico y Social.

 

La Comisión elaborará un programa de trabajo de acuerdo con los objetivos establecidos en el Anexo I y lo actualizará cuando sea necesario.
Hará una relación pormenorizada de los objetivos científicos y tecnológicos y especificará las fases de ejecución del programa, así como la financiación prevista para cada modalidad de realización.

La Comisión elaborará convocatorias de presentación de proyectos sobre la base del programa de trabajo.

 

La ejecución del programa corresponderá a al Comisión.

En los casos que determina el apartado 1 del artículo 7, la Comisión estará asistida por un comité consultivo compuesto por representantes de los Estados miembros y presidido por el representante de la Comisión.

El representante de la Comisión presentará al comité un proyecto de las medidas que deben tomarse.
El Comité emitirá su dictamen sobre este proyecto en un plazo que el presidente puede fijar según la urgencia de la cuestión o, en su caso, mediante voto.

El dictamen constará en acta; además, cada Estado miembro tendrá derecho a solicitar que su posición conste en acta.

La Comisión tendrá cuidadosamente en cuenta el dictamen emitido por el comité e informará a éste de la forma en que ha tenido en cuenta su dictamen.

 

- a la elaboración y actualización del programa de trabajo mencionado en el apartado 1 del Artículo 5,
- a la evaluación de los proyectos y las acciones concertadas de IDT previstos en el anexo III así como del importe estimado de la contribución comunitaria, cuando sea superior a 0,1 millones de ecus por año,
- a las medidas que deben tomarse para evaluar el programa,
- a cualquier ajuste del reparto indicativo del importe que figura en el anexo II y que no haya sido objeto de una decisión presupuestaria. 2. La Comisión informará al Comité, en cada una de sus reuniones, de la evolución de la ejecución del programa en su conjunto

.

 

En virtud del apartado 1 del artículo 228, se autoriza a la Comisión a entablar negociaciones con vistas a la celebración de acuerdos internacionales con terceros países europeos con objeto de asociarlos total o parcialmente al programa.

 

Los destinatarios de la presente Decisión son los Estados miembros.

 

CONTENIDO CIENTÍFICO Y TÉCNICO 

El presente programa específico refleja plenamente las orientaciones del Cuarto Programa Marco, aplica sus criterios y precisa sus objetivos científicos y tecnológicos.

El apartado 4.B del Anexo III, primera acción de dicho Programa Marco, es parte integrante del presente programa.

El presente programa se ejecuta en sinergia con los demás programas específicos en las áreas de las Ciencias y Tecnologías de la vida, así como en las áreas de la telemática, medida y ensayos en investigación socioeconómica.

SITUACIÓN 

La salud es un bien muy preciado para todos los ciudadanos europeos.
Es un sector económico importante que representa 7,25% del PIB y más de 6 millones de puestos de trabajo, es decir el 7% de la población activa.
Hay más de un millón de enfermeras calificadas, 850 000 médicos y 3 millones de camas de hospital que ocupa cada día el 0,8 % de la población.

La investigación es un elemento indispensable de cualquier estrategia que se proponga mejorar la salud de los ciudadanos y la competitividad de la industria de la salud.
Es importante que la investigación se focalice en proyectos que interesan a la comunidad y a los consumidores y que se estimule la transferencia rápida de la investigación hacia la aplicación técnica.

La investigación médica se ve enfrentada al desafío de controlar mejor los grandes azotes como el cáncer, el sida, las enfermedades cardiovasculares, las enfermedades neurológicas y elementales y los problemas derivados de la edad y las minusvalías.
El aumento de los costos de salud se ha convertido en una preocupación de todos los países de la CE, mientras que los ciudadanos europeos piden cada vez más atención de salud de elevada calidad.
El desarrollo de nuevos medicamentos necesita cada vez más tiempo y dinero, principalmente por la necesidad de responder a las exigencias de una reglamentación cada vez más compleja que debe racionalizarse en el contexto internacional.

En lo que se refiere a determinados problemas de salud fundamentales la competitividad de la industria de la salud debe protegerse y mejorarse.
Más que añadir un suplemento a las inversiones sustanciales ya hechas por los Estados miembros y la industria europea, la investigación comunitaria aportará un valor añadido por una acción integrada, por la sinergia de los empeños nacionales y por la interacción de todas las disciplinas de la investigación fundamental en la investigación clínica orientada hacia la resolución de problemas.

Se llevarán a cabo medidas que favorezcan la participación de las PYME, en particular, medidas de estímulo tecnológico y de interacción entre los parques científicos y las PYME de la industria de la salud, según las recomendaciones del Libro Blanco sobre crecimiento, competitividad y empleo.
Al establecer el Programa de Investigación en Biomedicina y Salud BIOMED I dentro del tercer programa marco, más de 6 000 equipos de investigación colaboran dentro de 400 redes creadas para favorecer la cooperación entre equipos de todos los países de la Comunidad Europea y del Espacio Económico Europeo y entre disciplinas complementarias, con el fin de atacar aquellos problemas de salud que difícilmente podrían resolverse en un contexto más restringido.

En el cuatro Programa Marco está previsto ir más allá de la concertación y participar en acciones de investigación de gastos compartidos cuando esté indicado para tareas de investigación concretas.
Hay numerosas prioridades que hay que establecer, teniendo en cuenta las grandes variaciones entre los sistemas de salud nacionales, las estructuras de investigación, las prácticas clínicas y los procedimientos terapéuticos.
Sólo se seleccionarán las propuestas que tengan un concepto científico sólido, una buena probabilidad de éxito y un valor añadido comunitario elevado y contribuyan a la salud y el bienestar de los ciudadanos europeos.

ACTIVIDADES DE IDT PROPUESTAS 

Los objetivos de la investigación sobre el sida, la tuberculosis y otras enfermedades infecciosas, cáncer, investigación farmacéutica, investigación sobre el cerebro e investigación sobre el genoma humano se realizarán mediante una concentración de medios, mientras que los demás objetivos se tratarán principalmente por medio de una concertación.

Investigación sobre sida, tuberculosis y otras enfermedades infecciosas 

Se han obtenido progresos importantes en la lucha contra el sida gracias a la concertación de la investigación sobre esta enfermedad llevada a cabo por la Comunidad Europea.
No obstante, continúa aumentando el número de seropositivos.
La inmunosupresión causada por el HIV en las personas infectadas permite la resurgencia de enfermedades antiguas, a veces ya resistentes a los medicamentos, y la aparición de cánceres oportunistas.

En una sociedad industrial en continuo cambio y que se caracteriza por una nueva permeabilidad de las fronteras, la movilidad de la personas, la inmigración y un cambio en los comportamientos sociales, el sida, la tuberculosis y otras enfermedades infecciosas repercutirán sobre la salud y la calidad de vida en la Comunidad Europea.

Se hará hincapié en la integración de ambos tipos de investigación, fundamental y clínica.
- la investigación viroinmunológica, la genética, la biología molecular y estructural del HIV y su variabilidad;
- la contribución a desarrollar una vacuna segura y eficaz contra el sida así como la definición de marcadores que evalúen su eficacia y sigan la evolución de la enfermedad; 
- la identificación, síntesis y evaluación de los componentes antivíricos y de los medicamentos contra el sida;
- la investigación clínica, centrada en los ensayos clínicos, el tratamiento del sida y de las enfermedades oportunistas. Se estudiarán también el pronóstico y la progresión de estas enfermedades así como las repercusión de los tratamientos;

- los estudios sobre la respuesta del huésped, la patogénesis, los modelos experimentales y pedologías nuevas como los priones, así como el mecanismo de resistencia a los tratamientos convencionales, incluido el problema de las infecciones nosocomiales;
- la prevención de la enfermedad, incluido el desarrollo de nuevos sistemas específicos de vigilancia para determinar el modo de distribución de antiguas y nuevas enfermedades. Se analizarán también los factores de riesgo para desarrollar el sida, las enfermedades oportunistas y los nuevos agentes infecciosos;

- la investigación socioeconómica en el área de los servicios de salud: necesidades en materia de cuidados y prevención, análisis de las consecuencias socioeconómicas y previsiones de su evolución en cooperación con la investigación en salud pública.


Investigación sobre el cáncer 

Mejorar el diagnóstico del cáncer, su tratamiento y su prevención exige integrar en la investigación ambas orientaciones, fundamental y clínica.
Es particularmente importante introducir los avances de la genética celular, molecular y del desarrollo en oncología y en epidemiología del cáncer, de forma que los nuevos datos biológicos referentes a las causas subyacentes del cáncer permitan, a su vez desarrollar nuevos enfoques.
El estudio de las interacciones huésped-tumor en el marco de la respuesta inmunitaria del tratamiento genético focalizado en las células cancerosas es fundamental, así como los son los estudios epidemiológicos que se proponen investigar los factores posibles de la carcinogénesis.

- los mecanismos moleculares de la génesis de los tumores y de las metástasis, incluida la caracterización de los genes y de las proteínas responsables, y, en colaboración con la investigación sobre el genoma humano, los factores genéticos de predisposición al cáncer;
- el control del crecimiento, la diferenciación y la muerte celulares normales y de las anomalías que pueden alterarlas y' por ese motivo, predispone al cáncer, incluido el desarrollo de modelos celulares y transgénicos precisos que convengan a la investigación sobre el cáncer; 
- las respuestas inmunitarias específicas y las posibilidades de detección precoz y de intervención curativa;
- la investigación que apoya la eficacia de las modalidades de tratamientos sistémicos, incluidos los agentes citotóxicos y los que modifican la respuesta biológica;
- la investigación dirigida a mejorar la dosimetría en radioterapia, tanto en el campo de la selectividad balística como en la manipulación de la respuesta de los tumores y de los tejidos normales a la irradiación;
- la calidad de vida como parámetro de evaluación de un tratamiento, incluidos los cuidados en fase terminal y la rehabilitación
.

Investigación farmacéutica

El objetivo general es el desarrollo de las bases científicas y técnicas necesarias para evaluar nuevos medicamentos, en particular los destinados al tratamiento de las enfermedades neurológicas y mentales, inmunológicas y víricas.

Estas acciones van también dirigidas a apoyar las actividades de la Agencia Europea de Evaluación de Medicamentos y a proporcionarle internacionalmente el respaldo de investigación necesario para armonizar exigencias técnicas para el desarrollo de productos farmacéuticos.
La investigación se realizará en colaboración con la industria, los centros de investigación, los hospitales, las universidades y las autoridades responsables del control de la eficacia, la seguridad y la calidad de los nuevos medicamentos.

- La farmacotoxicología: validación previa de métodos alternativos in vitro que puedan utilizar células y tejidos humanos y, en los casos inevitables, modelos animales, con el fin de reducir, mejorar y sustituir la experimentación animal. Se dará preferencia a los ensayos que hayan alcanzado las etapas más avanzadas hacia la validación (como los desarrollados en el marco del programa BIOTECH).
Estos estudios de validación previa deberían, idealmente, seleccionar a los mejores candidatos de estudios de validación propiamente dichos para el Centro Europeo de Validación de Métodos Alternativos (CEVMA).
Se explorará también la contribución de la imaginería funcional a la investigación en neurofarmacología.

- La farmacovigilancia: desarrollo de redes de vigilancia para la detección precoz de los posibles efectos secundarios de los nuevos medicamentos, de acuerdo con el marco reglamentario existente y prestando una atención especial a los esfuerzos internacionales de armonización. Esta investigación comprenderá el estudio de la exposición de los pacientes a los medicamentos, la armonización de los términos y los criterios diagnósticos, el análisis de los signos de efectos secundarios potenciales, el análisis de las estadísticas de morbilidad y mortalidad, estudios transnacionales de control de casos, registros transnacionales y estudios de cohorte.

- Los ensayos clínicos: apoyo a la colaboración intraeuropea en ensayos clínicos aleatorios de calidad científica elevada, con el fin de estimular la elaboración de mejores procedimientos diagnósticos y terapéuticos y contribuir a la evaluación económica del medicamento. El desarrollo de redes europeas de ensayos clínicos de nivel científico elevado contribuiría a una evaluación objetiva de nuevos procedimientos diagnósticos o terapéuticos en menos tiempo y conservando su valor científico.
La investigación en esta área se dirigirá principalmente a la creación de registros de ensayos clínicos, la investigación en las metodologías de metaanálisis y los ensayos clínicos aleatorios para las enfermedades menos conocidas, incluido un inventario de éstas y un depósito de los medicamentos conocidos como huérfanos» disponibles para ensayos clínicos en Europa
.


Investigación sobre el cerebro 

La elevada prevalencia de las enfermedades mentales y la incidencia cada vez mayor de las enfermedades neurodegenerativas representan una carga considerable para los países de la Comunidad Europea y absorben más del 20% de los costos en el área de cuidados de salud.

Las nuevas oportunidades que ofrecen la biología molecular, hacer la genética, la nueva instrumentación y la tecnología de la información contribuirán a lograr progresos importantes en el área de las neurociencias y a mejorar la prevención y el tratamiento.
Se fomentará particularmente la investigación que integre los aspectos fundamentales con las aplicaciones clínicas y el desarrollo industrial.

- la investigación sobre la fisiopatología y los mecanismos básicos responsables de las enfermedades del sistema nervioso, investigación que tendrá que integrar los enfoques moleculares, celulares y clínicos;
- la investigación sobre las lesiones del sistema nervioso central, la regeneración y la plasticidad y el desarrollo de estrategias terapéuticas con e! fin de limitar las lesiones y favorecer la regeneración y la reparación;
- la investigación transdisciplinar dirigida a mejorar la comprensión de la bases inmunológicas y genéticas de las enfermedades del sistema nervioso, en estrecha cooperación con las actividades sobre el análisis del genoma humano y los programas biotecnológicos. Igualmente, la creación de cultivos celulares y, en caso necesario, de modelos animales de enfermedades del cerebro humano para el estudio de la patogencidad y la puesta a punto de agentes terapéuticos;

- la contribución al desarrollo de mejores técnicas de diagnóstico por la imagen cerebral que, combinadas con las ciencias informáticas, contribuirán a una mejor comprensión de las estructuras del cerebro, de sus funciones y de su metabolismo, para determinar la distribución de las proteínas y de otras estructuras en el cerebro y caracterizar las estructuras anatómicas y los mecanismos psicológicos que pueden interaccionar con la función y la disfunción cognitivas. Esta orientación integrará la contribución de varias disciplinas, así como la ingeniería biomédica, y reunirá las tecnologías más avanzadas y las infraestructuras dispersas por toda Europa;

- la investigación sobre los mecanismos del dolor, incluido el desarrollo de nuevos tratamientos y la realización de ensayos clínicos para evaluar la eficacia de los tratamientos disponibles; 
- la investigación sobre el comportamiento de los toxicómanos, que deberán integrar los enfoques fundamental y clínico con el fin de reducir la demanda de drogas;
- el desarrollo de programas que combinen la epidemiología y la prevención a largo plazo a fin de evaluar el impacto de enfermedades neurológicas y psiquiátricas y las ventajas de su tratamiento en las minorías y los grupos de riesgo elevado; 
- la investigación sobre las ciencias cognitivas, incluido el desarrollo de modelos de comportamiento neuronal, el aprendizaje, la memoria y la psicolingüística. 
.

Investigación sobre el genoma humano 

Las realizaciones, actividades e infraestructuras puestas en pie en los precedentes programas deben consolidarse y, si es necesario, adaptarse para responder a las necesidades futuras.
Se fomentará la investigación fundamental y, más particularmente, los estudios funcionales destinados a garantizar que los avances en genética sirvan para mejorar la salud humana.
Se estimulará el desarrollo de tecnologías adecuadas y las aplicaciones que contribuyan al bienestar de los pacientes.
En particular, se realizarán en Europa ensayos para desarrollar la terapéutica genética somática en los casos en que las condiciones y la aceptación justifiquen un esfuerzo focalizado, por ejemplo, en el caso de la mocoviscidosis.

Se estimulará la puesta en común y la armonización de los bancos de datos genéticos, incluida la participación de la Comunidad Europea en la gestión de la base de datos internacional del genoma humano (GDB).
Se mantendrán los contactos con las organizaciones internacionales adecuados o con foros (por ejemplo, HUGO, organización del genoma humano).

La confidencialidad de los datos personales recogidos durante la investigación será conforme al mejor código de protección de datos.
No se hará en este programa ninguna investigación que modifique o se proponga modificar la constitución genética del ser humano alterando las células germinales o una etapa cualquiera del desarrollo embrionario de forma que estas modificaciones resulten hereditarias.

- la cartografía genética y el análisis del genoma, incluyendo la construcción de mapas de transcripción integrados, la secuenciación de regiones cromosómicas específicas y la explotación de enfoques comparativos ;
- en análisis de la función del gen, incluidos la mejora de la técnicas de focalización en los genes y el desarrollo de modelos animales como el ratón;
- el análisis de la regulación genéticas, incluyendo la identificación de secuencias de regulación y el análisis de los mecanismos de regulación de la expresión de genes específicos, sobre todo los implicados en enfermedades; 
- el diagnóstico de enfermedades genéticas, incluyendo los factores no genéticos y el desarrollo de los protocolos de evaluación del riesgo y del consejo genético, insistiendo en la prevención potencial;
- -la terapéutica genética somática, incluyendo el desarrollo de vectores de transferencia de materia genético a las células in vitro , el desarrollo de métodos para introducir genes corregidos in vitro de forma eficaz y segura y la coordinación de los ensayos clínicos sobre la terapéutica genética somática;
- las bases de datos, incluyendo la recogida, el almacenamiento y el análisis de los datos experimentales y el desarrollo de una base de datos integrada sobre el genoma;
- el desarrollo de tecnología, incluida la promoción de la investigación dirigida al desarrollo de métodos adaptados para cumplir todos los objetivos anteriormente citados.
.

Investigación en medicina laboral y ambiental 

Los objetivos consisten en ampliar el conocimiento científico necesario para mejorar la seguridad y la protección de la salud de los trabajadores a fin de evitar los accidentes en el lugar de trabajo y prevenir las pedologías unidas al trabajo así como reducir los riesgos que corre la población por factores ambientales.

- la identificación y el control de los factores de riesgo en el lugar de trabajo y la cuantificación de la exposición, más particularmente a los riesgos biológicos y químicos con efectos a corto y largo plazo;
- el desarrollo de técnicas de gestión de la seguridad, incluida la definición de buenas prácticas de seguridad y la evaluación de su eficacia para reducir la morbilidad; 
- la educación de salud y las medidas preventivas destinadas a reducir los accidentes en el lugar de trabajo y la exposición a factores de riesgo; 
- la interacción entre los factores de riesgo en el lugar de trabajo y en el medio ambiente y la etimología de las enfermedades laborales o en relación con el medio ambiente 
.

Investigación sobre otras enfermedades que tienen una repercusión socioeconómica importante

Para el ciudadano europeo, las aportaciones de actividades horizontales como la biología molecular, la fisiología, la genética, la estadística, la epidemiología y las tecnologías genéricas se miden según los beneficios que consigue como individuo.
Dos tercios de la población mueren de enfermedades cardiovasculares y de cáncer y son objeto de preocupación cada vez mayor las enfermedades infecciosas, como el sida, y las enfermedades que van unidas a las condiciones de trabajo.
La población espera de la integración de ambos tipos de investigación, fundamental y clínica, que dé respuestas que permitan mejorar la prevención, el diagnóstico y el tratamiento de estas enfermedades con fuerte repercusión socioeconómica y de las S 000 enfermedades poco conocidas a las que es más fácil atacar a escala internacional.

Investigación cardiovascular 

Con el fin de acelerar los descubrimientos sobre los mecanismos fisiopatológicos que llevan al desarrollo de enfermedades cardiovasculares y de traducir estos resultados en prevención y tratamientos, se estimulará la investigación multidisciplinar asociando la competencia de médicos y científicos de formación diferente en la investigación cardiovascular fundamental y clínica y en la genética molecular.

- el análisis de los mecanismos celulares y moleculares responsables del desarrollo de las enfermedades cardíacas y vasculares, incluida la investigación sobre el crecimiento, las lesiones y la reparación de las células cardíacas y vasculares y la inflamación cardiovascular asociada;
- la investigación sobre el desarrollo de agentes que actúen sobre la prevención de las lesiones y del crecimiento excesivo, limitando las lesiones y favoreciendo la reparación; 
- la investigación sobre la comprensión de la base genética de las enfermedades cardiovasculares, incluidos la identificación y la decodificación de los genes, la investigación sobre el modo de expresión, el papel funcional de los genes y sus modificaciones y el desarrollo de modelos animales, cuando sea necesario así como de estrategias terapéuticas;
- la investigación clínica, incluida la evaluación, verificación y definición de la contribución que la investigación fundamental ha aportado a la comprensión de la aparición y desarrollo de los fenómenos patológicos; la validación de los programas de detección preclínicos así como de los ensayos clínicos multicéntricos para evaluar los dispositivos y los procedimientos terapéuticos;
- la investigación sobre las técnicas de exploración no invasiva y de diagnóstico por la imagen que permitan estudiar la estructura, el metabolismo y la función del corazón y los vasos sanguíneos; la investigación sobre un programa combinado de prevención a largo plazo y de epidemiología, en el que se incluya la evaluación de eventuales diferencias en los factores de riesgo, el impacto de los factores psicosociales en la incidencia y la prevalencia de las enfermedades cardiovasculares y las ventajas de su tratamiento en los grupos de riesgo elevado 
.

Investigación sobre enfermedades crónicas y problemas derivados del envejecimiento 

Vista la importancia socioeconómica que tiene el hacerse cargo de las enfermedades crónicas, la investigación se dirigirá en particular a las enfermedades autoinmunes y a la inmunogenética, así como las alteraciones de las células T; se dará prioridad a la investigación transdisciplinar e integrada.

Investigación sobre enfermedades poco frecuentes 

Las enfermedades poco frecuentes son enfermedades (alrededor de 5000 tipos) en las que el principio de subsidiariedad encuentra una aplicación evidente.

Ningún país por sí solo dispone del número y la diversidad de casos y ningún país por sí mismo puede permitirse gastar los recursos necesarios para realizar la investigación fundamental y clínica sobre estas enfermedades.
Sin embargo, a escala de la Comunidad Europea y desde el punto de vista científico general, estos casos «excepcionales» se vuelven relativamente parecidos y ofrecen posibilidades de experimentación que permiten efectuar una investigación a fondo sobre los mecanismos de base de las enfermedades y las minusvalías y ofrecen posibilidades de poner en conexión la investigación genética con la expresión bioquímica y física de la enfermedad.
Ejemplos de esto son la talasemia, enfermedades metabólicas congénitas debidas a peroxisomas anormales, etc. Entre las actividades se incluirá un inventario de estas enfermedades poco frecuentes y, en colaboración con la parte del programa que se refiere a la investigación sobre medicamentos, la constitución de un banco de medicamentos «huérfanos» con vistas a la investigación clínica.

Investigación en salud pública, incluida la investigación sobre servicios de salud 

Podrían evitarse más de 110000 muertes antes de los 65 años, debidas a 21 afecciones comunes, si simplemente cada región europea procurase alcanzar la tasa de mortalidad nacional más baja registrada para cada una de estas afecciones.

Las tradiciones, las prácticas y la organización jurídica y administrativa de los servicios y los sistemas de salud pública son tan diferentes en los Estados miembros que parece irrealizable una armonización en esta área.
No obstante, los objetivos de la investigación deberían consistir en ayudar a las Estados miembros a reforzar la información sobre temas de salud pública, ayudar a formular los objetivos, las políticas y las estrategias y ponerlos en práctica, contribuyendo así a la cohesión de las medidas tomadas para proteger la salud en la Comunidad Europea.

- la investigación sobre la educación en materia de salud y sobre la prevención, atención primaria y evaluación de las necesidades, incluidas las de los nuevos grupos en situación de dependencia; evaluación de la eficacia de las iniciativas tomadas en política de salud y de las tecnologías de la salud; 
- la coordinación y la comparación de la base de datos europeas en materia de salud; 
- el impacto del mercado interior en la atención de salud dentro de la Comunidad; la reglamentación y la desreglamentación así como el equilibrio entre los sistemas de salud financiados por los sectores públicos y privados; 
- los aspectos económicos y de organización de los sistemas de salud; 
- la definición de una orientación europea para la introducción de nuevas tecnologías en los servicios de salud 
.

Investigación sobre la tecnología y la ingeniería biomédicas 

La evaluación de las tecnologías de salud y la investigación prenormativa se vuelven cada vez más importantes en un gran mercado europeo, con sus directivas referentes a productos médicos y las actividades de normalización que la «acompañan».

Tanto para la industria como para aquéllos que deciden a cualquier nivel, además de las consideraciones en materia de seguridad es esencial tener acceso en el momento oportuno a una información objetiva sobre la eficacia de los productos médicos, antes de introducirlos en el mercado de la salud.

- las técnicas y la robótica de intervención mínima: robótica,imágenes tridimensionales, tecnologías de las microestructuras y nanotecnología para su aplicación a la cirugía, así como para ampliar la gama de indicaciones clínicas de las intervenciones médicas mínimas; nuevas orientaciones ergonómicas de los quirófanos para la cirugía poco invasiva;
- las técnicas de diagnóstico por la imagen: resonancia magnética,ultrasonidos, biomagnetismo, tomografía por emisión de positrones, etc., así como diagnóstico por la imagen por microondas y espectroscopia, y diagnóstico por infrarrojos. Evaluación integrada y comparativa de las diversas tecnologías de imaginería biomédica;

- la investigación sobre los biosensores, en particular en relación con su valor clínico, como ocurre en el control de la glucosa para la diabetes y en el del oxígeno y los iones en los cuidados intensivos;
- responder a las necesidades de la población, cada vez mayor, de los ancianos, así como de los minusválidos, exige aumentar la investigación en tecnología de rehabilitación. La tendencia muy marcada hacia la cirugía de sustitución en medicina incita a investigar más en biomateriales, en particular para mejorar las propiedades mecánicas y la biocompatibilidad de los polímeros así como sobre los órganos artificiales, como el corazón artificial y el páncreas artificial.
Es también necesario, por las mismas razones, fomentar la investigación dirigida a la modelización de las funciones humanas, así como la investigación en biomecánica, hemodinámica y biorreología;

- la ingeniería celular: poner a trabajar conjuntamente la biología celular y molecular y la ingeniería clínica, mecánica y eléctrica permite obtener resultados que abren nuevas posibilidades de aplicación clínica. Los campos de interés serán: las tecnologías referentes a las investigaciones celulares, la orientación de las células, por ejemplo, para aplicaciones en las redes neuronales y la estimulación funcional, la ingeniería de tejidos que permita poner a punto sistemas que imiten a los biológicos, sensores que se basan en las interacciones celulares gracias al reconocimiento a nivel molecular, interacción entre células y energías y utilización de materiales biológicos y de sus propiedades, por ejemplo, para los dispositivos electrónicos y las superficies biocompatibles

.

La investigación en ética biomédica 

La investigación en ética biomédica, de naturaleza horizontal, se interesará por las normas generales del respeto a la dignidad humana y de la protección del individuo en el contexto de la investigación biomédica y sus aplicaciones clínicas.
Estudiará el impacto social y la percepción por parte del público de los problemas asociados al progreso biomédico.

- la procreación médicamente asistida, incluyendo la predeterminación del sexo, el diagnóstico preimplantatorio y prenatal, la investigación sobre embriones, la utilización de tejido ovárico fetal, el embarazo posmenopáusico y la donación de esperma y ovocitos; 
- el análisis del genoma humano y sus aplicaciones clínicas, incluidos el diagnóstico, la detección y la somatoterapia;
- el fin de la vida, incluyendo los cuidados paliativos, la eutanasia, la prolongación artificial de la vida por técnicas médicas y la reanimación;
- la asignación de recursos: aspectos éticos y sociales de las opciones que deben tomarse en materia de presupuesto de salud y asignación de recursos; 
- el transplante: utilización de tejidos y órganos humanos, así como la organización de bancos de tejidos y órganos humanos; 
- el consentimiento informado del paciente en cuanto al diagnóstico, el tratamiento, la prevención o la investigación, incluido el consentimiento de poblaciones vulnerables como los prisioneros o los pacientes incapacitados; 
- el carácter confidencial y privado de la información médica, genética o no, insistiendo sobre el problema específico que plantean los sistemas modernos de información, como la informatización y la transferencia automática de datos
.

Objetivos tratados mediante actividades horizontales 

Las actividades de investigación en Ética Biomédica del programa BIOMED II y las actividades sobre los aspectos éticos, jurídicos y sociales de la biomedicina de la unidad horizontal «Aspectos Éticos y Jurídicos» se realizarán conjuntamente a fin de beneficiarse de competencias interdisciplinares.

Con el fin de mejorar el diálogo y la comprensión mutua entre las diferentes posiciones sociopolíticas y bioéticas nacionales, en el respeto de las diferencias culturales que existen entre estados, se crearán grupos de trabajo para preparar informes y estudios comparativos que interesen al Parlamento Europeo y al Consejo.

Para seleccionar y discutir áreas de divergencia nacional e internacional, se pondrán en marcha talleres bien definidos y, si es necesario, actividades de investigación que tengan una orientación multidisciplinar.
Se apoyarán también las acciones que tengan como objetivo la percepción por parte del público de las nuevas tecnologías.
Esta actividad horizontal tiene en cuenta la Convención Europea de Bioética y sus protocolos adicionales que se están elaborando.

La experimentación y los ensayos en animales deberían sustituirse en lo posible por métodos in vitro o de otro tipo.
En ninguno de estos tres programas específicos se llevará a cabo ningún tipo de investigación que modifique o tenga como finalidad modificar la construcción genética de los seres humanos mediante cambios en las células germinales o en cualquier otra fase del desarrollo embrionario que hagan hereditarias esas modificaciones; tampoco se hará ningún tipo de investigación que tenga por objeto sustituir un núcleo de una célula de un embrión tomado en una célula de una persona, otro embrión o del desarrollo posterior de un embrión (clonación).

Las actividades de demostración en el marco de la Investigación en Biomedicina y Salud facilitarán los ensayos comparativos multicéntricos europeos de nuevos medicamentos, las nuevos orientaciones terapéuticas y los prototipos preparados para ensayar nuevos equipos médicos.
Se prestará una atención especial a la demostración de las tecnologías más recientes en diagnóstico clínico y por la imagen, de los detectores implantables para el seguimiento y la reeducación de estados patológicos, de órganos artificiales, materiales biocompatibles, nuevos tratamientos de cáncer y técnicas de irradiación.
Se utilizará un método que vaya de abajo arriba, en colaboración con los demás programas de las Ciencias de la Vida, para localizar las mejores oportunidades de demostración precompetitiva, con el fin de demostrar la viabilidad técnica de estas nuevas tecnologías así como, si es el caso, su ventaja económica.
La implicación precoz de hospitales y médicos clínicos en esta demostraciones permitirá una difusión eficaz de los conocimientos y una rápida apreciación de los beneficios que pueden sacarse de adoptar en la práctica métodos, medicamentos y equipos tan innovadores.
Manteniendo como primera prioridad el beneficio para el paciente, las demostraciones en estas áreas tendrán en cuenta las necesidades específicas de la industria farmacéutica y de la ingeniería biomédica y se organizarán con ayuda de una colaboración que incluye a las industrias de fabricación, las profesiones médicas, quienes prestan atención médica y las autoridades de salud.

 

Tabla

Distribución indicativa de los recursos

La distribución entre las diferentes áreas no excluye que los proyectos puedan referirse a varias de ellas.

MODALIDADES DE REALIZACIÓN DEL PROGRAMA 

Las modalidades de participación financiera de la Comunidad son las previstas en el Anexo IV de la decisión relativa al Cuarto Programa Marco.

Las modalidades de participación de las empresas, centros de investigación y universidades, así como las modalidades de difusión de los resultados se precisan en las medidas previstas en el artículo 130 J. 

No obstante, se aplicarán a la ejecución del presente programa las precisiones siguientes: la participación en el programa está abierta, con ayuda financiera de la Comunidad a todas las entidades jurídicas establecidas y que ejercen habitualmente actividades de IDT , en la Comunidad o en un país tercero asociado, total o parcialmente, a la ejecución del programa en cuestión, como consecuencia de un acuerdo firmado entre la Comunidad y dicho país tercero; al Centro Común de Investigación.

La participación en el programa está abierta, sin ayuda financiera comunitaria, y a condición de que su participación presente algún interés para las políticas comunitarias: a las entidades jurídicas establecidas en un país que haya firmado con la Comunidad un acuerdo de cooperación científica y técnica relativo a acciones abarcadas por el programa, a condición de que esta participación sea conforme a las disposiciones del acuerdo susodicho, a las entidades jurídicas establecidas en un país europeo, a las organizaciones internacionales de investigación.

La participación en el área «Análisis del Genoma Humano» está abierta, sin ayuda financiera de la Comunidad, a toda entidad jurídica a condición de que su participación presente algún interés para las políticas comunitarias.

La participación de las organizaciones internacionales europeas podrá financiarse sobre las mismas bases que la de las organizaciones comunitarias en casos debidamente especificados.

El presente programa se realiza en forma de Participación financiera de la Comunidad en actividades de IDT ejecutadas por terceros o por las instituciones del CCI en asociación con terceros.

Acciones de gastos compartidos que abarcan las modalidades siguientes: 
- los proyectos de IDT ejecutados por las empresas, centros de investigación y universidades, incluidos los consorcios para proyectos integrados que los reagrupan alrededor de un tema común; 
- los proyectos de investigación fundamental dentro de redes temáticas que se--crearán entorno a tecnologías genéricas de importancia estratégica y asociarán a empresas, centros de investigación y universidades; 
- la estimulación tecnológica dirigida a fomentar y facilitar la participación de las PYME mediante la concesión de una prima que cubre la fase exploratoria (incluida la búsqueda de socios) de una acción de IDT y mediante la investigación cooperativa. Esa prima se concederá tras seleccionar los bocetos de propuestas que puedan presentarse en todo momento; 

- el apoyo a la financiación de infraestructuras o instalaciones indispensables para realizar una acción de coordinación (actividad reforzada de coordinación);
- las acciones de demostración definidas en el Anexo III del Programa Mareo, dirigidas, mediante primas de viabilidad y subvenciones directas a los tecnólogos, entre otras cosas, a superar los obstáculos concretos que impidan la utilización de las nuevas tecnologías y establecer nexos de unión entre productores y usuarios de tecnologías.

Acciones concertadas, que consisten en coordinar, sobre todo en forma de redes de concertación de los proyectos de IDT ya financiados por autoridades públicas u organismos privados.
La acción concertada puede servir también para la coordinación necesaria al funcionamiento de las redes temáticas que, mediante proyectos de IDT de acciones de gastos compartidos &lsqb;véase el primer guión de la letra a) del apartado 2.1&rsqb; reagrupan, alrededor de un mismo objetivo tecnológico o industrial, a fabricantes, usuarios, universidades y centros de investigación.

Medidas específicas, como las medidas en favor de la normalización, y medidas dirigidas a poner en marcha instrumentos generales al servicio de los centros de investigación, las universidades y las empresas.
La participación de la Comunidad cubre hasta el 100% de los costos de las medidas.

Medidas de preparación, acompañamiento y apoyo que abarcan las modalidades siguientes:
- estudios de apoyo del presente programa y de preparación de eventuales acciones futuras;
- conferencias, seminarios, talleres u otras reuniones científicas o técnicas, incluidas las reuniones de coordinación intersectorial o multidisciplinar;
- recurso a capacidades de pericia externas, incluido el acceso a bases de datos científicos; 
- publicaciones científicas, incluyendo la difusión, la promoción y la explotación de los resultados (en coordinación con las actividades realizadas por la tercera acción); 
- estudios de evaluación de las consecuencias socioeconómicas y de los riesgos tecnológicos eventuales asociados al conjunto de los proyectos del presente programa;
- actividades de formación asociadas a las investigaciones que abarca el programa;
- evaluaciones independientes (incluidos estudios) de la gestión y de la realización de las actividades del programa;
- medidas de apoyo al funcionamiento de redes de sensibilización y asistencia descentralizada en favor de las PYME en coordinación con la acción Euromanagement-auditorías de IDT .

Las actividades relativas a la difusión y la explotación de los resultados conseguidos por este programa serán complementarias de las llevadas a cabo por la acción y se pondrán en práctica en estrecha coordinación con ésta.
Los colaboradores de los proyectos IDT constituyen redes privilegiadas de difusión y de explotación de resultados y se verán reforzados por publicaciones, conferencias, promoción de resultados, estudios potencialidades tecnicoeconómicas, etc .
Con el fin de garantizar una explotación óptima, deberán tenerse en cuenta los factores que puedan favorecer una utilización ulterior de los resultados, ya desde el comienzo del los proyectos IDT y a todo lo largo de los mismos.


Búsqueda de genes para el diseño de nuevas medicinas

La identificación de genes humanos implicados en enfermedades permite a los investigadores producir proteínas potencialmente terapéuticas y acelerar el desarrollo de fármacos eficaces

La mayoría de los lectores están familiarizados con la idea de que un gen es algo que transmite caracteres hereditarios de una generación a la siguiente.
Lo que quizá no sepan es que la causa de la mayoría de las enfermedades, no sólo las hereditarias, se debe a un mal funcionamiento de los genes.
En el cáncer, aterosclerosis, osteoporosis, artritis y enfermedad de Alzheimer, por ejemplo, se producen cambios específicos en las actividades de ciertos genes. 
Las enfermedades infecciosas suelen también provocar la activación de algunos genes del sistema inmunitario del paciente.
Por último, la acumulación de daños en los genes, como resultado de toda una vida de exposición a radiaciones ionizantes y agentes químicos dañinos, guarda probable relación con cambios que se producen durante el envejecimiento.

Si supiéramos cuándo y en que parte del cuerpo humano se activan los genes, razonamos unos colegas y yo hace algunos años, podríamos aplicar esos conocimientos para predecir, prevenir, tratar y curar enfermedades.
Cuando un gen se activa, o se "expresa", como dicen los genéticos, la secuencia de unidades químicas, o bases, de su ADN dicta las órdenes necesarias para fabricar una proteína específica.
Las proteínas dirigen todas las funciones celulares.
Actúan como componentes estructurales, como catalizadores que llevan a cabo los múltiples procesos químicos de la vida y como elementos de control que regulan la reproducción y especialización celular, así como la actividad fisiológica en todos sus niveles.
El desarrollo de un ser humano desde un huevo fecundado hasta el adulto maduro es, en última instancia, el resultado de una serie de cambios ordenados en el patrón de expresión génica en los diferentes tejidos. 

Saber cuáles son los genes que se expresan en los tejidos sanos y enfermos nos permitiría, por un lado, identificar las proteínas necesarias para el normal funcionamiento de los tejidos y, por otro, conocer las alteraciones que se producen en las enfermedades.
Podríamos, por tanto, desarrollar nuevas estrategias para el diagnóstico de algunas enfermedades y crear fármacos capaces de modificar la actividad de las proteínas o genes afectados.
Algunas de las proteínas y genes que identificásemos podrían también utilizarse por otros investigadores.
Lo que estábamos imaginando venía a ser una suerte de anatomía molecular.

Teníamos claro desde el principio el enorme trabajo que supondría identificar todos los genes que se expresan en cada uno de los muchos tipos de tejidos del cuerpo.
Una célula humana encierra unos 100.000 genes, de los cuales sólo una pequeña fracción (unos 15.000) se expresa en cada tipo de célula, si bien los genes que se expresan varían de un tipo celular a otro.
Por esa razón, centrarse sólo en uno o dos tipos celulares no nos revelaría qué genes se están expresando en el resto del cuerpo.
Teníamos que estudiar también tejidos en todos los estadios del desarrollo humano.
Además, para identificar los cambios de expresión génica que contribuyen a las enfermedades, deberíamos analizar tejidos procedentes de individuos enfermos y de individuos sanos. 

Para nuestra fortuna, los avances de la técnica habían facilitado este tipo de trabajo.
Se puede conocer con cierta facilidad qué genes se expresan en determinado tejido.
La estrategia diseñada por nosotros permite, además, identificar con gran rapidez genes de interés clínico.
Fijémonos, por ejemplo, en la aterosclerosis.
Esta enfermedad común se caracteriza por la acumulación de una sustancia grasa, denominada placa, en la luz de las arterias, principalmente en las que alimentan el corazón.
Con nuestra estrategia podemos generar una lista de los genes que se expresan en las arterias normales y saber cuánto se expresa cada uno de ellos.
Podemos comparar esa lista con otra similar obtenido de pacientes con aterosclerosis.
Las diferencias entre las listas nos revelarán los genes (y por tanto las proteínas) implicados en la enfermedad.
Nos indicarán también qué efecto ha ejercido la enfermedad sobre la expresión de los genes, si la ha reforzado o disminuido.
Los investigadores pueden entonces producir, in vitro , las proteínas humanas determinadas por esos genes.

Una vez sintetizada la proteína en su forma pura, se prepara un ensayo para detectar la presencia de la misma en un paciente.
Por seguir con el ejemplo, un ensayo que detectara la sobreproducción de una proteína presente en las placas pondría de manifiesto uno de los signos tempranos de la aterosclerosis, cuando los tratamientos son más eficaces.
Además, los farmacólogos pueden utilizar las proteínas purificadas para fabricar nuevos fármacos.
Un compuesto químico que inhiba la producción de una proteína presente en una placa puede considerarse un fármaco contra la aterosclerosis. 

Nuestra aproximación, que yo denomino genómico-médica, se sale un tanto de las principales corrientes de investigación en genética humana.

Son muchos los expertos enrolados en el Proyecto Genoma Humano, un empeño internacional dirigido a descubrir la secuencia completa de bases químicas del ADN humano.
Esta información, de gran valor para los estudios evolutivos y de expresión génica, resultará especialmente útil para las investigaciones sobre enfermedades hereditarias. 
Sin embargo, el proyecto genoma no es el camino más rápido de descubrir genes, ya que la mayoría de las bases que integran el ADN no forman parte de los genes propiamente dichos.

Tampoco pondrá de manifiesto el proyecto genoma qué genes están implicados en enfermedades.

En 1992 creamos una compañía, Human Genome Sciences (HGS), para poner en obra nuestras ideas.
De entrada se trataría de un proyecto conjunto entre HGS y el Instituto para la Investigación del Genoma. una organización sin ánimo de lucro financiada por HGS.
Al director de ese instituto, J. Craig Venter, se deben algunas de las ideas claves en la investigación genómica.
Seis meses después, SmithKline Beecham. uno de los principales laboratorios farmacéuticos del mundo, se unió al proyecto, para al cabo del primer año seguir trabajando por su cuenta.

Posteriormente se nos asociaron Schering-Plough, Takeda Chemical Industries de Japón, Merck KGaA de Alemania y la francesa Synthelabo.

Alguien podría preguntarse por qué nos preocupábamos por los genes, si lo importante a la hora de desarrollar nuevos fármacos son las proteínas que aquellos determinan, y no los propios genes.
Bastaría con analizar directamente las proteínas producidas por cada célula.
Sin embargo, el simple hecho de conocer la composición proteínica de una célula no nos permite desarrollar fármacos.

Para ello, hay que poder fabricar cantidades substanciales de las proteínas que interesan, lo que exige, a su vez, aislar los genes correspondientes e introducirlos en células que sean capaces de expresar dichos genes en grandes cantidades. 

Nuestra estrategia de búsqueda de genes tiene como punto de partida un producto intermediario crítico, que se forma en las células cuando se expresa un gen.
Ese producto intermediario es el ARN mensajero (ARNm).
Lo mismo que el ADN, consta de secuencias de cuatro bases.

Cuando la célula fabrica el ARNm correspondiente a un gen, copia la secuencia de bases del ADN del gen en cuestión. 
La molécula de ARNm sintetizada sirve luego de molde para construir la proteína específica que el gen cifra.
La importancia del ARNm radica en que la célula lo fabrica sólo cuando el gen correspondiente se halla activo.
Conociendo la secuencia de bases del ARNm podemos aislar el gen y fabricar la proteína que cifra.

Pero no resulta fácil manipular el ARNm.
De ahí que no trabajemos con éste, sino con copias estables de ADN obtenidas a partir de dichos ARNm.
Estas copias se denominan ADN complementario (ADNc) de las moléculas de ARNm.
Para fabricar los ADNc seguimos un proceso inverso al que cursan las células para fabricar ARNm a partir del ADN.

Las copias de ADNc así fabricadas suelen ser réplicas de fragmentos de los ARNm, no de las moléculas completas, ya que éstas pueden llegar a tener varios miles de bases. 

De hecho, podemos obtener ADNc procedentes de distintas partes de un mismo gen.
No obstante, en la práctica, podemos estar seguros de que un ADNc que contenga poco más de mil bases es suficiente para identificar en exclusiva a un gen.
Y es así porque resulta altamente improbable que dos genes diferentes compartan una secuencia idéntica de miles de bases.
De la misma manera que un capítulo de un libro tomado al azar identifica inequívocamente a la obra, una molécula de ADNc identifica al gen de donde procede.

Una vez que hemos conseguido un ADNc, podemos hacer las copias que queramos.
De esa manera podemos conseguir suficiente material para poder determinar su secuencia de bases.

Como conocemos las reglas que las células utilizan para traducir secuencias de ADN en las secuencias de aminoácidos que constituyen las proteínas, el orden de las bases nos dicta la secuencia de aminoácidos del correspondiente fragmento de proteína.

Posteriormente, esa secuencia se compara con todas las secuencias de proteínas conocidas, lo que a veces aporta información sobre la función de la proteína completa, ya que las proteínas que comparten secuencias de aminoácidos similares suelen realizar funciones parecidas. 

La de analizar secuencias de ADNc puede convertirse en una tarea lenta, si bien en los últimos años se han desarrollado instrumentos que ejecutan el trabajo de una forma fiable y automática.
Necesitábamos, además, otras mejoras técnicas para que nuestra estrategia fuera factible.
Los equipos de secuenciación, cuando operan a la escala que estábamos contemplando, generan una cantidad inmensa de datos.
La suerte es que ahora contamos con máquinas capaces de manejar esa información y estamos desarrollando programas que nos ayuden a encontrarle sentido a tamaño aluvión de datos.

Para identificar los genes que se expresan en una célula, se analizan las 300 a 500 primeras bases de uno de los extremos de cada molécula de ADNc.
Esas secuencias parciales de los ADNc sirven de marcadores de los genes y a veces se denominan "secuencias expresadas marcadas".
Hemos elegido esa longitud para nuestras secuencias parciales de ADNc porque es lo suficientemente corta como para que el análisis sea rápido, pero también lo suficientemente larga como para que identifique a un gen sin ambigüedades.
Si asociamos una molécula de ADNc a un capítulo de un libro, una secuencia parcial sería la primera página del capitulo.
Con ella se puede identificar el libro e incluso nos puede dar una idea sobre su argumento.
De la misma manera, las secuencias parciales de ADNc pueden decirnos cosas sobre el gen del que proceden.
En HGS producimos, cada día, casi un millón de bases de secuencias "en rama".

Nuestro método es muy eficaz.
En cinco años no completos hemos identificado miles de genes, muchos de los cuales podrían intervenir en alguna enfermedad.
Otros investigadores han iniciado también programas para generar secuencias parciales de ADNc.

Los ordenadores de HGS reconocen que muchas de las secuencias parciales que producimos corresponden a algunos de los 6000 genes que otros expertos han identificado por medios distintos.
Otras corresponden a un gen que nosotros habíamos identificado con anterioridad.
Cuando no podemos asignar una nueva secuencia parcial a un gen ya conocido, la cosa se vuelve más interesante.
En esos casos, nuestros ordenadores realizan una búsqueda en todas las bases de datos disponibles, incluidas las nuestras, para comprobar si la nueva secuencia parcial se solapa con alguna previamente descrita.
Cuando encontramos un solapamiento claro, disponemos las secuencias parciales solapantes en segmentos "contiguos", de longitud creciente. 
Los contiguos son, por tanto, secuencias incompletas que, según inferimos, deben estar presentes en alguna parte de algún gen.
Este proceso evoca al que podríamos llevar a cabo si tomamos de un texto frases como "..lugar de la Mancha.." y "...Mancha, de cuyo nombre.." y las combinamos después en un fragmento reconocible de El Quijote , de Miguel de Cervantes.

Al mismo tiempo, intentamos deducir la función verosímil de la proteína correspondiente a la secuencia parcial.
Una vez realizada la predicción sobre la estructura de la proteína, la clasificamos según su parecido con las estructuras de otras proteínas conocidas. 
A veces encontramos que una secuencia coincide con otra proteína humana, pero no es raro que se parezca a la de una bacteria, hongo, planta 0 insecto.

Otros organismos producen muchas proteínas con funciones similares a las humanas.
Nuestros ordenadores están continuamente actualizando estas clasificaciones provisionales. 

Hace unos tres años, teníamos caracterizados cuatro contiguos.

Tras compararlos con las bases de datos, hicimos la predicción de que los genes correspondientes a cada uno de ellos deberían producir proteínas similares a las que en bacterias y levaduras se encargan de corregir mutaciones que se producen en el ADN.

Nos decidimos a caracterizar la secuencia completa de los cuatro genes, que presumíamos importantes, ya que se sabía que los fallos en la reparación de mutaciones podían producir cáncer de colon.
Cuando cierta autoridad en cáncer de colon nos pidió ayuda para identificar genes que pudiesen intervenir en esa enfermedad, de los cuales él conocía uno de ellos, pudimos avanzarle que ya estábamos trabajando con otros tres candidatos posibles. 

Posteriores investigaciones han confirmado que mutaciones en cualquiera de esos cuatro genes pueden causar cáncer de colon, ovario o endometrio.
Una de cada 200 personas, de América del Norte y Europa, porta una mutación en uno de esos genes encargados de reparar errores que se producen en el ADN.
Sabiendo esto los expertos pueden poner a punto pruebas para analizar ese tipo de genes en las personas que tienen algún familiar con esos tipos de cánceres.
Si la persona analizada presenta alguna predisposición genética a la enfermedad, se la somete a un seguimiento más estrecho. 
La detección precoz de un tumor resulta vital, ya que se puede eliminar quirúrgicamente.
Este tipo de pruebas se utilizan ya en programas de investigación clínica para identificar personas que presentan algún riesgo.

Nuestras bases de datos recogen más de un millón de secuencias génicas parciales derivadas de ADNc agrupadas en 170.000 contiguos.
Creemos que tenemos secuencias parciales de casi todos los genes humanos que se expresan.
Y lo creemos porque. cuando los investigadores registran secuencias génicas en las bases de datos públicas, comprobamos que en el 95 por ciento de los casos nosotros ya poseemos una secuencia parcial de ellas.
Con frecuencia, agrupando varias secuencias parciales se deja al descubierto algún gen nuevo.
En conjunto, más de la mitad de los nuevos genes que identificamos se parecen a algún gen con función conocida.
Con el tiempo, esta proporción habrá de ir en aumento.

Si un tejido produce un número extraordinario de secuencias de ADNc que derivan del mismo gen, se nos está diciendo que ese gen produce una gran cantidad de ARNm.
Esto suele suceder cuando las células sintetizan grandes cantidades de la proteína correspondiente, lo que a su vez es una señal de que la proteína puede estar desempeñando una función vital.
HGS presta también una atención especial a los genes que sólo se expresan en un tipo particular de tejido, o en muy pocos de ellos, pues muy probablemente esos genes tienen que ver con enfermedades que afectan a dichos tejidos.
De los miles de genes que hemos descubierto, hay unos 300 que parecen revestir un interés clínico particular.

Utilizando este tipo de aproximación, basada en secuencias parciales de ADNc, se ha podido estimar cuántos genes participan en las funciones de defensa, metabolismo y otras de parecida importancia celular.
Tamaña cantidad de información sobre secuencias parciales de ADNc que se está almacenando, abre una nueva gama de posibilidades para la biomedicina, muchas de las cuales son objeto de exploración sistemática.

Bases de datos como las nuestras son muy útiles para descubrir proteínas que sirvan como marcadores de enfermedades.
Lo vemos en el cáncer de próstata. 
Uno de los ensayos utilizados para detectar cáncer de próstata se basa en la medición de los niveles en sangre de cierta proteína, el antígeno específico de próstata.
Los pacientes que tienen cáncer de próstata suelen presentar niveles muy altos de esa proteína.

Mas, para nuestro infortunio, los tumores benignos, de crecimiento lento, y los malignos, que precisan de una terapia agresiva, pueden producir niveles elevados del antígeno, razón por la cual el ensayo es ambiguo.

HGS y sus asociados han analizado ARNm de muchas muestras de tejidos sanos de próstata y de tumores de próstata benignos y malignos.

Hemos encontrado unos 300 genes que se expresan en la próstata, pero no en otros tejidos.
De tales genes, casi 100 son activos sólo en los tumores de próstata y una veintena se expresan exclusivamente en tumores considerados malignos. 
Ahora utilizamos esos 20 genes y sus productos proteicas para poner a punto ensayos que nos permitan identificar los tumores malignos de próstata.

Estamos llevando a cabo una estrategia similar para los cánceres de mama, pulmón, hígado y cerebro. 

Las bases de datos de secuencias parciales de ADNc valen también para identificar los genes responsables de enfermedades raras.

Desde hace tiempo se sabe que cierta forma de ceguera infantil se debe a un defecto hereditario relacionado con el metabolismo de la galactosa.
Una búsqueda en nuestra base de datos identificó dos genes humanos, de los que no se tenía noticia, cuyas proteínas correspondientes eran previsiblemente similares a ciertas enzimas que, en levaduras y bacterias, aparecían implicadas en el metabolismo de dicho azúcar.
No tardó en confirmarse que ese tipo de ceguera venía determinado por un defecto hereditario en uno u otro de esos dos genes.
En el futuro, las enzimas o los propios genes podrían emplearse para prevenir la enfermedad.

Las secuencias parciales de ADNc constituyen una herramienta poderosísima para la búsqueda de micromoléculas con capacidad terapéutica.

Los métodos para crear y ensayar esos fármacos diminutos han mejorado espectacularmente en los últimos años.
Hay equipos automáticos que analizan en un santiamén compuestos sintéticos y naturales y comprueban si afectan a una proteína humana implicada en alguna enfermedad.

No obstante, el limitado número de posibles proteínas diana frena el desarrollo de ese tipo de estrategias.

Cuantas más proteínas humanas se investiguen, más se podrá avanzar en esa línea.
Y nuestro trabajo está colaborando a ello.
De hecho, suministramos a Smith-Kline Beecham más de la mitad del material que utilizan en la búsqueda de nuevos productos de interés potencial.

Las bases de datos del estilo de las nuestras no sólo facilitan el escrutinio aleatorio en busca de moléculas eficaces.
El conocimiento de la estructura de una proteína permite diseñar drogas que interaccionen de forma específica con la proteína.
Se acude a esta técnica, denominada diseño racional de fármacos, para crear nuevos inhibidores de proteasas que han mostrado su eficacia contra el VIH (nuestras bases de datos no han tenido que ver en ello).
Confiamos en que las secuencias parciales de ADNc posibilitarán una atención mayor al diseño racional de fármacos.

En los osteoclastos podemos ejemplificar el servicio que prestan nuestras bases de datos.
Estas células habituales de los huesos producen una enzima capaz de degradar el tejido óseo.
Ciertas enfermedades, as' la osteoartritis y la osteoporosis, producen la enzima en demasía.
En nuestros ordenadores encontramos una secuencia de un gen que se expresaba en osteoclastos, que parecía cifrar la enzima destructora.

Su secuencia era similar a la de un gen conocido que determina una enzima degradadora del cartílago.
Por último, confirmamos que el gen de osteoclastos era el responsable de la enzima degradadora y demostramos que no se expresaba en otros tejidos.

Los resultados no carecían de interés:
permitían diseñar procedimientos para atacar la proteína sin preocuparnos de que pudiese perjudicar a otros tejidos.
Fabricamos la proteína y Smith-Kline Beecham la utilizó en un programa de búsqueda de posibles terapias, combinando las técnicas de escrutinio rápido y diseño racional de fármacos.
La compañía se ha apoyado también en nuestros datos para buscar moléculas eficaces contra la aterosclerosis. 

Desde un punto de vista médico, los receptores asociados a proteínas G son un auténtico filón de genes y proteínas.
Las G atraviesan la membrana exterior de la célula, reciben señales biológicas procedentes de otras células y las transmiten al interior celular.
Con toda seguridad, los fármacos capaces de inhibir tales receptores vitales podrán utilizarse para tratar la hipertensión, úlceras, migraña, asma, el resfriado común y trastornos psiquiátricos. 
La HGS ha encontrado más de 70 nuevos receptores asociados a proteínas G.
Ahora estamos comprobando sus efectos, introduciendo en las células los genes para esos receptores y evaluando cómo las células que sintetizan las correspondientes proteínas responden a diversos estímulos. 
Hay dos genes de interés particular:
determinan proteínas que parecen estar críticamente implicadas en la hipertensión y en la diabetes adulta.
Nuestros socios de la industria farmacéutica buscan ya micromoléculas que inhiban las señales biológicas transmitidas por esos receptores.

Por último, creemos que algunos genes y proteínas humanas que estamos descubriendo, quizá con algunas modificaciones, serán ellos mismos la base de algunas nuevas terapias. 
Muchas proteínas humanas se utilizan ya como fármacos. 
La insulina y el factor coagulante para los hemofílicos son dos ejemplos conocidos.
Las proteínas que estimulan la producción de eritrocitos se emplean también para acelerar la recuperación de los pacientes tras quimioterapia. 

Proteínas de unos 200 genes totalmente caracterizados por HGS tienen posibles aplicaciones médicas.

Hemos sintetizado ya la mayoría de esas proteínas y hemos puesto a punto ensayos para medir su actividad celular. 
Algunas están dando también resultados muy prometedores en ensayos con animales de experimentación. 
Entre esas proteínas hay varias quimiocinas, moléculas que estimulan el sistema inmunitario.

El desarrollo farmacéutico nunca será un proceso rápido, porque las medicinas, ya sean proteínas, genes o micromoléculas, deben someterse a ensayos exhaustivos. 
Las secuencias parciales de ADNc pueden, no obstante, acelerar el descubrimiento de nuevos agentes terapéuticos. 

Gracias a la utilización sistemática de los procedimientos automáticos e informáticos desarrollados para buscar genes se ha alcanzado una visión global de dónde se expresan los genes, o lo que es lo mismo, la anatomía de la expresión génica humana. 

Figura 1

PROPORCIONES DE GENES dedicados a las principales tareas que se llevan a cabo en una célula humana,según se deduce de un estudio de 150.000 secuencias parciales.

El criterio utilizado para asignar provisionalmente cada una de las funciones fue la similitud con otros genes, humanos o no, de función conocida.

Figura 2

ROBOT utilizado para distinguir colonias bacterianas que portan secuencias de ADN humano. 

El aparato desecha las colonias azules, que son las que no contienen esas secuencias.
Mediante análisis de las secuencias presentes en las bacterias, se llega a la identificación de los genes humanos.

Figura 3

PROTEINAS HUMANAS producidas a partir de genes descubiertos en Human Genome Sciences. 

Se presentan algunas que han demostrado ya su eficacia en células humanas aisladas y en animales de laboratorio. 
Se están sometiendo a ensayo para evaluar sus posibilidades terapéuticas. 

Tabla 3

Proteína.
Actividad.
Posibles aplicaciones.


Factor de crecimiento de queratinocitos.
Estimula la regeneración de la piel.
Cicatrizar heridas, estimular el crecimiento capilar, proteger contra los efectos secundarios de la quimioterapia.


Proteína inhibidora 1 del progenitor del mieloide.
Previene la destrucción de las células de médula ósea ocasionada por la quimioterapia.
Protección contra los efectos secundarios de la quimioterapia.


Factor de crecimiento de las neuronas motoras.
Previene la muerte traumática de las neuronas motoras.
Tratamiento de la enfermedad de Lou Gehrig, daños traumáticos al sistema nervioso, accidentes cerebrovasculares y atrofia muscular debida al envejecimiento.


Factor inhibidor de monocitos.
Inhibe a los macrófagos.
Tratamiento de la artritis reumatoide y otras enfermedades autoinmunitarias y relacionadas con los macrófagos.


Así se fabrican y purifican moléculas de ADNc 

Las células utilizan el ARN mensajero para sintetizar proteínas.
Nuestra estrategia para descubrir genes consiste en crear copias de ADN complementario (ADNc) del ARN mensajero.
Para determinar la secuencia de bases de los ADNc debemos clonar y fabricar un gran número de copias de cada uno de ellos.
Los biólogos moleculares han desarrollado técnicas para insertar ADNc en bucles especiales de ADN, vectores que se replican dentro de las bacterias. 
Una colección de moléculas de ADNc de un tejido determinado constituye una genoteca.

Los investigadores de HGS han preparado genotecas de ADNc humano de casi todos los órganos y tejidos normales y también de muchos afectados por alguna enfermedad.
Para conseguir copias de una genoteca, los vectores se introducen y multiplican en bacterias.

Las bacterias se inoculan después en cajas con un gel nutritivo para que se repliquen y formen colonias.
Cada colonia deriva de una sola bacteria.
A continuación, utilizamos un robot que automáticamente reconoce y separa del gel las colonias que han incorporado una molécula de ADNc.
Para ello, el robot se guía por el color. 
Los vectores que utilizamos están diseñados de tal manera que, si no consiguen combinarse con un ADNc, producen un pigmento azul.
El robot, que es capaz de analizar hasta 10.000 colonias al día, identifica las que contienen ADNc humano evitando las colonias azules.
El ADNc de cada colonia seleccionada, debidamente multiplicado, se purifica automáticamente. 

Obtención de una secuencia parcial de ADNc

Para generar secuencias parciales de ADNc, hay que descomponer químicamente las moléculas de ADNc hasta generar una colección de fragmentos cuyas longitudes difieren en una sola base.
Durante ese proceso, la base presente en uno de los extremos de cada fragmento se combina con un colorante fluorescente.
Existen cuatro tipos de colorantes, uno por cada tipo de base.
Las máquinas ordenan los fragmentos marcados por tamaños.
Posteriormente, un láser excita las marcas de colores una a una.
El resultado es una secuencia de colores que puede leerse electrónicamente y que se corresponde con la secuencia ordenada de bases de uno de los extremos del ADNc que se está analizando.
El ordenador ensambla secuencias parciales, que pueden llegar a tener centenares de bases cada una, y completa la secuencia de un gen.


Terapia génica sin virus

Muchas de las dificultades que encuentra la transferencia de genes víricos pueden superarse gracias a la introducción de otros sistemas biológicos.
La investigación experimental realizada permite abrigar esperanzas de su futuro empleo en terapia y en vacunación En buena medida, la investigación en torno a la terapia génica se apoya en el empleo de virus modificados para introducir genes humanos que codifican proteínas de potencial eficacia curativa. 
Se busca con ello que las células invadidas por un virus transfieran el gen en cuestión al núcleo celular.
Las células deberán entonces "expresar", o fabricar, la proteína necesaria especificada por el gen.

Los virus son medios eficaces para trasladar genes hasta el interior celular.
A lo largo de la evolución han adquirido mecanismos especializados que les permiten unirse a tipos específicos de células y descargar su contenido en el seno de la célula.
Pero el recurso terapéutico a los virus, empleados de vectores para la introducción de genes, entraña ciertos problemas.
Algunos virus degradan el ADN de las células que infectan, con los consiguientes resultados nocivos.
Y lo que es peor, los virus atenuados podrían transformarse en el interior del organismo y recuperar su patogenicidad.
Por no hablar de la posibilidad real de que el paciente desencadene una respuesta inmunitaria contra el virus.
Las respuestas de ese tipo anulan de inmediato la terapia génica, pues destruyen el virus o las células infectadas antes de que el gen terapéutico desarrolle su acción en favor del paciente.

Por todo ese cúmulo de razones se investiga, desde hace tiempo, la posibilidad de introducir genes terapéuticos en el interior de las células sin tener que recurrir a agentes infecciosos.
Además, la medicina ha llegado al convencimiento, en su experiencia de los últimos cinco años, de que en muchas enfermedades que requieren terapia génica no bastará con un solo procedimiento para obtener la curación definitiva, sino que se necesitará repetir el tratamiento una y otra vez.
Las técnicas que no se apoyan en los virus podrían resultar muy adecuadas para un uso repetido, porque no provocan respuestas inmunitarias que alteren los vectores víricos.

En el procedimiento que yo he estudiado, cuya eficacia se halla ahora en fase de prueba en humanos, se emplean complejos constituidos por ADN y lípidos no inmunogénicos. 
También en los últimos años hemos asistido a un descubrimiento sorprendente:
la inyección de ADN "desnudo" en animales de experimentación y en pacientes induce la expresión de las proteínas codificadas. 

Esta segunda estrategia, como tendremos ocasión de ver, encierra unas posibilidades prometedoras en el dominio de las nuevas vacunas.

Problemas eléctricos

A la ciencia se le abrió un horizonte insospechado cuando supo que podía alterar las células de una forma selectiva mediante la introducción en ellas de ADN foráneo.
John Holland, de la Universidad de California en San Diego, y algunos de sus contemporáneos demostraron ya en los años cincuenta que las células podían captar ácidos nucleicos (ARN y ADN) extraídos de virus y expresar las proteínas que cifraban.
Este descubrimiento sirvió de incentivo para mejorar el rendimiento de la transferencia de genes a las células (transfección génica).

Llegados los sesenta, se advirtió que el obstáculo principal para la captación celular de ADN se debía a un fenómeno de cargas;
en disolución acuosa, así el medio que baña las células en el organismo, la molécula tiene una carga eléctrica negativa.
Por eso, las membranas de las células tienden a repelerla, al estar también ellas cargadas negativamente. 
Hubo, pues, que desarrollar técnicas que combinaran ADN con moléculas que neutralizaran su carga, permitiendo de ese modo su absorción.
Una de esas técnicas se servía de un polímero orgánico dotado de carga positiva, el DEAE-dextrano.
En otro procedimiento se recurría al fosfato de calcio.

Viose que, por estos medios, los cultivos de células humanas podían captar genes y expresarlos de manera permanente. 
Se obtuvo una demostración espectacular con la incorporación del gen de la quinasa de timidina, aislada del virus del herpes simple.
Algunas células incorporaban el gen cuando se presentaba formando complejo con el fosfato de calcio;
producían entonces dicha enzima de manera estable.

A finales de los setenta arrancó la moderna industria biotecnológica, impulsada por el descubrimiento de técnicas para extraer genes de células y empalmarlos dentro de plásmidos: lazos flexibles de ADN que se multiplican naturalmente en el interior de bacterias.
Las técnicas del ADN recombinante permitieron, por tanto, producir un gran número de réplicas de genes específicos.
El grupo de Paul Berg, de la Universidad de Stanford, conjugó la técnica del ADN recombinante con la de la transfección química celular al introducir plásmidos procedentes de bacterias en células cultivadas de mamíferos. 
La expresión de genes empalmados en este tipo de plásmidos sugería que los genes captados se habían incorporado en el núcleo, por una sencilla razón:
los genes no pueden expresarse si no actúan sobre ellos las proteínas nucleares.
Estos métodos de transfección no vinculados a virus se aplicaron entonces para producir líneas celulares de mamíferos, que la industria utiliza hoy para fabricar proteínas recombinantes de uso en clínica; por ejemplo, el factor VIII, que se administra a los hemofílicos para corregir problemas relacionados con la coagulación de la sangre que pueden poner en peligro su vida.

Los lípidos en el candelero

A pesar de su valor comercial, muchos reputaron ineficaces las técnicas químicas para la terapia génica. 
Pese a tal escepticismo, Berg y Demetrios Papahadjopoulos, de la Universidad de California en San Francisco, consiguieron la transfección celular con genes, mediante la exposición de las células a liposomas cargados con genes.
Los liposomas son unas perlitas huecas constituidas por membranas lipídicas en su zona externa y una solución acuosa en el interior.
Claude Nicoloau, de la facultad de medicina de Harvard, repitió el resultado.
De ello se infería también que los genes de los plásmidos penetraban en el núcleo, donde la maquinaria de la célula actuaría sobre los genes introducidos promoviendo su expresión en proteínas.

Descritos en los años sesenta por Alec D. Bangham, del Instituto Babraham en Cambridge, los liposomas pueden formarse espontáneamente cuando se suspenden en una solución acuosa determinados lípidos.
Los liposomas se asemejan a las células animales en el sentido de que su membrana externa consta de una doble capa lipídica.
Tal peculiaridad se debe a que las moléculas de lípidos en ella presentes tienen un extremo hidrófilo y otro hidrófobo. 
En soluciones acuosas forman membranas de doble capa, en la que las cabezas hidrófilas se orientan hacia el entorno acuoso exterior y también hacia el centro del liposoma lleno de agua.
Esta similaridad con las células indujo a pensar en la posibilidad de que liposomas "cargados" con alguna sustancia medicinal se fundieran con las células y vertieran su contenido en el interior de las mismas.

Los resultados obtenidos con los liposomas eran bastante prometedores.
Hubo, sin embargo, un problema de índole técnica que retrasaba su aplicación práctica en la introducción de plásmidos en la célula. 
El diámetro interno de un liposoma - entre 0,025 y 0,1 micrometros- es mucho menor que la dimensión mayor de un plásmido de ADN, que puede medir hasta dos micrometros. 
Por causa de tal discordancia, cuando los liposomas se sintetizaban en presencia de plásmidos, sólo se encapsulaba un número exiguo de plásmidos muy plegados. 

Pero algunos no cejamos hasta encontrar la manera de mejorar el proceso de encapsulación.
Trabajaba entonces en Syntex Research en Palo Alto.
Con otros compañeros consideré la posibilidad, en teoría, de modificar los liposomas al objeto de que pudieran captar plásmidos de manera más eficiente e introducir con mayor facilidad su contenido en las células.

Nos proponíamos preparar liposomas en los que algunos lípidos normales quedaran reemplazados por otros que portaran una carga positiva en el extremo hidrófilo.

De esa forma, pensábamos, los liposomas interaccionarían mejor con el ADN y ARN, así como con las superficies celulares. 
Sin embargo, por aquel entonces -era 1983- escaseaban los ejemplos de lípidos cargados positivamente (catiónicos) que tuvieran la forma adecuada que les permitiera organizarse y dar lugar a liposomas.

Esto nos llevó a sintetizar diversos lípidos catiónicos que, de acuerdo con nuestras predicciones, gozaran de las propiedades adecuadas.
Las moléculas se comportaron como habíamos esperado, y los liposomas catiónicos resultantes se trabaron firmemente a la superficie de las células en los cultivos de tejidos.

Más aún, con sólo mezclar plásmidos y una masa de lípidos catiónicos unas ocho veces mayor, conseguíamos capturar todo el ADN presente.
Sentí la satisfacción de ver abierto el camino que nos habría de conducir hasta las condiciones necesarias para producir complejos físicamente estables.

Frutos del descubrimiento

Las estructuras formadas a partir de mezclas de plásmidos y lípidos catiónicos son más variables y complicadas que un simple liposoma.
A menudo, por ejemplo, nos encontramos con plásmidos encerrados en estructuras lipídicas tubulares.
Y en las condiciones adecuadas, un tubo lipídico que contenga un plásmido puede plegarse hasta formar una partícula densa con una pared lipídica.
Una estructura de este tipo se parece ya bastante a ciertos virus.
Puesto que las estructuras formadas a partir de lípidos catiónicos difieren tanto de los meros liposomas, se ha convenido ahora en darles un nombre nuevo: el de lipoplejos.

Creía yo que habría que modificar aún más los lipoplejos antes de emplearlos para descargar su contenido en el interior celular.
Mas, para sorpresa de todos, un doctorando de mi laboratorio demostró que estas estructuras realizaban la transfección celular a una velocidad notable.
La mezcla de lípidos catiónicos con ADN se habría de convertir, desde entonces, en técnica estándar para insertar genes en cultivos celulares;
en los catálogos comerciales se ofrecen hoy muchas preparaciones de lípidos catiónicos con esta finalidad.

No hace mucho comenzaron las investigaciones de lipoplejos en el hombre.
El primer candidato terapéutico consistió en lipoplejos que portaban el gen de una proteína del sistema inmunitario, la HLA-B7, uno de los antígenos del complejo principal de histocompatibilidad.
Cuando las células cancerosas expresan HLA-B7, estimulan el sistema inmunitario del paciente, que las reconoce entonces como foráneas y puede destruirlas selectivamente.
En los protocolos clínicos patrocinados por Vical, más de 90 pacientes, que no respondían a los tratamientos anticancerosos habituales, recibieron inyecciones en sus tumores de lipoplejos que contenían ADN con la información para sintetizar HLA-B7.

En la mayoría de los casos quedó demostrado que el tratamiento estimulaba la producción de la proteína HLA-B7.
Sesenta de los pacientes padecían melanoma maligno;
en una tercera parte de esa fracción, el tumor en el que se había inyectado el lipoplejo disminuyó de tamaño o desapareció.
El melanoma avanzado se disemina a menudo por todo el organismo, por lo que la inyección del fármaco quizá no alcance a curar la mayoría de los casos.
Pero nuestros hallazgos preliminares, esperanzadores, indican que los lipoplejos podrían ser de utilidad en el tratamiento del melanoma.
En algunos casos el tratamiento con lipoplejos consigue que remita el tamaño, incluso de tumores no inyectados.
Otros estudios con lipoplejos que portan el gen de la HLA-B7 se proponen establecer la ausencia de efectos nocivos y la eficacia de tratamientos semejantes en episodios de cáncer de colon, riñón y mama que no pueden extirparse quirúrgicamente.

La empresa Vical patrocina también una prueba clínica de una formulación de lipoplejo que contiene el gen de la interleuquina - 2 (IL-2).
Se inyecta esta proteína del sistema inmunitario para el tratamiento del cáncer de riñón;
comporta, sin embargo, graves efectos secundarios.
Ahora bien, inyectados en la masa tumoral, los lipoplejos podrían estimular el sistema inmunitario mediante la producción local de elevadas concentraciones de IL-2, al tiempo que se evitarían la mayoría de los efectos tóxicos colaterales.

En la empresa Genzyme se investiga el uso de lipoplejos para el tratamiento de la fibrosis quística.
Las estructuras en cuestión portan un gen correspondiente a la proteína defectuosa en la enfermedad: el regulador de la conductancia transmembrana de la fibrosis quística. 

Los niveles de expresión génica alcanzables en animales con formulaciones de lipoplejos son, en la mayoría de los casos, equiparables a los que se consiguen con sistemas de transferencia de genes basados en virus.
Pero todavía median importantes diferencias entre ambos sistemas por lo que se refiere al rendimiento.
Algunos virus logran una eficiencia de casi el 100 por cien en la transferencia de su genoma al interior de las células;
así, 1000 virus pueden infectar casi 1000 células del tipo adecuado.
Para obtener la transfección de 1000 células con lipoplejos necesitaríamos unos 10 millones de réplicas del gen en un número comparable de lipoplejos;
dicho de otro modo, este planteamiento es unas 10.000 veces menos eficiente. 

Para mejorar ese rendimiento de los lipoplejos, nosotros, y no sólo nosotros, trabajamos en la estrategia que consiste en incorporar en la membrana externa proteínas especializadas o fragmentos de proteínas que se asemejen a las que orientan a los virus en su búsqueda de determinados tipos celulares. 
Podríamos quizás agregar incluso otras moléculas para facilitar la supervivencia del gen y su funcionamiento en las células transfectadas.
Por ejemplo, podrían incorporarse las proteínas de fusión de los virus, que en condiciones normales permiten a éstos salvarse del sistema interno de destrucción. 
Se investiga también la manera de unir los genes a las señalizadoras nucleares, proteínas víricas que indican a los genes víricos el camino que lleva al núcleo de la célula.

Compartimos esas líneas de trabajo con otro estudio, el de hallarle aplicaciones al ADN desnudo.
A finales de los años ochenta, mucho antes de que comenzaran las pruebas clínicas con lipoplejos, Jon A. Wolff, Robert W. Malone y el autor, junto con algunos más, realizamos un descubrimiento sorprendente.
Con la esperanza de determinar qué formulación lipídica era la más eficaz para transferir genes a las células, habíamos decidido medir la expresión de un gen determinado, fácilmente detectable después de que hubiéramos inyectado, en tejidos murinos diversos, lipoplejos portadores del gen en cuestión.

Guardo muy presente en la memoria el día en que, tras analizar los datos de nuestro primer experimento, vimos que habíamos obtenido niveles de expresión génica comparables con los mejores resultados conseguidos en la transfección de cultivos celulares.
Pero hubo más.
Algo sin precedentes.
El ADN que nos había servido de control en algunas pruebas, daba, por sí solo, niveles de expresión semejantes o incluso superiores a los conseguidos con las formulaciones lipídicas.

El ADN desnudo

Se repitieron los experimentos, varias veces, en distintos laboratorios.
Los resultados fueron siempre los mismos: 
el ADN desnudo, inyectado en el músculo de un animal, se expresaba en proteína.
Con ser eso mucho, conseguimos, además, una concentración local muy elevada de proteína, hasta 100 nanogramos del producto del gen por gramo de tejido muscular.

Pese a la repulsión eléctrica entre las superficies celulares y el ADN, había células que asimilaban la molécula.
Todavía desconocemos el mecanismo responsable.
Quizás una ligera lesión tisular o la elevación de la presión local en el lugar de la inyección tengan algo que ver con ello.

En principio, parecía posible que la inyección de ADN desnudo en los músculos de los pacientes produjera la proteína deseada en cantidades terapéuticas. 

Piénsese, para ponderar ese fenómeno, en la necesidad apremiante de mejorar las vías de administración de insulina en diabéticos, o de los factores VIII o IX de la coagulación de la sangre para el tratamiento de pacientes hemofílicos.
Pero las elevadas concentraciones locales de proteína producida en el interior del músculo, en estos trabajos pioneros, no hubieran bastado para hacer frente a tales patologías cuando las proteínas quedaran diluidas en tres litros de plasma sanguíneo circulante. 

Gracias a las mejoras introducidas en el diseño de plásmidos, hemos avanzado algunos pasos.
Podemos, por ejemplo, estimular la producción de hematíes en ratones inyectándoles plásmidos desnudos que cifran la eritropoyetina.
Se administra esta hormona para promover el aumento del número de nuevos glóbulos rojos en pacientes sometidos a quimio y radioterapia.
Quizás en el futuro una inyección semejante de plásmidos recombinantes en el músculo sea menos cara que la administración directa de la eritropoyetina. 

A más corto plazo, las posibilidades del ADN desnudo se centran en el dominio de las vacunas.
Basta una cantidad muy pequeña de proteína para provocar una respuesta inmunitaria protectora.
La inmunología nos enseña que las proteínas pueden despertar dos tipos de respuesta inmunitaria.
La primera, la inmunidad humoral, se desarrolla después de que el sistema inmunitario destruya el agente patógeno.
Una vez degradado el microorganismo invasor, ciertas células se encargan de engullirlos;
son, éstas, células especializadas que presentan en su superficie las proteínas foráneas a los linfocitos B , células secretoras de anticuerpos. 
Los linfocitos B responden produciendo anticuerpos que reconocen las proteínas foráneas. 

Estos anticuerpos se unirán rápidamente al agente patógeno originario tan pronto como se lo encuentren de nuevo, neutralizándolo o dejándolo marcado para la destrucción por otros componentes del sistema inmunitario. 

El segundo tipo de respuesta, conocido como inmunidad celular, se produce cuando patógenos invasores colonizan las células y les obligan a multiplicarlos.
En la superficie de la célula infectada aparecen fragmentos cortos de las proteínas del patógeno.
El sistema inmunitario responde con la producción de otros linfocitos: los linfocitos T activados que reconocen tales fragmentos.
Estos linfocitos no producen anticuerpos, sino que destruyen directamente las células que exhiben los fragmentos críticos. 
Si el agente patógeno invade de nuevo el organismo, las células infectadas presentarán los fragmentos y serán objeto de aniquilación.

El plásmido de ADN desnudo invade las células. 
Nada tendría pues de extraño, razoné con Dennis A. Carson, que estimulara la inmunidad humoral y la celular.

En nuestro primer experimento, en el que intervino también Gary H. Rhodes, demostramos que, al inyectar en el ratón un plásmido que codificaba una proteína del virus de la inmunodeficiencia humana (VIH), se estimulaba en el animal la producción de anticuerpos que se unían a la proteína del VIH.
Rhodes pondría luego de manifiesto que también los plásmidos despertaban respuestas inmunitarias celulares.
En las pruebas de laboratorio, los linfocitos T de estos animales atacaban células que presentaban fragmentos de proteínas del VIH.

Mirando hacia el futuro

Debemos a Suezanne E. Parker un logro extraordinario.
Demostró, en mi laboratorio, que podían emplearse los plásmidos que llevaban un gen del virus de la gripe para inmunizar ratones y protegerlos así frente a lo que, por su propia naturaleza, constituiría una dosis vírica letal.
Por su parte, el grupo de Margaret Liu, de los laboratorios Merck, ha confirmado estos resultados en animales y los ha extendido con el desarrollo de una serie de vacunas potenciales de ADN capaces de producir respuestas celulares y humorales duraderas.
Los laboratorios Merck han iniciado ya las pruebas clínicas de una vacuna para la gripe con ADN desnudo.

No parece que esté lejano el día en que comiencen las pruebas clínicas de vacunas de ADN contra el herpes, la malaria y el VIH.
Ya a más largo plazo, la tuberculosis, papiloma, Chlamydia y hepatitis podrían constituir nuevos objetivos.
Además de la prevención de la enfermedad, con estas vacunas podría estimularse el sistema inmunitario de las personas enfermas.
Se prepara una prueba clínica con ADN desnudo para el tratamiento del linfoma, un cáncer de los leucocitos de la sangre. 

Los lipoplejos y el ADN desnudo no son los únicos planteamientos que baraja la terapia génica sin virus. 
Otros expertos laboran con diversos polímeros catiónicos que forman complejos con el ADN.
Tales poliplejos, nombre genérico de dichas estructuras, empiezan a atraer la atención en las pruebas de laboratorio y en los ensayos clínicos.

La terapia génica sin virus persigue, asimismo, otra meta de interés sumo: el desarrollo de sistemas de dispensación que permitan inyectar en la sangre secuencias de ADN y hacerlas llegar a los tejidos apropiados, tales como el pulmón, hígado o médula ósea. 
Mejor sistema de dispensación sería, por supuesto, poder administrar los genes en píldoras, lo que facilitaría aún más la generalización de la terapia génica.
Y si con los sistemas de dispensación se pudiera dirigir la descarga de los genes hacia células específicas, se habría conseguido, en varios aspectos, dar un gran paso en la estrategia del tratamiento del cáncer.
En última instancia, podría emplearse la terapia génica para corregir genes mutados en las células de quienes sufren una enfermedad hereditaria o cáncer.
A este respecto, la técnica de direccionamiento de genes ofrece una vía plausible; y ha demostrado su éxito en cultivos celulares.

La introducción de genes en las células a través de lipoplejos, poliplejos y ADN desnudo se ha convertido ya en un importante campo de investigación farmacéutica. 
Si se mantiene el ritmo actual, los próximos decenios verán surgir muchos productos, basados en esas técnicas, que podrán administrarse de manera rutinaria en el tratamiento y prevención de enfermedades comunes.


Inmunogenética

Cada día nuestro organismo se enfrenta a una formidable serie de invasores: virus, bacterias y muchos otros microorganismos patológicos cuyo objetivo es superar nuestras defensas naturales.
Estas defensas, conocidas globalmente como sistema inmunológico, consisten en un complejo conjunto de billones de células.
El sistema inmunológico debe ser capaz de enfrentarse a una multitud de microorganismos invasores y debe ser capaz de distinguir entre lo propio, y lo «ajeno con un elevado grado de precisión.

Como era de esperar, la base genética del sistema inmunológico es compleja.
El estudio de la genética del sistema inmunológico, conocido como inmunogenética , se ha beneficiado enormemente del desarrollo de las nuevas técnicas de trazado del mapa y clonación génica.
En el estudio de los genes responsables de la respuesta inmunológica, se ha utilizado la mayoría de las técnicas descritas en capítulos anteriores (p. ej., análisis de ligamiento, paseo cromosómico, determinación de la secuencia del DNA).
Se han descubierto machos genes nuevos y se han estudiado intensamente sus funciones e interacciones.
En este capítulo se expone un breve resumen de la inmunología básica v se estudian los genes fundamentales para nuestra capacidad de defensa contra una gama muy diversa de patógenos. 
También se examinan algunos aspectos de la enfermedad autoinmune y se estudian algunos de los principales trastornos por inmunodeficiencia.

RESPUESTA INMUNOLÓGICA: CONCEPTOS BÁSICOS

Sistema inmunológico innato 

Cuando se detecta un microorganismo extraño, la primera línea de defensa de nuestro organismo incluye a los fagocitos (un tipo de célula que fagocito y destruye el microorganismo) y al sistema de complemento .
Las proteínas del sistema de complemento pueden destruir directamente a los microbios perforando sus membranas celulares y, al recubrir la superficie microbiana, también pueden atraer a los fagocitos v a otros agentes inmunológicos hacia los microbios (a causa de esta función de asistencia se originó el término " complemento"). 
Estos componentes del sistema inmunológico pueden distinguir de muchas formas a los cuerpos extraños.
Por ejemplo, a menudo identifican a las bacterias porque éstas sintetizan unos péptidos característicos iniciados con formilmetionina.

Con frecuencia reconocen a los virus ya que éstos producen RNA de doble filamento, relativamente raro en los mamíferos. 
Estos componentes de la fase inicial de la respuesta inmunológica, denominados en conjunto sistema inmunológico innato , reconocen únicamente características generales de los microbios extraños. 

El sistema inmunológico innato, que incluye a los fagocitos y al sistema de complemento, es una parte precoz de la respuesta inmunológica y reconoce características generales de los microorganismos invasores.

Sistema inmunológico adaptativo 

Aunque el sistema inmunológico innato puede contribuir a controlar una infección en sus fases iniciales, a veces es incapaz de vencerla.
De esta tarea se encarga un componente más especializado del sistema inmunológico, el denominado sistema inmunológico adaptativo .
Como indica su nombre, esta parte del sistema inmunológico es capaz de "adaptarse" a las características del microorganismo invasor estableciendo una respuesta inmunológica más específica y más efectiva.

Los componentes fundamentales de la respuesta inmunológica adaptativa son los linfocitos T (o células T ) y los linfocitos B (o células B ).
Estas células se originan en los órganos linfoides primarios del organismo (la medula ósea para las células B y el timo para las células T).
En el timo se seleccionan las células T que toleran los péptidos propios del organismo y se eliminan las que atacarían a estos péptidos.
A continuación, las células T y B emigran hasta los tejidos linfoides secundarios, como ganglios linfáticos, bazo y amígdalas, donde se encuentran con los microorganismos extraños.
Los linfocitos B maduros producen los anticuerpos circulantes que combaten las infecciones. 
Puesto que en este proceso intervienen agentes que circulan por el torrente sanguíneo, a veces se denomina a esta parte del sistema inmunológico sistema inmunológico humoral .

Los linfocitos T ayudan a los linfocitos B a responder a las infecciones con más eficacia y pueden matar directamente a las células infectadas.
A esta parte del sistema inmunológico se la denomina a veces sistema inmunológico celular .
Se calcula que el cuerpo contiene varios billones de células B v T.

Los linfocitos B son un componente del sistema inmunológico adaptativo;
producen anticuerpos circulantes en respuesta a la infección.
Los linfocitos T, otro componente del sistema inmunológico adaptativo, interactúan directamente con las células infectadas, destruyéndolas y ayudando a su vez a la respuesta de las células B.

Respuesta de las células B: sistema inmunológico humoral

Uno de los principales elementos de la respuesta inmunológica adaptativa se inicia cuando unos tipos especializados de fagocitos fagocitan los microbios invasores y después presentan, sobre sus superficies celulares, péptidos procedentes de estos microbios.
Estas células, que incluyen u los macrófagos y a las células dendríticas , se denominan células presentadoras de antígenos (ACP : antigen presentig cell ).
Las células B también son capaces de presentar de manera similar péptidos extraños en sus superficies celulares.

El péptido extraño es transportado de modo especializado hasta la superficie de las APC por una molécula del complejo mayor de histocompatibilidad ( MHC : major histocompatibility complex ) de clase II (fig. 8-1).

Este complejo, que se expone en el medio ambiente extracelular, es identificado por otra célula, el linfocito T cooperador .
Esta célula tiene en su superficie un receptor capaz de unirse (es decir, formar enlaces no covalentes) al complejo MHC- péptido.
Como se muestra en la figura 8-2 (pág. 158), otras proteínas en la superficie celular, como las moléculas de adhesión celular y las moléculas coestimuladoras , también participan en el proceso de unión.
La unión del complejo MHC-péptido estimula los linfocitos T cooperadores para que secreten citocinas, sustancias que estimulan la proliferación de otras células.
En particular, estas citocinas contribuyen a la estimulación de la subserie de los linfocitos, en cuyos receptores de superficie celular, denominados inmunoglobulinas , se unen a ciertas estructuras moleculares de los microorganismos invasores.
La capacidad de la inmunoglobulina para unirse a una molécula extraña específica (es decir, 
SU afinidad por la molécula) está determinada por su forma y por otras características. 

En la respuesta inmunológica humoral, las partículas extrañas son presentadas junto con las moléculas de MHC de clase 11 por las APC.

Estos complejos son identificados por las células T cooperadoras, que después estimulan la proliferación de las células B cuyas inmunoglobulinas pueden unirse al patógeno extraño.

Se calcula que, tras la exposición inicial a un cuerpo extraño, tan sólo uno de cada millón de linfocitos B produce inmunoglobulinas capaces de unirse al microbio.
Este número es demasiado pequeño para combatir con eficacia la infección.
Además, es probable que la afinidad de unión de la inmunoglobulina sea relativamente escasa.
Sin embargo, una vez estimulada esta pequeña población de linfocitos B, comienza un proceso de adaptación durante el cual se producen, en cada división mitótica, variaciones menores en la secuencia de DN'A (hipermutación somática, v. más adelante).
A su vez, estas variaciones de la secuencia de DNA producen modificaciones en Las características de unión de las inmunoglobulinas (p ej., la forma de la proteína).

Algunas de estas inmunoglobulinas variantes poseerán mayor afinidad de unión con el microorganismo.
Las celarlas B que producen estas inmunoglobulinas experimentan una selección favorable y proliferan con rapidez.
Estas células B se convierten en células plasmáticas secretoras de inmunoglobulinas hasta el torrente sanguíneo.
Las inmunoglobulinas secretadas, estructuralmente idénticas a las inmunoglobulinas que actúan como receptores en la superficie de las células B, son los anticuerpos.
Ahora se comprende la razón por la cual el sistema inmunológico adaptativo recibe este nombre: 
implica la selección inicial de células B cuyos receptores pueden unirse con el patógeno v la subsiguiente depuración de estas células B para lograr mayor afinidad de unión.

Durante la respuesta de células B a un péptido extraño aumenta la afinidad de unión de las inmunoglobulinas por el patógeno invasor.
Al madurar, las células B se convierten en células plasmáticas secretoras de anticuerpos.

Tras la estimulación inicial por el patógeno, el proceso de diferenciación y maduración de las células B en células plasmáticas productoras de anticuerpos requiere entre ~ y 7 días de duración. 
Cada célula plasmática es capaz de secretar aproximadamente unos 10 millones de moléculas de anticuerpo por hora.
Los anticuerpos se unen a los antígenos (antígeno = generador de anti cuerpos") de superficie del patógeno y pueden destruir directamente el microorganismo.
Con más frecuencia, el anticuerpo señala el microorganismo patógeno para que sea reconocido y destruido por otros Componentes del sistema inmunológico como las proteínas del sistema de complemento y los fagocitos.

Otra actividad importante de la respuesta inmunológica humoral es la creación de células memoria , una subserie de células B de elevada afinidad, que persisten en el organismo una vez desaparecida la infección. 
Estas células que ya han experimentado un proceso de selección muy elevado en respuesta al patógeno, proporcionarán una respuesta más rápida cuando el mismo patógeno aparezca de nuevo durante la vida del individuo.
La eficacia de las vacunas estriba en que inducen la formación de células memoria que pueden responder melar a un microorganismo patógeno no específico.

Figura 8.1

Respuesta inmunológica humoral.

Las moléculas de MHC de clase 11 en las células presentadoras de antígeno transportan los péptidos extraños hasta la superficie de la célula, donde el péptido extraño es reconocido por una célula T cooperadora.
La célula T secreta citocinas que estimulan las células B cuyas inmunoglobulinas se unirán al antígeno extraño. 

Estas células se convierten en células plasmáticas, que secretan anticuerpos en la circulación que se unen a los microbios, contribuyendo a combatir la infección. 

Figura 8.2

Imagen detallada de la unión entre una célula T cooperadora y una célula B. 

Además de la unión del receptor de la célula T al complejo péptido MHC, cierto número de otras moléculas interactúan entre sí, como el complejo B7-CD28 coestimulador.

Sistema inmunológico celular 

Algunos microorganismos, como los virus, tienden a insertarse con rapidez en el interior de las células del cuerpo. 
En esa localización resultan inaccesibles a los anticuerpos, que son unas proteínas hidrosolubles que no pueden atravesar la membrana lipídica de la célula. 

Para combatir estas infecciones, interviene un segundo componente del sistema inmunológico adaptativo, el sistema inmunológico celular.
Un miembro fundamental del sistema inmunológico celular es la molécula MHC de clase I , localizada en las superficies de casi todas las células del organismo.
En una célula normal, la molécula de MHC de clase I se une con unos pequeños péptidos (8-10 aminoácidos de longitud) procedentes del interior de la célula.
Esta molécula emigra a la superficie de la célula, transportando el péptido y presentándolo al exterior de la célula. 
Puesto que este péptido es propio del organismo, no se desarrolla una respuesta inmunológica. 
Sin embargo, en una célula infectada, la molécula de MHC de clase I se une a pequeños péptidos procedentes del microorganismo infeccioso.

La presentación en la superficie de la célula del péptido extraño por parte de la molécula de MHC de clase I alerta al sistema inmunológico, en especial a las células T. 
Recuerde que los linfocitos T, mientras se desarrollan en el timo, aprenden a tolerar los péptidos «propios» pero son muy agresivos contra los péptidos extraños.
El complejo MHC-péptido se une al receptor de superficie de la célula T apropiada, estimulando a la célula T para que emita una sustancia química que destruya la célula infectada (fig. 8-3).
Debido a su capacidad para destruir células de este modo, estos linfocitos se denominan linfocitos T citotóxicos (o linfocitos T asesinos ).
También se les conoce como células T CD8 a causa de la presencia de moléculas CD8 en su superficie (las células T cooperadores tienen moléculas CD4 en sus superficies celulares y, por lo tanto, constituyen la subpoblación de linfocitos T CD4).
Cada linfocito T citotóxico puede destruir una célula infectada en 1-5 minutos.

El sistema inmunológico celular es capaz de destruir las células infectadas del organismo.

Los péptidos del patógeno son presentados en las superficies celulares por las moléculas de MHC de clase I.
Estos péptidos son identificados por los linfocitos T citotóxicos (asesinos), que destruyen la célula infectada.

Como ocurre con los linfocitos B, únicamente una pequeña proporción de las células T del organismo tendrán capacidad de unirse al patógeno infeccioso.
Al secretar citocinas, los linfocitos T cooperadores estimulan la proliferación de linfocitos T citotóxicos que se unen a las células infectadas.
Además, las células dendríticas circulantes captan y presentan los péptidos extraños en sus superficies celulares, y posteriormente emigran a los tejidos linfoides secundarios, donde residen la mayoría de las células T, contribuyendo a alertar a las células T apropiadas contra la infección. 

El sistema inmunológico humoral está especializado en combatir las infecciones extracelulares, como las causadas por bacterias circulantes.
El sistema inmunológico celular combate las infecciones intracelulares, como virus y parásitos celulares.
Sin embargo, esta distribución de tareas no es estricta y existe una gran cantidad de interacción entre los dos componentes del sistema inmunológico adaptativo. 

Figura 8.3

En una célula infectada con un virus, los péptidos virales son transportados hasta la superficie de la célula por moléculas de MHC de clase I.

El receptor de la célula T de una célula T citotóxica CD8 se une al complejo péptido-MHC.
Una vez reconocido el péptido como extraño, las células T citotóxicas secretan unas sustancias químicas que matan la célula infectada.

PROTEÍNAS DE LA RESPUESTA INMUNOLÓGICA: BASES GENÉTICAS DE LA ESTRUCTURA Y LA DIVERSIDAD

Genes y estructura de las inmunoglobulinas 

Como se muestra en la figura 8-4, cada molécula de anticuerpo (o inmunoglobulina) consta de cuatro cadenas: un par idéntico de cadenas pesadas , más largas, y un par idéntico de cadenas ligeras , más cortas.
Las cadenas están unidas por puentes disulfuro.
Existen cinco tipos diferentes de cadenas pesadas ( xxx ) y dos tipos de cadenas ligeras ( xxx ).

Los cinco tipos de cadenas pesadas determinan la clase principal (o isotipo ) al que pertenece una molécula de inmunoglobulina (Ig):
xxx corresponden a lo isótopos de inmunoglobulinas G, M, A, D y E, respectivamente.
Los linfocitos B inmaduros producen únicamente IgM, pero cuando maduran se produce una redistribución de los genes de cadenas pesadas denominado cambio de clase (o switch ) Este proceso produce las otras cuatro clases principales de inmunoglobulinas, cada una de ellas diferente en cuanto a composición de aminoácidos, carga, tamaño y contenido en hidratos de carbono.

Cada clase tiende a localizarse en determinadas partes del cuerpo y a responder a un tipo diferente de infección. 
Los dos tipos de cadenas ligeras pueden asociarse con cualquiera de los cinco tipos de cadenas pesadas.

Las cadenas pesada y ligera contienen una región constante y otra región variable , localizadas en los extremos carboxi-terminal y amino-terminal de las cadenas, respectivamente.
La distribución de genes que codifican la región constante determina la clase mayor, de la molécula de Ig (p. ej., IgA, IgE). 
La región variable es responsable de la identificación y la unión al antígeno, y por lo tanto varía en las clases de inmunoglobulinas.
Tres segmentos genéticos distintos codifican las cadenas ligeras: C para la región constante, V para la región variable y I para la región que une las regiones constante y variable.

Cuatro segmentos genéticos codifican las cadenas pesadas, con C, V y J codificando de nuevo las regiones constante, variable y de unión, respectivamente' y una región de "diversidad" (D) localizada entre las regiones de unión y variable.
Los genes que codifican la cadena ligera xxx se localizan en el hombre en el cromosoma 2 y los que codifican la cadena ligera xxx se encuentran en el cromosoma 22.
Los genes que codifican las cadenas pesadas se encuentran en el cromosoma 14.

Las moléculas de inmunoglobulina constan de dos cadenas pesadas idénticas y dos cadenas ligeras idénticas.
La región constante de cadena pesada determina la clase mayor a la que pertenece una inmunoglobulina. 
Las regiones variables de las cadenas ligeras y pesadas identifican y se unen a los antígenos.

Figura 8.4

Una molécula de anticuerpo consta de dos cadenas ligeras idénticas y dos cadenas pesadas idénticas.

La cadena ligera incluye tres tipos de regiones: variables, de unión y constantes; la cadena pesada, además de estas regiones, incluye una región de diversidad localizada entre sus regiones variable y de unión. 

La parte superior de la figura muestra un modelo molecular de estructura de anticuerpo.

Bases de la diversidad de las inmunoglobulinas 

Nuestros cuerpos se enfrentan a una enorme diversidad de microorganismos infecciosos.
Puesto que el sistema inmunológico no puede "conocer" de manera anticipada el tipo de microbios que se encontrará, el sistema debe contener una inmensa reserva de células inmunológicas estructuralmente diversas, de modo que, al menos, algunas puedan responder (es decir, unirse) a cualquier microbio invasor.
En realidad, el sistema inmunológico humoral es capaz de generar al menos diez mil millones de anticuerpos estructuralmente distintos. 
En algún momento se pensó que, dado que cada anticuerpo tiene una única secuencia de aminoácidos, cada uno de ellos debía ser codificado por un gen diferente. 
Sin embargo, esta hipótesis de "un gen, un anticuerpo" no podía ser correcta, ya que el organismo humano tiene únicamente entre 50.000 y 100.000 genes.
Estudios posteriores han demostrado que varios son los mecanismos responsables de la diversidad en la generación de los anticuerpos: 

Múltiples genes de inmunoglobulina de línea germinal .
Los estudios de genética molecular (clonación y determinación de la secuencia del DNA) han demostrado que, para cada cadena pesada y ligera, un individuo tiene entre 100 y 200 segmentos V situados de modo contiguo en su línea germinal v seis segmentos J diferentes. 
En la región de la cadena pesada existen al menos 30 segmentos D.

Recombinación somática (recombinación VDJ) .
Cuando las moléculas de inmunoglobulinas se forman durante la maduración de los linfocitos B' una combinación específica de segmentos aislados V y J se selecciona para la cadena ligera y otra combinación de segmentos aislados V, J y D se selecciona para la cadena pesada. 
Esto ocurre mediante la deleción específica de las secuencias de DN-A que separan los segmentos V, J y D aislados antes de su transcripción en RNAm (fig. 8-5). 

Este proceso es efectuado por unas enzimas denominadas recombinasas , codificadas en parte por los genes RAG1 y RAG2 Este proceso de "cortar y pegar" se denomina recombinación somática en contraposición a la recombinación de línea germinal que tiene lugar durante la meiosis.
La recombinación somática produce un resultado bastante único: a diferencia de la mayoría de las otras células del organismo, cuya composición en DNA es idéntica, los linfocitos B maduros varían en términos de sus secuencias de DNA inmunoglobulínico redistribuidas. 
Debido a la existencia de muchas combinaciones posibles de segmentos aislados V, J y D, la recombinación somática puede generar un gran número de tipos diferentes de moléculas de anticuerpos.

3 Diversidad de unión .
Cuando se ensamblan las regiones V' D y J se producen unas ligeras variaciones en la posición en la que se unen, y es posible que se inserten o delecionen un pequeño número de nucleótidos en las uniones entre regiones.
Ello origina una variación mayor de la secuencia de aminoácidos. 

4 Hipermutación somática .
Tras la estimulación por un cuerpo extraño, los linfocitos B experimentan un proceso de "diferenciación secundaria" caracterizado por la hipermutación somática , antes mencionada.
La tasa de mutación de los genes codificadores de inmunoglobulinas se aproxima a 10-3 por par de bases y por división celular.
Esto provoca otra variación en las secuencias de DNA que codifican inmunoglobulinas y, por lo tanto, en sus características de unión a péptidos.
La hipermutación somática de los genes V, D y J produce una notable diversidad de anticuerpos y puede deberse a la presencia de zonas calientes de mutación en estos genes.

5 Combinaciones múltiples de cadenas pesadas y ligeras .
La combinación aleatoria de diferentes cadenas pesadas y ligeras en el ensamblaje de la molécula de inmunoglobulina provoca una diversidad adicional. 

Todos estos mecanismos contribuyen a la diversidad de anticuerpos. 
Considerándolos de forma conjunta, se ha estimado que pueden, en potencia, producir un número de anticuerpos distintos del orden de xxx .

Figura 8.5

Recombinación somática en la formación de una cadena pesada de una molécula de anticuerpo.

La cadena pesada funcional es codificada únicamente por un segmento de los múltiples segmentos V, D y J.


Información vital

El Proyecto Genoma Humano está generando un río de información que ayudará a descubrir los gérmenes ocultos de las enfermedades hereditarias.Podríamos así entrar en una nueva era de la medicina, aunque no vendrá exenta de riesgos desconocidos

A finales de este año, si la empresa Myriad Genetics consigue su propósito, miles de mujeres sanas recibirán dos malas noticias.
La primera, que un familiar cercano, quizás una hermana, padece cáncer de mama.
La segunda, que a ese familiar su médico le habrá dicho que el cáncer lo ha producido, casi con toda probabilidad, una mutación que sus consanguíneos también pueden tener.
El doctor sugerirá a la paciente que comunique a sus familiares femeninos cercanos la conveniencia de que se sometan a una prueba para detectar dicha mutación. 
Las mujeres que la porten deberán considerar la posibilidad de someterse a una doble mastectomía profiláctica, ya que es muy probable que ellas desarrollen cáncer de mama.
¿Cuál es esa probabilidad?
Aunque difícil de precisar, pues tales mutaciones aún no se han estudiado a fondo, la probabilidad podría elevarse hasta el 85 por ciento.

Muchísima gente habrá de enfrentarse al dilema planteado por las pruebas para detectar genes asociados con enfermedades graves. BRCA1 , el gen del cáncer de mama, cuyo examen a gran escala se dispone Myriad abordar este mismo año, constituye uno más de entre las decenas que los médicos consideran necesarios someter a las pruebas de detección de mutaciones.
Este tipo de pruebas son, en mayor o menor grado, uno de los frutos indirectos del Proyecto Genoma Humano, un audaz programa federal estadounidense de 375.000 millones de pesetas y 15 años de duración, cuyo objetivo es analizar molecularmente la herencia genética humana.

Si bien el proyecto comenzó formalmente hace sólo unos cinco años y medio, los progresos registrados ya superan las mejores expectativas.
Francis S. Collins, director del norteamericano Centro Nacional de Investigaciones sobre el Genoma Humano (NCHGR), del Instituto Nacional de la Salud, opina que el trabajo puede estar terminado "quizá dos años antes de lo previsto", hacia el 2003.

Las técnicas desarrolladas al abrigo de este enorme desafío han permitido cuadruplicar la tasa de descubrimientos de genes humanos relacionados con enfermedades, según una estimación del NCHGR.
No hay semana, podría decirse, sin que se caracterice un nuevo segmento de ADN de interés clínico.
Un ritmo que seguirá acelerándose. 

Cuando se aproxime el final, las previsiones son que cada hora se secuencia un nuevo gen, es decir, se determine el orden de las subunidades químicas que lo forman.

El proyecto respondía a un objetivo:
lograr el camino más seguro para luchar contra enfermedades de las cuales hace mucho tiempo se conocía su carácter hereditario y también contra otras, como el cáncer, que guardan una relación más sutil con los genes. 
Pero esa andadura hacia el nirvana genético será larga.
Los primeros resultados de uno de los frutos más ansiosamente esperados, la terapia genética, han sido decepcionantes.
Y al menos un biólogo implicado en este proyecto, Daniel W. Drell, del Departamento de Energía, prevé una "virulenta evolución legal" en el tema de las patentes sobre genes, ya que las compañías tratan por todos los medios de ocultar aquellas secuencias de ADN que encierran valor comercial.
La comercialización de este acervo común de nuestra especie ha disparado las protestas de los defensores de los derechos humanos, que ven en ello una afrenta a la naturaleza.
Al propio tiempo, un eminente genético ha advertido que las terapias basadas en genes serán demasiado caras para pensar en un uso generalizado de las mismas.

Ya han empezado a aparecer los abusos relacionados con la nueva ciencia del genoma.
Hay constancia de niños sometidos a pruebas de detección de ciertas enfermedades, a pesar de que los especialistas en consejo genético coinciden en que, para evitarles perjuicios, ellos deben quedar al margen de tales ensayos.
Por otra parte, como el riesgo de contraer una determinada enfermedad suele ser causa de rechazo a la hora de conseguir un empleo o de contratar un seguro de enfermedad o vida, muchos pacientes, y sus hijos, empiezan a negarse a ser sometidos a pruebas genéticas, por beneficioso que pudiera resultarles desde una óptica médica.
Otros acceden bajo un nombre falso.

Algunos líderes del proyecto genoma han reconocido desde el principio que la genética humana puede aplicarse con fines legítimos y otros menos legítimos.
Por ello, han insistido en la necesidad de invertir el dinero preciso en el estudio de los aspectos éticos, legales y sociales.
Sin embargo, los avances se producen a tal velocidad, que casi superan a los intentos, por parte de las sociedades profesionales y de los entes legislativos, de orientar el uso de las nuevas técnicas.

En 1990, los planificadores estimaban que, en dilucidar la secuencia de subunidades químicas, o bases, de todo el genoma, se emplearían varias decenas de miles de laborantes-año (toda la secuencia escrita, utilizando un carácter para cada base, ocuparía unas 390.000 páginas de Investigación y Ciencia , sin ilustraciones).

Además, como paso previo a la secuenciación, que revelaría finalmente la posible función de cada uno de los casi 100.000 genes humanos, y su localización en los cromosomas, habría primero que elaborar una suerte de "plano de situación" genético.

Los investigadores necesitaban un mapa genético. 
A grandes rasgos, un mapa genético es un diagrama que describe las relaciones cromosómicas de miles de secuencias "marcadoras" conocidas, en función de cómo éstas se separan y recombinan durante las sucesivas generaciones humanas. 
Se requerían también mapas físicos. 
En éstos, se describe la ordenación, a lo largo de un cromosoma, de multitud de sitios "señalados" por secuencias reconocibles.
Con el mapa genético, los investigadores pueden comparar el patrón hereditario de una característica genética determinada con el que presenta alguna de las secuencias "marcadoras".
De ese cotejo se infiere el lugar donde se encuentra el gen responsable de la característica genética en cuestión. 
Los computadores se encargan después de ir situando los rimeros de datos que emergen de las máquinas de secuenciación sobre los mapas físicos.

Con los dos tipos de mapas es posible encontrar rápidamente el lugar donde se encuentran los genes asociados con enfermedades. 

Aunque el Congreso de los Estados Unidos ha dedicado menos de los 25.000 millones de pesetas por año solicitados por el proyecto genoma, las fructíferas colaboraciones internacionales han dado un empujón considerable a los trabajos.
A finales de 1994 se había perfilado ya un buen mapa genético que abarcaba el genoma entero, y existen ya mapas físicos de una calidad excelente que cubren el 95 por ciento del genoma.
Este año se espera que esté ya disponible un mapa físico con marcadores espaciados cada 100.000 bases, la distancia ideal. 
Todo está, pues, a punto para comenzar la secuenciación a gran escala.

Tras los esfuerzos realizados en la secuenciación de genomas de otros organismos, se ha aprendido a acelerar el proceso y abaratar los costes.

Hay en desarrollo nuevas técnicas de secuenciación. 
Collins opina, sin embargo, que no serán necesarios enfoques radicalmente novedosos para conseguir que en el 2005 tengamos la secuencia del genoma humano con un 99,9 por ciento de fiabilidad.
Las mejoras en eficacia observadas en los dos últimos años le han convencido de que la técnica actual es adecuada para realizar el trabajo.

El Departamento norteamericano de Energía, que financia una parte importante del proyecto genoma, tiene en marcha varias operaciones de secuenciación a escala "piloto". 
El primer gran programa de secuenciación se inauguró a finales del año pasado en el Centro Sanger de Gran Bretaña, financiado por la Wellcome Trust.

Por lo que concierne a los Estados Unidos, el grueso del trabajo corre a cargo del centro de secuenciación de Robert H. Waterston, en la Universidad de Washington.

La avalancha de datos genéticos que se avecina aún no ha afectado a mucha gente.
Pero es cuestión de tiempo.
En los laboratorios se aíslan ya rutinariamente mutaciones genéticas asociadas con el cáncer, la enfermedad de Alzheimer y algunos tipos de enfermedades cardiovasculares. 
Desarrollar pruebas para detectar mutaciones en un gen conocido se ha convertido en un asunto bastante sencillo.
Genzyme, empresa afincada en Cambridge, Massachusetts, anunció el pasado otoño una técnica de diagnóstico capaz de analizar simultáneamente ADN de 500 pacientes para 106 mutaciones diferentes en siete genes.

Cuando se tenga suficiente información sobre los efectos de las mutaciones, los resultados de las pruebas serán de gran utilidad para el médico, ya que se podrá determinar la probabilidad de que una persona desarrolle una enfermedad y quizá sugerir algún tipo de tratamiento para aumentar la esperanza de vida.
Pero los estudios necesarios para conocer los efectos de las mutaciones requieren mucho tiempo.
Y los datos genéticos pueden acarrear perjuicios inmediatos y graves.
En particular, pueden precipitar cambios psicológicos gravosos y abrir la puerta a la discriminación.

En el pasado, la discriminación genética quedó relegada principalmente a los miembros de familias afectadas con enfermedades raras y que mostraban un patrón claro de herencia.
Por ejemplo, los miembros de familias con Huntington, una enfermedad neurodegenerativa muy grave que se manifiesta en la edad adulta, tienen grandes dificultades a la hora de contratar seguros sanitarios, y a veces les resulta imposible.
Según Collins, hasta el momento se conocen "unos pocos centenares', de personas que han perdido sus trabajos o sus seguros por el mero hecho de que con su patrimonio genético corrían el riesgo de contraer en el futuro una enfermedad; 
en la mayoría de los casos, porque un miembro de su familia había sido diagnosticado con una enfermedad de base genética conocida.
Pero, según las predicciones de Collins, a medida que aumente el número de pruebas de detección "veremos que eso sucederá a mayor escala, ya que todos corremos algún tipo de riesgo". 

En la actualidad, las pruebas para detectar mutaciones asociadas con algún cáncer, por ejemplo, sólo se realizan en el marco de investigaciones que se llevan a cabo en grandes hospitales, ya que la interpretación de los resultados es todavía harto imprecisa.
Pero hay fenómenos incontrovertibles.
En familias con casos de cáncer de mama hereditario, que constituyen menos del 10 por ciento de todos los episodios, presentar una mutación en el gen BRCA1 acarrea un riesgo del 85 por ciento de padecer la enfermedad, y un riesgo también del 45 por ciento de contraer cáncer de ovario.
Algunas mujeres de esas familias, que saben que son portadoras de un gen BRCA1 mutado, han decidido someterse a una mastectomía profiláctica y a una ooforectomía (extirpación de los ovarios).
Ello puede reducir el peligro de contraer el cáncer, pero no lo elimina.

La incertidumbre es mayor, sin embargo, en el caso de una mujer que porte un gen BRCA1 mutado pero no tenga una historia familiar de cáncer de mama.
En esas situaciones, se desconoce el peligro, pero puede ser menor.
Tampoco se sabe si el pertenecer a un grupo étnico determinado afecta a la probabilidad de contraer la enfermedad (en los judíos ashkenazi, por ejemplo, se da con mayor frecuencia un tipo determinado de mutación en el gen BRCAl, y un gen que causa neurofibromatosis tiene efectos más severos en blancos que en negros).
Estas y otras fuentes de incertidumbre plantean graves dilemas a la hora del tratamiento.
La elección entre cirugía radical y vigilancia intensiva, en forma de frecuentes mamografías, puede ser crucial.
La reciente identificación de un segundo gen de cáncer de mama, el BRCA2 , complica el asunto aún más.

Aunque los avances en el conocimiento del genoma llegarán algún día a eliminar esos dilemas, la mayoría de los científicos no esperan que ello ocurra en un futuro previsible.
Para las mujeres a las que ya se les ha diagnosticado un cáncer de mama, no se sabe muy bien qué significado pueda tener, a la hora del tratamiento, que la prueba del BRCA1 haya dado positiva.
Incluso con un resultado negativo en la prueba, una mujer sigue teniendo el mismo riesgo, un octavo, que cualquier mujer de EE.UU.
Estos factores han llevado a la Sociedad Norteamericana de Genética Humana y a la Asociación Nacional de Afectadas de Cáncer de Mama, un grupo de apoyo, a exigir que, por ahora, las pruebas del BRCA1 se realicen sólo en el marco de la investigación. "Lucharemos contra la venta de esas pruebas antes de que exista un acuerdo sobre cómo utilizarlas", dice Mary Jo Ellis Kahn, miembro de la asociación que ha sobrevivido a un cáncer de mama y en cuya familia se han dado varios casos de la enfermedad.

Este tipo de prevención choca con las previsiones comerciales.
Myriad Genetics, con sede en Salt Lake City, contempla ofrecer las pruebas para detectar el gen BRCA1 a todas las mujeres diagnosticadas con cáncer de mama o de ovario, y a sus familiares cercanas, a finales de 1996. 
Si aceptaran sólo las personas diagnosticadas, serían más de 200.000 pruebas al año.
Peter D. Meldrum , presidente de Myriad, afirma que su compañía está estudiando los riesgos para poblaciones diferentes. 
Collins, sin embargo, opina que los planes de Myriad son "potencialmente prematuros", ya que, según él, no es seguro que se sepa lo suficiente sobre las mutaciones y las opciones de tratamiento como para justificar las pruebas a gran escala a finales de año.

OncorMed , establecida en Gaithersburg, vende ya pruebas de BRCA I para su uso en protocolos de investigación con mujeres de alto riesgo.

La Asociación Nacional de Afectadas ha reclamado, con carácter de urgencia, el estudio del tipo de restricciones que deberían incluirse en esos protocolos.
La misma compañía tiene entre manos la investigación de las mutaciones asociadas con una forma de cáncer de colon.

El organismo norteamericano competente, la Oficina de Alimentación y Fármacos (FDA), no ha regulado los servicios de pruebas genéticas.

En enero, sin embargo, una comisión sobre pruebas genéticas creada por el programa genoma abordaba la posibilidad de solicitar a la FDA que limitase las pruebas de mutaciones asociadas a algún tipo de cáncer, hasta que no hubiese una información sólida sobre las posibilidades de tratamiento.

Algunas mujeres que saben que son portadoras de un gen BRCA1 mutado (sólo unas docenas, por ahora) han decidido ocultar esa información a sus agentes de seguros, según Barbara B. Biesecker, del NGHGR.
Temen que las aseguradoras consideren las mutaciones como un estado preexistente y, por tanto, se nieguen a cubrir los tratamientos relacionados con tal "circunstancia".

El asunto es espinoso.
Las compañías de seguros médicos suelen negarse a ofrecer pólizas, o exigen primas desproporcionadas, a los individuos con una clara historia familiar de cáncer.
El pasado año, la Asociación Nacional de Afectadas de Cáncer de Mama vio rechazada en repetidas ocasiones la petición de un seguro de enfermedad colectivo para los miembros de la directiva en Washington, porque eran sólo ocho, y algunos de ellos habían sobrevivido a un cáncer de mama. 
Para tener cobertura debían conseguir un número mayor de asociados.

La tendencia al secretismo en las pruebas genéticas parece haber calado.
En cierta reunión de la Sociedad de Genética Humana, celebrada el pasado otoño, Thomas H. Murray, de la Universidad Case Western Reserve, preguntó a su auditorio si conocían pacientes que hubiesen solicitado someterse a pruebas genéticas de este tipo de forma anónima, o bajo un nombre falso.
Se levantaron manos por toda la sala.
En muchos estudios clínicos, a los pacientes se les advierte formalmente que los resultados de las pruebas pueden acarrear complicaciones con las aseguradoras, si éstas acceden a los historiales médicos de los pacientes.
Para evitarlos, los investigadores obtienen a veces unos documentos legales, o "certificados de confidencialidad", que impiden a los tribunales acceder a los datos obtenidos en una investigación. 

Otros pacientes evitan simplemente hacerse las pruebas, renunciando así a cualquier beneficio médico que ello pudiera reportarles.
Las personas con von Hippel-Lindau (VHL), rara enfermedad hereditaria que produce tumores en el cerebro y en los riñones, suelen encontrar muchos problemas a la hora de contratar un seguro de enfermedad, debido a que las atenciones quirúrgicas que pueden necesitar son muy caras.
Aunque no hay una terapia profiláctica para prevenir ese tipo de tumores, las personas con enfermedad de VHL pueden alargar su esperanza media de vida sometiéndose a tratamientos regulares de resonancia magnética y a la posterior extirpación quirúrgica de los tumores. 
Según William C. Dicson, de la Asociación de Familiares de VHL, muchos padres con este síndrome no quieren que sus hijos pasen por las pruebas para detectar mutaciones en el recientemente descubierto gen de la VHL, ya que temen que el diagnóstico genético impida que se les extienda un seguro medico.

Los padres que padecen policistitis renal, quizá la enfermedad hereditaria grave más común, también suelen decidir no someter a sus hijos a las pruebas genéticas, por las misma razones relacionadas con los seguros, según indica Gregory G. Germino, un investigador de la facultad de medicina de la Universidad Johns Hopkins.
Aproximadamente 600.000 norteamericanos padecen esa enfermedad, aunque en su mayoría lo ignoran.
En 1994, y con técnicas desarrolladas en el marco del proyecto genoma, se descubrió el gen PKDI, responsable último de muchos de esos casos. 
Según Germino, las pruebas para detectar dicho gen pueden a veces mejorar la terapia médica en los niños. 

Este tipo de informes ha disparado las alarmas de los responsables sanitarios.
Un grupo de trabajo encargado de las implicaciones sociales, éticas y legales del programa genoma humano, junto con el Plan Nacional sobre el Cáncer de Mama, una iniciativa presidencial, acaba de recomendar la prohibición del uso de información genética, o el requisito de una prueba genética, como base para limitar o denegar un seguro médico por parte de las aseguradoras. 

Los agentes de seguros no suelen preguntar directamente por los resultados de las pruebas genéticas.
Las indagaciones sobre la salud o causa de la muerte de los padres de la persona en cuestión suelen ser suficientes para clasificar a muchas de esas personas entre las de alto riesgo. 
Pero las aseguradoras pueden considerar el requisito de las pruebas genéticas para determinadas pólizas. "Ahora no hacen preguntas sobre pruebas genéticas, pero seguro que esta actitud cambiará", comenta Nancy S. Wexler , presidenta de la Fundación de Enfermedades Hereditarias. 
Wexler tiene un 50 % de probabilidad de padecer la enfermedad de Huntington.

Como los pacientes con enfermedades genéticas se muestran reacios a identificarse, es difícil evaluar el impacto de la discriminación.
Pero hay nuevos datos que refuerzan los primeros datos anecdóticos y que sugieren que el fenómeno está muy extendido.
En una de las primeras encuestas amplias, programada para su publicación, en Engineering and Scientific Ethics , el equipo de Lisa N. Geller, de la Facultad de Medicina de Harvard, describen cómo enviaron cuestionarios a personas que, aunque libres de síntomas, tenían algún riesgo de contraer una enfermedad genética.

De los 917 que respondieron, 455 aseguraban que habían sufrido algún tipo de discriminación al revelar el diagnóstico genético.

Entrevistas complementarias realizadas por los investigadores suministraron detalles de aseguradoras (médicas y de vida) que rehusaron o cancelaron las coberturas, agencias de adopción que solicitaban a los posibles padres "pasar" una prueba genética (y que en una ocasión malinterpretaron los resultados) y empresarios que despidieron o no contrataron porque el implicado padecía alguna enfermedad genética, tratable médicamente, o porque tenía alguna posibilidad de padecerla.
Paul R. Billings, del Hospital de Veteranos en Palo Alto, uno de los autores del estudio, ha declarado que 'la gente rechazará las pruebas genéticas por miedo a la discriminación". 
En otro estudio, realizado por E. Virginia Lapham, de la Universidad de Georgetown, el 22 % de un grupo de 332 personas que tenían alguna enfermedad genética en su familia, declaró que esa circunstancia le había impedido la contratación de un seguro médico.

Varios países europeos han tomado medidas para evitar los abusos asociados con los datos genéticos.
El seguro médico básico no constituye ningún problema especial en Europa, donde está regulado como derecho fundamental de los ciudadanos que los gobiernos deben proteger.
Con todo, en Francia, Bélgica y Noruega existen leyes que impiden el uso de información genética por parte de las compañías aseguradoras y la mayoría de los empresarios.
En los Países Bajos se garantiza un seguro de vida básico, y en Alemania hay ciertas medidas protectoras.

En EE.UU. varios estados han promulgado leyes que limitan la discriminación fundada en los datos genéticos. 
Las leyes federales prohíben la discriminación en el empleo, y varios proyectos presentados ahora ante el Congreso tratarán de impedir la discriminación en los seguros por motivos genéticos.
Pero las perspectivas son inciertas.

Los posibles perjuicios psicológicos derivados de las pruebas genéticas están también recibiendo una atención cada vez mayor.
Como los resultados de las pruebas pueden afectar a todos los miembros de una familia, en su sentido más amplio, las relaciones entre aquellos pueden verse seriamente alteradas debido a los sentimientos de culpabilidad.
El miedo a esas consecuencias puede explicar la práctica escasa de una prueba que está disponible desde hace varios años, y que permite identificar a los portadores del gen de la fibrosis quística, a pesar de que tales individuos carecen de riesgo.

Los expertos en consejo genético coinciden en que, debido a los posibles perjuicios, los niños no deben ser sometidos a pruebas de detección de mutaciones que pronostiquen enfermedades que no se desarrollarán hasta la edad adulta, a menos que exista la posibilidad de una intervención médica.
Según esto, los niños deben quedar exentos de pruebas detectoras de la enfermedad de Huntington, ya que no existe una terapia preventiva contra el desarrollo de los desarreglos motores y mentales que la caracterizan.
Sin embargo, hay padres que sí solicitan esas pruebas para sus hijos: en algún caso para no pagar la educación superior si al final el muchacho va a sucumbir a la enfermedad. 

La principal red norteamericana de laboratorios dotados de capacidad técnica para realizar este tipo de pruebas se denomina Helix.
Según un estudio acometido por Dorothy C. Wertz y Philip R. Reilly, del Centro Shriver de Retraso Mental, en Waltham, el 23 por ciento de los laboratorios de la red habilitados para detectar la mutación responsable de la enfermedad de Huntington han realizado las pruebas a menores de 12 años.
Más del 40 por ciento de los laboratorios Helix han aplicado las pruebas directamente a los pacientes, sin asistencia médica.
No siempre el público comprende el significado del diagnóstico genético, comenta Wertz.
Y no sólo eso, agrega: muchos médicos no están capacitados para emitir un consejo genético.

En el haber de las pruebas debemos hacer constar, no obstante, que ciertos pacientes con familias afectadas con cáncer de colon hereditario y posiblemente también cáncer de mama, han adoptado decisiones médicas sensatas como resultado de descubrimientos facilitados por el programa genoma. 
Algunos portadores de una mutación que predispone al cáncer de colon, por ejemplo, han aceptado la cirugía del colon en cuanto han empezado a producirse los cambios amenazadores. 
Es ése un procedimiento que probablemente ha salvado sus vidas.
Nuevas terapias ansiosamente esperadas, sin embargo, parecen más lejanas.

Collins recuerda que sólo seis años después de que un equipo que él codirigía encontrara el gen asociado con la fibrosis quística, en 1989, ya se estaban ensayando con pacientes medicamentos diseñados para contrarrestar los efectos producidos por el gen mutado. 
Sin embargo, se ignora cuánto tardará en llegar un tratamiento definitivo.

La perspectiva de tratamiento que más ha atraído la atención de la gente es la terapia génica, si bien resulta más descriptivo hablar de transplante de genes.
No obstante, los intentos para tratar la hipercolesterolemia familiar, la fibrosis quística y la distrofia muscular de Duchenne han resultado fallidos hasta el pasado año, aparentemente porque las células de los pacientes no incorporaban suficiente cantidad de genes transplantados.
Los primeros tratamientos para corregir la deficiencia en desaminasa de adenosina, descritos en esta revista por W. French Anderson ( véase "Terapia génica" INVESTIGACIÓN Y CIENCIA, noviembre de 1995), muestran un efecto bastante discreto en el mejor de los casos.
El pasado diciembre, una revisión del NIH concluía que "hasta el momento, no ha quedado definitivamente demostrada la eficacia clínica de ningún protocolo de terapia génica".

A pesar de todos los problemas, la emergente revolución genética está ya causando movimientos sísmicos en el mundo de los negocios.
Las compañías farmacéuticas han dedicado miles de millones de pesetas a la búsqueda de genes responsables de enfermedades, ya que ellos nos ilustran sobre el tipo de moléculas candidatas a convertirse en el punto de mira de medicamentos o reactivos para la diagnosis.

J. Craig Venter, del Instituto de Investigaciones Genómicas, de Gaithersburg, ha ideado una estrategia más directa, que analiza ARN mensajeros, los productos que se crean cuando se activan los genes.
La técnica produce unas marcas químicas que revelan cierta información básica relativa a los genes asociados, sin gran dispendio de trabajo.

Algunos expertos no quedaron muy impresionados por la propuesta de Venter, ya que no proporcionaba una información tan global como la que ofrece la secuenciación a gran escala.
Pero funciona.
Dos compañías se han embarcado en una suerte de microscópica fiebre del oro en busca de genes lucrativos utilizando la estrategia de Venter.
Human Genome Sciences (casa matriz de la organización de Venter) e Incyte Pharmaceuticals, de Palo Alto, andan ahora patentando secuencias afanosamente.

Williams A. Haseltine, presidente de Human Genome Sciences, asegura que su compañía ha identificado el 90 por ciento de los genes humanos y ha utilizado docenas de ellos para fabricar proteínas con propiedades de interés terapéutico. 
Los hallazgos de Human Genome Sciences, según Haseltine, han provocado también un "notable impacto" en el programa de desarrollo de nuevos fármacos de su principal socio, SmithKline Beecham.
Por su parte, Incyte asegura haber identificado "muchísimos" genes humanos y ha encontrado clientes para sus bases de datos.

Las perspectivas de explotación comercial del genoma despiertan inquietud en ciertos medios.
Se recela, sobre todo, del Proyecto Diversidad del Genoma Humano.
Este plan, sin vinculación formal con el proyecto genoma, tiene como objetivo el estudio de las variaciones de secuencias genéticas entre diferentes grupos en todo el mundo.

Los patrocinadores del proyecto diversidad, concebido inicialmente por Luigi Luca Cavalli-Sforza, de la Universidad de Stanford, señalan que las secuencias generadas por el propio proyecto genoma derivarán principalmente de ADN de donantes europeos y norteamericanos.
Según Cavalli-Sforza, si se estudiase la existencia de variación, del 0,1 por ciento, entre el ADN de gente de todo el mundo, se cosecharía una información muy valiosa sobre la adaptación.

Henry T. Greely, profesor de derecho de Stanford y coorganizador del proyecto diversidad, reconoce que los datos sobre variación genética podrían invitar a los racistas a realizar análisis arbitrarios para justificar la discriminación. 
Pero se defiende alegando que el proyecto asumirá su responsabilidad en la lucha contra tales abusos, y apunta que los datos ya disponibles ilustran sobre la superficialidad de las diferencias raciales.
La mayor parte de esa variación del 0,1 por ciento entre seres humanos se da entre miembros de la misma raza, y no entre razas distintas.

El proyecto ha recibido duras críticas.
La Fundación Internacional para el Avance Rural (RAFI), una modesta organización con sede en Ottawa que se opone a las patentes de seres vivos, dice, por boca de Jean Christie, que se opone al proyecto diversidad porque éste producirá líneas celulares que pueden ser patentadas por las compañías cazagenes de los países ricos.

Greely insiste en que las normas del proyecto eliminan la posibilidad de que las muestras puedan utilizarse con fines lucrativos sin el consentimiento de los donantes.
A pesar de esa seguridad, un comité de la Organización de las Naciones Unidas para la Educación, la Cultura y la Ciencia (UNESCO) ha criticado al proyecto la falta de contactos con grupos indígenas durante su fase de planificación. 
La controversia levantada por la concesión de una patente al NIH sobre una línea celular derivada de un oriundo de Papúa Nueva Guinea ha avivado el debate sobre la explotación.
Se entiende, pues, que el proyecto diversidad del genoma encuentre dificultades a la hora de recabar subvenciones de fondos públicos.

No sabemos cómo acabará el tema de las patentes. 
Collins afirma que pretender patentar todos los genes secuenciados resultaría "desestabilizador", ya que pondría en peligro la cooperación entre investigadores.
Tradicionalmente.
Los especialistas han gozado de libertad a la hora de llevar a cabo investigaciones no sometidas a patentes. 
Esta libertad puede verse amenazada, comenta Rebecca Eisenberg, experta en patentes de la Universidad de Michigan. 
Cuantos más especialistas se plieguen a la voluntad de las empresas, por vínculos laborales, con mayor facilidad podrá patentarse el genoma.

Antes de que un gen pueda patentarse, el "inventor" debe conocer a grandes rasgos por lo menos la función que desempeña, para cumplir así el requerimiento legal de utilidad.
Ahora bien, la industria controla la mayoría de los recursos investigadores con capacidad para descubrir eficazmente las propiedades útiles de tales genes.
Por tanto, la comercialización parece una consecuencia inevitable de la exploración científica del genoma, como lo ha sido para otro tipo de exploraciones. 
Aunque la Oficina de Patentes de EE.UU. ha accedido recientemente a examinar algunas cuestiones planteadas por las patentes de genes, una vuelta atrás en este tema, con la consiguiente anulación de las patentes, como solicitan algunos críticos, parece improbable.
Y mientras siga habiendo dinero para invertir en estos temas, los países ricos continuarán probablemente beneficiándose de ello.

Resulta, no obstante, obligado preguntarse qué beneficio sacará de todo esto el resto del mundo.
Algunas de las terapias génicas sometidas a ensayo se adaptarán a determinados pacientes.
Pero James V. Neel, de la Universidad de Michigan, uno de los pioneros en genética humana, ha a vertido ya que "las terapias individuales serán demasiado caras" para su uso generalizado.
Actuaciones "humildes", como un mejor control de la dieta y el ejercicio pueden aliviar el impacto de la diabetes adulta, con una mejor relación costo-eficacia que la medicina genética, señala Neel.
Al tiempo que apremia a los genéticos a prestar al medio ambiente, cada vez más degradado, que muchos seres humanos soportan, la misma atención que al ADN. 

Con todo, la carrera del gen sigue su curso.
Se encontrarán mejores medicinas, algunos harán fortuna y otros resultarán perjudicados.
Porque de lo que no cabe duda es de que, si bien todos los seres humanos comparten ADN, no todos compartirán sus beneficios.
Según un informe de la Organización Mundial de la Salud, en 1993 murieron 12,2 millones de niños menores de 5 años en los países en vías de desarrollo.
Más del 95 por ciento de esas muertes pudieron haberse evitado, según la OMS, si esos niños hubiesen estado bien nutridos y hubiesen tenido acceso a los cuidados médicos que ya son una práctica normal en los países que pueden costeárselos. 
Para los desheredados de la Tierra, la medicina genética es todavía un sueño muy lejano.

Figura 1

Cromosoma 17

Este mapa genómico parcial, que se muestra aquí y en las páginas siguientes, se publicó el pasado mes de diciembre.
Ha sido realizado por expertos del Instituto de Investigaciones Biomédicas Whithead y Genéthon, de Francia.


Clonación y análisis del DNA mediante PCR

Principios en los que se basa la PCR 

La PCR es un método de clonación de DNA en un sistema libre de células.

Reacción de PCR estándar 

La PCR (acrónimo inglés de Polimerase Chain Reaction , reacción en cadena de la polimerasa) es un método rápido y versátil de amplificación in vitro de secuencias de DNA específicas dentro de una muestra.
Habitualmente la reacción se diseña para permitir la amplificación selectiva de una o varias secuencias diana de DNA presentes en una mezcla compleja de secuencias (por ejemplo, DNA genómico total o una población compleja de cDNA).
Para que la amplificación selectiva sea posible es absolutamente necesario disponer de un mínimo de información sobre la secuencia de la diana.
Esta información permite la construcción de dos oligonucleótidos, habitualmente de 15 a 30 nucleótidos de longitud, que actúan como cebadores (iniciadores, en inglés primers ) en la reacción.
Estos oligos reciben también el nombre de amplímeros.

Cuando se mezclan con DNA genómico desnaturalizado los amplímeros se unen específicamente a las secuencias complementarias, que se sitúan inmediatamente adyacentes a la región genómica que se quiere amplificar. 
Los amplímeros están diseñados para que puedan iniciar la reacción de síntesis de DNA en presencia de una DNA polimerasa termoestable adecuada y de los precursores de DNA (los cuatro desoxinucleótidos trifosfato, dATP, dCTP, dGTP y dTTP).
Cada amplímero servirá para iniciar la síntesis de una hebra de DNA complementaria a una de las hebras del segmento de la diana, y las dos hebras de nueva síntesis serán complementarias entre sí. 

La PCR es una reacción en cadena porque las hebras de DNA de nueva síntesis sirven a su vez de molde para reacciones de síntesis en ciclos posteriores.
Tras unos 30 ciclos, la PCR habrá generado unas xxx copias de la secuencia diana específica, cantidad que puede ser fácilmente detectada tras electroforesis en gel como una banda discreta de un tamaño específico. 
Se utiliza una DNA polimerasa termoestable debido a que la reacción pasa por ciclos en los que se cambia la temperatura. 
Cada uno de estos ciclos consta de los tres pasos que se indican en la página 141.

i Desnaturalización: típicamente calentando la muestra a 93-95 °C para el DNA genómico humano
ii Reasociación: a temperaturas que varían entre 50 y 70°C , dependiendo de la Tm del dúplex esperado (la temperatura de reasociación se fija habitualmente 5 °C por debajo de la Tm calculada para los híbridos amplímero /DNA);
iii Síntesis de DNA: típicamente a 70-75 °C .

Algunos microorganismos cuyo hábitat natural son las fuentes hidrotermales han servido como base para la purificación de DNA polimerasas termoestables.
Por ejemplo, la Taq polimerasa, ampliamente utilizada, proviene de Thermus aquaticus .
Esta enzima es estable hasta xxx , y su temperatura de trabajo óptima es de 80°C .

Diseño de cebadores y especificidad de la amplificación

La especificidad de la amplificación depende necesariamente de la especificidad de reconocimiento de la secuencia específica respecto de otras presentes en la muestra por parte de los oligonucleótidos cebadores.
Para muestras complejas como el DNA genómico total de una célula de mamífero, a menudo basta con diseñar dos cebadores que tengan unos 20 nucleótidos de longitud.
En estas condiciones la probabilidad de una correspondencia accidental perfecta con otras secuencias que no sean la buscada, que además estén en la orientación y a la distancia adecuada, es muy pequeña.
Sin embargo, a pesar de las precauciones que se toman para que sólo sean estables los dúplex entre diana y cebador con un elevado grado de homología, a veces también ocurren amplificaciones espúreas.
Esto puede suceder si uno o ambos cebadores elegidos contienen parte de una secuencia repetitiva.
Por ello, en el diseño de los cebadores se tienen en cuenta las secuencias repetitivas conocidas, y también se tiene la precaución de evitar largas tiras de un mismo nucleótido.

El apareamiento fortuito del extremo 3' del cebador es crítico, ya que aporta el extremo 3' OH libre para el inicio de la síntesis por parte de la DNA polimerasa: 
se pueden obtener productos espúreos de híbridos cebador- diana que no tengan una homología importante si el extremo 3' del cebador es exactamente complementario a la secuencia (le la diana.
Una forma de minimizar el problema de la amplificación de productos espúreos es el empleo de cebadores encajados ( nested primers ).
Los productos de la amplificación inicial se diluyen y son utilizados como DNA diana para una segunda reacción en la que se utiliza un juego de cebadores diferente, cuyas secuencias están situadas próximas a los cebadores anteriores, en una zona interna del fragmento teóricamente amplificado en primer lugar.

Las principales ventajas de la PCR como método de clonación son su rapidez, su sensibilidad y su robustez 

La PCR es una técnica popular gracias a su sencillez; 
su amplio rango de aplicaciones tiene su base en las tres principales ventajas del método.

Rapidez y sencillez de uso de la PCR

La PCR permite clonar DNA en pocas horas, utilizando equipos relativamente poco sofisticados.
Una reacción de PCR típica consiste en 30 ciclos de desnaturalización, síntesis y reasociación.
Cada ciclo dura típicamente 3-5 minutos y se utiliza un termociclador automatizado para cambiar las temperaturas correspondientes a cada paso del ciclo.
Esto supera ampliamente el tiempo requerido para la clonación en células, que suele ser de semanas, o incluso meses.
Por supuesto, el diseño y síntesis de los oligonucleótidos cebadores también lleva tiempo, pero este proceso ha sido simplificado gracias a la aparición de programas informáticos para el diseño de los cebadores, y a la proliferación de casas comerciales especializadas en la síntesis de oligonucleótidos por encargo.
Una vez que se pone a punto, la reacción puede ser repetida de forma sencilla.

Sensibilidad de la reacción de PCR

La PCR puede amplificar secuencias a partir de cantidades ínfimas de DNA diana, incluso a partir del DNA contenido en una sola célula.
Esta elevada sensibilidad ha permitido el desarrollo de nuevos métodos para el estudio de la patogénesis molecular y la aparición de numerosas aplicaciones en ciencia forense, en diagnóstico, en el análisis del ligamiento genético utilizando la tipificación de espermatozoides individuales y en estudios de paleontología molecular, donde las muestras pueden contener muy pocas células.
Sin embargo, el hecho de que el método tenga una sensibilidad tan elevada significa también que se deben extremar las precauciones para evitar la contaminación de la muestra con DNA extraños, por ejemplo a partir de células desprendidas del propio operador.

Robustez del método

La PCR permite la amplificación de secuencias específicas de material que contiene DNA muy degradado, o incluido en un medio que hace problemática su purificación convencional. 
Esto hace que el método resulte muy adecuado para estudios de antropología y paleontología molecular, por ejemplo para el análisis de DNA recuperado de individuos momificados y para intentar identificar DNA de muestras fósiles que contienen poquísimas células de criaturas extintas hace ya mucho tiempo.
El método se ha empleado con éxito también para la amplificación de DNA de muestras de tejidos fijadas con formol, lo cual ha tenido importantes aplicaciones en patología molecular v, en algunos casos, en estudios de ligamiento genético. 

Las principales desventajas de la PCR son la necesidad de disponer de información sobre la secuencia, el tamaño corto de los productos que genera y la infidelidad de la replicación del DNA

A pesar de su tremenda popularidad, la PCR, como método para la clonación selectiva de secuencias de DNA específicas, tiene ciertas limitaciones.

Necesidad de disponer de información sobre la secuencia del DNA diana

Para poder construir oligonucleótidos específicos que actúen como cebadores para la amplificación selectiva de una secuencia particular de DNA se necesita disponer de alguna información previa sobre la propia secuencia a amplificar.
Esto implica, por regla general, que la región de interés ya ha sido parcialmente caracterizada, a menudo mediante la aplicación de métodos de clonación basados en sistemas celulares.
Sin embargo, existen varias técnicas desarrolladas recientemente que reducen e incluso hacen desaparecer esta necesidad de disponer de información previa sobre la secuencia del DNA diana, en casos en los que se deben cumplir ciertos objetivos.
Por ejemplo, es posible clonar secuencias de DNA no caracterizadas que pertenecen a una familia génica o a una familia de secuencias repetitivas de las que se disponga información de alguno de sus miembros, utilizando oligonucleótidos degenerados.
En algunos casos, la PCR puede utilizarse también en ausencia de información sobre la secuencia del DNA diana, de manera que se permite la amplificación indiscriminada de secuencias de DNA cuando en la muestra el DNA es extremadamente escaso.
Pese a que la PCR se puede aplicar para asegurar la amplificación de todo el genoma, carece de la ventaja de los sistemas celulares de clonación, que ofrecen un método para individualizar los clones de DNA que componen una biblioteca de DNA genómico.

Tamaño corto de los productos de la PCR

Una desventaja clara de la PCR como método de clonación de DNA ha sido el tamaño de las secuencias de DNA que permite clonar.
A diferencia de la clonación de DNA en sistemas celulares, donde pueden clonarse secuencias de hasta 2 Mb, la información de que se dispone sobre secuencias clonadas por PCR sitúa el tamaño de los fragmentos clonados entre 0 y 5 kb, tendiendo hacia el extremo inferior. 

Los fragmentos pequeños se amplifican muy fácilmente, pero conforme aumenta su tamaño se hace más difícil obtener una amplificación eficiente.
Sin embargo, recientemente se han puesto a punto unas condiciones de reacción en las que se pueden llegar a amplificar fragmentos largos, incluyendo un fragmento de 42kb derivado del genoma del bacteriófago xxx .
Las condiciones para la PCR de largo alcance ( long range PCR) se basan en la combinación de las condiciones estándar con un sistema de dos polimerasas, de forma que se obtienen unos niveles óptimos de actividades DNA polimerasa y xxx exonucleasa.
Esta última actividad sirve para que funcione el mecanismo corrector de lectura de pruebas. 

Lectura de pruebas gracias a la actividad xxx exonucleasa asociada a la DNA polimerasa

La fidelidad de la replicación del DNA in vivo es extremadamente elevada:
durante la replicación de los genomas de mamíferos, por ejemplo, sólo una de cada xxx bases es copiada de manera incorrecta.
La incorporación equivocada tiene lugar con una frecuencia baja, dependiendo de las energías libres de las bases apareadas de forma correcta o incorrecta. 
Cambios muy pequeños en la geometría de la hélice pueden estabilizar pares G-T (con dos puentes de hidrógeno;
recuérdese la presencia frecuente de pares G-U en el RNA, página 7).
El proceso de copiado in vivo exhibe una frecuencia de error mucho menor que la que se desprendería de las meras limitaciones termodinámicas.
Esto ocurre gracias a la existencia de mecanismos correctores de lectura de pruebas ( proofreading), uno de los cuales es una propiedad común de las DNA polimerasas.
A diferencia de las RNA polimerasas, las DNA polimerasas tienen un requerimiento absoluto de un extremo 3' hidroxilo libre de una hebra apareada que actúe de cebadora para poder iniciar la extensión de la cadena. 
Además, la presencia de un nucleótido mal apareado en el extremo 3' de la hebra cebadora hace que esta hebra no sirva para iniciar o continuar la síntesis del DNA.
Muchas DNA polimerasas, incluyendo la de E. coli , presentan una actividad xxx exonucleasa integrada.
Cuando se inserta una base incorrecta durante la síntesis del DNA el proceso se detiene.

En lugar de seguir la síntesis, la actividad xxx exonucleasa elimina un nucleótido por vez a partir del extremo 3' hidroxilo, hasta que llega a un par de bases terminal correctamente apareado.
Esto permite que se reinicie la síntesis de DNA.
Por lo tanto, las DNA polimerasas son capaces, por regla general, de autocorregirse, eliminando los errores introducidos por la actividad DNA polimerasa durante la síntesis del DNA.

Infidelidad en la replicación del DNA

La clonación del DNA en sistemas celulares pasa por la replicación del DNA in vivo , proceso asociado a una elevada fidelidad de copiado debido a la existencia de mecanismos de lectura de pruebas y de corrección.
Sin embargo, cuando el DNA se replica in vitro la tasa de errores cometidos durante el copiado se dispara. 
La DNA polimerasa termoestable más utilizada en PCR es la Taq , obtenida de T. aquaticus .
Esta polimerasa carece de la actividad xxx exonucleasa que permite a otras polimerasas comprobar que los nucleótidos añadidos son los correctos (lectura o verificación de pruebas, en inglés proofreading ).
Esto hace que la tasa de errores cometidos durante la replicación catalizada por esta enzima sea bastante elevada:
para una secuencia de 1 kb que pasa por 20 ciclos de amplificación, aproximadamente el 40% de las nuevas hebras de DNA sintetizadas por PCR contendrá al menos un nucleótido incorrecto como consecuencia de un error de copiado.
Esto implica que, incluso cuando se usa como molde una única secuencia de DNA, el producto final de una reacción de PCR es una mezcla de secuencias extremadamente parecidas, aunque no idénticas. 
Pese a ello, si se secuencia el DNA de todo el producto obtenido por PCR se obtiene la secuencia correcta.
Esto se debe a que las variaciones individuales se producen esencialmente al azar, por lo que para cada posición las bases correctas superan con mucho alas bases incorrectas. 
Sin embargo, esto significa que cualquier análisis posterior del producto puede ser difícil.
Por ejemplo, para los estudios de tipo funcional se suele necesitar que el fragmento que se estudie sea clonado en un vector de expresión adecuado.
Al clonar los productos de PCR en un sistema celular se tiene el problema de que la transformación selecciona a partir de una sola molécula, y los clones celulares seleccionados para ser amplificados contendrán moléculas idénticas, derivadas de un solo representante de las moléculas de la muestra original, que bien podría ser portador de un error de copia introducido durante la amplificación por PCR.
En consecuencia, cuando se hace un experimento de este tipo se deben aislar varios clones y secuenciar los insertos, a fin de determinar la secuencia correcta (la secuencia consenso) antes de seleccionar los que contengan la secuencia auténtica para ser analizada en experimentos posteriores.

Recientemente se ha reducido considerablemente el problema de la infidelidad de la replicación del DNA durante la reacción de PCR gracias al empleo de DNA polimerasas alternativas que sí presentan la actividad xxx exonucleasa que permite la lectura de pruebas.
Un ejemplo es la DNA polimerasa de Pyrococcus furiosus .
El producto de PCR resultante tiene muchas menos mutaciones debidas a errores de copia:
para un segmento de 1kb de DNA que ha pasado por 20 ciclos efectivos de replicación, aproximadamente el 3,5% , de las hebras de DNA son portadoras de una base mal incorporada.

Aplicaciones de la PCR

A pesar de que la PCR se desarrolló con éxito hace sólo 10 años, su simplicidad y versatilidad han hecho que se cuente entre los métodos más ubicuos de la genética molecular, con una amplia gama de aplicaciones generales.

La PCR se utiliza frecuentemente para detectar polimorfismos y mutaciones patogénicas

Ensayos de polimorfismos de dianas de restricción y repeticiones en tándem

Los RSP ( Restriction Site Polymorphisms ) son polimorfismos que generan cambios en alguna diana de restricción (creación o destrucción) entre los diferentes alelos.
Estos polimorfismos pueden detectarse por hibridación de transferencias Southern de muestras de DNA genómico digeridos con la enzima de restricción adecuada y separadas por tamaño en electroforesis en gel de agarosa, utilizando sondas de DNA que representen el locus de interés.

Los RFLP resultantes presentarán dos alelas, tino que corresponde al conjunto de fragmentos que tiene la diana de restricción y otro que corresponde a los que carecen de él.
Una alternativa sencilla a la metodología de RFLP es la detección de RSP mediante PCR, basada en el diseño de cebadores que flanqueen la zona polimórfica, la posterior digestión de los productos de PCR con las enzimas de restricción adecuadas y la separación de los fragmentos por electroforesis en gel de agarosa.

Los polimorfismos de repeticiones cortas en tándem (STRP), también llamados marcadores microsatélite, son secuencias cortas, de uno a cuatro nucleótidos, que se repiten en tándem varias veces, v se caracterizan por la existencia de muchos alelas.
Por ejemplo, las repeticiones (CA)n/(TG)n son a menudo polimórficas cuando la n es mayor que 12, y, han sido muy utilizadas como marcadores polimórficos en el genoma humano (véase más adelante).
Sin embargo, últimamente se están identificando cada vez más polimorfismos marcadores basados en repeticiones de tri y tetranucleótidos. 
En todos los casos los STRP pueden estudiarse cómodamente mediante PCR.
Se diseñan los cebadores de forma que reconozcan secuencias situadas en los flancos de un locus STRP específico, lo que permite la amplificación de alelos cuyos tamaños difieren en un múltiplo entero de la unidad que se repite.
Los productos de PCR se pueden fraccionar utilizando electroforesis en gel de poliacrilamida. 
Para favorecer la identificación de los productos se mezcla con los precursores un precursor marcado, normalmente un nucleótido radioactivo o fluorescente.
Antes de la electroforesis se desnaturaliza el producto de la PCR, para asegurar que el fraccionamiento por tamaño es correcto. 
En la Figura 6.6 se muestra un ejemplo para el marcador CA.

Cribado de mutaciones

Gracias a su rapidez y simplicidad, la PCR es muy adecuada para la obtención de una cantidad de DNA suficiente para realizar un cribado de mutaciones.
Cuando se tiene una secuencia parcial del DNA genómico o del cDNA de un gen asociado con una enfermedad o con algún fenotipo interesante, se pueden diseñar inmediatamente reacciones de PCR específicas para ese gen.
La amplificación del segmento génico apropiado permite el examen rápido de la existencia de mutaciones asociadas, haciendo extensivo el análisis a un elevado número de individuos.
En contraste, si se pretendiera hacer lo mismo utilizando clonación en sistemas celulares el proceso sería tan sumamente lento y laborioso que no podría ser considerado una alternativa seria.

La posibilidad de identificar fronteras exón/intrón y secuenciar los extremos de los intrones de un gen de interés permite el cribado de mutaciones genómicas:
se ponen a punto ensayos de amplificación específicos de exones individuales del gen de interés, diseñando cebadores que reconozcan secuencias intrónicas próximas a la frontera con el exón investigado.
Los productos de PCR resultantes se analizan mediante métodos rápidos de cribado de mutaciones, muchos de los cuales se basan en el análisis de productos de tamaño no superior a las 200 bp.

En los seres humanos el tamaño medio de los exones es de 180 bp, lo que resulta muy conveniente para este tipo de análisis.
Sin embargo, en caso de que se trate de exones muy grandes, lo habitual es diseñar una serie de cebadores que permitan generar productos exónicos solapados que cubran toda su extensión.

La PCR permite también amplificar secuencias de cDNA para el cribado de mutaciones.
Este cribado de mutaciones en el cDNA puede ser el único método para estudiar mutaciones que afecten a un gen si no se dispone de información sobre su organización de intrones/exores.
Para realizar el estudio sobre cDNA se purifica en primer lugar el mRNA a partir de un tejido conveniente, por ejemplo células sanguíneas.
Este mRNA se pasa a cDNA utilizando transcriptasa inversa, y el cDNA así obtenido se utiliza como sustrato para una reacción de PCR.
Esta versión de la reacción estándar sobre DNA genómico recibe el nombre de RT-PCR (acrónimo del inglés Reverse Transcriptase PCR , o transcriptasa inversa PCR ).
El método es obviamente adecuado para genes que se expresan mucho en células de fácil acceso, como las células sanguíneas. 
Sin embargo, dado que en todos los tejidos existe una cierta transcripción ectópica (transcripción de bajo nivel que se da para casi todos los genes en casi todos los tejidos) el método se ha aplicado también para el estudio de transcritos de genes que no se expresan de forma significativa en células sanguíneas, como por ejemplo el gen de la distrofina( DMD ).

Figura 6.1

La PCR es un método de amplificación de secuencias de DNA in vitro , que utiliza oligonucleótidos de secuencia definida como cebadores (iniciadores) de la reacción

Los oligonucleótidos cebadores A y B son complementarios a secuencias de DNA situadas en hebras opuestas y a ambos lados de la región que se quiere amplificar. 
Los cebadores que se asocian con las hebras acaban siendo incorporados a las hebras de nueva síntesis.
El primer ciclo genera dos nuevas hebras de DNA, cuyos extremos 5' quedan fijados por la posición de los cebadores y sus extremos 3' son variables [los extremos 3' están "raídos" ( ragged)).
En los ciclos posteriores las hebras de nueva síntesis actúan a su vez como molde para la síntesis de hebras complementarias del tamaño deseado (los extremos 5' quedan definidos por el cebador y los extremos 3' quedan fijos ya que la síntesis no puede ir más allá del extremo reconocido por el otro cebador).
Tras unos pocos ciclos, el producto de tamaño deseado fijo comienza a ser predominante.

Figura 6.4

La PCR permite identificar fácilmente polimorfismos de dianas de restricción, y por ello constituye una alternativa a los laboriosos ensayos de RFLP

Los alelos 1 y 2 se pueden diferenciar por un polimorfismo que modifica la secuencia de nucleótidos de una diana específica para la nucleasa de restricción R:
el alelo 1 posee la diana, pero el alelo 2 tiene uno o más nucleótidos cambiados, X, X' , por lo que carece de ella.
Es posible diseñar cebadores para PCR que flanqueen la región que contiene la diana de restricción, de forma que se amplifique un producto corto.
La digestión del producto de la PCR utilizando la enzima de restricción R y el fraccionamiento por tamaño de los fragmentos resultantes permite la identificación sencilla de ambos alelos.

Figura 6.5

La PCR se puede utilizar para identificar polimorfismos de repeticiones cortas en tándem (STRP)

El ejemplo ilustra la identificación (tipificación) del polimorfismo de una repetición del dinucleótido (CA)/(TG), que presenta tres alelos como consecuencia de la variación del número de repeticiones de (CA).
En la autorradiografía, cada alelo queda representado por una banda superior más intensa y dos bandas "sombra" inferiores menos intensas.
Los individuos 1 y 2 tienen los siguientes genotipos (entre paréntesis):

Figura 6.6

Ejemplo de tipificación de una repetición CA

El ejemplo de la figura muestra la tipificación de los miembros de una familia grande utilizando el marcador (CA)/(TG) D17S800.
Las flechas de la izquierda señalan la banda superior (banda principal) que puede apreciarse en los diferentes alelos 1-7. Obsérvese que los alelos individuales exhiben una banda superior intensa seguida de dos "bandas sombra" inferiores, una de intensidad intermedia situada inmediatamente por debajo de la banda superior, y otra que es muy débil y que se sitúa inmediatamente por debajo de la primera banda sombra.
Los genotipos correspondientes a los diferentes individuos de la familia son (entre paréntesis): xxx .
Obsérvese que en el último caso la banda media es particularmente intensa debido a que contiene tanto la banda principal del alelo 4 como la banda media del alelo 3.
Se supone que el apareamiento desfasado de las hebras es el principal mecanismo responsable de la producción de las bandas sombra en las repeticiones en tándem de dinucleótidos (Hauge y Litt, 1993).

Figura 6.7

Los productos de PCR destinados al cribado de mutaciones se obtienen a partir de DNA genómico utilizando cebadores específicos de intrones que flanqueen exones o bien utilizando RT-PCR.

A DNA genómico.
Los exones 1-4 pueden ser amplificados independientemente a partir de DNA genómico utilizando los pares de oligos específicos de intrones xxx , etc.

B RT-PCR.
Esta reacción se basa en que existe un mínimo de mRNA del gen de interés presente en células de fácil acceso, como las células sanguíneas, y esto permite su conversión a cDNA. 
El cDNA puede ser utilizado como molde en una reacción de PCR con pares de cebadores específicos de exones como por ejemplo xxx etc. que permiten la obtención de fragmentos solapados


Figura 6.2

Diseño de cebadores para PCR

Tabla

Tamaño.
Habitualmente unos 20 nt para secuencias diana presentes en DNA genómico complejo; puede ser mucho menor si el DNA diana es menos complejo .


Composición de base.
Evitar repeticiones en tándem de uno o más nucleótidos. .
Se debe elegir un %GC global de forma que la Tm , de cada oligonucleótido sea igual o casi idéntica.


Extremos 3' .
Debe evitarse que las dos bases de los extremos 3' de los cebadores sean complementarias entre sí. .
De lo contrario podrían producir dímeros de cebadores que reducirían la eficiencia de la amplificación .


Figura 6.3

La PCR tiene numerosas aplicaciones de carácter general

La figura ilustra las aplicaciones generales. 
Las aplicaciones específicas se describen en otros capítulos.
El paseo cromosómico se refiere a la posibilidad de acceder a DNA no caracterizado a partir de una secuencia vecina conocida.

Tabla

Tipificación de marcadores genéticos .
RFLP; STRP.


Obtención de DNA para el cribado de mutaciones.
Cribado de mutaciones genómicas y RT PCR.


Detección de mutaciones puntuales.
Mutaciones que alteran dianas de restricción - el mismo principio que en la Figura 6.4 .
Otras mutaciones por amplificación específica de alelo (ARMS).


Clonación de cDNA.
A partir de la secuencia de aminoácidos utilizando DOP-PCR;clonación de los extremos de un cDNA utilizando RACE.


Clonación de DNA genómico.
Clonación de nuevos miembros de una familia de DNA mediante DOP-PCR.
Amplificación del genoma completo o amplificación subgenómica (por ejemplo, bandas cromosómicas microdiseccionadas) mediante DOP-PCR o PCR iniciada con oligonucleótidos conectores/adaptadores .


Paseo genómico.
PCR inversa.
PCR con conectores burbuja.
IRE-PCR.


Obtención, de DNA para secuenciación .
DNA de cadena sencilla mediante PCR asimétrica .
DNA de doble cadena para secuenciación directa o para clonación convencional seguida de secuenciación .


Mutagénesis in vitro .
Usando mutagénesis por añadidos en 5' para crear un producto de PCR recombinante.
Usando cebadores con apareamientos incorrectos para cambiar un único nucleótido previamente determinado.


Identificación de genes de enfermedades humanas

Principios básicos y estrategias en la identificación de genes de enfermedades

Antes del año 1980 eran muy pocos los genes humanos identificados como loci de enfermedades.
Estos éxitos iniciales se basaron mayormente en hechos excepcionales:
las bases bioquímicas de las enfermedades en cuestión ya habían sido establecidas y la purificación del producto génico no comportaba mayor dificultad.
Sin embargo estas ventajas no existen para la gran mayoría de las enfermedades que resultan de mutaciones que afectan a genes humanos.
En los años ochenta la aplicación de la tecnología de DNA recombinante ofreció nuevas estrategias para el cartografiado y la identificación de genes subyacentes a desórdenes de genes únicos y cánceres somáticos, y el número de genes de enfermedades identificados como tales comenzó a crecer con extraordinaria rapidez.
Con el advenimiento posterior de los estudios rápidos de ligamiento basados en PCR v de las tecnologías de cribado de mutaciones por PCR, la identificación de nuevos genes de enfermedades pasó a ser habitual y en la actualidad no pasa una semana sin que se descubra un nuevo gen.

Es importante hacer notar que no todos los 65,000 a 80,000 genes humanos serán identificados como genes de enfermedades. 
Algunos genes son indispensables para la función embrionaria, de forma que las mutaciones deletéreas que los afecten pueden provocar letalidad embrionaria y por tanto nunca se observa su efecto en humanos.
En otros casos, la abolición de la función génica puede no tener efecto sobre el fenotipo debido a la existencia de redundancia genética , es decir, que existan otros genes no alélicos que suplen la misma función. 
El catálogo de trastornos hereditarios humanos, Mendelian Inheritance in Man , contiene actualmente una lista de 5000 caracteres mendelianos. 
Como veremos en el Capítulo 15, no existe una correspondencia uno a uno entre los genes y los síndromes clínicos.
Algunas veces mutaciones diferentes de un mismo gen pueden provocar la aparición de fenotipos distintos, v es frecuente que el mismo fenotipo de una enfermedad pueda ser causado por mutaciones en genes distintos.

Los desórdenes hereditarios más comunes son los que resultan más difíciles de estudiar utilizando genética molecular ya que generalmente implican una combinación de diferentes genes (combinación oligogénica o trastornos poligénicos, ver Capítulo 18), así como diferentes agentes ambientales que actúan de disparadores de los procesos patológicos.
De forma similar los cánceres más comunes implican fenómenos celulares en los que hay varios genes implicados.
En consecuencia, no es extraño observar que los genes de enfermedades humanas que han sido aislados hasta la fecha corresponden mayormente a los genes de copia única, responsables de los trastornos de genes de copia única, o bien a cánceres somáticos donde existe un gen de susceptibilidad principal.
Este capítulo se centrará en los intentos de identificación de estos últimos tipos de genes;
las estrategias empleadas para la identificación de genes implicados en las enfermedades más comunes serán tratadas en el Capítulo 18.

Para la identificación de genes de enfermedades se pueden describir cuatro estrategias generales

La elección de la estrategia a seguir para identificar el gen de una enfermedad depende de los recursos disponibles (modelos animales, anomalías cromosómicas, bibliotecas de clones, etc.), o bien de lo que se conoce acerca de la patogénesis de la enfermedad.
Varias de las estrategias posibles comienzan por intentar identificar varios genes candidato , que deben ser examinados individualmente para hallar evidencias que les señalen como el locus de la enfermedad.
Se han utilizado varios métodos para identificarlos. 
Sin embargo, en general situar la enfermedad en el mapa mediante su localización subcromosómica específica es el paso inicial que resulta más fructífero. 

La clonación posicional puede ser extraordinariamente laboriosa, y no suele intentarse en serio hasta que se obtiene la posición aproximada del gen dentro de una región de 1 a 2 Mb.
Existen algunos genes para los que es imposible elaborar mapas tan ajustados.
Si un trastorno es raro, puede ser difícil identificar entrecruzamientos que reduzcan la localización del gen de la enfermedad un intervalo pequeño, y la probabilidad de identificar a un paciente con una anomalía citogenética que pueda refinar la localización puede ser virtualmente cero.
En estos casos la única manera de proceder es adoptar una estrategia relacionada con la identificación de genes candidato.

Estrategias generales para la identificación de genes de enfermedades humanas

Clonación funcional

Alguna información disponible sobre la función se aprovecha para aislar un clon del gen.
Si el producto génico es conocido su purificación parcial puede permitir varias estrategias para la identificación del gen correspondiente.
Como alternativa la búsqueda del gen puede basarse en un ensayo funcional.
Esta estrategia, en realidad, ha sido útil pocas veces.

Clonación posicional

Este método implica que el gen se aísla conociendo únicamente su localización subcromosómica, sin utilizar información sobre su función bioquímica o su patogénesis.
La estrategia general consiste en intentar construir mapas físicos y genéticos de la región, refinar la localización subcromosómica y entonces identificar genes en la región para investigar como candidatos de la enfermedad.

La clonación posicional continúa siendo ardua, y a medida que se acumula información que permite estrategias de genes candidato posicionales va perdiendo importancia.

Estrategias de genes candidato independientes de la posición

Se puede tener un gen candidato para un trastorno humano sin disponer de información sobre su localización cromosómica.
Esto puede ocurrir si el fenotipo se parece a otro fenotipo humano o de animales cuyo gen es conocido, o si la patogénesis molecular sugiere que el gen puede ser miembro de una familia génica conocida.
Estas estrategias han tenido éxito en contadas ocasiones, y se han visto superadas por las de genes candidato posicionales. 

Estrategias de genes candidato posicionales 

Una vez que una enfermedad ha sido totalmente cartografiada resulta cada vez más factible utilizar las bases de datos para identificar genes candidato.
Con más y más genes humanos siendo localizados en posiciones subcromosómicas específicas, las estrategias basadas en genes candidato posicionales están llamadas a dominar ampliamente el campo. 

Los genes candidato para alguna enfermedad humana deben investigarse para hallar evidencias de asociación con la enfermedad

Cada una de las estrategias detalladas en el Recuadro 14.1 genera genes candidato de la enfermedad, los cuales deben ser investigados individualmente para determinar la probabilidad de que realmente estén asociados con ella. 
Un buen candidato debería tener un patrón de expresión coherente con el fenotipo de la enfermedad, aunque muchas veces la patología viene determinada por un pequeño subgrupo de los tejidos que expresan los genes afectados.
Existen varias maneras de demostrar que ungen candidato ocupa probablemente el locus de la enfermedad.

Cribado de mutaciones

La búsqueda de mutaciones puntuales específicas de paciente que afecten al gen candidato es, con mucho, el método más popular, debido a que se puede aplicar casi siempre v es un método comparativamente rápido.
La identificación de mutaciones en el gen candidato en varios individuos afectados es una poderosa evidencia de que se ha identificado el gen correcto, pero la demostración formal requiere evidencias adicionales.

Restablecimiento del fenotipo normal in vitro 

En algunas enfermedades, particularmente aquellas en las que las mutaciones provocan pérdida de función, el fenotipo es reversible.
Si una línea celular que exhibe el fenotipo mutante puede cultivarse a partir de las células de un paciente, la transfección de un alelo clonado normal a las células cultivadas provenientes del enfermo puede restablecer el fenotipo normal al complementar la deficiencia genética original.

Producción de un modelo de la enfermedad en ratón

Una vez que se identifica un posible gen de enfermedad se puede construir un modelo en ratón utilizando la tecnología de transgénicos.
Se caracteriza el gen ortólogo del ratón y, si se sabe que el fenotipo resulta de una pérdida de función, se provoca la pérdida dirigida de la función del gen del ratón introduciendo una mutación deletérea en la línea germinal. 
En inglés, esta poderosa tecnología recibe el nombre de gene targeting .
Cabe esperar que los ratones mutantes presenten un fenotipo parecido al de la enfermedad humana, aunque esto no siempre sucede.
Si el fenotipo de la enfermedad humana resulta de una ganancia de función, entonces los intentos para lograr un modelo en ratón pasan por la introducción del alelo de la enfermedad en la línea germinal.

Clonación funcional

Clonación funcional significa que la identificación del gen desconocido causante de la enfermedad se basa en la información disponible sobre su función.
En esencia se emplean dos estrategias: las basadas en la disponibilidad del producto génico purificado y las basadas en la existencia de un ensayo funcional para el gen en cuestión.

La purificación parcial de un producto génico permite la producción de oligonucleótidos o de anticuerpos específicos que pueden utilizarse para la identificación del gen

Si se conocen las bases bioquímicas de una enfermedad hereditaria, puede ser posible purificar y parcialmente caracterizar algo del producto génico.
Si esto se logra, la identificación del gen de la enfermedad puede pasar por dos estrategias.

Oligonucleótidos específicos del gen

Esta estrategia confía en la capacidad de purificar suficiente producto proteico para poder hacer secuenciación de aminoácidos.
Existen enzimas proteolíticas, como la tripsina, que cortan los enlaces peptídicos del producto proteico por posiciones específicas (la tripsina corta por el extremo carboxilo de residuos de lisina o arginina).
El mismo efecto producen ciertos reactivos químicos, como el bromuro de cianógeno (que corta por el extremo carboxilo de los residuos de metionina).
La secuencia de aminoácidos de los péptidos resultantes de la digestión se puede determinar mediante secuenciación química.
Para la secuenciación química se realizan series repetidas de reacciones en un secuenciador de aminoácidos .
En cada ciclo de la secuenciación se expone el péptido a un agente químico que se une covalentemente al aminoácido N-terminal y posteriormente separa este aminoácido del esqueleto peptídico.
Al individualizarse, el aminoácido puede ser identificado por métodos cromatográficos.
Los péptidos solapantes pueden identificarse gracias a los solapamientos observados en la secuencia, lo cual permite reconstruir secuencias más largas que las de los péptidos resultantes de la digestión inicial.

La secuencia de aminoácidos resultante se inspecciona para identificar regiones que contengan aminoácidos con una mínima degeneración de codones (por ejemplo, metionina y triptófano están codificados por un solo codón, AUG y UGG, respectivamente).
Una vez que se identifican regiones adecuadas se sintetizan oligonucleótidos que contengan todas las posibles combinaciones de codones que generen la secuencia de aminoácidos deseada.
La mezcla resultante de oligonucleótidos parcialmente degenerados se marca y se usa como sonda para cribar bibliotecas de cDNA.
Como sólo uno de los oligonucleótidos de la mezcla corresponderá a la secuencia auténtica es importante mantener reducido el número de oligonucleótidos diferentes, para tener así mayor probabilidad de identificar la diana correcta.
Una vez que se aísla un clon de cDNA adecuado puede emplearse para cribar una biblioteca de DNA genómico para aislar así clones de DNA genómico que permitan la completa caracterización del gen.

La identificación del gen de la hemofilia A es un ejemplo de esta estrategia.

Los análisis bioquímicos de muestras de suero de individuos con hemofilia A habían establecido previamente que existía una deficiencia genética en uno (le los factores de coagulación, el factor VIII.
La purificación del factor VIII a partir del plasma no es tarea sencilla, en parte porque se encuentra en muy poca cantidad. 
Una estrategia fue purificar grandes cantidades de factor VIII a partir de volúmenes grandes de sangre de cerdo, utilizando métodos estándar de purificación de proteínas.
El producto purificado permitió la producción de oligonucleótidos específicos del len que fueron utilizados como sonda para el cribado de bibliotecas. 

Cuando se emplean mezclas complejas de oligonucleótidos el cribado de bibliotecas por hibridación puede ser bastante tedioso, debido a que los resultados están enormemente influidos por las condiciones de hibridación.
Una alternativa más rápida es emplear los oligonucleótidos parcialmente degenerados como cebadores de reacciones de PCR. 
Una estrategia utilizada al principio del desarrollo de la técnica fue utilizar dos grupos de oligos degenerados correspondientes a regiones diferentes de la proteína. 
Utilizando un cDNA adecuado como molde pudo amplificarse un cDNA específico que abarcaba los codones comprendidos entre las dos regiones.
Sin embargo, esta aproximación demanda disponer de una considerable información previa sobre la secuencia de la proteína.
Una alternativa más conveniente es preparar cDNA y ligarlo a un vector. 
La PCR se realiza entonces empleando un cebador derivado de la proteína (los oligos parcialmente degenerados) y un cebador específico de aluna secuencia en el vector. 

Anticuerpos específicos 

Sólo con poder disponer de una pequeña cantidad del producto proteico normal se pueden generar anticuerpos específicos. 
La proteína, o un péptido derivado de ella, se conjuga a un agente poderosamente inmunogénico, tal como la proteína hemocianina de lapa, y la molécula compuesta se inyecta a un ratón o a un conejo.
El agente inmunogénico activa los linfocitos B, y la proteína o el péptido de interés activa los linfocitos T colaboradores, estimulándose así la producción de anticuerpos.
Los anticuerpos producidos por el ratón o el conejo que sean específicos de la proteína o el péptido de interés pueden ser utilizados siguiendo diversos métodos para identificar el correspondiente cDNA.

Una estrategia del inicio de la aplicación de estas técnicas consistió en el enriquecimiento del mRNA codificante del producto mediante sistemas in vitro de síntesis proteica libre de células .
Este método fue el empleado para la identificación del gen de la fenilcetonuria. 
Se sabía que la fenilcetonuria era causada por la falta de la enzima fenilalanina hidroxilasa.
La enzima fue purificada a partir de hígado, ya que se sabía que allí se expresaba.
Se produjeron anticuerpos específicos, que fueron utilizados para inmunoprecipitar polisomas que contenían mRNA de la fenilalanina hidroxilasa. 
El mRNA purificado fue posteriormente convertido a cDNA y esto permitió el aislamiento de un clon específico de cDNA.

Este tipo de aproximaciones experimentales se ha visto superado por el cribado de bibliotecas de cDNA utilizando anticuerpos .
El cribado con anticuerpos requiere disponer de una biblioteca de cDNA construida clonando el cDNA de los tejidos relevantes en un vector de expresión. 
Se espera que los insertos de los clones recombinantes se expresen dentro de la célula huésped, produciendo polipéptidos foráneos.
Esto permite el empleo de anticuerpos específicos para cribar filtros de colonias derivadas de tales bibliotecas de cDNA de expresión , y la obtención de positivos abre el camino para la identificación de los clones que codifican el producto de interés.

Los genes de algunas enfermedades se han identificado mediante ensayos de complementación funcional

En algunos casos se puede realizar un ensayo de complementación funcional , incluso cuando se desconoce el producto génico.
Se han empleado mutantes de levadura para identificar genes humanos que desempeñan funciones bioquímicas específicas.
Los mutantes definidos de levadura son relativamente abundantes, y el análisis genético puede llegar a ser particularmente sofisticado porque es muy fácil obtener recombinación homóloga. 
Para proteínas que han sido muy conservadas por la evolución, la proteína humana puede ser capaz de complementar un mutante de levadura defectuoso en la proteína equivalente.
El procedimiento implica la transformación de las células de levadura mutantes con diferentes fragmentos de DNA humano hasta dar con el que rescata el fenotipo normal. 
Esta estrategia ha tenido éxito en la identificación de los genes humanos que especifican varias enzimas de la biosíntesis de las purinas y de las pirimidinas, así como algunos factores de transcripción crucialmente importantes, etc. 

La clonación por complementación funcional también ha permitido identificar genes de enfermedades humanas, particularmente aquéllos implicados en la reparación del DNA.
Existen varias líneas celulares de mamíferos que son deficientes en sus mecanismos de reparación de DNA.
Se caracterizan por presentar respuestas anormales al ser expuestas a la radiación UV o a mutágenos químicos.
Estas células mutantes, o alternativamente células derivadas de pacientes con una deficiencia en reparación de DNA, pueden transformarse con fragmentos de DNA humano normal o con cromosomas humanos completos hasta lograr un fenotipo de reparación eficiente.
Utilizando este tipo de métodos pudieron obtenerse clones de cDNA para el gen del grupo C de la anemia de Fanconi ( FACC ).

Clonación posicional

La clonación posicional implica la clonación de un gen del que se ignora prácticamente todo excepto su localización subcromosómica.
La localización inicial suele definir una región candidata relativamente grande, de 10 Mb o más.

Estas localizaciones iniciales pueden haber sido deducidas a partir de varias metodologías, de las cuales las más comunes son:

- Análisis de ligamiento .
Identifica un marcador que presente un lod superior a 3 en estudios familiares.
Éste es el procedimiento más común de localización de enfermedades hereditarias.

- Cribado de pérdida de heterozigosis .
Identifica una región cromosómica que se pierde habitualmente en tumores.
Los genes supresores de tumores importantes en cáncer suelen ser localizados inicialmente de esta manera

- Anomalías cromosómicas .
Pueden implicar a una región grande en una enfermedad.
Por ejemplo, las translocaciones y deleciones identificaron Xp21 coto la localización del gen de la distrofia muscular de Duchenne (DMD).


Pasar de una localización tan inicial a la identificación del gen constituye un logro muy importante. 
La primera clonación posicional fue publicada en 1986 y marcó el inicio de una nueva era para la genética molecular humana.
Uno tras otro se fueron aislando genes de enfermedades importantes como la DMD, la fibrosis quística (CF), la enfermedad de Huntington (HD), la poliquistosis renal adulta, el cáncer colorrectal, el cáncer de mama, etc.
Sin embargo, la clonación posicional puede ser extremadamente ardua, y hacia 1995 sólo 50 enfermedades hereditarias habían sido identificadas de esta forma. 
Además, casi todos los éxitos habían sido posibles gracias a otros factores además de la clonación posicional pura - normalmente el descubrimiento de una mutación de gran escala en uno de los pacientes, o la existencia de desequilibrio de ligamiento.
En la actualidad, con el siempre creciente número de genes humanos situados sobre el mapa y con la expectativa de que hacia 1998 se dispondrá de un mapa físico que contenga la gran mayoría de los genes humanos, las estrategias basadas en candidatos posicionales están listas para tomar el relevo a la pura clonación posicional. 

Afinar la región candidata requiere disponer de clones y marcadores

Para poder afinar la región candidata hace falta disponer de una serie de clones y marcadores de la región de que se trate.
Se necesitan marcadores polimórficos que permitan generar un mapa genético de alta resolución de la región, y se necesitan clones que suministren tales marcadores y que permitan comenzar el cartografiado físico. 
Gracias al avance del Proyecto Genoma Humano muchos de los recursos necesarios ya han sido generados a escala del genoma completo.
En muchas regiones cromosómicas los mapas genéticos de alta resolución ya existentes permiten situar el gen de una enfermedad entre marcadores separados por sólo 1 cM, siempre y cuando se disponga de suficiente material familiar.
Los esfuerzos en marcha para construir mapas físicos detallados de los cromosomas humanos hacen que sea posible que para la región de interés ya se disponga de cóntigos ordenados (habitualmente de clones YAC).
Si la región es inestable o resulta difícil de clonar en YAC puede resultar una ventaja utilizar vectores de clonación diferentes (P1, PAC o BAC).
Los grandes insertos de estos clones pueden subclonarse en vectores más manejables, por ejemplo cósmidos, para estudios más detallados.
Algunas veces estos recursos permiten la identificación de mutaciones de gran escala asociadas a enfermedades, y estas mutaciones, a su vez, pueden ayudar a apuntar la localización del gen de la enfermedad.

En la actualidad, estos clones pueden encontrarse cribando bibliotecas de grandes insertos que son de dominio público y están almacenadas en algunos pocos centros de referencia. 
Lo único que puede hacer falta realizar en el laboratorio propio es ordenar en forma de cóntigo un número modesto de clones.
Al final de la década de los 80, cuando los mapas genéticos y especialmente los físicos estaban todavía en una fase primitiva y las librerías disponibles tenían insertos pequeños se necesitaban puntos de inicio adicionales para poder ensamblar los cóntigos dentro de las regiones candidatas.
Se diseñaron varios ingeniosos métodos para rescatar clones de DNA de regiones que contenían el gen de alguna enfermedad. 

Clonación por microdisección de cromosomas

Empleando técnicas de micromanipulación se puede separar un trozo pequeño de cromosoma de una preparación citogenética situada sobre un portaobjetos de microscopía. 
La microdisección se realiza sobre varias células con la ayuda de agujas extremadamente tinas o rayos láser. 
Los fragmentos, que típicamente corresponden a una banda cromosómica, se agrupan y su DNA es extraído y utilizado como base para la construcción de bibliotecas de DNA.
Este tour de force técnico ha sido útil para la generación de clones de DNA de varias regiones subcromosómicas, previamente poco caracterizadas, asociadas con alguna enfermedad.
Sin embargo, las bibliotecas de microdisección son difíciles de construir;
la contaminación por DNA externo a la región ha sido un problema y la complejidad (el número de secuencias de DNA diferentes) ha sido a menudo pobre.
Esta estrategia se ha visto superada por el advenimiento del cartografiado a gran escala de clones YAC.

Figura 14.1

Cuatro maneras de identificar el gen de una enfermedad humana

Obsérvese que los dos métodos de genes candidato comienzan con la hipótesis de un gen candidato particular, pero que en última instancia los cuatro métodos identifican genes candidato para la enfermedad de interés, cuya asociación con la enfermedad debe ser comprobada posteriormente.

Figura 14.2

El gen del factor VIII, el locus de la hemofilia A, fue clonado mediante el cribado de bibliotecas de DNA con oligonucleótidos deducidos directamente del producto proteico.

La figura ilustra una de las vías de obtención de clones de DNA del factor VIII.

Tras la digestión del factor VIII porcino purificado para dar lugar a péptidos, y la posterior secuenciación de aminoácidos de los péptidos obtenidos, se inspeccionaron las secuencias resultantes para identificar regiones con pocos codones redundantes.
El panel superior muestra una secuencia de 15 aminoácidos desde His8 a Met22 en uno de los péptidos, con las posibles permutaciones de codones representadas arriba (con los nucleótidos variables en rojo).
Se seleccionó esta secuencia porque presentaba una redundancia de codones relativamente baja:
dos aminoácidos, Trp y Met, están especificados por un solo codón, y otros siete pueden ser especificados únicamente por dos codones alternativos. 
A partir de esta información se preparó un oligonucleótido parcialmente degenerado de 45 bp, que fue utilizado como sonda de hibridación primaria para el cribado de una biblioteca de DNA genómico porcino. 
El cribado secundario y los posteriores se basaron en una sonda que era un oligonucleótido antisentido parcialmente degenerado, de 15 bp, que correspondía a la secuencia de Trp18 a Met22.
Una vez aislado, el clon genómico del factor VIII porcino fue utilizado como sonda para cribar bibliotecas de DNA humano a fin de identificar el gen humano. 

Figura 14.3

Las estrategias de genes candidato posicionales están destinadas a imponerse sobre otros métodos de identificación de genes de enfermedades humanas

Pruebas genéticas en individuos y poblaciones

Los genetistas no tienen el monopolio del diagnóstico basado en el DNA.
Por ejemplo, la PCR es una herramienta básica para los microbiólogos y los virólogos interesados en la identificación de patógenos, mientras que los hematólogos, oncólogos y otros patólogos emplean el examen del DNA como una de las bases del diagnóstico.
Sin embargo, en este capítulo nos limitaremos a hablar del diagnóstico basado en el DNA en el contexto de la genética molecular humana.

Diagnóstico clínico y ensayos de laboratorio

Las definiciones clínicas de las enfermedades resultan siempre algo arbitrarias.

En esencia, el clínico está diciendo, de forma más o menos convincente, que "este paciente que tengo que tratar se parece a un grupo de otros pacientes a los que he Visto anteriormente o sobre los que he leído, y que de alguna forma presentan características comunes". 
Un buen médico es un individuo sumamente hábil en realizar tales definiciones fenotípicas.
Para el cuidado de los pacientes v para muchos otros propósitos éste es el método más útil de clasificación, y en algunas ramas de la medicina no existen otras bases para establecer una clasificación mas fundamental de los fenotipos. 

Los genetistas son afortunados ya que, al menos para las enfermedades mendelianas, existe un principio subyacente natural básico.
Todas las enfermedades mendelinas pueden ser clasificadas, en primer lugar en función del locus implicado y en segundo por el alelo mutado particular que ocupa el citado locus.
Dado que los diagnósticos no son convenciones sencillas, evolucionan a medida que lo hace también el conocimiento de la genética subyacente.
Las enfermedades pueden o bien agruparse (distrofias musculares de Duchenne y de Becker) o bien separarse (la retinitis pigmentosa está en la OMIM bajo siete encabezamientos autosómicos dominantes, uno autosómico recesivo y tres ligados al X).
Algunas veces la clasificación original de todo un espectro de enfermedades basada en la clínica acaba siendo completamente cambiada por el análisis molecular, tal como ha ocurrido con la osteogénesis imperfecta.

Las clasificaciones clínica y genética de las enfermedades no compiten una contra otra.
Cada una tiene sus aplicaciones, y una clasificación satisfactoria ha de incluir ambos aspectos.
Para la gestión de los pacientes y para establecer prognosis no resulta demasiado útil clasificar las enfermedades en grupos alélicos. 
Como se ve en los ejemplos del Capítulo 15, la patología molecular es una ciencia muy imperfecta.
Entre los problemas que presenta se incluyen

- El que mutaciones de varios loci diferentes puedan producir el mismo síndrome clínico (por ejemplo, sordera congénita)
- El que mutaciones diferentes de un mismo locus puedan producir diferentes síndromes clínicos, por ejemplo en las hemoglobinopatías y en las mutaciones en RET
- El que las enfermedades genéticas suelan ser variables, incluso dentro de la misma familia, de forma que conocer la existencia de la mutación no necesariamente permite predecir las características que presentará el paciente.
La neurofibromatosis tipo 1 (MIM 162200) es uno de los muchos ejemplos disponibles.


De esta forma, las descripciones basadas en DNA de las enfermedades genéticas complementan, más que superan, a las descripciones clínicas tradicionales. 
Sin embargo, la predicción genética es un área en la que la identificación del locus afectado, y cuando se puede de la mutación exacta, resulta fundamental. 
La genética es una rama inusual de la medicina, ya que muchas de las consultas las realizan personas fenotípicamente normales.
Estas personas quieren saber si sus hijos aún no nacidos están afectados, o si es posible que acaben desarrollando los síntomas de una enfermedad de manifestación tardía.
El diagnóstico del trastorno que afecta a la familia requiere de las habilidades clínicas tradicionales, mientras que para el consejo genético se puede calcular un riesgo estadístico basado en principios mendelianos estándar o en datos empíricos.
Sin embargo, sólo los datos de los exámenes realizados en el laboratorio pueden dar una respuesta definitiva.
La demostración que un paciente ha heredado o no una mutación que se sabe afecta a la familia es el requerimiento fundamental de las predicciones genéticas acertadas.

Pruebas directas e indirectas

La cantidad de información extraíble de los exámenes del DNA depende del conocimiento sobre el gen o genes implicados, aunque en principio el diagnóstico basado en DNA puede realizarse de dos maneras esencialmente distintas. 

- Pruebas directas : se examina el DNA de la persona que hace la consulta para ver si es o no portadora de una mutación patogénica.

- Pruebas indirectas ( gene tracking ) : se utilizan marcadores ligados para descubrir, en estudios familiares, si la persona que hace la consulta ha heredado o no el cromosoma portador de la enfermedad de uno de sus progenitores.


En general es preferible el examen directo, aunque como se muestra en la Figura 16.1 y en la discusión de más adelante, esto no siempre es posible. 
Incluso cuando es científicamente posible realizar el examen directo, puede ocurrir que no siempre sea practicable en el contexto de un servicio de diagnóstico de rutina. 

Pruebas directas

El mejor diagnóstico basado en DNA es el que se establece a partir de un examen directo del DNA de la persona interesada, a fin de comprobar si la secuencia de su gen es normal o mutante.
Sin embargo, las pruebas directas no son siempre practicables.
En primer lugar, debemos conocer por supuesto qué gen examinar, y debemos conocer también la secuencia "normal" (silvestre).
Suponiendo que el gen haya sido clonado, el principal obstáculo a la comprobación directa de la existencia de mutaciones es la heterogeneidad alélica, como se discutirá más adelante.

Las pruebas directas se realizan casi siempre por PCR - las escasas aplicaciones basadas en hibridaciones de transferencias Southern incluyen las pruebas de X frágil y de mutaciones completas de distrofia miotónica y de algunas translocaciones cromosómicas en casos de cáncer.
La sensibilidad de la PCR nos permite emplearla para una gran variedad de muestras. 
Éstas pueden incluir

- Muestras de sangre: la fuente de DNA de adultos más utilizada
- Enjuagues y raspados bucales: el rendimiento de DNA es suficiente para realizar una o dos pruebas; especialmente favorecidos en programas de cribado poblacional
- Biopsias de vellosidades coriónicas: son la mejor fuente de DNA fetal (mejor que la amniocentesis) 
- Una o dos células separadas de embriones en fase de ocho células - se utilizan para el diagnóstico previo a la implantación tras una fecundación in vitro 
- Cabello, semen, etc. para investigaciones criminológicas
- Especímenes patológicos de archivo , que pueden ser utilizados en caso de necesidad para tipificar individuos fallecidos de los que no se ha almacenado DNA.
La amplificación de DNA fijado o degradado funciona mejor para secuencias cortas, de 100 bp o menos

- Tarjetas de Guthrie: en estas tarjetas se envía al laboratorio la gota de sangre seca de los recién nacidos para las pruebas neonatales de fenilcetonuria (PKU) en el Reino Unido y en otros países; las pruebas no consumen toda la gota, por lo que constituyen una posible fuente de DNA en caso de muerte del niño

No todas las pruebas mutacionales emplean DNA.
La comprobación del RNA empleando RT-PCR tiene ventajas cuando se criban genes con muchos exones o se buscan mutaciones de corte v empalme.
Sin embargo, la obtención de RNA es mucho más complicada, requiere una manipulación especial de las muestras v un rápido procesamiento.
También vale la pena destacar que los ensayos basados en proteínas pueden volver a entrar en escena tras el eclipse sufrido en los últimos 20 años.
Un ensayo funcional basado en proteínas puede clasificar los productos de una serie alélica muy heterogénica en dos grupos, los funcionales v los no funcionales - lo cual, después de todo, es la cuestión esencial de la mayoría de los diagnósticos.

Las pruebas de mutaciones conocidas y desconocidas en un gen plantean problemas diferentes y requieren la aplicación de métodos distintos

Cribado de mutaciones

En el Capítulo 14 describimos las muchas técnicas que se pueden utilizar para explorar la presencia de mutaciones en un gen.
La Tabla 16.1 resume las ventajas y las desventajas (desde el punto de vista del Laboratorio de diagnóstico) de algunos métodos ampliamente utilizados.
Su empleo en investigación puede producir un balance bastante diferente. 
Para el diagnóstico de rutina, todos estos métodos sufren dos limitaciones

- Son bastante laboriosos v caros para un servicio de diagnóstico, que necesita generar respuestas rápidamente v ajustándose a un presupuesto modesto
- Detectan diferencias entre la secuencia del paciente y la secuencia publicada "normal" - pero en general no distinguen entre cambios patogénicos y cambios al azar no patogénicos 

Ninguno de estos problemas es insuperable.
Cada año que pasa disminuye el trabajo v el coste por genotipo v se desarrollan nuevos métodos automatizados de búsqueda de mutaciones.
Diferentes grupos están trabajando en el desarrollo de " chips de DNA ", que contengan tiras de oligonucleótidos unidos a un sustrato de sílice.
Estos chips podrían utilizarse para la búsqueda de cualquier mutación en una sola reacción.
Una de las estrategias contempla hibridar la diana al chip, extendiendo los híbridos un nucleótido con DNA polimerasa v un terminador de cadena fluorescente, y mirando aquellos sitios en los que no no habido extensión.
Estos sitios marcarían apareamientos incorrectos debidos a una desviación de la secuencia diana respecto de la secuencia silvestre estándar.
También, con el avance en nuestro conocimiento sobre la función génica v con el creciente número de datos sobre mutaciones relacionadas con enfermedades, decidir si un cambio determinado de la secuencia es patogénico está siendo cada vez más fácil.
No obstante, para el futuro inmediato las pruebas de mutaciones desconocidas en un laboratorio de servicios siguen siendo un problema considerable. 

Detección de mutaciones 

Comprobar si la muestra de un paciente tiene o no un cambio de secuencia conocido es un problema diferente y mucho más sencillo que barrer un gen para comprobar si contiene alguna mutación.
La Tabla 16.1 relaciona los principales métodos disponibles para comprobar mutaciones conocidas, con una descripción detallada a continuación. 
Se pueden hacer pruebas de mutaciones conocidas en los siguientes casos

- Enfermedades en las que se supone existe homogeneidad alélica, de forma que todos los individuos afectados en una población presentan el mismo particular cambio de secuencia de DNA
- Enfermedades en las que la mayoría de los individuos afectados presentan una o un número limitado de mutaciones específicas
- Diagnóstico familiar.
Los métodos generalistas de detección de mutaciones pueden ser necesarios para definir la mutación en la familia analizada, pero una pez que ésta es caracterizada, los otros miembros de la familia pueden analizarse buscando únicamente la mutación particular

- En investigación, para comprobar muestras de controles.
Un problema frecuente de la donación posicional es que una vez identificado un gen candidato se ve que existe un cambio en este gen en un paciente afectado de la enfermedad de interés.
Entonces, la cuestión que se plantea es si este cambio representa una mutación patogénica (confirmándose así que el gen candidato es el gen de la enfermedad) o si puede ser un polimorfismo no patogénico.
Una estrategia común para resolver este dilema es explorar un panel de unas 100 muestras provenientes de individuos normales a ver si presentan el cambio (lo cual, por supuesto, no resuelve el problema de variantes neutras que sean muy poco frecuentes)


Existen varios métodos para la comprobación sencilla de cambios específicos en la secuencia de DNA

Presencia/ausencia de una diana de restricción

Como hemos visto en el Capítulo 5, el cambio de una única base responsable de la anemia falciforme elimina una diana de reconocimiento de la enzima de restricción MstII, CCTNAGG.
Esto proporciona una prueba sencilla y directa para 1a mutación falciforme.
Normalmente la prueba se realiza utilizando PCR en lugar de hibridación de transferencias Southern.

Pese a que se conocen cientos de enzimas de restricción, casi todas reconocen dianas simétricas palindrómicas, y la mayoría de las mutaciones puntuales patogénicas no afectan a estas secuencias.
Además, las dianas de enzimas de restricción muy raras y oscuras no resultan adecuadas para el uso diagnóstico, debido a que las enzimas son caras y suelen ser de baja calidad.
Sin embargo, algunas veces se puede introducir una diana diagnóstica mediante una forma de mutagénesis basada en PCR, utilizando cebadores cuidadosamente elegidos.
En la Figura 16.2 se muestra un ejemplo.

Empleo de sondas basadas en oligonucleótidos específicos de alelo (ASO)

Estas cortas sondas sintéticas hibridan únicamente con secuencias con las que tengan un apareamiento perfecto.
Su empleo ha sido descrito en el Capítulo 5.

Para el diagnóstico se suele utilizar un procedimiento de manchas de puntos inverso (reverse dot-blot).

Prueba ARMS: amplificación por PCR basada en cebadores específicos de alelo

Los principios en los que se basa el método fueron ya descritos en el Capítulo 6.
Se llevan a cabo reacciones apareadas de PCR.
Uno de los cebadores (el cebador común) es el mismo en las dos reacciones, mientras que el segundo cebador de cada reacción presenta versiones sutilmente diferentes, una específica para la secuencia normal y la otra específica para la secuencia mutante. 
Normalmente se incluye un par de cebadores de control, destinados a amplificar una secuencia no relacionada pero presente en todas las muestras.

Esto permite comprobar que la reacción de PCR ha funcionado bien.
El cebador común puede escogerse de manera que el producto obtenido tenga el tamaño que se desee, de forma que resulta sencillo diseñar reacciones multiplex , o múltiples, es decir, que permiten la detección simultánea de varios productos de PCR utilizando varios pares de cebadores a la vez.
En estos sistemas no es necesario introducir un par adicional de control, ya que los cebadores específicos de la secuencia normal y de la mutante se pueden combinar para asegurar que todos los tubos den algún producto de amplificación. 

El método ARMS es adecuado para el cribado de muchas muestras en busca de un panel de mutaciones determinado empleando PCR múltiple.
Las compañías que se dedican a la producción de kits de diagnóstico han diseñado muchas variantes a partir de estos conceptos básicos.

Ensayo de ligación de oligonucleótidos ( Oligonuucleotide ligation essay , OLA)

La prueba OLA para detección de sustituciones de bases se basa en dos oligonucleótidos de 20 meros, diseñados para hibridar con secuencias adyacentes, s, que coinciden en la posición de la mutación.
La DNA ligasa unirá covalentemente a los dos oligonucleótidos solamente cuando su hibridación con el molde sea perfecta. 
La prueba se realiza sobre un molde amplificado por PCR, y sólo se realiza una reacción de ligación. 
Uno de los oligos está etiquetado con biotina, mientras que el segundo lleva una molécula informadora como digoxigenina.
Los productos de reacción se transfieren a placas de microtitulación revestidas de estreptoavidina, que se une a la biotina.
Si existe ligación, la molécula informadora también quedará retenida en el pocillo después de los lavados.
La OLA es una técnica adecuada para las pruebas automatizadas a gran escala, ya que no implica centrifugación o electroforesis. 

Figura 16.1

Diagnóstico, consejo y predicción genética

El diagrama de flujo muestra la realización de un diagnóstico clínico partiendo del fenotipo del paciente y los diferentes métodos aplicables dependiendo del patrón de herencia y del estado de conocimiento genético para dar consejo y para realizar pruebas predictivas.

Figura 16.2

Introducción de una diana de restricción artificial con fines diagnósticos 

La mutación xxx en el lugar de corte y empalme del intrón 4 del gen FACC ni crea ni suprime una diana de restricción.
El cebador de PCR acaba muy cerca de esta base alterada, pero tiene una base mal apareada (la G roja) en una posición no crítica, de manera que no afecta su hibridación ni la amplificación de las secuencias normales o mutantes.
El apareamiento incorrecto del cebador introduce una diana AGTACT, de Scal , en el producto de PCR derivado de la secuencia normal. 
La foto muestra los productos de digestión con Scal de pacientes homozigotos normales (N), heterozigoto (H) y homozigoto mutante (M).

Figura 16.3

Hibridación inversa de manchas de puntos (reverse dot-blot) para la detección de mutaciones específicas

Se ligan nueve oligonucleótidos a un pequeño filtro.
Cada uno de los oligos hibrida específicamente con una secuencia que se encuentra en uno o más alelos HLA-DR.
Un grupo de estos filtros ha sido hibridado con el producto marcado de la amplificación con PCR del DNA de nueve pacientes;
la figura muestra una imagen negativa de las autorradiografías resultantes.
Estos oligonucleótidos se emplean para distinguir entre subtipos de HLA-DR4; DR12 (inferior derecha) es negativo, mientras que DR7 (inferior centro) hibrida con la mayoría de los oligos.
Existen versiones comerciales de esta técnica que producen un código de barras de colores cuando se exponen al DNA problema.

Figura 16.4

Ensayo ARMS multiplex para la detección de cuatro mutaciones de fibrosis quística

Los cebadores comunes se han diseñado de forma que cada reacción produce un producto de tamaño distinto, cosa que permite varias reacciones en la misma mezcla ( multiplexing , o PCR múltiple).
Cada tubo contiene cebadores para dos secuencias normales y dos mutantes.
Los productos de PCR, de arriba abajo, detectan las mutaciones xxx .
En el carril izquierdo de cada par los cebadores de la ARMS amplifican los equivalentes normales de las secuencias xxx , y las mutantes G551D y G542X; en el carril derecho los cebadores amplifican los alelos opuestos en cada caso.
Carriles 1 y 2: no se detectan mutaciones: carril 3: heterozigoto compuesto para xxx y G551D; carril 4: heterozigoto compuesto para xxx ; carril 5: heterozigoto compuesto para xxx .

Tabla 16.1

Métodos de barrido de un gen para buscar mutaciones puntuales

Método.
Ventajas.
Desventajas .


Secuenciación.
Detecta todos los cambios.
Laboriosa.


Mutaciones completamente caracterizadas.
Genera excesiva información.


Movilidad en gel de los heterodúplex.
Muy sencilla.
Sólo secuencias cortas ( xxx ).


Sensibilidad limitada.


No revela la posición del cambio.


Análisis de polimorfismos de conformación de cadena sencilla (SSCP).
Sencillo.
Sólo secuencias cortas ( xxx ).


No revela la posición del cambio.


Electroforesis en gel de gradiente desnaturalizante (DGGE).
Alta sensibilidad.
La elección de los cebadores es crítica.


Los cebadores pinzados con GC son caros.


No revela la posición del cambio.


Corte por bases mal apareadas.


químico .
Alta sensibilidad.
Reactivos tóxicos .


enzimático.
Muestra la posición del cambio.
Experimentalmente difícil .


Test de interrupción de proteínas (PTT).
Alta sensibilidad para mutaciones terminadoras de cadenas.
Sólo para mutaciones terminadoras de cadena.


Caro.


Muestra la posición del cambio.
Experimentalmente difícil.


Mejor con RNA.


Tabla 16.2

Métodos para la comprobación de una mutación específica

Método.
Comentarios.


Digestión con enzimas de restricción del DNA amplificado por PCR; comprobación del tamaño de los productos en un gel.
Sólo cuando la mutación crea o suprime una diana de restricción natural o una que se ha introducido gracias al empleo de cebadores especiales en el PCR.


Hibridar el DNA amplificado por PCR con oligonucleótidos específicos de alelo (ASO) en manchas de puntos, manchas de ranura o transferencias Southern .
Método general para mutaciones puntuales .


PCR empleando cebadores específicos de alelo (prueba ARMS).
Método general para mutaciones puntuales; el diseño de los cebadores es crítico .


Ensayo de ligación de oligonucleótidos (OLA).
Método general para mutaciones puntuales .


PCR con cebadores situados a ambos lados del punto de rotura sospechado de una deleción o de una translocación.
Si la amplificación tiene éxito se muestra la presencia de la reordenación especificada.


Comprobar el tamaño de la expansión de tripletes repetidos.
Sólo para enfermedades de repeticiones de tripletes; las expansiones grandes requieren hibridación de transferencias Southern, las pequeñas pueden analizarse por PCR.


Estudio de la estructura y función de los genes humanos y creación de modelos animales de enfermedades

La estructura y la función de los genes humanos se pueden estudiar empleando diversos métodos.
Hay métodos bien establecidos que implican manipulaciones in vitro y la utilización de células en cultivo.
Más recientemente, la expresión y la función de los genes se han estudiado en animales, gracias a la introducción en ellos de genes exógenos o a la propia modificación de los genes exógenos mediante la tecnología de transgénicos y de manipulación dirigida de genes ( gene targeting ).
Estos métodos han sido utilizados también para la creación de modelos animales de ciertas enfermedades.

Obtención de clones de genes humanos

Los clones de genes humanos pueden aislarse siguiendo diversos caminos

Es posible obtener clones de genes humanos mediante clonación de DNA en sistemas celulares o a partir de PCR, partiendo tanto de DNA genómico como de cDNA.
Los estudios más completos sobre la estructura y la función de un gen suelen partir de DNA clonado en sistemas celulares.

Esta metodología, a diferencia de la PCR, permite producir una gran cantidad de DNA clonado, ofrece una amplia gama de tamaños de fragmentos susceptibles de ser clonados y además permite la rápida subclonación en útiles vectores diseñados para permitir la expresión del gen en células e incluso en organismos como los animales. 
La PCR puede ser una forma rápida de obtener un clon inicial del gen, pero normalmente los estudios posteriores utilizarán los productos de la PCR inicial para cribar bibliotecas genómicas y de cDNA, a fin de identificar y aislar clones de células que contengan la secuencia génica de interés.
Para obtener estos clones se pueden seguir varias rutas diferentes, que serán tratadas en las siguientes secciones de este capítulo.

Clonación por enriquecimiento de cDNA

Este método permitió la obtención de los primeros clones de genes humanos.
A pesar de que el DNA genómico de todas las células nucleadas humanas está compuesto por básicamente la misma colección de secuencias, las poblaciones de mRNA y las proteínas que fabrican las diferentes células pueden ser muy distintas.
En consecuencia, el cDNA total derivado de tipos celulares diferentes está enriquecido en ciertas secuencias génicas a expensas de otras, y la abundancia de péptidos y proteínas específicas puede ser extremadamente variable.
Por ejemplo, las secuencias génicas de xxx y xxx globina constituyen menos del xxx del DNA genómico en una célula humana, pero hay unos 15 g de hemoglobina por cada 100 ml de suero, con la correspondiente abundancia de secuencias de xxx y xxx globina en la población de mRNA de los glóbulos rojos.
Por tanto, existe una buena probabilidad de que en una biblioteca de cDNA de glóbulos rojos se puedan identificar clones de cDNA de globinas, y por ello no sorprendió que los genes de globinas estuvieran entre los primeros genes humanos clonados.
Más recientemente se han diseñado métodos basados en PCR que amplifican selectivamente cDNA de ciertos subgrupos de genes transcripcionalmente activos.

Estos métodos de exposición diferencial del mRNA ( mRNA differential display ) permiten el análisis de los patrones de expresión génica y facilitan el aislamiento de genes que se expresan en unas células y no en otras.

Clonación dirigida por proteínas

Es posible aislar y purificar una proteína específica de una célula, o más convenientemente, de fluidos extracelulares en el caso de proteínas secretadas.

Para hacerlo se pueden aplicar diversas técnicas de fraccionamiento de proteínas, que explotan diferencias en tamaño, solubilidad, carga y afinidad de unión. 
Las fracciones separadas en cada estadio de la purificación se someten a pruebas para ensayar una propiedad distintiva (por ejemplo, actividad enzimática), que permita la identificación de las fracciones enriquecidas en la proteína de interés. 

Una vez purificada parcialmente la proteína, se pueden utilizar dos caminos para aislar el cDNA correspondiente. 
Una opción es inyectar la preparación de proteína purificada a conejos o ratones a fin de obtener anticuerpos específicos, que por su parte pueden emplearse en el cribado de una biblioteca de expresión.
La alternativa es determinar la secuencia de aminoácidos de un polipéptido, como se explica en la página 400.
Esta secuencia se examina para diseñar oligonucleótidos específicos del gen, que pueden usarse como sondas de hibridación para el aislamiento de clones de DNA o de cDNA que correspondan a la secuencia buscada.

Clonación dirigida por la localización

Los proyectos de cartografiado físico y de clonación posicional pueden implicar intentos de identificar clones génicos de localizaciones subcromosómicas específicas. 

Clonación basada en homologías 

Los genes humanos pueden tener secuencias muy parecidas a las de los genes de otras especies ( genes ortólogos ), notablemente especies de mamíferos, y también a las de otros miembros de la misma familia génica en humanos ( genes parálogos ).
Debido a la presión evolutiva para conservar la función génica, son las secuencias codificantes las que definen el grado de parentesco entre secuencias; los intrones no suelen conservarse bien, a no ser que sean muy cortos.
Debido a que existe una estrecha homología entre genes de distinta y de la misma especie, un clon de DNA puede usarse para buscar otros clones de secuencia parecida.
Esto se puede realizar por hibridación molecular o mediante PCR.

Clonación por homología basada en hibridación.
Se marca el clon de DNA y se usa como sonda de hibridación para cribar bibliotecas genómicas o de cDNA.
Un clon genómico marcado puede identificar un clon de cDNA correspondiente o viceversa, y si el clon pertenece a una familia génica puede permitir la identificación de otros miembros de la familia.
Los clones de DNA de otras especies pueden utilizarse como sondas de hibridación para cribar bibliotecas humanas.
Siempre y cuando las sondas no sean muy cortas, las secuencias parecidas permiten la formación de heterodúplex estables: basta incluso una concordancia global del 70% de las posiciones de nucleótidos para obtener señales de hibridación adecuadas.
Ésta es la base de la hibridación de zoo-blots (que se podría traducir literalmente como "hibridación de manchas zoológicas"). 
La homología con ortólogos de mamíferos ha permitido el aislamiento de muchos genes humanos.
Además, ciertos genes pueden contener motivos extremadamente bien conservados durante la evolución, y esto ha permitido clonar genes por homología utilizando sondas derivadas de especies tan distantes como Drosophila melanogaster (por ejemplo, los genes HOX, PAX, WNT , etc.).

Clonación por homología basada en PCR.
En algunos casos ha sido posible diseñar como cebadores oligonucleótidos degenerados que corresponden a cortas secuencias de aminoácidos que están bien conservadas entre los productos de diferentes miembros de una familia génica.
La potencia de esta estrategia queda ilustrada en los siguientes dos ejemplos:

Algunas familias génicas codifican productos con dos o más regiones muy conservadas separadas por regiones menos conservadas.
Se eligen cebadores degenerados a partir de dos de estos motivos conservados .
Estos cebadores serán el cebador de 5' , a partir de uno de los motivos, y el cebador de 3' , a partir del segundo.
Partiendo de una fuente adecuada de DNA, (por ejemplo, un tejido en el que se presume que los miembros de la familia génica se expresan), los cebadores degenerados pueden utilizarse para amplificar las secuencias intermedias de diferentes transcritos que contienen a las regiones conservadas. 
La clonación y secuenciación posterior de los productos de PCR puede ser el punto de partida para el aislamiento de nuevos miembros de la familia génica.

Una estrategia parecida implica una PCR en la cual los dos cebadores son oligonucleótidos degenerados derivados de una sola región conservada (algunas veces llamada un bloque, o una ACR, de ancient conserved region , o región conservada antigua; entre los ejemplos se incluyen dedos de zinc, homeoboxes, etc.).
La idea en este caso es cribar clones genómicos a partir de una región cromosómica definida, por ejemplo una colección de YAC derivada de tal región, buscando precisamente ese tipo de motivos.
Esto puede llevar a la identificación de nuevos genes en la región cromosómica analizada.

Caracterización de clones al azar

Los métodos descritos en las secciones anteriores han sido y continúan siendo muy útiles para el aislamiento de clones genómicos.
Sin embargo, en la actualidad han sido desplazados por la caracterización a gran escala y al azar de clones de cDNA.
Los proyectos de EST humanas (EST: Expressed Sequence Tag , o etiqueta/motivo de secuencia expresada) implican la secuenciación al azar de clones de bibliotecas de cDNA, como preludio a una caracterización más completa del clon.
Esta vía permitirá la obtención de aproximadamente el 80% de todos los genes humanos (los otros genes puede ser que no se expresen o se expresen muy poco en el conjunto de bibliotecas de cDNA que están siendo cribadas para los proyectos de EST).

Estudios de estructura génica y cartografiado de transcritos

Tras la identificación de un clon genómico o de cDNA que contiene un gen no caracterizado, el primer paso es la obtención de los correspondientes clones de cDNA o de DNA genómico, respectivamente, cribando bibliotecas adecuadas.
Una prioridad inicial es la obtención de una secuencia completa del cDNA, y definir el principio y el final de la traducción, el o los lugares de poliadenilación y las fronteras entre intrones y exones.
Posteriormente pueden hacerse intentos de identificar y caracterizar elementos del promotor y otros elementos reguladores en la secuencia adyacente por 5' , y definir el lugar de inicio de la transcripción. 
También suele establecerse la secuencia completa de nucleótidos, especialmente cuando se trata de genes pequeños; para genes más grandes lo habitual es centrarse primero en obtener la secuencia del cDNA completo. 

Para completar un cDNA puede recurrirse a ensamblar clones de cDNA solapantes y al método de RACE-PCR

Muchos de los clones de las bibliotecas de cDNA contienen secuencias parciales.
Un método frecuente de construir bibliotecas de cDNA se basa en el empleo de un cebador oligo(dT) para unirse a la cola de poli(A) del extremo 3' del mRNA. 
Este cebador iniciará la síntesis de la primera cadena por parte de la transcriptasa inversa.
Sin embargo, los mRNA muy largos pueden ser un problema debido a la dificultad de completar la síntesis del cDNA, y estas bibliotecas presentan típicamente un sesgo general hacia secuencias de 3' : muchos de los clones de cDNA son secuencias comparativamente cortas que contienen únicamente el extremo 3' del cDNA.
Una forma de resolver este problema ha sido cebar la síntesis de la primera cadena con un cebador hexanucleotídico degenerado, como los que se emplean en el marcado del DNA in vitro , en lugar del cebador de oligo(dT).
Estos cebadores hibridan prácticamente al azar a lo largo del RNA, y pueden generar secuencias de cDNA lo suficientemente largas para que sean útiles una vez clonadas.
La obtención de la secuencia de un cDNA completo puede lograrse secuenciando varios clones de cDNA diferentes y organizándolos para formar una serie de clones de cDNA solapantes, tal como se hizo en el caso del gen de la fibrosis quística.
Este proceso puede considerarse por tanto el equivalente de cDNA del paseo cromosómico utilizando clones de DNA genómico (ver Figura 11.13 ) Dado que las copias completas de los mRNA poco abundantes han sido difíciles de obtener mediante clonación convencional, se han desarrollado métodos rápidos basados en PCR.
Un método bastante utilizado para la obtención de secuencias de cDNA completos es la técnica RACE ( rapid amplification of cDNA ends , o amplificación rápida de extremos de cDNA, también llamada anchor-PCR ).
La RACE-PCR es una forma de RT-PCR que amplifica secuencias entre una región previamente caracterizada en el mRNA y el extremo 5' o el 3' .
Se diseña un cebador de la región conocida interna y el segundo cebador se selecciona a partir de una secuencia de anclaje , que se añade artificialmente a uno de los extremos del cDNA (ver Figura 19.4 ).

Los lugares de inicio de la transcripción se pueden cartografiar mediante protección a nucleasa S1 y por primer extension (extensión de cebador

Las secuencias reguladoras importantes suelen estar situadas cerca de los lugares de inicio de la transcripción, y a menudo pueden ser identificadas una vez localizado este último punto.
A pesar de que la 5' RACE-PCR puede permitir el rescate de las secuencias del extremo 5' de un mRNA (y por tanto identificar el lugar de inicio de la transcripción), para definir este sitio se utilizan preferentemente dos métodos. 

Protección a nucleasa S1

La endonucleasa S1 es una enzima del moho Aspergillus oryzae que corta al RNA y al DNA monocatenarios pero no a las moléculas de doble cadena.
Para localizar el lugar de inicio de la transcripción de un gen se necesita un clon genómico que pueda contener este sitio.
Este clon de DNA es digerido con una endonucleasa apropiada para generar un fragmento que presumiblemente contenga el inicio de la transcripción.
Como se puede apreciar en la Figura 19.5A , la hibridación con el correspondiente mRNA y la digestión con nucleasa S1 define la distancia del lugar de inicio de la transcripción a partir del extremo no marcado del fragmento de restricción. 
Si se necesita una localización más precisa, el fragmento de DNA marcado en el heterodúplex puede secuenciarse empleando un método de secuenciación química. 
Obsérvese que, básicamente de la misma manera, se puede emplear esta técnica para cartografiar otras fronteras entre DNA codificarte y no codificante, tales como las fronteras entre exones e intrones (ver más adelante) y el extremo 3' de un transcrito.

Extensión de cebador ( primer extension )

Este método es muy parecido al de la protección a S1.
En este caso, el fragmento de restricción elegido debe ser más corto que el mRNA y el saliente monocatenario se rellena empleando transcriptasa inversa ( Figura 19.5B ).
Aquí también es posible llegar a una localización más precisa empleando el método de Maxam y Gilbert (1980) para secuenciar la hebra de DNA marcada. 

Existen varios métodos para cartografiar las fronteras entre exones e intrones

Una necesidad de las primeras fases del estudio de la estructura de un gen es el aislamiento de clones de DNA que cubran toda su longitud.
Disponiendo de clones de DNA genómico y de cDNA se tiene la oportunidad de localizar las fronteras entre exones e intrones.
No todos los genes humanos tienen intrones (ver Tabla 7.6 ).
Sin embargo, cuando los tienen suelen ser comparativamente grandes.
La presencia de intrones suele poder deducirse de la comparación de clones genómicos y de cDNA correspondientes.
Una vez establecida la secuencia del cDNA completo se pueden diseñar cebadores de secuenciación a partir de varios segmentos del cDNA v utilizarlos en la secuenciación cíclica del DNA de un molde adecuado, por ejemplo clones cosmídicos (ver página 333).
Las secuencias que se obtienen deberían atravesar las fronteras entre exones e intrones, salvo que los exones sean muy largos, en cuyo caso hace falta diseñar nuevos cebadores de secuenciación. 

Existen otros métodos para obtener mapas de las fronteras entre exones e intrones, entre los que se incluyen protección a nucleasa SI (página 551) y el método de paseo genómico basado en PCR.
Este último método incluye técnicas como la PCR de conectores-burbuja o la PCR inversa ( Figuras 6.11 y 6.12 ), que pueden ser empleados con un cebador específico de exón para amplificar un corto segmento de secuencias intrónicas vecinas a partir del DNA de un clon de YAC (e incluso a partir de DNA genómico total) y luego proceder a su secuenciación. 

Éste fue el método por el cual se establecieron las fronteras exón/intrón para el gen de la distrofina. 

Figura 19.1

Un anticuerpo que detecte un producto génico parcialmente purificado puede utilizarse para cribar una biblioteca de expresión de cDNA e identificar el correspondiente clon

Las bibliotecas de cDNA de propósito general utilizan vectores de clonación que no están diseñados para expresar el DNA insertado, sino simplemente para propagarlo. 
Sin embargo, los vectores de expresión están diseñados expresamente para permitir que los cDNA insertados sean expresados por la célula huésped para generar un producto polipeptídico foráneo.
Para cumplir esta función, el vector requiere que a los flancos del inserto se coloquen las secuencias de control adecuadas. 
Para expresión en huéspedes bacterianos, estas secuencias incluirán un promotor (P) fágico o bacteriano que sea fuerte y habitualmente inducible, además de un lugar de unión a ribosoma (RBS) situado hacia 5' del sitio de clonación múltiple MCS.
También se necesitan sitios de terminación de la transcripción (TER) situados a 3' del sitio de clonación, que eviten que la transcripción continúe y perjudique a la replicación del plásmido.
La clonación de cDNA en estos vectores produce una biblioteca de expresión , con células que contienen moléculas recombinantes (R1, R2, R3, R4 en la figura) y que son capaces de expresar el inserto para dar una proteína ( protl, prot2, etc.).
Las colonias bacterianas pueden individualizarse sembrando sobre placas y transfiriendo las colonias aisladas a filtros de colonias.
Las células bacterianas se lisan sobre el mismo filtro mediante un tratamiento con lisozima, y los filtros se lavan para eliminar el exceso de restos celulares.
La exposición de los filtros a un anticuerpo adecuado resulta en la unión específica del anticuerpo a la colonia que expresa la proteína de interés, y la detección del anticuerpo unido permite la identificación de la colonia.
Esta colonia se siembra en un cultivo líquido para lograr así el aislamiento del clon de cDNA buscado.

Figura 19.2

La clonación por homología basada en PCR emplea cebadores degenerados que corresponden a regiones de la proteína conservadas por la evolución 

Panel izquierdo .
Este ejemplo muestra los miembros de una familia génica que codifican proteínas con dos o más regiones bien conservadas, A y B. De la información de que se dispone, digamos de las secuencias 1 a 4, se derivan secuencias consenso para la región A (CONS-A) y B (CONS-B).
Se diseñan oligonucleótidos degenerados para que actúen como cebadores de 5' y de 3' de forma que correspondan con las secuencias de CONS-A y CONS-B respectivamente.
Cuando se emplean para amplificar secuencias de cDNA total obtenido de un tipo celular adecuado, pueden generar nuevos productos, digamos 5 a 7, además de los conocidos 1 a 4.
La clonación en células de los productos de estas PCR permite la secuenciación de clones individuales, y la nueva información sobre la secuencia permite el aislamiento de clones más grandes específicos del locus, empleando por ejemplo RACE-PCR.

Panel derecho .
Los miembros de algunas familias génicas codifican proteínas que tienen dominios grandes con un grado de conservación elevado.
Con la información de que se dispone se pueden diseñar oligonucleótidos degenerados que actúen de cebadores de 5' y de 3' y que correspondan a la secuencia conservada (que constituiría así una especie de STS degenerada).
Estos cebadores se pueden aplicar a ensayos en búsqueda de nuevos genes que contengan el motivo conservado.
Por ejemplo, se pueden analizar uno a uno los clones de un cóntigo de YAC recién caracterizado en búsqueda de alguno de ellos que contenga un gen que codifique por el motivo especial, por ejemplo un dedo de zinc .

Figura 19.3

El ensamblaje de una serie de clones de cDNA solapados permitió la obtención de una secuencia completa del cDNA del gen de la fibrosis quística (CFTR)

Los clones individuales de cDNA se muestran como cajas cuyos recuadros numerados representan exones.
El primer clon de cDNA que fue aislado. 10-1, se obtuvo del cribado de una biblioteca de cDNA de glándula sudorípara, empleando secuencias conservadas identificadas en los clones genómicos solapados E4.3 y H1.6.
Este clon fue utilizado entonces como sonda de hibridación para identificar clones solapantes en varias bibliotecas, y los nuevos clones fueron empleados a su vez como sondas para identificar clones aún más distales.
Los clones derivaban de diversas fuentes: T84 (línea celular de carcinoma de colon, todos los clones comenzados por T); pulmón (clones comenzados por CDL); páncreas (CDPJ5 ).

Además, los clones marcados en rojo fueron generados por RACE-PCR.
Entre ellos se incluyen los clones PA3-5 y TB2-7 (obtenidos por 5' RACE-PCR empleando un cebador específico del exón 2) y THZ-4 (por 3' RACE-PCR empleando un cebador derivado de la 3' UTS).
Obsérvese la presencia de secuencias intrónicas en algunos clones (esto se puede deber a la transcripción inversa de secuencias precursoras en el RNA, o bien a ocasionales secuencias con corte y empalme alternativo), así como de secuencias ajenas en otros casos.
La hibridación de clones o subclones con fragmentos de restricción del DNA genómico (panel inferior) ayudó a orientar los exones en el DNA genómico.
Se han omitido muchos de los clones de la figura original en beneficio de la claridad. 

Figura 19.4

La RACE-PCR puede facilitar el aislamiento de secuencias de 5' y de 3' del cDNA

Un paso inicial en PCR-RACE ( amplificación rápida de extremos de cDNA , rapid amplification of cDNA ends ) es la introducción de una secuencia específica en el extremo 3' o 5' por lo que constituye, de hecho, una forma de mutagénesis por adición en 5' 5'-add-on mutagenesis , ver páginas 155-156).

La 3' RACE-PCR emplea un cebador antisentido inicial que tiene una secuencia de extensión específica en su extremo 5' ( secuencia de anclaje , suele superar los 15 nucleótidos de longitud). 
Esta secuencia resulta incorporada en el transcrito de cDNA durante el paso de transcripción inversa.
El siguiente paso es el empleo de un cebador interno para generar una corta segunda cadena que acabe en una secuencia complementaria a la secuencia de anclaje original.
A partir de aquí se inicia la PCR, empleando como cebador con sentido el oligonucleótido interno, y como cebador antisentido un oligonucleótido correspondiente a la secuencia de anclaje.

La 5' RACE-PCR emplea inicialmente un cebador interno para iniciar la síntesis de un primer cDNA parcial (negro) a partir de un molde de mRNA (rojo).

Posteriormente se añade una cola de poli(A) al extremo 3' del cDNA utilizando la transferasa terminal.
La síntesis de la segunda cadena se inicia con un cebador poli(T) con sentido que incorpora una extensión específica (secuencia de anclaje).
Esta hebra se emplea posteriormente como molde para obtener una copia complementaria de la secuencia de anclaje en una reacción iniciada por el cebador interno. 
Una vez obtenida la doble cadena con la secuencia de anclaje se realiza la PCR empleando cebadores correspondientes a la secuencia interna y a la secuencia de anclaje.

Figura 19.5

El lugar de inicio de la transcripción puede cartografiarse empleando ensayos de protección a nucleasa S1 o ensayos de extensión de cebador

Ensayo de protección a nucleasa S1.
Se sospecha que el sitio de inicio de la transcripción está contenido en un fragmento de restricción del extremo 5' de un gen clonado.
El fragmento se marca en 5' y se desnaturaliza, permitiendo que hibride con RNA total de células en las que se piensa que el gen estudiado se expresa.
El mRNA correspondiente puede hibridar con la hebra antisentido para formar un heterodúplex RNA-DNA. 
El tratamiento posterior con nucleasa S1 resulta en la digestión progresiva de la secuencia protuberante de DNA de 3' , hasta el punto en el que el DNA está hibridado al extremo 5' del mRNA.
El fraccionamiento por tamaño en un gel desnaturalizante permite la identificación de la diferencia de tamaño entre el DNA original y del DNA resultante del tratamiento con la nucleasa S1.

Ensayo de extensión de cebador ( primer extension ).
En este caso. el fragmento de restricción sospechoso de contener el lugar de inicio de la transcripción se elige deliberadamente pequeño. 
La hibridación con un mRNA correspondiente dejará el mRNA con un extremo protuberante por 5' .
El DNA puede actuar como cebador para iniciar la síntesis de una primera cadena utilizando transcriptasa inversa, que extenderá su extremo 3' hasta que llegue al extremo 5' del mRNA.
El aumento de tamaño tras el tratamiento con transcriptasa inversa (+RT) comparado con el tamaño antes del tratamiento (-RT) permite situar al sitio de inicio de transcripción.
Ambos métodos permiten un cartografiado más preciso secuenciando el DNA tras el tratamiento con S1 o RT.

Tabla 19.1

Métodos habituales de fraccionamiento de proteínas

Cromatografía de filtración en gel .
La preparación de proteínas se fracciona de acuerdo al tamaño, tras ser aplicada a la parte superior de una columna de bolitas porosas hechas de un polímero insoluble como el dextrano o la agarosa (entre los ejemplos comerciales se incluyen Sephadex, Sepharosa y Bio-gel) 

Precipitación salina .
La preparación de proteínas se fracciona de acuerdo a la solubilidad diferencial a elevadas concentraciones de sal. 
Por ejemplo, 0,8 M de sulfato amónico bastan para precipitar el fibrinógeno, pero para precipitar la albúmina sérica hace falta una solución 2,4 M.

Cromatografía de intercambio iónico .
La separación tiene lugar de acuerdo a la carga neta.
Las proteínas cargadas negativamente pueden separarse en columnas de dietilaminoetil (DEAE)-celulosa, que tienen carga positiva; las proteínas positivas pueden separarse en columnas negativas como las de carboximetil-celulosa. 

Cromatografía de afinidad .
Este método potente y versátil depende de la propiedad de muchas proteínas de tener una elevada afinidad de unión por grupos químicos particulares.
Así, si se sabe que la proteína de interés se une al grupo químico X con afinidad alta, entonces se puede generar una columna en la cual el grupo X o un derivado se encuentre unido covalentemente a las bolitas de la matriz, y esto permitirá captar de una mezcla compleja a la proteína de interés. 

Terapia génica y otras estrategias terapéuticas basadas en genética molecular

Principios básicos de las estrategias de tratamiento de enfermedades basadas en genética molecular 

Una vez que se ha caracterizado el gen de una enfermedad humana, se pueden utilizar las herramientas de la genética molecular para diseccionar su función y explorar los procesos biológicos implicados en los estados normal y patológico. 

La información obtenida puede utilizarse para diseñar nuevas terapias empleando estrategias convencionales basadas en fármacos.
Además, las tecnologías de genética molecular han proporcionado recientemente una variedad de nuevas estrategias terapéuticas que dependen de: la capacidad de clonar genes individuales, transferirlos a células receptoras y expresarlos; la capacidad de rediseñar proteínas; y la capacidad de inhibir específicamente la expresión de genes predeterminados (caracterizados). 
Las nuevas estrategias terapéuticas basadas en genética molecular pueden categorizarse en dos grupos generales, dependiendo de si el agente terapéutico es un producto génico/vacuna o material genético: 

Fármacos y vacunas recombinantes obtenidas por ingeniería genética .
Entre las estrategias posibles se incluyen:

- Clonación por expresión de productos génicos normales: 
clonación de (habitualmente) genes humanos y su expresión en sistemas adecuados para producir grandes cantidades de un producto génico médicamente valioso; ver páginas 112-113 y 547. 

- Anticuerpos manipulados por ingeniería genética:
los genes de los anticuerpos pueden manipularse para formar nuevos anticuerpos, incluyendo anticuerpos parcial o completamente humanizados , para su uso como nuevos agentes terapéuticos; ver páginas 593-596.

- Vacunas obtenidas por ingeniería genética: 
nuevas vacunas contra el cáncer y vacunas contra agentes infecciosos


La terapia génica es la modificación genética de células de un paciente a fin de combatir alguna enfermedad.
Esta amplia definición incluye muchas estrategias posibles, y puede implicar la transferencia de genes humanos, de segmentos de genes humanos en forma de doble cadena, de genes de otros genomas, de oligonucleótidos y de varios genes artificiales, por ejemplo genes antisentido. 
Estas transferencias génicas pueden provocar la modificación genética de las células del paciente.
En la mayoría de los casos, la terapia génica se diseña para modificar genéticamente a las células enfermas, pero algunas estrategias van deliberadamente dirigidas a las células sanas , especialmente células del sistema inmunitario, y constituyen una forma de vacunación.

Los genes humanos clonados pueden ser utilizados como fuente de productos de importancia médica

Una vez que se ha clonado un gen humano se puede obtener una gran cantidad de producto génico recurriendo a sistemas adecuados de clonación por expresión (ver página 112).
Esto suele implicar la expresión del gen de interés en células bacterianas, que tienen la ventaja de que pueden cultivarse en grandes volúmenes, con lo que permiten la obtención de gran cantidad de producto. 
Incluso cuando se dispone de tratamientos consolidados, la estrategia basada en clonación y expresión del gen puede ser preferible debido a que se minimizan los riesgos para la seguridad.
Por ejemplo, los diabéticos habían sido tratados tradicionalmente con insulina preparada de vacas o cerdos.
Sin embargo, debido a la diferencia de secuencia de aminoácidos entre los productos de estos animales y la insulina humana, estos productos resultan potencialmente inmunogénicos y pueden producir efectos secundarios no deseados en individuos muy inmunorreactivos.

La administración de productos humanos purificados bioquímicamente también tiene sus riesgos.
Recientemente, muchos hemofílicos han desarrollado SIDA como consecuencia del tratamiento con factor VIII purificado a partir del suero de donantes humanos que no habían sido cribados.
La deficiencia en hormona del crecimiento se trataba con hormona del crecimiento humana purificada.
Sin embargo, algunos pacientes desarrollaron enfermedad de Creutzfeldt-Jacob (un raro trastorno neurológico que es el equivalente humano del scrapie de las ovejas y de la enfermedad de las vacas locas) debido a que la hormona había sido extraída de un gran número de pituitarias de cadáveres humanos.

La clonación de genes humanos de importancia médica y su expresión en sistemas de clonación y expresión adecuados resulta entonces atractiva.
Si la purificación bioquímica del producto a partir de una fuente humana o animal es difícil o imposible, constituye la única vía posible para diseñar una terapia basada en el producto.
Incluso cuando se puede purificar el producto bioquímicamente a partir de humanos o animales, la clonación y expresión permite obtener grandes cantidades del producto sin los riesgos concurrentes descritos en los párrafos anteriores.
La insulina recombinante humana fue comercializada por primera vez en 1982, y subsecuentemente se han comercializado otros muchos productos de interés médico obtenidos a partir de genes humanos clonados (ver Tabla 20.1 ).
Sin embargo, el tratamiento con los productos de genes clonados también tiene sus riesgos.
Por ejemplo, los pacientes que carecen completamente de un producto génico determinado pueden responder con una vigorosa reacción inmunológica al producto administrado, tal cono ocurrió con algunos pacientes con hemofilia A severa que habían sido tratados con factor VIII recombinante.

La clonación por expresión suele implicar el empleo de microorganismos, pero esta estrategia puede resultar poco adecuada en algunos casos.
Por ejemplo, la expresión de un gen humano en bacterias puede generar un producto que tiene diferencias con el producto génico humano normal:
el polipéptido puede tener la misma secuencia de aminoácidos, pero los patrones de glucosilación y otras modificaciones introducidas durante el procesamiento postraduccional pueden ser diferentes.
Esto puede significar que el producto génico no es particularmente estable en un ambiente humano, o bien puede provocar una reacción inmunológica, o hacer que su función biológica sea menos efectiva que lo deseado.
Se han utilizado sistemas de expresión alternativos, y cada vez se está prestando mayor atención a la construcción de animales de granja transgénicos , cuyos sistemas de modificación postraduccional son más parecidos a los sistemas humanos. 
Por ejemplo, un gen humano clonado puede fusionarse a un gen de oveja que especifique una proteína de la leche, y luego ser insertado en la línea germinal de la oveja. 
La oveja transgénica resultante puede secretar grandes cantidades de la proteína de fusión a la leche.
El diseño del gen de fusión normalmente contempla el poder cortar la proteína de fusión secretada empleando una proteasa específica para generar el residuo correspondiente a la proteína humana para facilitar su purificación.

La ingeniería de anticuerpos ha permitido la construcción de anticuerpos humanizados y de anticuerpos completamente humanos

Los anticuerpos son agentes terapéuticos naturales producidos por linfocitos B.
En cada precursor de linfocito B tiene lugar una reordenación cae componentes de los genes de anticuerpos, reordenación que es completamente específica de cada célula individual (ver página 191 en adelante). 
Otros mecanismos añaden aún más diversidad, incluyendo frecuentes mutaciones somáticas. 
Como resultado se tiene que cada individuo posee una población de linfocitos B que colectivamente un enorme repertorio de anticuerpos diferentes como sistema de defensa contra una diversa gama de antígenos foráneos. 
El anticuerpo puede considerarse una molécula adaptadora :
contiene sitios de unión para antígenos foráneos en su extremo variable (V), y para moléculas efectoras en su extremo constante (C). 
La unión de un anticuerpo puede bastar para neutralizar algunas toxinas y virus, pero lo más común es que el anticuerpo dispare el sistema del complemento y- la inactivación medida por células.

Los anticuerpos terapéuticos producidos artificialmente se diseñan de manera que sean monoespecíficos (es decir, reconocerán un único tipo de lugar antigénico), y pueden aplicarse específicamente al reconocimiento de ciertos antígenos asociados a enfermedades.

El reconocimiento puede llevar a la eliminación de la célula portadora de los citados antígenos. 
Dianas dignas de mención de una terapia basada en ellos podrían ser los cánceres, particularmente linfomas y leucemias, las enfermedades infecciosas (utilizando anticuerpos obtenidos contra antígenos de los patógenos relevantes) y trastornos autoinmunes (reconocimiento por anticuerpos de antígenos inapropiadamente expresados por células huésped).

El método habitual de producción de anticuerpos monoespecíficos es fusionar linfocitos B individuales de ratón o rata previamente inmunizados y células inmortales derivadas de un tumor de linfocitos B de ratón. 
Del resultado de la fusión, una mezcla heterogénea de células híbridas, se seleccionan las que son capaces de producir un anticuerpo en particular al tiempo que mantienen la propiedad de ser inmortales en cultivo.
Estos hibridomas se propagan como clones individuales, cada uno de los cuales puede proporcionar una fuente permanente y estable de un único tipo de anticuerpo monoclonal .

Hasta hace poco la estrategia de anticuerpos terapéuticos no era inmediata ni sencilla.
Ha sido muy difícil obtener anticuerpos monoclonales ( mAbs, de monoclonal antibodies ) humanos empleando la tecnología de hibridomas.
Se pueden obtener mAbs de ratón contra patógenos y células humanas, pero estos anticuerpos tienen una limitada aplicación terapéutica: 
los mAbs de roedores tienen una corta vida media en el suero humano;
sólo algunas de las diferentes clases son capaces de disparar los mecanismos efectores en humanos; y pueden provocar, además, una respuesta inmune en pacientes (anticuerpos humanos anti anticuerpos de ratón).
Sin embargo, una vez que se han clonado los genes de las inmunoglobulinas existe la posibilidad de diseñar combinaciones artificiales de segmentos de los genes de las inmunoglobulinas ( ingeniería de anticuerpos ).
El proceso se ve favorecido, además, porque los diferentes dominios de una molécula de anticuerpo vienen codificados en exones distintos, por lo que el intercambio de dominios puede realizarse fácilmente en el DNA, recurriendo al barajado de exones entre genes de anticuerpos diferentes.
Los genes de anticuerpos recombinantes que resultan pueden expresarse a fin de obtener anticuerpos quiméricos .

Anticuerpos humanizados

Un objetivo inmediato de la ingeniería de anticuerpos fue la producción de anticuerpos humanizados , es decir, anticuerpos recombinantes entre roedores y humanos. 
En principio, la humanización de los anticuerpos de roedor permite acceder a un gran acervo de mAbs de roedores para la terapia, incluyendo los específicos contra antígenos humanos que son difíciles de obtener en una respuesta inmune humana.
Las primeras versiones contenían los dominios variables de un anticuerpo de roedor, unidos a los dominios constantes de un anticuerpo humano:
la inmunogenicidad de los mAbs de roedor se reduce, al mismo tiempo que las funciones efectoras pueden seleccionarse para su aplicación terapéutica. 
Los anticuerpos pueden humanizarse aún más. 
La parte esencial del lugar de unión al antígeno es un subconjunto de determinantes de la región variable caracterizado por secuencias hipervariables, las regiones de determinación de la complementariedad, o CDR.
De acuerdo con esto, los anticuerpos humanizados de segunda generación fueron anticuerpos humanos a los que se había trasplantado la CDR:
los lazos de unión a antígeno del anticuerpo de roedor se integraban en un anticuerpo humano.
Ambos tipos de anticuerpos humanizados, los quiméricos V/C y los trasplantados con CDR, han sido construidos contra una amplia gama de patógenos microbianos y marcadores de superficie de células humanas, incluyendo antígenos tumorales.
En algunos casos han sido también empleados en clínica (ver Tabla 20.2 ).

Anticuerpos humanos

Se han adoptado dos estrategias para la construcción de anticuerpos totalmente humanos.
Una de ellas, la exposición en fagos , soslaya la tecnología de hibridomas, e incluso evita la inmunización.
En lugar de ello los anticuerpos se producen in vitro , imitando las estrategias de selección del sistema inmune (página 568).
Este procedimiento debería facilitar la construcción de anticuerpos humanos de valor terapéutico, además de su potencial como herramienta de investigación.
Una segunda estrategia reciente ha sido el empleo de ratones transgénicos .
Esta estrategia implica la transferencia a las células madre embrionarias del ratón de cromosomas artificiales de levadura que contienen segmentos grandes de los loci humanos de las cadenas pesada y ligera kappa de inmunoglobulinas. 
Los ratones transgénicos que se obtienen (ver página 572 para una descripción de los principios básicos para la construcción de ratones transgénicos) producen un repertorio diverso de cadenas pesadas y ligeras de inmunoglobulinas humanas y, tras su inmunización con toxina tetánica, pueden utilizarse para derivar anticuerpos monoclonales específicos de naturaleza humana.
Apareando estos ratones con ratones que están modificados de manera dirigida (página 577) para ser deficientes en la producción de inmunoglobulinas se obtuvo una cepa de ratones con elevada producción de anticuerpos, la mayoría de los cuales estaban compuestos por cadenas pesadas y ligeras de origen humano.
Estas cepas deberían permitir el desarrollo de anticuerpos monoclonales totalmente humanos con potencial terapéutico.

Las vacunas obtenidas por ingeniería genética tienen un gran potencial terapéutico

La tecnología del DNA recombinante está siendo aplicada también a la construcción de nuevas vacunas. 
Se están aplicando diversas estrategias:

- Inyección directa del DNA. 
La inyección directa de una pieza del DNA del virus de la gripe (conservada entre las diferentes cepas del virus) en células musculares del ratón ha resultado en una potente respuesta inhibitoria contra la gripe.
Si esta estrategia funciona de forma tan eficaz en humanos, el potencial terapéutico es muy considerable

- Modificación genética del antígeno .
Esto puede lograrse, por ejemplo, fusionando con un gen de citocinas para aumentar la antigenicidad 

- Modificación genética de microorganismos .
Esto puede implicar dos estrategias: 
(i) Inactivar genéticamente un organismo (por ejemplo, eliminando los genes necesarios para su patogénesis o supervivencia).
Esto es un método genético de atenuación , de manera que se puede usar una vacuna viva sin riesgo innecesario.

(ii) Insertar un gen exógeno que será expresado en bacterias o parásitos


Una aplicación prometedora es el empleo del bacilo Calmette- Guérin (BCG), genéticamente modificado, como vehículo para la inmunización. 

El BCG es un bacilo de tuberculosis atenuado utilizado como vacuna, de hecho la vacuna más utilizada en el mundo; 
es per se , inmunomodulatorio, y tiene una incidencia de complicaciones serias muy baja.
Se han desarrollado cepas recombinantes de BCG con vectores de expresión que contienen secuencias reguladoras del bacilo acopladas a genes que codifican antígenos foráneos. 
Estas cepas pueden provocar la síntesis de anticuerpos de larga vida, así como respuesta mediada por células contra antígenos foráneos en ratón.
Mantienen el potencial de expresar varios antígenos simultáneamente, por lo que podrían emplearse para la expresión de múltiples antígenos protectores contra diferentes patógenos.
Los organismos recombinantes de este tipo pueden emplearse no sólo como portadores de genes para agentes infecciosos, sino también para tumores ( vacunas anticancerosas ), y teóricamente autoantígenos.
Cabe destacar que algunas estrategias de terapia génica, como la inmunoterapia adoptiva constituyen formas de vacunación basadas en ingeniería genética (ver página 623). 

Las diferentes estrategias para la terapia génica

El término terapia genética es amplio:
abarca muchas estrategias diferentes, todas las cuales se diseñan para superar o aliviar la enfermedad introduciendo en las células de un individuo afectado genes, fragmentos de genes u oligonucleótidos. 
El material genético puede ser transferido directamente alas células dentro del paciente ( terapia génica in vivo ), o bien se pueden extraer las células del paciente, insertar el material genético en ellas in vitro y devolverlas al paciente una vez modificadas ( terapia génica ex vivo ).
Debido a que las bases moleculares de las enfermedades pueden variar enormemente, algunas estrategias de terapia génica resultan particularmente adecuadas para determinados tipos de trastornos, mientras que otras son adecuadas para otros.

Entre las enfermedades principales se incluyen:

- enfermedades infecciosas (resultan de la infección por patógenos víricos o bacteriano)
- cánceres (continuación impropia de la división celular y de la proliferación celular como consecuencia de la activación de un encogen, o de la inactivación de un gen supresor de tumor o de un gen de apoptosis - ver Capítulo 17)
- trastornos hereditarios (deficiencia genética en un producto génico, o expresión inadecuada de un gen determinada genéticamente)
- trastornos del sistema inmune (incluye alergias, inflamaciones y enfermedades autoinmunes - es la destrucción inadecuada de las células del organismo por sus propias células del sistema inmune)

Una idea fundamental subyacente al desarrollo de la terapia génica ha sido la necesidad de tratar enfermedades para las diales no existen tratamientos efectivos.
La terapia génica tiene el potencial de tratar todas las clases de trastornos descritas en el párrafo anterior. 
Es posible considerar diferentes estrategias según la base de la patogénesis.

La terapia génica actual es exclusivamente terapia génica somática, es decir, la introducción de genes en células somáticas de un individuo afectado. 
Las perspectivas de una terapia génica de células germinales humanas levanta muchos considerandos éticos, y en la actualidad no está permitida (ver página 628).

Estrategias generales de terapia génica (ver también la Figura 20.1 )

Terapia de aumento génico (GAT)

Para enfermedades causadas por la pérdida de función de algún gen, la introducción de copias adicionales del gen normal puede aumentar la cantidad del producto normal hasta un punto en el cual se restablece el fenotipo normal ( Figura 20.1 ).
El resultado es que la GAT va dirigida a trastornos clínicos de patogénesis reversible .
También ayuda el que no se tengan requerimientos precisos sobre la cantidad de expresión del gen introducido y que haya una respuesta clínica incluso ante una expresión baja.
La GAT ha sido aplicada particularmente a tratar trastornos recesivos autosómicos en los que una expresión modesta del gen introducido puede suponer una diferencia sustancial. 

Los trastornos de herencia dominante son mucho menos susceptibles de ser tratados:
las mutaciones de ganancia de función no son tratables con esta estrategia, e incluso cuando se trate de mutaciones de pérdida de función se requiere una elevada eficiencia en la expresión del gen introducido: 
individuos con un 50% de producto génico normal suelen estar afectados, por lo que el desafío consiste en aumentar la cantidad de producto génico hasta las concentraciones normales.

Supresión dirigida de células específicas

Esta estrategia general es popular en las terapias génicas contra el cáncer.
Los genes se dirigen a las células diana y se expresan en ellas de manera que provoquen su eliminación. 

- La eliminación directa es posible si los genes insertados se expresan para producir una toxina letal ( genes suicidas ), o bien se inserta un gen que codifica una prodroga , que confiere susceptibilidad a la eliminación por una droga o fármaco administrado subsecuentemente
- La eliminación indirecta se basa en genes inmunoestimuladores para provocar o potenciar una respuesta inmune contra la célula diana (ver página 623 en adelante)

Corrección dirigida de mutaciones

Si una mutación hereditaria produce un efecto dominante negativo (ver página 435) es poco probable que la terapia de aumento sirva para algo.
En lugar de ello se debe corregir la mutación residente. 
Debido a las dificultades prácticas asociadas esta estrategia aún no ha sido aplicada, pero en principio puede realizarse a diferentes niveles: a nivel génico (por ejemplo, aplicando métodos de manipulación dirigida de genes basados en recombinación homóloga - ver página 577); o a nivel del transcrito de RNA (por ejemplo empleando tipos particulares de ribozimas terapéuticas - o la edición terapéutica del RNA, ver página 614).

Inhibición dirigida de la expresión génica

Si las células enfermas presentan un nuevo producto génico o la expresión inadecuada de un gen (como ocurre con muchos cánceres, enfermedades infecciosas, etc.) se pueden emplear diferentes sistemas encaminados específicamente a bloquear la expresión de un único gen en el DNA, el RNA o la proteína (página 607 en adelante). 
En algunos casos también es posible la inhibición específica de la expresión de alelos concretos, lo que permite aplicar terapia génica a algunos trastornos que resultan de efectos dominantes negativos.

La tecnología de la terapia génica clásica

Distinguiremos entre las estrategias de terapia génica que sencillamente se basan en la transferencia y expresión de genes (que de forma algo arbitraria se describen como estrategias clásicas de terapia génica ), y otras estrategias (como la inhibición dirigida de la expresión génica in vivo y la corrección dirigida de mutaciones in vivo ), que serán consideradas independientemente comenzando en la página 607.

Tabla 20.1

Ejemplos de productos farmacéuticos obtenidos por clonación por expresión

Producto.
Para el tratamiento de.


Factor VIII de coagulación sanguínea.
Hemofilia A.


Factor IX de coagulación sanguínea.
Hemofilia B.


Eritropoyetina.
Anemia.


Insulina .
Diabetes.


Hormona del crecimiento .
Deficiencia en hormona del crecimiento.


Activador tisular del plasminógeno.
Trastornos trombóticos.


Vacuna contra hepatitis B.
Hepatitis B.


Interferón xxx .
Leucemia de células ciliadas, hepatitis crónica.


Interferón xxx .
Esclerosis múltiple.


Interferón xxx .
Infecciones en pacientes con enfermedad granulomatosa crónica.


Interleucina 2.
Carcinoma de célula renal .


Factor estimulador de colonias de granulocitos(G-CSF) .
Neutropenia tras quimioterapia.


DNasa (desoxirribonucleasa).
Fibrosis quística .


Tabla 20.2

Ejemplos del potencial clínico de los anticuerpos humanizados

Diana.
Potencial clínico.


CDw52.
Linfomas, vasculitis sistémica, artritis reumatoide.


CD3.
Trasplante de órganos.


CD4.
Trasplante de órganos, artritis reumatoide, enfermedad de Crohn.


Receptor de IL-2.
Leucemias y linfomas, trasplantes de órganos, enfermedad de rechazo del trasplante.


TNF-a .
Shock séptico.


HIV.
SIDA.


RSV.
Infección vírica respiratoria sincitial.


HSV.
Infección por herpes neonatal, ocular y genital.


Lewis-Y.
Cáncer .


xxx .
Cáncer .


PLAP.
Cáncer.


CEA.
Cáncer.



Los objetivos de la ingeniería genética

Técnicas de manipulación de genes

La ingeniería genética implica «cambiar genes».
Estos cambios pueden tener lugar como resultado de la transferencia de un gen desde el lugar donde se encuentra habitualmente a una célula que normalmente no lo tiene, o al modificar de alguna forma la secuencia de un gen, de modo que se obtenga un gen distinto.
Por supuesto, dicho gen modificado también se puede transferir a un nuevo tipo de células.
Para evitar la confusión con las técnicas clásicas de mejora, o con el proceso de transferencia por plásmidos que ocurre de forma natural, se admite que la ingeniería genética o manipulación genética también incluye algunos procesos que se realizan in vitro .

Mejora clásica

¿Por qué se necesita la ingeniería genética?
Es cierto que mediante los programas clásicos de mejora es mucho lo que se ha logrado y lo que se continuará logrando.
En la Sección 9.,3.,1 se han descrito los métodos empleados para la «mejora» de cepas microbianas mediante la conjugación o, con menor frecuencia, mediante la transferencia parasexual de genes entre cepas, y por tanto no se van a repetir aquí. 
En general, el científico que hace la mejora pretende introducir un gen deseable en un organismo que ya tiene muchos atributos atractivos, lo que se realiza cruzando el organismo con otro que tenga el gen deseado, pero que en muchos otros aspectos no es tan adecuado como el primero.
Se espera que la descendencia de tal cruce conlleve la combinación de las características favorables del primer organismo con el gen deseado del segundo. 
Por desgracia, la probabilidad de que se dé esa combinación perfecta de genes tiende a ser extremadamente baja.
De hecho, existe la misma probabilidad de obtener una descendencia que presente sólo las características no deseadas de cada progenitor.
Como consecuencia, se han de elegir aquellos descendientes que contengan el gen deseado y que también hayan conservado muchas de las características favorables del primer progenitor.
Si a continuación se cruzan dichos descendientes con el primer progenitor, la descendencia resultante contendrá, por lo general, menos genes no deseados que los que tenía la primera generación, aunque también algunos habrán perdido el gen deseado y, por tanto, han de descartarse.
Por repetición de este tipo de retrocruzamientos de descendientes seleccionados, se puede llegar a un nivel en el que, a efectos prácticos, sólo se han transferido al genoma del primer organismo el gen deseado.
Incluso entonces, han de realizarse nuevos cruces entre la descendencia para generar organismos que sean homozigotos para el gen «transferido» y, por consiguiente que sea una «verdadera» mejora.

Es obvio que la mejora clásica requiere mucho tiempo, incluso cuando se utilizan artilugios para aumentar las probabilidades a favor del científico que hace la mejora.
Se puede progresar más rápidamente si los organismos que se van a cruzar son muy similares, ya que esto reduce el número de retrocruzamientos necesarios.

Sin embargo, los genes de mayor utilidad suelen encontrarse en organismos muy diferentes del organismo de destino.
Si se puede realizar el cruce, la obtención de una cepa «verdadera» resulta muy laboriosa, aunque posible. 
Desgraciadamente, por lo general sólo hay fertilización entre organismos de la misma especie o de especies muy próximas, por lo que para cualquier organismo, el conjunto de genes disponibles a través de la mejora clásica es muy limitado. 
De aquí, la importancia de mantener reservas de individuos selectos de una amplia gama de variedades «extraordinarias» de plantas y animales, ya que constituirán el conjunto de genes para futuros programas de mejora.

La variedad en el conjunto de genes disponibles para una especie se puede aumentar creando mutantes, utilizando agentes químicos mutagénicos o radiación de alta energía, como se ha descrito para los microorganismos (véase Sección 9.,2).
Sin embargo, ya que no hay control sobre qué genes van a matar, la inmensa mayoría de dichas mutaciones son letales, perjudiciales o sin consecuencias.
Puede suceder que se genere una mutación beneficiosa, pero que no se detecte debido a la aparición simultánea de una mutación letal en otro gen de la misma célula.
Obviamente, la mutagénesis sólo es útil si se dispone de métodos muy eficaces para detectar las mutaciones beneficiosas (véase Sección 9.,2).
El método también se ve limitado a organismos unicelulares o a aquellos que puedan crecer a partir de cultivos celulares o embriones, ya que su modo de acción aleatoria excluye la posibilidad de provocar mutaciones idénticas en todas las células de un animal o de una planta mediante el tratamiento de todo el organismo. 
Como se verá más adelante, esta limitación también es aplicable a la mayoría de los tipos de ingeniería genética.

Protoplastos y clonación de células 

Como se describe en la Sección 9.,3.,2, la «fusión de protoplastos» puede aumentar enormemente la eficacia de la recombinación entre los genomas de cepas microbianas diferentes.
Esta técnica también se puede aplicar a células animales y vegetales.
Cuando las paredes de las células vegetales se digieren con pectinasas y celulosas en un medio iso-osmótico con el contenido de las células, se obtienen protoplastos esféricos, que son células desnudas rodeadas tan sólo por sus membranas plasmáticas.
Mediante una manipulación adecuada, estos protoplastos regenerarán las paredes celulares, crecerán y se multiplicarán, y cada protoplasto dará lugar a una cepa de células clonadas que puede cultivarse indefinidamente como tejido indiferenciado de callo. 
En muchos casos se pueden lograr plantas completas a partir del callo, modificando en el medio los niveles de las hormonas del crecimiento de las plantas, de forma que se puede obtener un número ilimitado de plantas clonadas a partir de un único protoplasto.

Más importante aún es el hecho de que, utilizando un reactivo tal como el polietilenglicol o un campo eléctrico pulsante, se puede inducir la fusión de unos protoplastos con otros antes de que se regenere la pared celular (Shepard y col., 1983).
Estas fusiones pueden tener lugar entre células de especies sexualmente incompatibles, formándose células híbridas.
Por desgracia, estos híbridos son inestables y, con el tiempo, se pierde un juego completo de cromosomas, pero no antes de que haya cierta transferencia de genes entre ambos.

Dado que el intercambio de DNA es al azar, sólo hay una pequeña posibilidad de que alguno de los productos de la fusión contenga un determinado gen «extraño», por lo que se debe utilizar algún tipo de selección para entresacar las células deseadas.
Con frecuencia esto sólo se puede hacer después de que se hayan regenerado las plantas a partir de cada línea celular, a menos que el gen deseado codifique un producto que pueda detectarse bioquímicamente en la etapa de callo.
A pesar de lo poco previsible que resulta y de la dificultad técnica, la fusión de protoplastos representa una valiosa contribución a las técnicas clásicas de la mejora vegetal.
Se ha utilizado, por ejemplo, para transferir resistencia a enfermedades desde especies primitivas de Solanum a modernas variedades comerciales de patata, que no pueden cruzarse sexualmente. 

También se puede inducir la fusión de células animales, formándose normalmente células híbridas inestables.
Sin embargo, como no se pueden regenerar animales completos a partir de células únicas, este método no se ha utilizado como una técnica adicional en programas clásicos de mejora animal.
No obstante, la fusión de células animales ha adquirido una importancia enorme desde que Milstein y Kohler descubrieron que las células normales productoras de anticuerpos se podían fusionar con células malignas (mieloma) para dar lugar a células de «hibridoma», produciendo cada una de éstas produciría un solo tipo de anticuerpo.
Las células del hibridoma pueden separarse unas de otras y cultivarse indefinidamente, produciendo cada clon un único anticuerpo, conocido como anticuerpo «monoclonal».
Estos anticuerpos están demostrando ser de un inmenso valor en medicina y en biología molecular, ya que son mucho más eficaces que las mezclas de anticuerpos diferentes para distinguir entre antígenos similares.

Otro método para obtener nuevas características en una planta se basa en el fenómeno de «variación de clones somáticos», por el cual se observa que las plantas clonadas a partir de diferentes células de la misma planta original pueden poseer características hereditarias diferentes.
No es posible predecir qué características nuevas pueden aparecer, y de las que aparezcan, muchas no serán deseables.
No obstante el método ha demostrado ser de utilidad para conseguir nuevas variedades mejoradas de patatas y de trigo.

Posibles productos de la ingeniería genética

Existe un importante campo de la ingeniería genética en el que la mejora clásica resulta totalmente inútil; se trata de la transferencia de un gen desde un organismo superior a una bacteria o a una levadura con el propósito de lograr sintetizar el producto del gen cultivando las células transformadas.
Pero, dado que los animales o las plantas de los que deriva el gen elaboran tales productos,
¿por qué hemos de preocuparnos de persuadir a organismos inferiores para que hagan el mismo trabajo?
La respuesta se basa generalmente en el coste de aislamiento de cantidades adecuadas de producto a partir de la fuente natural, o en la absoluta imposibilidad de obtener la materia prima suficiente.

Fármacos

Para apreciar el enorme potencial de la ingeniería genética en la producción de fármacos, baste considerar algunos de los polipéptidos que tienen valor farmacológico.
Por ejemplo, muchos hemofílicos pueden morir al no poder detener las hemorragias debido a la imposibilidad de sintetizar un polipéptido llamado factor VIII de la coagulación de la sangre humana, el cual se encuentra presente normalmente en la sangre humana, aunque en concentraciones muy pequeñas.
Afortunadamente, se puede evitar que dichas personas mueran por una hemorragia si se les inyecta el factor VIII, purificado a partir de enormes cantidades de sangre humana.
La preparación del factor VIII no es sólo un proceso muy caro, dada su baja concentración inicial en un fluido cuya disponibilidad es limitada y que ya es de por sí valioso, sino que, como cada preparación se obtiene a partir de la sangre de muchos miles de donantes, hay un riesgo importante de que con las inyecciones del factor de coagulación se puedan transmitir a los hemofílicos virus peligrosos, como el de la hepatitis o el del SIDA .

La obtención de otros polipéptidos, que también han de extraerse del suero humano o de tejidos, presenta problemas similares: el activador plasminógeno tisular podría utilizarse para disolver coágulos de sangre no deseados que bloquean vasos sanguíneos, aliviándose así las trombosis.
La calcitonina, posiblemente junto con la hormona para-tiroidea, podría ser de gran valor para estimular el fortalecimiento de huesos quebradizos que con tanta frecuencia se dan en personas mayores.
Se ha identificado un factor de crecimiento neuronal que podría inducir la reparación y regeneración de nervios lesionados. 
La hormona del crecimiento humano es un polipéptido que ya se utiliza para tratar niños que padecen insuficiencia hipofisaria, y que en caso contrario daría lugar a un marcado retraso del crecimiento o enanismo.
Por fortuna, la incidencia relativamente baja del enanismo hace que se pueda controlar utilizando la hormona extraída de hipófisis de cadáveres humanos, pero existen indicios de que la hormona del crecimiento humano también podría ser de utilidad en el tratamiento de quemaduras, para inducir la cicatrización de heridas y huesos fracturados y para tratar huesos debilitados por una descalcificación.

Dado que la hormona que se obtiene de hipófisis humanas se utiliza casi exclusivamente para tratar el enanismo, cualquier expansión en las aplicaciones de esta hormona está supeditada a su obtención mediante microorganismos sometidos a una manipulación genética.

Posiblemente es la insulina el ejemplo más conocido de un polipéptido obtenido por manipulación genética. 
Esta hormona es esencial para la regulación correcta de los niveles de glucosa en sangre.
Si no se produce en el páncreas, se presentará la diabetes, que debe mantenerse bajo control mediante inyecciones de insulina. 
Dado que se estima que hay unos 60 millones de diabéticos en el mundo, de los cuales unos 25 millones están en países desarrollados (es decir, que pueden pagar sus medicamentos), hay un enorme mercado para la insulina.
Para la mayoría de los diabéticos la insulina obtenido de cerdo o de ganado vacuno parece ser tan eficaz como la variedad humana, lo que era de esperar en vista de que sólo difieren de la insulina humana en uno o tres aminoácidos respectivamente Sin embargo, algunos diabéticos pueden desarrollar una respuesta alérgica a las insulinas animales, y existe cierta polémica acerca de los efectos colaterales de la utilización de una hormona «extraña».
Estas observaciones impulsaron los intentos de producir insulina «humana» mediante bacterias manipuladas genéticamente, y se pensaba que la primera compañía que tuviese éxito haría una fortuna.
Por desgracia para estas compañías, la investigación de métodos de modificar la insulina químicamente fue progresando, hasta el extremo de que ahora es posible modificar la insulina de cerdo para conseguir exactamente la misma estructura de la insulina humana.
La batalla entre los dos procesos de producción de insulina se librará en el ámbito económico y en vista de la abundancia de páncreas de cerdo, parece poco probable que la victoria sea para los biotecnólogos.

Otro grupo de polipéptidos que han originado una gran especulación son los interferones.
El cuerpo humano sintetiza estas moléculas de forma natural como respuesta a infecciones víricas; parece ser que tienen propiedades antivíricas y antitumorales.
No obstante, al igual que para muchos de estos compuestos, las cantidades que pueden elaborarse en los tejidos humanos (glóbulos blancos y fibroblastos en el caso de los interferones) son tan pequeñas que no fue posible realizar las pruebas clínicas adecuadas para establecer sus propiedades.
En vista de sus posibles efectos terapéuticos, se establecieron programas para producir interferones, a gran escala, en bacterias.
De esta forma se han obtenido tres tipos de interferones.
Aunque ciertamente no parecen ser los «fármacos maravillosos» que algunos hablan pronosticado, si parecen tener un efecto importante en el sistema inmunitario, originando un gran incremento de la resistencia a diversos virus.
Igualmente son eficaces en el tratamiento de determinados tipos de cáncer.
Irónicamente, los interferones pueden ser de máxima utilidad contra el cáncer revelando más información acerca de cómo el sistema inmunitario puede detectar y eliminar tumores.

Los sucesores de los interferones como fármacos «contra el cáncer» de mayor interés pueden ser el factor de necrosis tumoral (TNF) y la linfotoxina (LT).

Estos polipéptidos se elaboran en células humanas y son capaces tanto de reconocer células tumorales como de destruirlas selectivamente.
Aunque se trata de moléculas diferentes, ambas presentan algunas secuencias bastante extensas en común y hay dos regiones que son muy constantes (es decir, que probablemente tienen la misma estructura tridimensional en ambas moléculas) que podrían ser las responsables del reconocimiento de las células tumorales y de la destrucción celular.
Se han clonado los genes tanto del TNF como de la LT y las pruebas clínicas de las moléculas sintetizadas por las bacterias han de revelar su eficacia frente a tumores y, lo que es más importante, si producen efectos colaterales perjudiciales (Grey y col., 1984; Pennica y col., 1984).

Vacunas

Las aplicaciones médicas de los microorganismos manipulados genéticamente no se limitan a la síntesis de polipéptidos humanos.
La utilización de tales sistemas en la elaboración de vacunas resulta muy atractiva.
El objetivo que se persigue al fabricar una vacuna es que contenga moléculas que produzcan la misma respuesta antigénica que el virus para el que ofrece inmunidad, sin causar la proliferación nociva de virus en el organismo.
Esto puede lograrse mediante el empleo de virus que de alguna forma se hayan convertido en apatógenos, pero que todavía presentan el mismo aspecto externo.
Sin embargo, hay dos problemas.
La inactivación incompleta de los virus puede permitir que sobrevivan algunos de ellos y, en consecuencia, serian en realidad una fuente de infección; a este tipo de preparaciones se han atribuido algunos casos de glosopeda.
El otro problema es que el virus ha de cultivarse en animales y, en determinados casos, sólo pueden multiplicarse en el hombre.
En tales ocasiones han de utilizarse cultivos de células humanas, que son caros de mantener y de crecimiento muy lento en comparación con las bacterias.
Ya que el mercado de las vacunas puede ser inmenso (200 millones de personas en todo el mundo padecen hepatitis B; cada año se suministran al ganado 800 millones de dosis de vacunas contra la glosopeda), desde un punto de vista económico ha merecido la pena asumir estos problemas. 

Sin embargo, son las proteínas de la envoltura (capa externa) vírica las que son antigénicas, por lo que las preparaciones de proteína purificada procedente de esa estructura pueden actuar como una vacuna eficaz.
Esto abre inmediatamente la posibilidad de clonar y expresar en bacterias los genes de las proteínas de la envoltura vírica, utilizando el producto bacteriano como vacuna.
Tales vacunas tendrán la garantía de estar exentas de virus vivos y, desde luego, serán más baratas que las elaboradas utilizando cultivos de células animales.

Enzimas

Las enzimas sólo pueden utilizarse a escala industrial si su coste de producción es bajo (véanse secciones 13.,2, 14.,3 y 14.,6.,2), lo que, de hecho, significa que han de derivar de cultivos microbianos.
En vista de la inmensa variedad de reacciones que llevan a cabo los microorganismos, es probable que algún microorganismo ya produzca cualquier enzima que necesiten los ingenieros bioquímicos.
No obstante, también es probable que con una cierta manipulación genética se mejore el rendimiento del enzima, quizás transfiriendo su gen a otro hospedador que crezca más rápido y que sea más seguro y más eficaz, o uniendo el gen a un promotor más potente, a fin de incrementar su nivel de expresión.
Uniendo al gen las secuencias adecuadas, se podría lograr que la célula excretase el enzima, facilitándose así su aislamiento.

Muchos enzimas son inestables cuando se separan de la célula, por lo que no son adecuados para su uso industrial.
La ingeniería genética ofrece la posibilidad de modificar la estructura de un enzima para aumentar su estabilidad, modificar su pH o su temperatura óptima, cambiar su afinidad por el sustrato, etc., pero este «ajuste» de los enzimas depende de que mejore ampliamente nuestro conocimiento acerca de las forma exacta en que trabajan los enzimas.

Líneas de células mejoradas 

Las células se cultivan para obtener una molécula de valor, tal como un antibiótico, o a fin de generar una biomasa de alta calidad a partir de un sustrato barato, como en el caso de la producción de proteína unicelular. 
En ambos casos existen fuertes razones comerciales para utilizar células que crezcan lo más eficazmente posible en cuanto a conversión de energía, y que se adecuen tanto como sea posible a su función especifica. 
En la Sección 9.,3.,3 se presentan varios ejemplos en los que se emplea la ingeniería genética para potenciar la eficacia del crecimiento de microorganismos o para aumentar la producción de un compuesto útil.

Animales

Para el profano, el término «ingeniería genética» probablemente suscita imágenes de un moderno Dr.
Frankensteincreando nuevas criaturas monstruosas en su laboratorio.

Irónicamente, parece poco probable que la manipulación genética de genomas animales progrese en los próximos años tanto como la de otros organismos.
Como se verá más adelante, la manipulación genética implica la selección de las células cuyos genomas se han modificado adecuadamente, entre las que no se han modificado o que no se han modificado adecuadamente, seguida de la multiplicación de las células seleccionadas («clonación»). 
Este es el final del proceso cuando tratamos con organismos unicelulares, como bacterias o levaduras, o con cultivos celulares de animales o plantas; pero para obtener animales o plantas manipuladas genéticamente, han de regenerarse los organismos completos a partir de las células clonadas.
Por el momento somos incapaces de regenerar animales a partir de células clonadas, excepto en la limitada extensión de subdividir grupos de células en las primeras etapas del desarrollo embrionario.

Suponiendo que se pueda eliminar este obstáculo, existe todavía el problema de saber qué genes hemos de modificar.
Los genomas de los animales son inmensamente más complejos que los de las bacterias, incluso su análisis genético se ve dificultado porque las poblaciones de que se dispone para la investigación son relativamente pequeñas y sus tiempos de generación son largos.
A esto hay que añadir que características tales como la velocidad de crecimiento, la producción láctea, la resistencia a enfermedades y la morfología vienen determinadas por interacciones entre muchos genes diferentes, siendo evidente que todavía tenemos un largo camino que recorrer antes de que se pueda mejorar la calidad del ganado mediante ingeniería genética.
Sin embargo, a pesar de estos obstáculos ya ha habido algunos intentos fructíferos de expresar genes extraños en animales (véase Sección 11.,3.,3).

Plantas

Por el contrario, se ha caracterizado muy bien la genética de las principales plantas que se cosechan.
Se ha determinado la localización de una amplia gama de genes de función conocida y muchos de estos genes se han aislado, clonado e incluso se ha determinado su secuencia.
Estos análisis tan detallados han sido posibles gracias a la facilidad para obtener enormes cantidades de plantas a partir de cruces experimentales. 
Aún subsisten los problemas de la interacción de genes para determinar las características de la planta, pero con frecuencia se han podido identificar uno o dos genes que presentan una influencia especialmente fuerte en una determinada característica.
En consecuencia, hay muchos genes vegetales que codifican atributos tales como resistencia a enfermedades, a estrés o a herbicidas, y proteínas de reserva de semillas o componentes del sistema fotosintético que se podrían desear transferir de una especie a otra.
Es posible clonar células de la mayoría de las plantas por crecimiento como injerto o en cultivos en suspensión, y de muchos de éstos se pueden regenerar plantas completas. 
En tales casos parece no haber ninguna razón para que no se utilice la ingeniería genética como una alternativa más rápida que la mejora clásica y para salvar cualquier barrera que impida cruces entre plantas de especies diferentes.
Por desgracia, todavía no es posible regenerar plantas a partir de cultivos celulares de las monocotiledóneas más importantes que se cultivan (gramíneas), tales como el trigo o el maíz, y éste sigue siendo el obstáculo más importante para su manipulación genética (Ozias-Akins y Lorz, 1984).

Las fantasías del ingeniero genético de plantas no se detienen en la transferencia de genes de una planta a otra; van mucho más allá, como introducir genes microbianos en plantas.
Los genes más atractivos comprenden los que codifican los enzimas responsables de la degradación de ciertos herbicidas, y que por tanto podrían hacer que la planta fuese resistente a tales herbicidas.

Dicha resistencia seria muy útil en los cultivos, permitiendo la utilización de herbicidas de forma selectiva contra las malas hierbas después de que haya brotado el cultivo.
Otros genes potencialmente valiosos son los responsables de la elaboración de polipéptidos insecticidas en algunas bacterias, como Bacillus thuringiensis .

Probablemente la proposición más ambiciosa consiste en introducir en plantas los genes de fijación de nitrógeno desde un organismo tal como Klebsiella pneumoniae .
Esto implicaría la transferencia de un gran complejo de 17 genes, conocido como el grupo de genes nif y requeriría cierta manipulación de dichos genes para asegurar su expresión.
Dado el origen procariota del grupo de genes nif , se ha sugerido que la estrategia más sensata consistiría en transferirlos al cloroplasto, cuya maquinaria transcriptora y traductora tiene muchas características procariotas. 
Esta aproximación también tiene la ventaja de que los cloroplastos están acoplados a la producción de ATP y al potencial reductor que son necesarios para la fijación del nitrógeno (Merrick y Dixon, 1984).
Hay desde luego problemas, no siendo el menor de ellos la sensibilidad de la nitrogenasa al oxígeno que producen los cloroplastos y el no disponer de un método para transformar los cloroplastos. 
Sin embargo, en vista del masivo empleo anual de fertilizantes artificiales, existe toda clase de incentivos para resolver estos problemas.
Una alternativa, a la que se está prestando una considerable atención, consiste en introducir en los cultivos genes responsables de la asociación con microorganismos fijadores de nitrógeno, utilizando ingeniería genética. 

Es fácil dejarse llevar por las excitantes posibilidades de la manipulación genética y suponer que las ideas que se han expuesto darán su fruto en los próximos años.
Hay enormes presiones comerciales para progresar rápidamente, pero también hay muchos problemas técnicos y algunas lagunas importantes en nuestro conocimiento de los procesos que queremos modificar.

Resumen

La ingeniería genética se define como el cambio de genes utilizando procesos in vitro, en contraste con los procedimientos de mejora clásica en los que los genes se transfieren por medios sexuales o por conjugación.
Dado que tal transferencia de genes es un proceso relativamente aleatorio, hay que seleccionar las características deseables, y para generar «nuevos» genes hay que basarse en las mutaciones.
Una seria limitación de este sistema es que no se pueden transferir genes entre organismos que no estén relacionados.

Por el contrario, la ingeniería genética es particularmente útil cuando se desea introducir un gen de un organismo superior en una bacteria o en una levadura. 

Como resultado de tales manipulaciones se puede conseguir la síntesis microbiana de polipéptidos de actividad farmacológica, vacunas o enzimas para uso industrial. 
Dichas manipulaciones se pueden utilizar para mejorar las propiedades de los microorganismos de importancia industrial o para modificar la genética de los animales.
Sin embargo, en este último caso el principal problema radica en modificar todas las células de un organismo, ya que no se pueden regenerar animales a partir de cultivos unicelulares. 
Las plantas por el contrario, se pueden manipular más fácilmente, y hay perspectivas reales para dotarlas de resistencia a enfermedades y a herbicidas, dirigir la síntesis de polipéptidos insecticidas e introducir la fijación de nitrógeno en cultivos de plantas no leguminosas.

Técnicas de la ingeniería genética

Esquema de la clonación de genes 

La clonación de genes fue inicialmente posible por avances técnicos tales como el aislamiento de enzimas que rompen el DNA por sitios precisos (endonucleasas de restricción), o que unen covalentemente fragmentos de DNA (ligases) y, con frecuencia, los avances dependen aún del desarrollo de nuevos enzimas u otros reactivos bioquímicos.
Estamos viviendo una explosión de publicaciones de nuevas técnicas relacionadas con la clonación; esta proliferación de información técnica, junto con un exceso de «jerga», pueden hacer que el neófito no comprenda fácilmente los principios básicos de la clonación.
Una vez que éstos se entienden es mucho más fácil ver dónde encajan las nuevas técnicas en el proceso de clonación.
El objetivo de esta sección es esbozar las estrategias que se emplean en la clonación de genes y presentar las técnicas que se utilizan con mayor frecuencia.

Hay varias posibles razones para desear clonar un determinado fragmento de DNA .
Se podría desear introducir en una bacteria un gen que codifique un polipéptido valioso, de forma que las bacterias produjesen grandes cantidades del polipéptido.
Esta es la aproximación que ha recibido mayor publicidad, y que tiene como resultado la producción bacteriana de polipéptidos tales como la insulina, el interferón y la hormona del crecimiento. 
Como se verá más adelante, el DNA también se puede clonar en células eucariotas, con el propósito de utilizar las células como «fábricas» bioquímicas o para modificar las propiedades de las células y hacerlas más adecuadas a nuestras necesidades.
No obstante, en la práctica, la clonación de genes se lleva a cabo con mayor frecuencia para producir suficiente DNA para posteriores análisis como, por ejemplo, para determinar su secuencia.

La clonación de genes consiste esencialmente en la inserción en la célula de un determinado fragmento de DNA «extraño» de forma que el DNA insertado se replique y se transmita a las células hijas durante la división celular. 
Este proceso tiene lugar de forma natural, como lo demuestra la rápida dispersión de la resistencia múltiple a antibióticos entre poblaciones bacterianas sometidas a una adecuada presión selectiva.
Sin embargo, la transferencia de DNA sólo se da normalmente entre cepas de bacterias estrechamente relacionadas, y existe poco control sobre qué fragmentos de DNA se transfieren.
Por tanto, la transferencia natural de DNA o transducción, es demasiado imprevisible y limitada para que sea de utilidad en la clonación de genes.

Los principales pasos de la clonación de genes se resumen en la figura 11.1, incluyendo los procesos siguientes:

1 Aislamiento del gen (u otro fragmento de DNA ) a clonar.

2 Inserción del gen en otro fragmento de DNA , llamado vector, que hará posible que el DNA se incorpore a la bacteria y que se replique en su interior cuando las células crezcan y se multipliquen.
Aunque la figura ll.l muestra el empleo de un plásmido como vector, ya que éste es un sistema ampliamente utilizado, también se pueden utilizar otros tipos de vectores, tales como el DNA vírico o los cósmidos.

3 Transferencia de los vectores recombinantes a las bacterias, bien por transformación o por infección utilizando virus.

4 Selección de las células que contengan los vectores recombinantes deseados.

5 Cultivo de las bacterias, que puede prolongarse indefinidamente, para obtener tanto DNA clonado como sea necesario.

6 En esta etapa, normalmente, se recupera de las células el vector que contiene el DNA insertado, y el fragmento insertado se corta y se separa de las moléculas de vector.
No obstante, a veces el objetivo es conseguir la expresión del gen en las bacterias, especialmente si el gen codifica un polipéptido que es difícil o caro de preparar por otros medios.


Figura 11.1

Esquema de la clonación de genes

Procedimientos de clonación de genes

Endonucleasas de restricción 

Las endonucleasas de restricción desempeñan un destacado papel en la clonación de genes.
Estos enzimas, que se encuentran de forma natural en las bacterias como arma frente a virus invasores, cortan ambas cadenas de las moléculas de DNA siempre que se dé determinada secuencia de nucleótidos.
La clase de enzimas que más se utiliza para la clonación de genes se conoce como tipo II; cada uno de estos enzimas reconoce una secuencia determinada de nucleótidos, generalmente de cuatro o seis nucleótidos, y normalmente corta las cadenas de DNA por esta secuencia.
Los cortes pueden ser a distinta altura, originándose unas protuberancias unicatenarias cortas, o pueden encontrarse a la misma altura, en cuyo caso dan lugar a extremos «romos». 
La figura 11.2 ilustra los tipos de secuencia que pueden reconocer los enzimas de restricción y dónde se verifican los cortes.
Obsérvese que las secuencias que se cortan internamente (la mayoría) son simétricas; es decir, cuando se leen de 5' a 3', ambas cadenas tienen la misma secuencia.
Los cortes a distinta altura originan extremos unicatenarios que son idénticos y complementarios entre sí.
En consecuencia, a bajas temperaturas, las cadenas simples tienen cierta tendencia a asociarse por complementaridad de bases, originando una unión reversible entre fragmentos que se han cortado por el mismo enzima.
Se dice que tales fragmentos tienen extremos «adherentes» o «cohesivos». 
Hoy día se dispone a nivel comercial de una enorme variedad de endonucleasas de restricción, de forma que se pueden conseguir extremos romos o adherentes para la mayor parte de las secuencias de tetra - o de hexanucleótidos. 

El aislamiento de genes para clonar, la inserción de DNA en los vectores, la recuperación de los fragmentos insertados a partir de los vectores clonados y la construcción de nuevos vectores dependen de nuestra capacidad para cortar el DNA por puntos bien definidos-de aquí la capital importancia dé las endonucleasas de restricción en la clonación de genes .

Aislamiento del DNA a clonar

Dependiendo de los objetivos del experimento, el DNA puede proceder de diversas fuentes.
Se puede pretender aislar y clonar un gen determinado para posteriormente analizar su secuencia, o para investigar el control de su transcripción. 
Inevitablemente, el gen sólo constituirá una pequeña parte de todo el genoma (el 0,03 aproximadamente del genoma de Escherichia coli y tan solo el xxx de genomas de mamíferos), por lo que ha de encontrarse una forma de identificar el gen y «separarlo» de las otras partes del genoma, bien antes o bien después de clonarlo.
Esta tarea se simplifica si se dispone del RNA m correspondiente en una forma suficientemente purificada.
Si la proteína que codifica es un producto mayoritario de las células, el RNA m ya estará enriquecido en el tipo deseado.
Sin embargo, para enriquecer los RNA m menos abundantes (la mayoría) se necesitarán procesos tales como fraccionamiento por tamaño del RNA m, traducción in vitro de las fracciones, precipitación de los productos de la traducción con anticuerpos específicos, etc. 
En la Sección 11.,2.,6 se describen los métodos para obtener un RNA m específico, y no vale la pena que éste constituya el paso más difícil de la clonación, particularmente si el producto del gen no está bien caracterizado.

Figura 11.2

Secuencias de reconocimiento de algunas endonucleasas de restricción

Las flechas indican los puntos de escisión. 
Obsérvese que los extremos adherentes que se obtienen con xxx son complementarios de los que se obtienen con Bam HI, aunque el primero reconoce la secuencia de un tetranucleótido mientras que el último reconoce un hexanucleótido.

El RNAm purificado puede utilizarse para dirigir la síntesis de una cadena de DNA complementario (DNAc), utilizando el enzima transcriptasa inversa (Fig., 11.,3); como resultado se obtiene un híbrido de DNA/RNA .
Tras retirar la cadena de RNA utilizando un álcali, la cadena simple de DNA puede actuar como un molde para la síntesis de una cadena complementaria de DNA en presencia de la DNA polimerasa.
Se podría esperar que la doble cadena de DNAc resultante fuese idéntica al gen que codifica su RNAm , y así es en el caso de los genes procariotas.
Sin embargo, la mayoría de los genes eucariotas contienen secuencias intercaladas (intrones) que interrumpen sus secuencias de codificación. 
Se transcribe el gen completo, incluyendo los intrones; éstos se eliminan del RNAm y las piezas restantes (axones) se empalman para dar el RNAm maduro, que es el que se traduce y a partir del cual se obtiene el DNAc .
Como en el RNAm funcional no hay intrones, cuando el objetivo de la clonación sea la expresión del gen se puede utilizar el DNAc en lugar del gen verdadero.
Obviamente, por el centrar o, esta aproximación no se puede utilizar si se desea analizar la secuencia del gen nativo, incluyendo sus intrones.
En el último caso, el DNAc se puede marcar radiactivamente y utilizarse como una sonda de hibridación para identificar los fragmentos de DNA que contienen el gen.

Figura 11.3

Síntesis de DNAc

Cuando se manipula un genoma poco caracterizado se utiliza con frecuencia un procedimiento que consiste en formar una «genoteca», por el cual el genoma completo se corta al azar en grandes trozos de un tamaño de unas 10 kilobases (kb) o más, utilizando una digestión parcial con varios enzimas de restricción, y todas estas piezas se clonan sin ningún propósito de seleccionar unas secuencias determinadas.
Cuando las bacterias que contienen estos fragmentos se siembran en placas para que formen colonias individuales, cada colonia contendrá una porción del genoma, y existe una probabilidad determinada de que el gen deseado se encuentre en alguna de tales porciones.
Si se obtienen las colonias suficientes se puede estar casi seguro de que al menos una contendrá el gen.

Cuanto mayor sea el genoma y menor el tamaño medio de los fragmentos, mayor será el número de colonias que habrá que obtener para estar seguro de encontrar un gen determinado.
Mediante los cálculos oportunos se demuestra que para fragmentos de 10 kb se necesitan xxx colonias para el genoma de E. coli , y casi xxx para el Homo sapiens .
En el último caso, está claro que no se puede manejar el número de colonias que se requiere, y por lo tanto se han de clonar fragmentos mucho mayores, de un tamaño de unas 40 kb .
Como se describirá en la Sección 11.,2.,7, para clonar fragmentos de tal tamaño se necesitan vectores especiales.

Los métodos para identificar las colonias que contienen determinados genes se revisarán más adelante (véase Sección 11.,2.,6).

La química de los ácidos nucleicos ha avanzado tan rápidamente que la síntesis de largas moléculas de DNA de cualquier secuencia de nucleótidos deseada es ya una técnica de rutina en muchos laboratorios. 
De hecho, es posible comprar máquinas que realizan automáticamente cada paso de la síntesis; todo lo que tiene que hacer el operario es suministrar los reactivos y establecer la secuencia de nucleótidos a obtener.
En consecuencia, si se conoce la secuencia de aminoácidos de un polipéptido, puede ser más fácil sintetizar un «gen» que codifique esa secuencia que tratar de aislar el gen natural; esta aproximación se utilizó para la síntesis bacteriana de la hormona polipeptídica somatostatina.

Plásmidos como vectores de clonación 

Para clonar un fragmento de DNA no basta con transferirlo a una bacteria con la esperanza de que se replique junto con el propio DNA de la célula. 
El DNA extraño no se replicará a menos que contenga una secuencia de nucleótidos que la bacteria hospedadora reconozca como «origen de replicación». 
Por lo tanto, generalmente, es necesario unir el DNA a otro fragmento que contenga un origen de replicación. 

Estos orígenes de replicación existen de forma natural en los plásmidos, que son pequeñas moléculas circulares de DNA que se encuentran en muchas bacterias.
Son independientes del DNA cromosómico principal, y sin embargo se replican y generalmente pasan a las células hijas durante la división celular. 
Estos plásmidos pueden llevar genes muy importantes, incluyendo los de resistencia a antibióticos, los de producción de toxinas o de antibióticos, los de fijación de nitrógeno y los de enzimas necesarios para la degradación de un gran número de sustratos «poco frecuentes» tales como herbicidas o residuos industriales.
Sólo unos pocos de estos genes están presentes en cualquier plásmido, pero debido al frecuente intercambio de material genético entre los plásmidos y el DNA cromosómico, continuamente surgen nuevas colecciones de genes, contribuyendo de esta forma a que las poblaciones bacterianas respondan rápidamente a nuevas condiciones ambientales. 
Esto puede representar un problema serio en hospitales si se están utilizando distintos antibióticos, ya que la transferencia de genes a través de los plásmidos puede originar la aparición de bacterias con resistencia a varios antibióticos diferentes a la vez.

Los plásmidos ofrecen una fuente apropiada de orígenes de replicación para la clonación de genes, pero la mayoría de sus genes son superfluos, y hacen que la molécula sea difícil de manejar.
Por lo tanto, se han construido plásmidos a partir de otros que existen en la naturaleza, conservando sólo aquellas características que favorezcan la clonación.
Uno de los plásmidos más ampliamente utilizados, el xxx , se construyó cortando partes de plásmidos que aparecen naturalmente, utilizando enzimas de restricción y volviendo a unirlas en las orientaciones correctas, en una serie de operaciones que hacen que parezca fácil resolver el cubo de Rubik.
El plásmido resultante posee muchas de las características que son deseables en un vector de clonación (Fig., 11.,4). 
La molécula contiene un origen de replicación, que deriva de un plásmido relacionado con el plásmido natural Col El.
Este origen es particularmente útil por ser «relajado», es decir, su replicación no va unida a la del DNA cromosómico, y de aquí que la iniciación de la replicación del plásmido pueda ser más frecuente que la del DNA principal de la bacteria.
Como consecuencia, en cada célula se acumulan múltiples copias del plásmido.

Figura 11.4

Mapa del plásmido xxx 

Las regiones que codifican la resistencia a la ampicilina (ampR) y a la tetraciclina (tetR) , y el origen de replicación del DNA (ori) se indican con segmentos oscuros.
Se indican los puntos de corte de diversas endonucleasas de restricción, algunas de las cuales cortan el plásmido por un gen de resistencia a un antibiótico.
Cada una de los enzimas que se muestran cortan el plásmido por un solo punto, y por lo tanto origina una sola molécula lineal a partir del plásmido circular.

Este proceso puede llevarse aún más lejos si al cultivo celular se añade cloranfenicol (un inhibidor de la síntesis proteica), ya que la síntesis proteica es necesaria para la replicación cromosómica, pero no para la del plásmido.
De esta forma, se puede utilizar el cloranfenicol para «amplificar» el plásmido, obteniéndose hasta 3.000 copias por célula.

Este plásmido también se ha construido meticulosamente, de forma que contiene un único punto de reconocimiento para 20 endonucleasas de restricción diferentes.
Esto significa que cualquiera de los enzimas verificará un sólo corte en el plásmido circular, generando una sola molécula lineal a la que se puede unir un fragmento de DNA para su clonación.

Seguidamente la molécula se puede volver a cerrar regenerando un plásmido circular ligeramente mayor.
En los ejemplos de la figura ll.4 se ve que los puntos de restricción se distribuyen por todo el plásmido, y esto permite una gran flexibilidad en la estrategia para seleccionar las moléculas recombinantes, como a continuación se describe.

En el xxx se han conservado dos conjuntos de genes; éstos codifican la resistencia a la ampicilina y a la tetraciclina, que son marcadores de gran utilidad.
La presencia del plásmido en una bacteria confiere a ésta resistencia a dichos antibióticos, de forma que estas bacterias pueden seleccionarse cultivándolas en un medio que contenga ampicilina o tetraciclina.
Ademas, como los puntos de restricción están situados en los genes de resistencia a los antibióticos, se puede preparar la inserción del DNA extraño para que se inactive uno de los dos genes.
Por ejemplo, si se inserta el DNA en el punto Bam HI, se destruirá la resistencia a tetraciclina; por lo tanto los plásmidos recombinantes harán que las bacterias puedan crecer en presencia de ampicilina pero no ofrecerán protección frente a la tetraciclina. 

Esto proporciona las bases de un método de discriminación entre las bacterias que simplemente contienen el plásmido de aquellas que contienen el plásmido con el DNA insertado (véase figura ll.l). 
La bacterias se siembran a baja concentración en placas con un medio de cultivo, sólido por adición de agar, que contiene ampicilina.
Tras la incubación, todas las células que contengan un plásmido habrán crecido formando colonias.
En esta fase se podría saber qué bacterias contienen los plásmidos con DNA insertado inoculando células de cada colonia en una nueva placa del medio con tetraciclina en lugar de ampicilina.
Las bacterias que no puedan crecer en esta placa contendrán probablemente plásmidos con DNA insertado y los clones de células se pueden recuperar a partir de la colonia correspondiente en la placa madre.
En la práctica, la forma más sencilla de hacer esta selección es mediante la «replicación de placas», en la que se hace contactar una almohadilla de terciopelo estéril con la primera placa, cogiendo algunas células de cada colonia.
A continuación se inocula la segunda placa haciéndola contactar con la almohadilla de terciopelo.
Tras la incubación, las colonias se encontrarán en la segunda placa en idéntica posición a la de la primera, pero faltarán algunas. 
Entonces es muy fácil saber qué colonias no han crecido en el medio con tetracilina pudiendo recuperarse a partir de la primera placa y cultivarse para una posterior caracterización de los fragmentos insertados en sus plásmidos. 

El tamaño de los plásmidos que se utilizan como vectores para clonar se ha ido reduciendo al mínimo posible.
Esto se ha hecho por dos razones: primero, porque es mucho más difícil que las moléculas pequeñas de DNA sufran daños por cortes durante el aislamiento; este aumento de la estabilidad permite que se utilicen métodos relativamente violentos para procesos tales como la desproteinización de las preparaciones de plásmidos, haciendo que sea más fácil obtener buenos rendimientos de DNA puro.
En segundo lugar, las moléculas pequeñas se incorporan con mayor eficacia a la bacterias durante el proceso de «transformación», aspecto importante a considerar cuando se manejan cantidades muy pequeñas de un DNA valioso.
El tamaño del plásmido debe, por lo tanto, mantenerse lo más pequeño posible, por lo que es obvio que la inserción de grandes fragmentos de DNA en un plásmido dará como resultado una molécula físicamente inestable y que puede tener una eficacia de transformación tan baja que la haga impracticable.
Las limitaciones en el tamaño máximo del fragmento a insertar pueden originar problemas cuando, por ejemplo, se está preparando una genoteca de un organismo eucariota. 
En tal caso, suponiendo un tamaño medio de los fragmentos de 10 kb , sería necesario obtener unas xxx colonias de bacterias para estar razonablemente seguro de que al menos una tendrá un plásmido que lleve insertado un gen determinado.
Sin embargo, si se pudiesen clonar fragmentos de un tamaño de 40 kb , sólo se necesitarían xxx colonias.
Aunque esto puede parecer todavía un número elevado, de hecho, no es demasiado difícil de manejar.

Unión del DNA

El proceso de clonación del DNA no sólo depende de la disponibilidad de enzimas de restricción y de vectores, sino también de nuestra capacidad para unir fragmentos de DNA covalentemente.
Si los fragmentos de DNA que se van a unir se han cortado con el mismo enzima de restricción, dando extremos adherentes idénticos, al encontrarse tenderán a unirse por puentes de H entre pares de bases complementarias. 
No obstante, esta asociación es tan solo temporal, debido a la labilidad de los enlaces y al reducido número de bases implicado.
La unión puede hacerse permanente utilizando un enzima «ligase», que formará un enlace fosfodiéster entre los grupos 5'fosforil libre y el 3'hidroxil, uniendo así dos moléculas de DNA o circularizando una sola molécula lineal.
La principal fuente de ligase es el fugo T4, y dicho enzima requiere ATP para llevar a cabo la unión.
A fin de estabilizar el apareamiento de bases de los extremos adherentes, la unión se realiza a temperaturas de 4-10 °C con largos períodos de incubación para compensar la baja tasa de actividad enzimática a esas temperaturas.
Una característica importante de la DNA -ligasa del T4 es su capacidad para unir extremos romos de DNA , siempre las concentraciones de enzima y de DNA sean suficientemente altas.

Como puede observarse, el proceso de unión que se acaba de describir depende de los choques al azar de las moléculas de DNA para determinar qué fracciones se unen.
Por lo tanto, los productos de una mezcla de fragmentos de DNA y un vector lineal incluirán no sólo el vector con una única fracción de DNA insertado, sino también al vector circularizado sin ninguna inserción, fragmentos de DNA compuestos por uno o más fragmentos, fragmentos circularizados, dos o más moléculas del vector unidas, etc. 
Mediante una selección cuidadosa de las concentraciones del DNA del vector y del que se va «a insertar» se puede favorecer la formación de un plásmido que contenga una sola inserción, pero no se puede excluir totalmente la formación de productos no deseados.

Este tipo de unión es adecuado para algunos fines, especialmente cuando el producto recombinante se selecciona en pasos posteriores.
Sin embargo, hay formas de ejercer cierto control sobre los productos que pueden formarse.
Por ejemplo, si el vector se trata con una fosfatase alcalina después de la restricción, se eliminan los grupos 5'fosforil (Fig., ll.,5).
La ligase sólo unirá extremos 3' y 5' del DNA si el extremo S' está fosforilado, y, por lo tanto, al unir fragmentos de DNA sin tratar, sólo pueden formarse enlaces entre los fragmentos y el vector o entre fragmento y fragmento, impidiendo de esta forma la simple circularización de los vectores e incrementando la producción de moléculas recombinantes. 
Por supuesto, la unión fragmento-vector sólo puede realizarse a través de una cadena de DNA , pero es suficiente para mantener la molécula unida y las bacterias hospedadoras repararán las «fisuras» después de la transformación.

Figura 11.5

Utilización de una fosfatase alcalina para incrementar el rendimiento de moléculas recombinantes

En condiciones ideales tanto el vector como el DNA «a insertar» deben cortarse con el mismo enzima de restricción, o con dos enzimas que generen extremos idénticos (isoesquizómeros), pero esto no siempre es posible.
En tales casos, cualquier extremo adherente puede convertirse a la forma roma, bien eliminando los nucleótidos protuberantes con una nucleasa S1 (que degrada DNA de una sola cadena), o llenando los extremos unicatenarios utilizando una DNA -polimerasa y los cuatro nucleósidos trifosfatos.
En principio, las moléculas de extremos romos podrían unirse a continuación utilizando la DNA ligase del T4 .
Sin embargo, no suele hacerse, ya que probablemente las moléculas recombinantes así obtenidas no contendrían puntos de restricción en los lugares por donde se han unido, y por tanto sería difícil recuperar el DNA insertado a partir del vector tras la clonación.
En consecuencia, es mucho más útil unir los extremos romos de los fragmentos de DNA a los de los «engarces», que son oligonucleótidos obtenidos por síntesis química que contienen uno o más lugares de restricción (Fig., 11.,6).
Por ejemplo, si el vector contiene un punto de escisión Hind III en uno de sus genes de resistencia a antibióticos, los fragmentos de DNA , tras la conversión a formas de extremos romos, se unirán a engarces que contengan un lugar Hind III.

Posteriormente, al tratar los fragmentos unidos a los engarces con el enzima de restricción Hind III se obtendrán moléculas que puedan unirse al vector. 
Se dispone a nivel comercial de tantos engarces (y moléculas afines que se conocen como adaptadores) que se pueden solucionar casi todos los problemas de incompatibilidad entre el vector y el fragmento de DNA a insertar.

Para unir moléculas de extremos romos se puede utilizar una técnica que se conoce como «colas homopolímeras», que tiene la ventaja de que entre el vector y el DNA a insertar sólo puede haber enlaces intermoleculares. 

El vector y el fragmento de DNA se tratan independientemente con una transferasa terminal y con d ATP o con TTP , de modo que en el extremo 3' de uno se forman colas poli-d A y en el del otro colas pali- T .
Al mezclarlos, las colas complementarias forman moléculas híbridas estables que pueden utilizarse para la transformación. 
La desventaja de este método es que no crea automáticamente puntos de restricción a ambos lados del fragmento de DNA , por lo que puede ser difícil recuperar el DNA insertado.
Cuando se utilizan d GTP y d CTP de forma similar con moléculas que se han cortado con el enzima de restricción P st I , al unirse se regenera un punto de escisión PstI que puede utilizarse para recuperar el DNA insertado a partir del plásmido después de la clonación.

Transformación y cultivo de las bacterias

Antes de que el DNA recombinante pueda multiplicarse por clonación ha de incorporarse a una bacteria hospedadora adecuada, a la que entonces se llama «transformada» y que generalmente es una cepa de E. coli que no tiene sistema de restricción, el cual en condiciones normales degradaría el DNA extraño. 
Las bacterias sin tratar no incorporarán DNA de forma significativa, y por tanto han de tratarse previamente para hacerlas «competentes».
Este tratamiento previo generalmente incluye una incubación de las células en crecimiento exponencial con xxx a baja temperatura, tras lo cual se añade el DNA ; a continuación, con un moderado aumento de la temperatura se provoca la incorporación del DNA .
La eficacia de la transformación nunca es alta, viéndose influida por la cepa bacteriana y por el tamaño y forma del DNA .
Por cada microgramo (mg) de DNA superenrollado de xxx pueden obtenerse de xxx a xxx transformantes, pero esta tasa baja a menos de xxx de plásmido circularizado y disminuye progresivamente a medida que aumenta el tamaño de DNA insertado.
El DNA lineal es casi totalmente ineficaz en la transformación.

Incluso cuando su eficacia es máxima, la transformación tiene lugar tan solo con un 0,01 % de las moléculas de DNA .
Dado que el proceso favorece las moléculas pequeñas, esto puede representar un problema cuando se intentan clonar grandes fragmentos de DNA insertados en plásmidos, y deben tomarse algunas precauciones para evitar que los plásmidos no recombinantes vuelvan a circularizarse, utilizando fosfatase alcalina (véase Sección 11.,2.,4).

Figura 11.6

Utilización de engarces para unir moléculas de extremos romos

La selección de las bacterias transformadas depende generalmente de su resistencia a un antibiótico, y por tanto es importante incubar las células en un medio sin antibiótico durante una hora para permitir que se expresen los genes del plásmido de resistencia a antibiótico. 
A continuación se pueden sembrar las bacterias en un medio sólido que contenga el antibiótico para seleccionar las colonias que contengan el DNA recombinante, según se describe en la Sección 11.,2.,3.
Mis adelante se describirán los métodos para seleccionar colonias determinadas.
A veces se pretende clonar un único tipo de plásmido (recombinante o no) que ya se ha purificado y en este caso no hay necesidad de sembrar en placa las bacterias transformadas antes de cultivarlas en un medio selectivo Para más información acerca de la transformación y el cultivo de las bacterias véanse los Capítulos 6 y 7.

Avances y perspectivas de la ingeniería genética

Avances

¿Cuáles son los avances de la ingeniería genética en el área de la biotecnología? 
Hasta ahora son muy pocos los productos obtenidos por ingeniería genética que han llegado a producirse comercialmente, aunque muchos han llegado a la fase de planta piloto o se están sometiendo a ensayos clínicos. 

Las bacterias manipuladas genéticamente se han venido utilizando durante varios años para producir proteína unicelular.
Se dispone de insulina «humana» sintetizada por bacterias, si bien sostiene una dura competencia con la insulina de cerdo modificada químicamente.
Las únicas vacunas disponibles comercialmente por ahora son para animales, donde existe una gran demanda.
Estas vacunas no pueden ofrecer la prolongada protección que proporcionan las bacterias vivas atenuadas, pero su empleo también presenta menos riesgo.
Estos pocos productos, a pesar del éxito, posiblemente no pueden generar los beneficios suficientes como para cubrir las enormes cantidades de dinero que ya se han invertido en el desarrollo de la ingeniería genética, y ciertamente los inversores tendrán que esperar algunos años más antes de obtener un beneficio que merezca la pena.
Ya ha habido empresas que se han encontrado con problemas financieros al no considerar el tiempo que se requiere para resolver los enormes problemas asociados al desarrollo, producción y control de un nuevo producto.
Los Estados Unidos y Japón han mostrado con mucho el mayor compromiso con la ingeniería genética, y es de esperar que en los próximos años comiencen a obtener los muy considerables beneficios de su previsión. 
De hecho se espera que para el año 2000 se atribuyan a la ingeniería genética ventas en todo el mundo por 40 billones de dólares (Gregory, 1984), por lo que ahora existe gran interés para invertir en investigación. 

Los proyectos que probablemente van a proporcionar beneficios tanto comerciales como sociales en un futuro próximo incluyen la producción del factor VIII de coagulación de la sangre humana, interferones, factor de necrosis tumoral, linfotoxina y vacunas para el virus del herpes simple, el de la hepatitis B y otros virus humanos.
Se dispondrá de bacterias que degraden contaminantes tales como el 2,4,5-T o el DDT , o para producir compuestos tensoactivos, emulsionantes y otras moléculas especializadas.
Todos estos proyectos se han conseguido a nivel de laboratorio y ahora han de llevarse a cabo a gran escala.

Problemas

Expresión y estabilidad de los plásmidos

La ingeniería genética ha alcanzado un nivel en el que es relativamente sencillo aislar y clonar el gen de cualquier proteína que se haya caracterizado adecuadamente. 
Los problemas que requieren mas tiempo aparecen después de esta etapa.
Para empezar, es necesario conseguir niveles elevados de expresión de un gen en un hospedador extraño, idealmente bajo alguna forma de control, de modo que pueda retrasarse la expresión hasta que las células hospedadoras hayan alcanzado una elevada densidad celular.
La proteína resultante del gen podría no ser estable en la célula hospedadora, por lo que puede ser necesario utilizar un gen híbrido como una forma de «marcar» la proteína con un polipéptido protector; otra alternativa seria modificar el genoma de la célula hospedadora para evitar que degrade proteínas extrañas.
Si el gen extraño está en un plásmido, debe asegurarse que el plásmido se mantiene en el cultivo celular.
Cuando el gen extraño aumenta la eficacia de crecimiento de la célula hospedadora, como es el caso del gen de la glutamato deshidrogenasa transferido a M.methylothrophus , la estabilidad del plásmido queda asegurada automáticamente; pero en la mayoría de los casos, debe incluirse en el plásmido otro gen que ejerza una presión selectiva a favor de las células que contengan el plásmido.

Recuperación del producto .
Nuestras preocupaciones no terminan incluso cuando se ha conseguido la expresión de un gen en un plásmido estable.
La facilidad para extraer un producto puede ser un factor primordial para determinar si determinado proceso puede ser rentable.
Si el producto permanece en el interior de la bacteria, éstas han de recogerse, lisarse y separar el producto del resto.
Por otra parte, si se puede hacer que las bacterias excreten el producto, su obtención es mucho más sencilla, con la ventaja de que las células no se destruyen durante el proceso.

Las células eucariotas excretan las proteínas con una secuencia señal adecuada en su N-terminal, eliminando esta secuencia señal cuando la proteína sale de la célula, pero las procariotas generalmente tienen menor capacidad para excretar proteínas.
Sin embargo, se sabe que Bacillus subtilis excreta más de 50 proteínas, y por lo tanto, cuando la excreción tiene importancia, a menudo se utiliza este microorganismo en lugar de E. coli .
Los problemas de recuperación de enzimas de los cultivos celulares se discuten con detalle en el Capitulo 13.

Seguridad

Otro factor que debe considerarse detenidamente es la seguridad de cualquier sistema manipulado genéticamente.
Cuando se empezó a clonar, a mediados de los 70, parecía existir una posibilidad real de que los genes patógenos (tales como los que codifican la formación de tumores o de una toxina) podrían transferirse inadvertidamente a bacterias que sobrevivirían fuera del laboratorio y que podrían difundirse entre la población.
Los Institutos Nacionales de la Salud de los Estados Unidos y el Grupo Asesor sobre Manipulación Genética del Reino Unido elaboraron unas normas estrictas prohibiendo la clonación de DNA vírico o de DNA de tumores y especificando altos niveles de contención física para la mayoría de los demás experimentos de clonación.
Afortunadamente, se desarrollaron células hospedadoras seguras, tales como la 1.776, que tenía unos requerimientos nutritivos tan específicos, la pared celular frágil, alta sensibilidad a las sales biliares, etc., que era poco probable que sobrevivieran fuera del laboratorio.
Al mismo tiempo, se construyeron plásmidos que no poseían los genes tra necesarios para transferir el plásmido de una célula a otra por conjugación, reduciéndose así el riesgo de transferencia de un plásmido peligroso de un hospedador seguro a otro infectivo. 

Para que un gen clonado llegue a ser perjudicial, debe suceder todo lo siguiente:
El gen debe ser transferido a un hospedador infectivo, el nuevo hospedador debe evadirse del confinamiento y originar la infección, el gen clonado debe expresarse en el hospedador y el producto debe ser capaz de desplazarse de la bacteria a la persona infectada, donde podría llegar a ser patógeno si alcanza una localización determinada.
Dado que la probabilidad global de tal incidente es el producto de las probabilidades de cada paso, el riesgo comienza a parecer claramente insignificante, dado que se utilizan hospedadores y plásmidos deficientes, con niveles razonablemente estrictos de contención física.
En vista de todo esto, las normas se han ido relajando progresivamente y las únicas restricciones serias que se mantienen están en relación con la eliminación al medio ambiente de organismos manipulados genéticamente, lo cual seria necesario para la degradación microbiológica de contaminantes o para la extracción de petróleo potenciada por microorganismos.
Por supuesto, el trabajo con patógenos debe realizarse todavía en condiciones de contención estrictamente reguladas. 

Economía

No existe ninguna garantía de que un producto vaya a ser rentable por el simple hecho de que esté elaborado por organismos manipulados genéticamente.
Si el mercado es reducido no hay ninguna posibilidad de obtener beneficios, a no ser que todos los clientes sean jeques del petróleo. 
Este tipo de argumento puede aplicarse a la hormona del crecimiento, ya que hay relativamente pocos casos de enanismo en el mundo y por lo tanto los posibles ingresos son insuficientes para sufragar los gastos de desarrollo y de producción. 
Sin embargo, si se descubre que la hormona también es eficaz para el tratamiento de quemaduras y para favorecer la cicatrización, entonces el mercado será amplio y será muy rentable producir la hormona, en particular porque en tal caso la demanda superará con mucho las cantidades que pueden obtenerse a partir de cadáveres.
Este simple ejemplo puede aclarar que gran parte de la inversión en ingeniería genética ha de ser especulativa, excepto cuando existe una clara demanda de un producto sin que haya ninguna fuente alternativa satisfactoria.
Incluso en tales casos puede haber enormes riesgos, entre los que se encuentra el problema de una adecuada protección de patente para cualquier DNA recombinante nuevo.
A pesar de poder patentarse un organismo o un plásmido, no está claro aún qué protección se ofrece frente a la obtención de células o plásmidos que realicen la misma función de forma ligeramente distinta, por ejemplo, se pueden modificar algunas de las bases de un gen sin que cambie la proteína que codifica.

Necesidad de investigación básica 

Uno de los principales obstáculos en el camino de la ingeniería genética no tiene nada que ver con la manipulación genética; consiste en el hecho de que desgraciadamente todavía desconocemos muchos de los principios básicos que subyacen bajo las características que nos gustaría modificar.
Hasta que no tengamos un sólido conocimiento de la bioquímica que determina la resistencia a la enfermedad, la velocidad de crecimiento, la morfología, el rendimiento fotosintético, etc., será difícil manipular tales características por ingeniería genética.

El futuro

En este libro sólo se ha tratado la manipulación genética en relación con su aplicación directa a la biotecnología.
Sin embargo, no debe olvidarse que una de las principales aplicaciones de estas técnicas es para investigar la estructura y la función de los genes. 
Estos estudios básicos tendrán inevitablemente gran influencia en la biotecnología, tanto en lo que respecta a qué se clona, como a la forma en que se hace.
Podemos esperar que se sepa mucho más acerca de los factores que intervienen en la regulación de la expresión genética, y deberíamos ser capaces de utilizar esta información para optimizar la expresión de genes extraños.
El completo conocimiento de los mecanismos de excreción y de las modificaciones posteriores a la traducción contribuirán a la eficaz producción de polipéptidos totalmente activos y estables.

¿Qué avances podemos esperar para los próximos 10 años?
La vida seria muy aburrida sin sorpresas, y no hay razones para pensar que la ingeniería genética no vaya a proporcionarnos algunas.
No obstante, en base al trabajo que se está realizando actualmente, se puede prever en qué áreas generales es posible obtener resultados.

Farmacología

Mediante la clonación se obtendrá material suficiente para que puedan llevarse a cabo ensayos clínicos completos de aquellos polipéptidos, tales como interferones, encefalinas, cininas, etc., que en el organismo se producen de forma natural a niveles bajos y que probablemente actúen como «modificadores de la respuesta biológica». 
Si se pudiesen utilizar estos compuestos como fármacos «naturales», podría abrirse un área completamente nueva de la farmacología.

Parece probable que aumente el número de vacunas elaboradas mediante ingeniería genética, entre las que se incluirían varias para uso humano; hay esperanzas de obtener vacunas contra la malaria y el SIDA .

Enzimas industriales

A medida que aumenta la importancia del empleo industrial de enzimas (véase Capítulo 14) es de esperar que éstas se obtengan mediante microorganismos manipulados genéticamente.
Probablemente se modificará cada gen mediante mutagénesis in vitro para obtener un enzima en el que se haya mejorado la estabilidad, la cinética, y, quizás, que se pueda inmovilizar para un uso más eficaz a nivel industrial. 
Es probable que esta «ingeniería de proteínas» sea de una importancia inmensa para el desarrollo de la tecnología de enzimas y depende únicamente de las técnicas de ingeniería genética.
Un proyecto que ya está logrando progresos importantes es la producción de ligninasas, que puede poner a nuestra disposición una fuente de alimentos inmensa y nueva, basado en la degradación de residuos lignarios.
Para una discusión más profunda de las posibles aplicaciones de enzimas elaboradas por ingeniería genética, véase el Capitulo 15. 

Mejora

A nivel de organismos completos, podemos esperar mejoras significativas en la calidad de los cultivos al necesitarse menos fertilizantes, plaguicidas y al aumentar la resistencia a enfermedades, el rendimiento y la calidad de las proteínas vegetales. 
También es posible que haya avances similares en la calidad del ganado.
Ya se ha descrito la síntesis bacteriana de nuevos compuestos, al crearse nuevas rutas metabólicas. 
Por ejemplo, con Streptomyces se han sintetizado nuevos antibióticos híbridos llevando a un solo organismo los genes biosintéticos de cepas que producen antibióticos diferentes (Hopwood y col., 1985).

Alternativas

Se ha asegurado (Vana y Cuatrecasas, 1984) que la elaboración de péptidos por ingeniería genética sólo será de importancia temporal, ya que los avances en el conocimiento de las interacciones receptor-efector pueden permitirnos diseñar moléculas pequeñas que sean tan eficaces como polipéptidos grandes, que incluso pueden obtenerse por síntesis química.
Es cierto que se han encontrado péptidos cortos, entre 8 y 20 restos de longitud, que son más eficaces que las proteínas víricas completas para estimular la producción de anticuerpos para el virus de la glosopeda o el de la polio, y muchos polipéptidos con efectos biológicos son muy cortos (por ejemplo, calcitonina 32 restos, endorfinas 31, encefalinas 5).

La síntesis química ofrece la posibilidad de modificar algunos residuos, por ejemplo para utilizar D-aminoácidos, de forma que los polipéptidos no se degraden en el tracto digestivo, permitiendo así la administración oral de fármacos que de otra forma han de administrarse vía intravenosa.
La elección entre síntesis biológica o química se verá influenciada por factores tales como los avances tecnológicos en la síntesis de péptidos a gran escala, el coste de la materia prima, el grado de pureza necesario, etc., y no es posible predecir cuánto tiempo ha de transcurrir para que los fármacos se puedan diseñar racionalmente para que se ajusten a los receptores con que se unen.

Es probable que la ingeniería genética se utilice durante muchos años para elaborar péptidos y que en un futuro inmediato sea el único método viable para producir enzimas.
La ingeniería genética ya se ha convertido en una herramienta para la mejora, y su importancia en este campo aumentará sin lugar a dudas.

Resumen

El estudio de los avances incluye la biosíntesis de insulina, vacunas, factores de coagulación de la sangre y factores antivíricos y cancerígenos.

Los principales problemas que quedan por resolver son: 
la expresión eficaz y controlada de los genes clonados, la estabilidad de los plásmidos y recuperación del producto.

La nueva tecnología presenta algunos riesgos potenciales. 
Como consecuencia, se han dictado ciertas normas y se han desarrollado vectores y células hospedadoras «seguras». 

La rentabilidad determinará inevitablemente el futuro de la ingeniería genética, y debe haber suficiente demanda de un producto para justificar la gran inversión necesaria para llevarlo al mercado.
Todavía no está claro con qué eficacia pueden proteger las patentes a los productos obtenidos por ingeniería genética. 

Un breve estudio de los posibles productos de la ingeniería genética incluye los «modificadores de la respuesta biológica», las vacunas, los enzimas para uso industrial, las plantas y los animales mejorados y nuevos compuestos elaborados por bacterias.
No obstante, se ha sugerido que la síntesis química de oligonucleótidos puede desafiar su elaboración mediante la ingeniería genética.


INGENIERÍA DE TEJIDOS:URDIMBRE POLIMÉRICA 

Los obstáculos que se oponen a la fabricación de nuevos órganos a partir de células y polímeros sintéticos, aunque notables, pueden superarse.

Tal como vienen repitiendo los demás artículos de este informe especial, la ingeniería de tejidos se ha convertido en un nuevo campo de la medicina.
Hace tan sólo unos años, la mayoría de los expertos opinaba que los tejidos humanos sólo podrían reemplazarse por trasplantes procedentes de donantes o por piezas artificiales hechas de plástico, metal y circuitos integrados.
Muchos pensaban que nunca se construirían órganos bioartificiales (híbridos de células vivas con polímeros naturales o artificiales) y que el empleo de órganos de animales era la única solución ante la escasez de donaciones.

Sin embargo, investigaciones innovadoras e imaginativas en laboratorios de todo el mundo están demostrando la viabilidad de órganos biohíbridos.
Las compañías dedicadas a la ingeniería de tejidos han puesto en movimiento cifras milmillonarias, inversión que crece cada año. 
Pero el sector debe superar obstáculos importantes antes de que esta inversión empiece a reportar beneficios médicos ostensibles y logre aliviar el sufrimiento causado por los daños de tejidos muy diversos.

El objetivo primordial de los implicados estriba en obtener una fuente de células fiable.
Las células animales, una opción, ofrecen el inconveniente del rechazo inmunitario y las dudas sobre su seguridad.
La prudencia aconseja, pues, acudir a células humanas.

La reciente identificación de células madre en embriones humanos -células capaces de dar origen a distintos tejidos- aporta una posible solución al problema. 
Pero se está muy lejos de poder manipular cultivos de células madre embrionarias, para producir células totalmente diferenciadas que pudieran emplearse en la reparación o creación de órganos específicos.

Una meta más inmediata sería el aislamiento de células progenitoras a partir de tejidos.
Estas ya han avanzado algunos pasos hacia la especialización, pero su escaso grado de diferenciación posibilita todavía su transformación en tipos celulares diversos.
Por poner un ejemplo, el equipo encabezado por Arnold I. Caplan ha aislado a partir de médula ósea humana ciertas células progenitoras que, en condiciones de laboratorio, pueden ser forzadas a convertirse en osteoblastos, que dan lugar a los huesos, o en condrocitos, que componen el cartílago. 
De forma similar, Lola Reid ha identificado células progenitoras ovaladas y pequeñas en hígados humanos adultos; podemos modificar tales células en cultivo y formar hepatocitos maduros -células que producen bilis y destruyen toxinas- u obtener células epiteliales, que tapizan los conductos biliares.

Otro planteamiento consistiría en generar líneas celulares del tipo "donante universal".
Para obtener estas células habría que eliminar, o enmascarar con otras moléculas, las proteínas de superficie que delatan como extrañas las células procedentes de donantes. 
Los laboratorios Diacrin emplean esta estrategia para producir células de cerdo aptas para trasplantes en humanos. 
Diacrin también planea adoptar la técnica de "enmascaramiento" en los trasplantes con donantes humanos incompatibles.
En Estados Unidos se han autorizado los ensayos con células hepáticas humanas "enmascaradas" en ciertos trastornos del hígado.

En principio, no es de esperar que el receptor rechace las células donantes universales; se podrían generar varios tipos celulares a partir de tejidos diferentes y mantenerlas creciendo en cultivo hasta que fueran necesarias.
Pero no acabamos de barruntar qué respuesta darán las células donantes universales en ensayos clínicos a gran escala.

La búsqueda de métodos para producir células y tejidos no ha sido nada fácil.
Sólo se ha identificado una pequeña parte de las señales bioquímicas que dictan la diferenciación de las células madre embrionarias y de las células progenitoras en tipos celulares especializados.
Todavía no se pueden aislar cultivos de células madre, ni de células progenitoras a partir de médula ósea, sin tenerlas que mezclar con células de tejido conectivo, como los fibroblastos. (Deben evitarse los fibroblastos porque se dividen rápidamente y provocan un crecimiento excesivo de los cultivos de células madre.) Hay que desarrollar también procedimientos más avanzados para cultivar grandes cantidades de células en los biorreactores, cámaras de crecimiento equipadas con agitadores y sensores que regulan la cuantía adecuada de nutrientes, gases (oxígeno y dióxido de carbono, por ejemplo) y productos de desecho.
Los métodos actuales producen a menudo muy pocas células o películas de tejido más delgadas de lo deseable.

Pero están apareciendo nuevas soluciones.
Durante muchos años, los investigadores se esforzaron en hacer crecer segmentos de cartílago de espesor suficiente para aplicaciones médicas; por ejemplo, en la substitución de cartílago desgastado de la rodilla.
No obstante, cuando el cartílago crecía más allá de cierto grosor, los condrocitos del centro quedaban demasiado lejos del medio de crecimiento y no podían obtener nutrientes y gases, ni responder a las señales físicas y químicas que regulan el crecimiento o eliminar los desechos.
Gordana Vunjak-Novakovic y Lisa Freed resolvieron el problema cultivando los condrocitos sobre una urdimbre tridimensional de polímeros en el interior de un biorreactor.
La trama relativamente suelta y la agitación del biorreactor aseguraban que todas las células se unieran de modo uniforme con la urdimbre polimérica y se bañaran en el medio de cultivo. 

La potenciación de las propiedades mecánicas de los tejidos durante su crecimiento en los biorreactores será crucial, pues muchos tejidos se remodelan, o cambian su organización global, en respuesta a tensiones, estiramientos o compresiones. 
Así, un cartílago bioartificial aumenta de tamaño y contiene más cantidad de colágeno y de las demás proteínas que forman la matriz extracelular si se cultiva en recipientes rotatorios, que exponen el tejido en desarrollo a variaciones en las fuerzas del fluido. (La matriz extracelular es una suerte de red de telaraña que sirve de soporte para el crecimiento y organización de las células en tejidos.) El cartílago cultivado contiene proteínas de la matriz extracelular que lo hacen más consistente y duradero, capaz de una mejor respuesta fisiológica ante fuerzas externas.

Por una vía similar, John A. Frangos ha demostrado que los osteoblastos cultivados en perlas de colágeno y agitados en un biorreactor producen más minerales óseos que los que crecen en una placa estacionaria.
Laura E. Niklason ha comprobado que las arteriolas obtenidas por ingeniería a partir de células endoteliales (que tapizan la pared de los vasos) y células de la musculatura lisa en forma de tubo desarrollan propiedades mecánicas propias de vasos sanguíneos naturales si el medio es recorrido por pulsos que imitan la presión sanguínea generada por los latidos del corazón.
Otros grupos de investigación -incluido el nuestro- elaboran métodos para desarrollar músculo esquelético y cardiaco, tejidos que adquieren mayor vigor conforme se van sometiendo a tensiones.

Aprender a regular el comportamiento celular representa otro reto importante.
Los sistemas vivos revisten una increíble complejidad.
Nuestro hígado humano, por ejemplo, contiene seis tipos diferentes de células, que se organizan en unas formaciones microscópicas denominadas lóbulos.
Cada célula puede realizar cientos de reacciones bioquímicas diferentes.
Agréguese a ello que la actividad bioquímica de cada célula depende a menudo de su interacción con otras células y con la matriz extracelular que atraviesa cada tejido.
David J. Mooney ha demostrado que los hepatocitos producen diferentes niveles de determinada proteína según la adherencia del material sobre el que medran.
Antes de desarrollar órganos como hígados bioartificiales injertables -uno de los principales objetivos de la ingeniería de tejidos- los expertos deberán comprender mejor cómo hacer cultivar hepatocitos y otras células del hígado en condiciones que maximicen su capacidad de desempeñar funciones fisiológicas normales.

Comprender los mecanismos de remodelación será esencial para dar forma a los órganos y tejidos bioartificiales que se integran en el receptor.
En las pruebas de laboratorio con productos bioartificiales que han cosechado los mejores resultados, el trasplante estimula el crecimiento de las células y de los tejidos del receptor, que han terminado por substituir a los polímeros artificiales y a las células trasplantadas del injerto.
En colaboración con Toshiharu Shinoka y John E. Mayer, hemos demostrado que una laminilla de válvula cardiaca fabricada a partir de polímeros artificiales, células epiteliales y miofibroblastos de cordero (un tipo de células que promueve la cicatrización) se hizo más fuerte, más elástica y más delgada tras ser trasplantada a una oveja.
Transcurridas 11 semanas, la laminilla no era ya de polímeros artificiales, sino que, remodelada, contenía sólo matriz extracelular de oveja.
Con todo, es muy pobre nuestro conocimiento de las señales bioquímicas y los factores de crecimiento que dictan los procesos de remodelado.

La creación de nuevos materiales que sean a la vez biodegradables y no induzcan la formación de tejido cicatrizante es un campo muy vivo de la ingeniería de tejidos.
La mayoría de los materiales que se emplean hoy en la urdimbre son sintéticos, como el material biodegradable de las suturas, o naturales, como el colágeno o el alginato (substancia gelatinosa que se extrae de las algas).
La ventaja de los materiales sintéticos reside en que se puede controlar su resistencia, su velocidad de degradación y su microestructura durante su producción; pero las células se adhieren con mayor facilidad a los materiales naturales.

Se busca ahora combinar lo mejor de ambas estrategias para diseñar nuevas generaciones de materiales con propiedades específicas.
En esa línea, se preparan polímeros biodegradables con regiones biológicamente activas que remedan la matriz extracelular natural de un tejido.
Uno de estos polímeros contiene la secuencia RGD de la fibronectina, una proteína de la matriz extracelular.
Las siglas RGD remiten a los símbolos de los aminoácidos que la componen: arginina (R), glicina (G) y ácido aspártico (D).
Es usual que muchos tipos de células se engarcen en la fibronectina a través de RGD.
Por tanto, los polímeros que contienen esta sustancia podrían proporcionar un entorno más natural para las células en crecimiento.

También hay quien se propone producir polímeros conductores de la electricidad -que se podrían utilizar para hacer crecer nervios mediante ingeniería de tejidos- o polímeros que se aglutinen rápidamente.
Estos polímeros de decantación pronta pueden servir en sustancias bioartificiales inyectables, como las que se podrían emplear en el relleno de un hueso roto.

El proceso de angiogénesis, o inducción del crecimiento de los vasos sanguíneos, será crucial para el sostén de muchos órganos obtenidos por ingeniería de tejidos; en particular, páncreas, hígados y riñones, que requieren un gran suministro sanguíneo.
Se ha logrado estimular la angiogénesis en tejidos bioartificiales revistiendo la urdimbre polimérica con factores de crecimiento inductores de la formación de vasos.
Habrá ahora que examinar cuáles son los mejores métodos para liberar los factores de crecimiento y controlar su actividad a fin de que los vasos se formen sólo en el momento y lugar donde se requieran.

Conviene desarrollar nuevos métodos de preservación de tejidos que aseguren su integridad en el transporte y durante el trasplante.
No estaría de más fijarse en las técnicas dominadas en los trasplantes de órganos. 
Los cirujanos han aprendido que las lesiones de un órgano trasplantado se presentan, sobre todo, durante la reperfusión, momento en que el órgano se conecta a un suministro sanguíneo en el receptor.
La reperfusión induce la formación de radicales libres de oxígeno, que horadan las membranas y matan las células.
Para evitar los daños de la reperfusión, los cirujanos suelen añadir a la solución de preservación sustancias químicas que absorben los radicales libres.
Así, será preciso encontrar moléculas más eficientes para proteger a los tejidos bioartificiales de las lesiones debidas a la reperfusión y de las isquémicas, que se producen cuando el riego sanguíneo es insuficiente.
También se deben perfeccionar las técnicas de crioconservación para que los órganos y los tejidos puedan mantenerse congelados hasta que sean necesarios;
los métodos que se emplean con las células deben desarrollarse aún más para que funcionen en tejidos de mayor tamaño.

Estamos seguros de que los científicos y las administraciones eliminarán todos los obstáculos descritos en este artículo y que en los próximos años se comercializarán diferentes productos obtenidos por ingeniería de tejidos. 
Aún quedan muchos retos por superar, pero algún día -quizá dentro de unos años- injertar a los pacientes tejidos y órganos obtenidos por ingeniería de tejidos podría resultar tan rutinario como la introducción hoy de un divertículo coronario ( bypass ).


BASES DE LA INMUNOLOGÍA 

Inmunidad innata 

Se habita un mundo potencialmente hostil pleno de una asombrosa variedad de agentes infecciosos (fig. 1-1) con diversas formas, tamaños, composición y carácter subversivo, listos para utilizar al hombre como fértil santuario para propagar sus "genes egoístas" si no se hubiese desarrollado una serie de mecanismos de defensa igualmente eficaces y geniales (excepto en el caso de muchas parasitosis, en las cuales la situación puede describirse más como una tregua difícil y a menudo insatisfactoria).
Son estos mecanismos de defensa los que pueden instaurar un estado de inmunidad ( immunitas , libre o exento de) contra la infección y cuyo estudio es el objetivo de la disciplina denominada "Inmunología".

Además de factores constitucionales mal conocidos por los cuales una especie presenta susceptibilidad y otra resistencia congénitas a ciertas infecciones, se ha identificado una serie de sistemas antimicrobianos inespecíficos (p. ej.: fagocitosis) que son innatos en el sentido de que no son afectados de manera intrínseca por el contacto previo con el agente infeccioso.
Estos sistemas serán comentados y se examinará cómo, en el estado de inmunidad adquirida específica , su eficacia puede estar muy aumentada.

Barreras contra la infección 

La forma más sencilla de evitar la infección es impedir que los agentes infecciosos se introduzcan en el organismo (fig. 1-2).
La principal línea de defensa es la piel, que cuando se encuentra indemne es impermeable a la mayoría de los microorganismos; al producirse soluciones de la continuidad cutánea, como por ejemplo en las quemaduras, la infección se transforma en un problema.
La mayor parte de las bacterias no sobrevive por mucho tiempo sobre la piel debido a la acción inhibitoria directa del ácido láctico y los ácidos grasos de las secreciones sudorípara y sebácea y el pH ácido que generan.
Una excepción es el Staphylococcus aureus , el cual suele infectar los folículos pilosos y las glándulas, que son relativamente vulnerables.

Figura 1-1

La importante cantidad de agentes infecciosos con que se enfrenta el sistema inmunológico.

Aunque por lo general no se clasifican de esta manera por su carencia de pared celular, se incluyen por conveniencia entre las bacterias a los micoplasmas.
Los hongos adoptan muchas formas y se especifican valores aproximados para algunas de las morfologías más pequeñas.

El moco secretado por las membranas mucosas que revisten las superficies internas del organismo actúa como barrera protectora que bloquea la adherencia de las bacterias a las células epiteliales.
Los microbios y otras partículas extrañas atrapadas en el moco son eliminadas mediante mecanismos como el movimiento ciliar, la tos y los estornudos.
Entre los otros factores mecánicos que contribuyen a proteger las superficies epiteliales debe incluirse también la acción higienizante de las lágrimas, la saliva y la orina.
Muchos de los líquidos corporales secretados contienen sustancias bactericidas, como el ácido en el jugo gástrico, la espermina y el cinc en el semen, la lactoperoxidasa en la leche y la lisozima en las lágrimas, las secreciones nasales y la saliva.

Un mecanismo totalmente diferente es el del antagonismo microbiano que se asocia con la flora bacteriana normal del organismo.
Esta última inhibe la proliferación de muchas bacterias y hongos potencialmente patógenos en sitios superficiales por competencia por los alimentos esenciales o porque producen sustancias inhibitorias.
Por ejemplo, la invasión de agentes patógenos se ve limitada por el ácido láctico producido por especies particulares de bacterias comensales que metabolizan el glucógeno secretado por el epitelio de la mucosa vaginal.
Cuando los comensales protectores se alteran por la acción de antibióticos aumenta la susceptibilidad a infecciones por oportunistas como Candida y Clostridium difficile .
Los microorganismos comensales del intestino también pueden producir colicinas, una clase de bactericidinas que se fijan a la superficie con carga negativa de las bacterias susceptibles e insertan un apéndice helicoidal hidrofóbico en la membrana; luego la molécula sufre una transformación para tornarse hidrofóbica y formar en la membrana un canal dependiente del voltaje que destruye a la célula por supresión del potencial energético.
Incluso a este nivel, la supervivencia es difícil.

Ante la introducción de los microorganismos intervienen dos mecanismos de defensa principales, a saber: factores químicos solubles de acción destructora, como son las enzimas bactericidas, y la fagocitosis, en la cual una célula literalmente "come" los agentes nocivos.

Las células fagocíticas destruyen los microorganismos 

Fagocitos "profesionales" 

La fagocitosis y digestión de los microorganismos es realizada por dos tipos celulares principales identificados por Metchnikoff a fines del siglo pasado como micrófagos y macrófagos .

Neutrófilo polimorfonuclear 

Esta célula, la más pequeña de las dos, comparte un precursor hematopoyético común con los otros elementos formes de la sangre y es el leucocito predominante en el torrente sanguíneo.
Se trata de una célula indivisible de vida breve con núcleo polilobulado y un conjunto de gránulos que prácticamente no se tiñen con las técnicas histológicas comunes como hematoxilina-eosina, a diferencia de lo que ocurre con los gránulos de los leucocitos eosinófilo y basófilo) (figs. 1-3 y 1-5).
Los gránulos del neutrófilo son de tres tipos: los gránulos primarios o azurófilos que contienen mieloperoxidasa, cierta cantidad de lisozima y un tipo de proteínas catiónicas; los gránulos secundarios o "específicos" que poseen lactoferrina, lisozima y una proteína fijadora de vitamina B12; y gránulos terciarios emparentados con los lisosomas convencionales que contienen hidrolasas ácidas.
Los abundantes depósitos de glucógeno pueden ser utilizados para la glucólisis, lo que permite a estas células funcionar en condiciones anaeróbicas.

El macrófago 

Estas células derivan de promonocitos de la médula ósea que, luego de su diferenciación a monocitos sanguíneos, finalmente se establecen en los tejidos como macrófagos maduros para constituir el sistema fagocítico mononuclear (fig. 1-4).

Se encuentran en todo el tejido conectivo, rodean la membrana basal de los vasos sanguíneos de pequeño calibre y su concentración es particularmente grande en los pulmones (macrófagos alveolares), el hígado (células de Kupffer) y en el bazo y los ganglios linfáticos, donde se hallan estratégicamente ubicados tapizando los sinusoides para filtrar el material extraño.
Otros ejemplos son las células mesangiales en el glomérulo renal, la microglia del sistema nervioso y los osteoclastos del tejido óseo.
A diferencia de lo que sucede con los polimorfonucleares, los macrófagos son células de vida prolongada con un importante retículo endoplasmático rugoso y mitocondrias (fig. 1-5) y mientras que los neutrófilos proporcionan un mecanismo de defensa de gran relevancia contra las bacterias piógenas (generadoras de pus), puede decirse en general que los macrófagos se encuentran mejor adaptados para combatir aquellas bacterias (fig. 1-3g), virus y protozoarios capaces de vivir dentro de las células del huésped.

Figura 1-2

Las primeras líneas de la defensa contra la infección: protección en las superficies externas del organismo.

Figura 1-3

Células involucradas en la inmunidad innata.

a) Monocito, con su núcleo en forma de herradura y un citoplasma pálido moderadamente abundante.

Nótense los tres neutrófilos polimorfonucleares multilobulados y el pequeño linfocito (abajo a la izquierda).
Coloración de Romanowsky.
b) Dos monocitos coloreados para esterasa no específica con xxx naftil acetato.
Nótese el citoplasma vacuolado.
La célula pequeña con coloración focal en la parte superior es un linfocito T.
c) Cuatro neutrófilos polimorfonucleares y un eosinófilo.
Se observan con claridad los núcleos multilobulados y los gránulos citoplasmáticos, muy coloreados en el caso del eosinófilo.
d) Neutrófilo polimorfonuclear con gránulos citoplasmáticos coloreados para fosfatasa alcalina.
e) Neutrófilos en fase temprana en la médula ósea.
Los gránulos azurófilos primarios (CP) en principio acumulados cerca del núcleo, se trasladan hacia la periferia donde se generan en el aparato de Golgi los gránulos específicos del neutrófilo a medida que la célula madura.
El núcleo se hace lobular en forma gradual (NL).
Giemsa.
f) Células inflamatorias provenientes del sitio de una hemorragia cerebral se observa un gran macrófago activo en el centro con eritrocitos fagocitados y vacuolas prominentes.
Hacia la derecha se observa un monocito con núcleo en forma de herradura y cristales citoplasmáticos de bilirrubina (hematoidina).

Están claramente delineados varios neutrófilos multilobulados.
Giemsag) Macrófagos en cultivo de monocapa luego de la fagocitosis de microbacterias (coloreados de rojo).
Carbol-fucsina con verde de malaquita como colorante de contraste.
h) Numerosos macrófagos alveolares de gran tamaño ubicados dentro de los espacios aéreos del pulmón.
i) Basófilos con gránulos fuertemente coloreados comparados con un neutrófilo (parte inferior).
j) Mastocito proveniente de la médula ósea.
Presenta un núcleo central redondo rodeado por gránulos fuertemente coloreados.
Se observan en la parte interior dos pequeños precursores de eritrocitos.

Coloración de Romanowsky.
k) Mastocitos tisulares de la piel coloreados con azul de toluidina.
Los gránulos intracelulares son metacromáticos y se colorean de púrpura rojizo.
Nótense los acúmulos relacionados con los capilares de la dermis.

Fagocitosis 

Antes de que pueda producirse la fagocitosis el microbio debe adherirse a la superficie del polimorfonuclear o del macrófago, un fenómeno mediado por un mecanismo de reconocimiento bastante primitivo en el que es probable que participen carbohidratos.
Según su naturaleza, una partícula fijada a la membrana del fagocito puede iniciar la fase de ingestión al activar un sistema actomiosínico contráctil que se encarga de emitir seudópodos alrededor de la partícula en cuestión (figs. 1-6 y 17); a medida que receptores adyacentes se fijan en forme secuencial a la superficie del microbio (como si fuera un "cierre de cremallera"), la membrana plasmática del fagocito es fraccionada hasta rodear la partícula y encerrarla por completo en una vacuola (fagosoma; figs. 1-6 y 18).
Los fenómenos siguientes se suceden rápidamente en menos de un minuto los gránulos citoplasmáticos se fusionan con el fagosoma para liberar su contenido alrededor del microorganismo aprisionado (fig. 1-8) que es sometido a toda una batería de sustancias microbicidas.

Destrucción del microbio 

Mecanismos oxígeno- dependientes 

El invasor ve dificultada su acción en el momento en que se inicia la fagocitosis.
Se produce un aumento extraordinario de la actividad de la vía de las hexosas monofosfato que genera NADPH, el cual se utiliza en última instancia para reducir el oxígeno molecular unido a un citocromo particular de la membrana plasmática xxx , lo que provoca un gran aumento del consumo de oxígeno.

Como consecuencia, el oxígeno se convierte en anión superóxido, peróxido de hidrógeno, O2 libre y radicales hidroxilo, todos ellos poderosos agentes microbicidas.
Además, la combinación de peróxido, mieloperoxidasa y iones haluro constituye un potente sistema halogenante capaz de destruir tanto bacterias como virus (Cuadro I - I ).

Mecanismos oxígeno- independientes 

También se extrae del cuadro l-l que la dismutación del superóxido consume iones hidrógeno y eleva levemente el pH de manera que se permite la función óptima de las proteínas catiónicas.
Aunque estas proteínas aún no están bien caracterizadas, se sabe que lesionan la membrana bacteriana tanto por acción de proteinasas neutras (catepsina G) como por la transferencia directa a la superficie del microbio de una proteína que aumenta la permeabilidad bacteriana, de proteínas catiónicas de alto peso molecular y de las llamadas defensinas.
El pH bajo, la lisozima y la lactoferrina son factores bactericidas o bacteriostáticos no dependientes del oxígeno y, en consecuencia, pueden actuar en condiciones anaeróbicas.

Por último, los microorganismos muertos son digeridos por enzimas hidrolíticas y los productos de la degradación son liberados al exterior (fig. 1-7).

En este momento puede considerarse muy segura la notable potencialidad antimicrobiana de las células fagocíticas.
Pero aún existen obstáculos que deben tenerse en cuenta; el importante armamento con que cuenta el organismo resulta inútil a menos que el fagocito pueda 1) ser atraído hacia el microorganismo, 2) adherirse a Él y 3) responder mediante la activación de la membrana que inicia la fagocitosis.
Algunas bacterias producen sustancias químicas como el péptido formil-met-leu-phe que atraen a los leucocitos, un proceso conocido como quimiotaxis; algunos microorganismos se adhieren a la superficie del fagocito y otros proporcionan de manera espontánea la señal de membrana de iniciación apropiada.
No obstante, los prolíficos microorganismos mutan constantemente para producir nuevas especies que puedan vencer las defensas al carecer de estas propiedades.
Ante las dudas sobre el camino a seguir, el organismo humano ha resuelto estos problemas a través de millones de años de evolución, mediante el desarrollo del sistema del complemento.

El complemento facilita la fagocitosis 

El complemento y su activación 

Complemento es el nombre dado a un complejo grupo de unas 20 proteínas que, junto con los mecanismos de la coagulación, la fibrinolisis y la formación de quininas, constituye uno de los sistemas enzimáticos en cascada del plasma.
Estos sistemas producen una respuesta rápida y muy amplificada a un estímulo desencadenante mediada por un fenómeno en cascada, donde el producto de una reacción es el catalizador enzimático de la siguiente.

Algunos de los componentes del complemento se designan con la letra "C" seguida por un número que se relaciona más con la cronología de su descubrimiento que con su posición en la secuencia de la reacción.
El componente más abundante y de mayor importancia es C3 con un peso molecular de 195 kDa que se encuentra en el plasma en una concentración de aproximadamente 1,2 mg/ml .

Figura 1-4

El sistema de fagocitos mononucleares (incluido previamente Junto a las células endoteliales y los polimorfonucleares bajo el término 'sistema reticuloendotelial" o SRE). 

Los precursores promonocitos de la médula ósea se desarrollan a monocitos de la sangre circulante. que eventualmente se distribuyen en todo el organismo como macrófagos maduros ( xxx ), como se muestra.
La otra célula fagocítica importante, el neutrófilo polimorfonuclear, está confinado en su mayor parte al torrente sanguíneo, excepto cuando es reclutado hacia los sitios de inflamación aguda.

Figura 1-5

Ultraestructura de las células fagocíticas.

a) Neutrófilo.
Se observan con nitidez el núcleo multilobulado y los gránulos citoplasmáticos.
(cortesía del Dr. D McLaren)
b) Monocito (x 10000).
Núcleo en forma de herradura.
Son evidentes las vesículas fagocíticas y pinocíticas.
Los gránulos lisosomales, las mitocondrias y los perfiles aislados de retículo endoplasmático rugoso.

Mecanismo de desdoblamiento del C3 

En circunstancias normales, un enlace tioléster interno en el C3 se activa muy lentamente por reacción con agua o con trazas de una enzima proteolítica plasmática para formar un compuesto intermediario reactivo (el producto de desdoblamiento xxx o una molécula funcionalmente similar llamada xxx o xxx .
En presencia de xxx , la molécula en cuestión puede formar un complejo con otro componente del complemento, el factor B, que, luego es desdoblado por una enzima plasmática normal (el factor D) para producir xxx .
Obsérvese que por convención Un segmento de línea sobre un complejo denota actividad enzimática y que al desdoblarse un componente del complemento el producto mayor recibe en general el sufijo 'b' y el de menor tamaño adquiere el sufijo 'a'.

Figura 1-6

Fagocitosis y muerte de una bacteria. 

Etapas 3/4, aumento respiratorio; etapa 5, lesión por acción de intermediarios oxígeno reactivos, etapas 6/7, lesión por la acción de peroxidasa, proteínas catiónicas, lisosima y lactoferrina.

El xxx tiene una nueva actividad enzimática importante: es una 'convertasa del xxx ' que puede desdoblar el xxx para generar xxx y xxx .
Se comentarán brevemente las importantes consecuencias biológicas del desdoblamiento del xxx en relación con las defensas frente a los microorganismos, pero en condiciones normales debe existir algún mecanismo que limite este proceso hasta un cierto nivel, puesto que también puede dar origen a más xxx , por lo que se trata de un circuito de retroalimentación (feedback) positivo potencialmente fuera de control (fig. 1-9).
Como sucede con todas las cascadas con estas características aquí también existen poderosos mecanismos reguladores.

El control de la concentración del xxx 

En solución, la convertasa xxx es inestable y el factor B es desplazado con facilidad por otro componente, el factor H, para formar un complejo susceptible de ser afectado por el inactivador del xxx conocido como factor I (fig. 1-10; ver además pág. 240).
El xxx inactivado ( xxx ) es biológicamente inactivo y sufre una degradación adicional por acción de las proteasas de los líquidos corporales.

Figura 1-7

Adherencia y fagocitosis. 

a) Fagocitosis de Candida albicans por un leucocito polimorfonuclear (neutrófilo).
La adherencia a la superficie inicia la inclusión de la partícula fúngica dentro de prolongaciones de citoplasma.
Son abundantes los gránulos lisosomales pero escasas las mitocondrias (x 15000).
b) Fagocitosis de C albicans por un monocito mostrando formación casi completa del fagosoma ( flecha ) alrededor de un microorganismo e ingestión total de otros dos (x 5000).
(Gentileza del Dr. H Valdimarsson).

Figura 1-8

Formación de un fagolisosoma.

a) Neutrófilo 30 minutos después de la ingestión de C albicans .
El citoplasma ya está parcialmente desgranulado y se están fusionando dos gránulos lisosomales ( flechas ) con la vacuola fagocítica.
Se evidencian dos lóbulos del núcleo (x 5000).
B) La imagen a) con mayor aumento, donde se observan gránulos en fusión descargando su contenido dentro de la vacuola fagocítica ( flechas ). 
(x 33000; gentileza del Dr. H Valdimarsson).

Activación del mecanismo de desdoblamiento del xxx 

Ciertos microorganismos pueden activar la convertasa xxx y generar grandes cantidades de productos del desdoblamiento del xxx al estabilizar la enzima sobre sus superficies (ricas en carbohidratos), protegiendo así el xxx del factor H.
Otra proteína, la properdina, actúa sobre esta convertasa fijada y la estabiliza aun más.
A medida que el xxx es desdoblado por la enzima fijada a la membrana bacteriana, modifica su conformación, por lo que el enlace tioléster interno potencialmente reactivo queda expuesto y se une en forma covalente a grupos hidroxilo o amino de la superficie celular microbiana (fig. 1-10).

Así, cada grupo catalítico favorece el depósito de una gran cantidad de moléculas de xxx sobre el microorganismo.
Esta serie de reacciones provocadas directamente por los microorganismos que llevan a la degradación del xxx , recibe el nombre de 'vía alterna' de la activación del complemento (fig. 1-11).

Tabla 1-1

Sistemas antimicrobianos en las vacuolas fagocíticas.

Especies microbicidas en negrita.
O2 -anión superóxido; xxx singulete de oxígeno (activado); OH, radical hidroxilo libre.

La vía posterior al xxx 

El reclutamiento de una molécula mes de xxx en el complejo enzimático xxx genera una convertasa del xxx que lo activa por una fragmentación proteolítica con la liberación de un polipéptido pequeño ( xxx ) y la permanencia de un fragmento grande ( xxx ) unido al xxx en forma laxa.
La fijación consecuente del xxx y del xxx al xxx forma un complejo con un sitio de unión a la membrana transitorio y afinidad por la cadena peptídica xxx del xxx .
La cadena o, del xxx se apoya en la membrana y dirige las modificaciones de la conformación del xxx que lo transforman en una molécula anfipática capaz de insertarse en la bicapa lipídica (colicinas, pág .

14) y polimerizarse formando un 'complejo de ataque a la membrana' anular (MAC; figs. 1-12 y 2-3).
Se forma así un canal que atraviesa la membrana, totalmente permeable al agua y los electrolitos por lo que, debido a la alta presión coloidosmótica intracelular, se produce un ingreso neto de xxx y agua a la célula bacteriana que con frecuencia ocasiona su lisis.

El complemento posee numerosas funciones biológicas 

Pueden ser agrupadas convenientemente bajo tres encabezados:

Reacciones de adherencia 

Las células fagocíticas poseen receptores para el xxx y el xxx que facilitan la adherencia de los microorganismos tapizados de xxx a la superficie celular (ver pág. 193).

Fragmentos con actividad biológica 

El xxx y el xxx , los pequeños péptidos separados de las moléculas progenitores durante la activación del complemento, poseen varias acciones importantes.
Los dos péptidos actúan directamente sobre los fagocitos, en especial los neutrófilos, para estimular el importante aumento del consumo de oxígeno asociado con la producción de metabolitos del oxígeno y para incrementar la presencia de los receptores de superficie que reaccionan con xxx e xxx .
Además, ambos son 'anafilotoxinas' por su capacidad de desencadenar la liberación de mediadores a partir de los mastocitos (figs. 1-3 y 1-13) y sus equivalentes de la circulación, los basófilos (fig. 1-3), un fenómeno muy importante en el tema tratado, por lo que se han incluido detalles de los mediadores y sus acciones en la fig .

1-14; obsérvense en particular las propiedades quimiotácticas de estos mediadores y sus efectos sobre los vasos sanguíneos.
El C5a es un poderoso agente quimiotáctico para los neutrófilos, capaz de actuar directamente sobre el endotelio capilar para producir vasodilatación y un aumento de la permeabilidad, un efecto que en apariencia se prolonga por acción del leucotrieno xxx liberado a partir de neutrófilos, macrófagos y mastocitos activados.

Lesiones de la membrana 

La inserción del complejo de ataque en la membrana de una célula puede producir la lisis celular, pero el complemento presenta una relativa ineficacia para lisar la membrana celular de las células del huésped (autólogas) debido a la presencia de proteínas de control.

Figura 1-11

Activación microbiana de la vía alternativa del complemento por estabilización de la enzima xxx , y su control por los factores H e I.

Aunque desde el punto de vista filogenético es la vía más antigua, fue descubierta con posterioridad a otra vía que se estudiará en el próximo capítulo por lo que se la denominó "alterativa", lo cual puede llevar a confusión.

Figura 1-12

Vía post xxx que genera xxx y el complejo de ataque de membrana xxx ( MAC ).

a) Esquema de montaje molecular.
El cambio de conformación en la estructura de la proteína xxx que la convierte de molécula hidrofílica en anfipática (con regiones hidrofóbicas e hidrofílicas) puede ser interrumpido por un anticuerpo formado contra péptidos lineales derivados de xxx ; dado que el anticuerpo no reacciona con las formas solubles o ligadas a membrana de la molécula, debe estar detectando una estructura intermedia revelada transitoriamente en una redistribución estructural profunda.
b) Micrografía electrónica de un complejo de membrana xxx incorporado a membranas liposomales que muestran claramente la estructura anular.
El complejo cilíndrico se visualiza desde el costado, insertado en la membrana del liposoma de la izquierda, y desde la base en el de la derecha.
Aunque en si es una interesante estructura, es probable que la formación del cilindro anular xxx no sea esencial para la perturbación citotóxica de la membrana de la célula blanco, puesto que esto puede obtenerse por inserción de moléculas anfipáticas de xxx en cantidades demasiado pequeñas para poder formar un MAC definido.
(Gentileza del Prof. J Tranum-Jensen y del Dr. S Bhakdit).

MOLÉCULAS QUE RECONOCEN AL ANTÍGENO 

Las inmunoglobinas 

La estructura básica es una unidad tetrapeptídica 

La molécula de anticuerpo está compuesta por dos cadenas pesadas idénticas y dos cadenas livianas idénticas, unidas por puentes disulfuro intercatenarios (fig. 3-1).
Estas cadenas pueden separarse por reducción de los enlaces disulfuro y acidificación.
En el tipo de anticuerpo más abundante, la inmunoglobulina G, la región bisagra expuesta extiende su estructura debido al elevado contenido de prolina, y por lo tanto es vulnerable al ataque proteolítico; la molécula es escindida por acción de la papaína para formar dos fragmentos idénticos, cada uno con un único sitio de combinación para el antígeno (Fab; fragmento de unión con el antígeno ), y un tercer fragmento que carece de la capacidad de ligar el antígeno, denominado Fc ( fragmento cristalizable ).
La pepsina actúa en un punto diferente y escinde el Fc del remanente de la molécula, por lo que queda un fragmento 5S grande denominado xxx , por ser divalente con respecto a la unión con el antígeno al igual que el anticuerpo original (fig. 3-2).

Se demostró la localización de los sitios de combinación con el antígeno por medio de un estudio de los anticuerpos purificados formados contra un grupo dinitrofenilo (DNP) mezclado con el compuesto: xxx .

Los dos grupos DNP se encuentran lo suficientemente separados entre sí como para no interferir en sus respectivas reacciones con los anticuerpos, por lo que pueden unir por sus extremos a los sitios de combinación con el antígeno de dos anticuerpos diferentes.
Cuando se visualizan por coloración negativa al microscopio electrónico, se observa una serie de formas geométricas que representan las diferentes estructuras que pueden esperarse si una molécula con bisagra en forma de Y, con sitio de combinación en el extremo de cada brazo de la Y, intenta formar complejos con el antígeno divalente.
Pueden distinguirse trímeros triangulares, tetrámeros cuadrados y pentámeros pentagonales (fig. 3-3).
En la figura 3-4 se indica cómo se obtienen estas formas poliméricas.

De la forma de los polímeros producidos utilizando el fragmento de pepsina xxx (fig. 3-3e) se infiere la posición del fragmento Fc, el cual no se encuentra involucrado en la combinación con el antígeno.

Figura 3.1

Modelo de anticuerpo con dos cadenas polipeptídicas pesadas y dos cadenas livianas unidas por medio de puentes disulfuro intercatenarios.

En el diagrama el residuo amino terminal (N) se encuentra a la izquierda de cada cadena.

Figura 3.2

Degradación de una inmunoglobulina a sus cadenas peptídicas constituyentes y los fragmentos proteolíticos que muestra la divalencia del xxx de la pepsina para la unión con el antígeno y la univalencia del xxx de la papaína.

Luego de la digestión con pepsina se forma el fragmento xxx que representa la mitad del C terminal de la región xxx y permanece unido por medio de enlaces no covalentes.
A la porción de la cadena pesada en el fragmento xxx se la representa con el símbolo xxx .

Las secuencias de aminoácidos revelan variaciones en la estructura de las inmunoglobulinas 

La población de anticuerpos en cualquier individuo dado es increíblemente heterogénea, por lo tanto la determinación de las secuencias de aminoácidos no resultaba de utilidad hasta demostrar la posibilidad de obtener el producto homogéneo de un clon aislado.
La oportunidad se presentó al estudiarse las proteínas de los mielomas .

En la enfermedad humana conocida como mieloma múltiple, una célula productora de una inmunoglobulina individual particular se divide una y otra vez de la misma forma incontrolada que una célula cancerosa, Sin tener en cuenta la necesidad del huésped.
Por ello el paciente posee enormes cantidades de células idénticas derivadas de la célula original, como si fuera un clon, y todas sintetizan la misma inmunoglobulina-la proteína M o de mieloma- que aparece en el suero, en ocasiones en muy altas concentraciones.
Por purificación de la proteína de mieloma puede obtenerse Un preparado de inmunoglobulina de estructura exclusiva.
También pueden obtenerse anticuerpos monoclonales por fusión de células individuales formadoras de anticuerpos con un tumor de linfocitos B, para producir un clon de células en constante división dedicado a formar ese anticuerpo (véanse figs. 2-10 y 7-11).

La secuencia de varias de estas proteínas ha revelado que las porciones N-terminales tanto de las cadenas pesadas como de las cadenas livianas presenta considerable variabilidad, mientras que las partes remanentes de las cadenas son relativamente constantes y se agrupan en un número restringido de estructuras.
Por convención se habla de regiones constantes y variables de las cadenas livianas y pesadas (fig. 3-5).

Ciertas secuencias de las regiones variables muestran una notable diversidad y su análisis sistemático localiza estas secuencias hipervariables en tres segmentos sobre la cadena liviana (fig. 3-6) y en tres sobre la cadena pesada.

Isotipos 

En base a la estructura de las regiones constantes de las cadenas pesadas, se clasifican las inmunoglobulinas en grupos principales denominados clases que pueden subdividirse en subclases.
En el ser humano, por ejemplo, existen cinco clases: inmunoglobulina G xxx .
Pueden diferenciarse no sólo por sus secuencias sino también por las estructuras antigénicas a que dan origen estas secuencias.
Así, al inyectar un conejo con proteína de mieloma xxx humana, es posible obtener un antisuero que puede ser absorbido por mezclas de mielomas de otras clases para eliminar anticuerpos de reacción cruzada, que a su vez podrán reaccionar con xxx , pero no con xxx (fig. 3.7).

Dado que todas las estructuras de las regiones constantes de las cadenas pesadas ( xxx ) que dan origen a las clases y subclases se expresan juntas en el suero de un individuo normal, se denominan variantes isotípicas (cuadro 3-1).
Del mismo modo, las regiones constantes de las cadenas livianas ( xxx ) existen en formas isotópicas denominadas xxx asociadas con todos los isotipos de cadenas pesadas.
Como las cadenas livianas de un anticuerpo dado son idénticas, las inmunoglobulinas son xxx pero nunca mixtas (salvo que sean armadas especialmente en el laboratorio).
Por lo tanto, la xxx existe como xxx , etcétera.

Figura 3.3

(a) (d) Micrografías electrónicas (x 1.000.000) de complejos formados al enfrentar los haptenos divalentes de DNP con anticuerpos anti-DNP de conejo.

La "tinción negativa" con ácido fosfotúngstico es una solución electrodensa que penetra en los espacios entre las moléculas de proteína.
Así, la proteína se visualiza como una estructura "clara" con el haz de electrones.
El hapteno une las moléculas de anticuerpo en forma de Y para armar (a) dímeros, (b) trímeros.
(c) tetrámeros y (d) pentámeros (véase fig. 3-4).
La flexibilidad de la molécula en la región bisagra es evidente por la variación del ángulo de los brazos de la "Y".
(e) Como en (b); trímeros formados utilizando el fragmento xxx del cual se han digerido las estructuras xxx por acción de la pepsina (x 500000).

Puede observarse que los trímeros carecen de las proyecciones de xxx en cada ángulo evidente en (b). 
(Según Valentine R C y Green N M &lsqb;1967&rsqb; J Mol Biol 27, 615; cortesía del Dr Green y con la autorización de Academic Press, Nueva York).

Figura 3-4

Tres moléculas de anticuerpo anti-DNP unidas en forma de trímero por el antígeno divalente xxx .

Compárese con la figure 3-3b.
Cuando se eliminan los fragmentos xxx por acción de la pepsina, ya no se observan piezas angulares (fig. 3-3e).

Figura 3-5

Variabilidad de secuencia de aminoácidos en la molécula de anticuerpo.

Se utilizan los términos "región V" t "región C" para designar las regiones variable y constante, respectivamente, xxx son términos genéricos para estas regiones sobre la cadena liviana y xxx especifican regiones variable y constante sobre la cadena pesada.
Como se determinó con anterioridad, las cadenas pesadas de cada par son idénticas, al igual que las cadenas livianas de cada par.

Figura 3-6

Esquema de Wu y Kabat sobre la variabilidad de aminoácidos en la región variable de las cadenas pesada y liviana de las inmunoglobulinas. 

Se compara la secuencia de las cadenas de gran número de proteínas monoclonales de mieloma y se computariza la variabilidad en cada posición en función del número de aminoácidos diferentes hallados dividido por la frecuencia del aminoácido más común; es obvio que a mayor número mayor variabilidad.
Se definen con claridad las tres regiones hipervariables (verde) en las cadenas pesada (a) y liviana (b), referidas en general como Regiones Determinantes Complementarias (Complementary Determining Regions, CDR).
Se denominan regiones de marco a las secuencias peptídicas intervinientes.
(Cortesía del Prof. E A Kabat).

Alotipos 

Este tipo de variación depende de la existencia de formas alélicas (codificadas por aleros o genes alternativos en un único locus) que proveen marcadores genéticos (cuadro 3-1).
De manera similar a cómo los eritrocitos de individuos genéticamente diferentes pueden diferir en los términos del sistema A, B, O de grupos sanguíneos, las cadenas pesadas de xxx difieren en la expresión de sus grupos alotípicos.
Son alotipos característicos las especificidades xxx sobre xxx marcador sobre xxx que se reconocen por la capacidad de la xxx del individuo de bloquear la aglutinación de los eritrocitos recubiertos por anti-rhesus D con el alotipo xxx en los sueros de pacientes con artritis reumatoidea que contienen los factores reumatoideos anti-xxx apropiados (fig. 3-8).
Las diferencias alotípicas en un locus xxx determinado suelen involucrar uno a dos aminoácidos de la cadena peptídica.
Por ejemplo, el locus xxx de la xxx (cuadro 3-1).
Un individuo con este alotipo presenta la secuencia peptídica: xxx en cada una de sus moléculas de xxx .

Otra persona, cuyo xxx fuera a-negativo presentaría la secuencia xxx , es decir, dos aminoácidos diferentes.
Hasta el momento, se han detectado 25 grupos xxx en las cadenas pesadas xxx y tres adicionales (los grupos xxx denominados previamente grupos xxx ) en la región constante xxx .

Se han encontrado marcadores alotípicos en las inmunoglobulinas de conejos y ratones utilizando reactivos preparados por inmunización de un animal por medio de un complejo inmune obtenido con anticuerpos de otro animal de la misma especie.
Al igual que en otros sistemas alélicos, los individuos pueden ser homocigotos o heterocigotos para los genes que codifican los marcadores; éstos se expresan en forma codominante y se heredan según las leyes de Mendel.
Por ejemplo, si se toman los alotipos xxx de las cadenas livianas de conejo: un animal con genotipo xxx expresará el alotipo xxx , mientras que un conejo con genotipo xxx derivado de padres xxx expresará el marcador xxx sobre una fracción y el xxx sobre otra fracción de sus moléculas de inmunoglobulina.

Idiotipos 

Según se ha visto es posible obtener anticuerpos que reconozcan variantes isotípicas y alotípicas; también es posible obtener antisueros específicos contra moléculas de anticuerpos individuales y discriminar entre un anticuerpo monoclonal y otro sin considerar las estructuras isotípicas o alotípicas (fig. 3-7).

Estos antisueros definen los determinantes individuales característicos de cada anticuerpo, denominados en conjunto el idiotipo ( Kunkel y Oudin).
No sorprende el hecho de que los determinantes idiotípicos se encuentren localizados en la porción variable del anticuerpo asociado con las regiones hipervariables (fig. 3-9).

Los anti-idiotipos que reaccionan con un anticuerpo y no con otro se dice que reconocen idiotipos privados y proveen de apoyo adicional al concepto de que cada anticuerpo tiene una estructura exclusiva.
A menudo las moléculas de anticuerpo con estructuras de aminoácidos muy similares pueden también compartir idiotipos ( p.ej. xxx y xxx en la figura 3-9) y se habla entonces de idiotipos públicos o de reacción cruzada .

Los sueros anti-idiotipos proveen de reactivos útiles para demostrar la existencia de la misma región V en diferentes cadenas pesadas y en células diferentes, para la identificación de complejos inmunes específicos en el suero del paciente, para reconocer el amiloide tipo xxx en los individuos que excretan proteína de Bence-Jones, para la detección de proteína monoclonal residual luego de la terapéutica y tal vez para seleccionar linfocitos con ciertos receptores de superficie.
El lector puede sorprenderse o no al saber que es posible obtener sueros autoanti-idiotípicos, dado que esto implica que los individuos pueden formar anticuerpos contra sus propios idiotipos.
Esto tiene importantes consecuencias, como se verá al tratar la teoría reticular de Jerne en el capítulo 8.

Figura 3-7

Cómo utilizar las proteínas monoclonales del mieloma para producir anticuerpos específicos para distintas estructuras de xxx . 

El conejo forma anticuerpos dirigidos contra diferentes partes del mieloma humano por xxx .
Los anticuerpos específicos contra las partes comunes a otras clases de xxx pueden absorberse por medio de mielomas de otras clases que forman anticuerpos que reaccionan con estructuras G específicas de clase y específicas de región variable (idiotipo; Id) sobre la molécula original.
Del mismo modo, la absorción posterior con otros mielomas xxx captarán los anticuerpos xxx específicos comunes liberando un antisuero dirigido sólo contra los determinantes idiotípicos.
(A fin de simplificar el concepto, se han dejado de lado las subclases y los alotipos, pero el mismo principio puede extenderse a la generación de antisuero específico para estas variantes).
El conejo produce una mezcla de anticuerpos policlonales dirigidos contra cada sitio estructural sobre el antígeno, por ejemplo se producen por clones derivados de gran variedad de células progenitores específicas para el antígeno, cada una de las cuales reacciona por estereoquímica en forma ligeramente diferente frente a la misma estructura (véase p. 72).

Tabla 3-1

Resumen de variantes de inmunoglobulina

Figura 3-8

Demostración de la especificidad alotípica sobre xxx por inhibición de la aglutinación. 

Se aglutinan eritrocitos RhD recubiertos por anti-D que contiene el alotipo, por medio de un suero de artritis reumatoidea seleccionado por la capacidad del factor reumatoideo xxx de combinarse con el alotipo.
Si el xxx agregado contiene el alotipo en cuestión, bloquea los sitios de combinación sobre RF, que ya no podrán aglutinar a los eritrocitos.

Las inmunoglobulinas se pliegan en dominios globulares adecuados: a diferentes funciones 

Los dominios de las inmunoglobulinas presentan una estructura característica 

Además de los puentes disulfuro intercatenarios que unen las cadenas livianas y pesadas, existen uniones disulfuro internos intracatenarios que forman asas en la cadena peptídica.
Como predijo Edelman, las asas se pliegan formando dominios globulares compactos (fig. 3-10) con una estructura proteica de lámina plegada característica.

Es significativo que las secuencias hipervariables aparecen en un extremo del dominio variable donde forman parte de las asas de giro xxx y donde se acumulan una muy junto a la otra.

El dominio variable liga al antígeno 

Los acúmulos de asas hipervariables en los extremos de las regiones variables, donde se localiza el sitio de unión con el antígeno (figs. 3-3 y 3-4), son los candidatos obvios a actuar en el reconocimiento del antígeno (fig. 3-11); esto ha sido confirmado por análisis cristalográfico de rayos X de los complejos formados entre los Fragmentos Fab de los anticuerpos monoclonales y sus respectivos antígenos.
Esto fue demostrado sin lugar a dudas al probar que la especificidad del antígeno de un anticuerpo monoclonal de ratón podía conferirse a una molécula de inmunoglobulina humana reemplazando las secuencias hipervariables humanas con las del ratón (véase fig. 7-12).
La heterogeneidad secuencial de las asas hipervariables en las tres cadenas livianas y las tres cadenas pesadas garantiza una notable diversidad en la especificidad de combinación con el antígeno por medio de las variaciones de forma y naturaleza de la superficie que crean.
Así, cada región hipervariable puede considerarse una estructura independiente que contribuye a la complementariedad del sitio de unión para el antígeno y se habla de determinantes de complementariedad .

Ciertos experimentos en los que se examinó el potencial de combinación con el antígeno de cadenas aisladas sugirieron que tanto las regiones variables de las cadenas pesadas como las de las cadenas livianas contribuyen a la especificidad para el anticuerpo.
En general, se asoció un grado variable de actividad residual con las cadenas pesadas, pero relativamente escaso con las cadenas livianas; sin embargo, al recombinarse siempre se encontró un aumento significativo en la capacidad de unión con el antígeno.

Los aminoácidos asociados con el sitio de combinación pueden identificarse por medio del "marcado por afinidad".
En esta técnica, se equipa a un hapteno (un agrupamiento químico bien definido contra el que puedan formarse anticuerpos, p.ej. 
DNP en la figura 3-3) con una cadena lateral químicamente reactiva que formará enlaces covalentes con aminoácidos adyacentes luego de combinarse el hapteno con el anticuerpo, por lo que quedan marcados los residuos en las adyacencias del sitio de combinación.
Una modificación introducida por Porter y col utiliza el principio de " flick-knife".
El hapteno con una cadena lateral azídica se combina con su anticuerpo y es luego iluminado con luz ultravioleta; esto convierte a la azida en el radical nitreno reactivo que se une por enlaces covalentes con casi cualquier grupo orgánico con el que entre en contacto ( p. ej. fig. 3-12).

El marcado por afinidad se une tanto a las cadenas livianas como a las pesadas en las regiones hipervariables .

Figura 3-9

Correlatos estructurales de idiotopes (determinantes individuales sobre un idiotipo) en anticuerpos antidextrán.

Se muestran las secuencias de aminoácidos en las regiones variables de las cadenas pesadas en los anticuerpos antidextrán monoclonales del ratón.

Todos los anticuerpos presentan cadenas xxx .
Las líneas indican identidad de la secuencia de la primer proteína, xxx ; las letras (código de Dayhoff) muestran las diferencias o regiones correlacionadas con idiotopes (zonas de los recuadros centrales).
El idiotope de reacción cruzada xxx se asocia con las estructuras de la segunda región determinante de complementariedad xxx , mientras que los idiotopes privados xxx son característicos de la región xxx en estos anticuerpos.
La presencia de los idiotopes sobre cada molécula de anticuerpo se demuestra por reacción con antisuero específico para xxx (sobre la derecha).
Los idiotopes de reacción cruzada también pueden estar asociados con la región xxx de otros sistemas (según J M Davie y col (1986) Ann Rev Imninol 4, 147, con autorización por Annual Reviews Inc.).

Los dominios de la región constante determinan la función biológica secundaria 

Las clases de anticuerpo difieren una de otra en muchos aspectos: en su vida media, su distribución en el organismo, su capacidad de fijar el complemento y su unión a los receptores xxx de la superficie celular.

Dado que todas las clases presentan las mismas cadenas livianas xxx e iguales dominios de las regiones variables liviana y pesada, estas diferencias deben residir en las regiones constantes de la cadena pesada.

Ha sido posible localizar estas actividades biológicas en los numerosos dominios de la cadena pesada al emplear proteínas de mieloma que presentan delaciones de dominio espontáneas, o fragmentos enzimáticos obtenidos con papaína xxx , pepsina xxx , la porción C-terminal de xxx y plasmina ( xxx de conejo carece de xxx pero retiene la mitad N-terminal de xxx ).
Por supuesto, en la actualidad todo esto puede realizarse con proteínas obtenidas por ingeniería genética.

En la figura 3-13 se presenta un modelo de la molécula de xxx , donde se indica la disposición espacial y Las interacciones de los dominios de xxx y se atribuyen las diferentes funciones biológicas a las estructuras relevantes.
En principio, el dominio de la región V forma la unidad de reconocimiento (véase fig . 

2-1) y los dominios de la región constante median las funciones biológicas secundarias.

RECONOCIMIENTO DEL ANTÍGENO 

Detección y aplicación 

Luego de explorar la intimidad de las interacciones antígeno-anticuerpo a nivel molecular, en este capítulo se tratarán los aspectos prácticos de los métodos empleados para detectar y sacar partido del reconocimiento del antígeno.

Precipitación 

La clásica reacción de precipitina 

Cuando se mezcla una solución de antígeno con las proporciones correctas de un antisuero potente, se forma un precipitado.
El análisis cuantitativo de esta interacción por el método que se muestra en la figura 5-1 presenta el contenido de anticuerpo del suero inmune y además una indicación de la valencia del antígeno, es decir el número efectivo de sitios dominantes de combinación (determinantes).
Esto puede variar mucho de acuerdo con el antígeno, su tamaño y las especies que forman el anticuerpo.
En los antisueros hiperinmunes de conejo que contienen predominantemente anticuerpos xxx , la ovoalbúmina puede tener una valencia de 10 y la tiroglobulina humana hasta 40 determinantes en su superficie.

En la curva de precipitina de la figura 5-1 puede notarse que a medida que se agrega más antígeno, se alcanza un estado óptimo, tras el cual se forma menos precipitado.
En esta etapa se observa que el sobrenadante contiene complejos solubles del antígeno ( xxx ) y el anticuerpo ( xxx ), de composición xxx (Fig. 5-2).
Si se agrega un gran exceso de antígeno ( xxx ; fig. 5-1) el análisis por ultracentrifugación revela que los complejos obtenidos serán en su mayor parte xxx , resultado que se atribuye directamente a los dos sitios de combinación o divalencia de la molécula de anticuerpo xxx (véase estudio E.M., fig. 3-3 y análisis Scatchard, fig. 4-8).
Entre estos extremos el cruzamiento entre el antígeno y el anticuerpo por lo general da origen a estructuras tridimensionales reticuladas, tal como sugiere Marrack. que coalescen, en su mayoría por interacciones xxx , y forman grandes agregados que precipitan.
A menudo los sueros contienen hasta un 10% de anticuerpos no precipitantes que son univalentes debido a la presencia asimétrica de un azúcar en una región xxx que bloquea por estereoquímica el sitio de combinación adyacente.

Cuantificación por nefelometría 

Los pequeños agregados que se forman cuando se mezclan soluciones diluidas de antígeno y anticuerpo crean una opacidad o turbidez mensurable a través de la dispersión de una fuente luminosa incidente (nefelometría).
Puede obtenerse mayor sensibilidad al utilizar luz monocromática proveniente de un láser y por adición de polietilenglicol a la solución para incrementar el tamaño del agregado.
En determinados laboratorios que cuentan con el equipamiento apropiado este método reemplaza la inmunodifusión radial simple para la estimación de inmunoglobulinas, xxx , proteína C reactiva, etcétera.

Visualización de la reacción de precipitación en geles 

Doble difusión de Ouchterlorny 

Por este método, el antígeno y el anticuerpo ubicados en cubetas cortadas en el gel de agar difunden Uno hacia el otro y precipitan formando una línea opaca en la región donde se unen en las proporciones óptimas.

Un preparado que contiene varios antígenos dará origen a líneas múltiples.
Las relaciones inmunológicas entre dos antígenos pueden determinarse ubicando las reacciones de precipitación en cubetas adyacentes: las Líneas formadas por cada antígeno pueden confluir en su totalidad e indicar identidad inmunológica, pueden mostrar un "espolón" como es el caso de antígenos parcialmente relacionados, o pueden cruzarse, por tratarse de antígenos no relacionados (fig. 5-3).
En la figura 5-4 se explican los orígenes de estos esquemas.
Debe notarse que aun en el caso de líneas confluentes, esto solo indica identidad inmunológica en términos del antisuero empleado, y no necesariamente identidad molecular.
Por ejemplo, los anticuerpos purificados del hapteno dinitrobenceno darán una línea de confluencia cuando se forman contra conjugados de dinitrofenil(DNP)-ovoalbúmina y contra DNP-albúmina sérica ubicadas en cubetas adyacentes.

Cuando se enfrentan los reactivos en proporciones equilibradas, la Línea formada suele ser cóncava hacia la cubeta que contiene el reactante de mayor peso molecular, ya sea antígeno o anticuerpo.
Esto es consecuencia de la menor velocidad de difusión de las moléculas de mayor tamaño.

El método de precipitación puede hacerse más sensible al incorporar el antisuero al agar y permitir que el antígeno difunda en él (Feinberg).
Este método de inmunodifusión radial simple se emplea para la estimación cuantitativa del antígeno.

Figura 5-1

Reacción cuantitativa de precipitina entre anti-ovoalbúmina de conejo y ovoalbúmina (el método clásico de Heidelberger & Kendall, de 1935).

Los anticuerpos son predominantemente xxx .

Cantidades crecientes de ovoalbúmina se agregan a un volumen constante del antisuero colocado en cierto número de tubos.
Luego de la incubación se centrifugan los precipitados y se pesan.
Cada sobrenadante es dividido en dos partes iguales: por agregado de antígeno a uno y anticuerpo al otro, se demuestra la presencia de anticuerpo o antígeno reactivos respectivamente.
El contenido de anticuerpo del suero puede calcularse a partir del punto de equivalencia, en el cual virtualmente no hay antígeno o anticuerpo presente en el sobrenadante.

Por lo tanto todo el antígeno agregado será acomplejado en el precipitado con todo el anticuerpo disponible y el contenido de anticuerpo de 0,1 ml de suero estará dado por xxx .
El análisis del precipitado formado en presencia de un exceso de anticuerpo xxx , cuando los sitios de combinación con el antígeno están saturados por completo, da una medida de la relación molar del anticuerpo respecto del antígeno en el complejo y por lo tanto un estimado de la valencia efectiva del antígeno.

Inmunodifusión radial simple (SRID) 

Cuando el antígeno difunde de una cubeta al agar que contiene el antisuero convenientemente diluido, al principio se encuentra en una concentración elevada y forma complejos solubles; a medida que el antígeno difunde más, la concentración disminuye hasta alcanzar el punto en el cual los reactantes están más cerca de las proporciones óptimas y se forma un anillo de precipitado.

Cuanto mayor sea la concentración del antígeno mayor será el diámetro del anillo (fig. 5-5).
Si se incorporan por ejemplo tres patrones de concentración conocida del antígeno, puede obtenerse una curva de calibración que puede utilizarse para determinar la cantidad de antígeno que se encuentra en las muestras desconocidas estudiadas (fig. 5-6).
Este método se emplea de rutina en la inmunología clínica, en particular en las determinaciones de inmunoglobulinas y también para sustancias tales como el tercer componente del complemento, la transferrina, la proteína C reactiva, y la proteína embrionaria, xxx fetoproteína, asociada a ciertos tumores hepáticos.

Inmunoelectroforesis 

Se describe el principio en la figura 5-7.
El método tiene valor para la identificación de los antígenos por su movilidad electroforética, en particular cuando están presentes otros antígenos.
En la inmunología clínica, esta técnica provee información semicuantitativa referente a las concentraciones de inmunoglobulina y la identificación de las proteínas de mieloma (fig. 5-8).

Figura 5-2

Representación en Un diagrama de los complejos formados entre un antígeno tetravalente hipotético y un anticuerpo bivalente mezclados en distintas proporciones.

En la práctica, es poco probable que las valencias del antígeno estén ubicadas en un mismo plano o que estén formadas por determinantes idénticos como sugiere la figura.
(a) Con un exceso extremo de anticuerpo. las valencias del antígeno están saturadas y la relación molar xxx se aproxima a la valencia del antígeno.
(b) En la equivalencia se forman grandes filigranas que se agregan para formar un clásico precipitado inmunitario.
Esta segunda agregación y subsecuente precipitación tiende a inhibirse por una elevada concentración salina.
(c) Con un exceso extremo de antígeno con el cual las dos valencias de cada molécula de anticuerpo se satura con rapidez, tiende a predominar el complejo xxx .
(d) Un hapteno monovalente puede fijarse pero es incapaz de formar enlaces cruzados con moléculas de anticuerpo.

Figura 5-3

Líneas múltiples formadas en la determinación de Ouchterlony (precipitación por doble difusión) cuando el antisuero de conejo (cubeta central) reacciona en el gel de agar con cuatro preparados de antígeno, diferentes (cubetas periféricas).

Se muestran identidades nulas y parciales de antígenos, respectivamente, por entrecruzamientos y formación de espolones entre las líneas de precipitina.

Figura 5-4

(a) Línea de confluencia obtenida con dos antígenos que no pueden distinguirse por el antisuero utilizado.

(b) Formación de espolón por tratarse de antígenos parcialmente relacionados con un determinante común x pero determinantes individuales y v z que reaccionan con una mezcla de anticuerpos dirigida contra x e y.
El antígeno con determinantes x y z sólo puede precipitar anticuerpos dirigidos contra x.
Los anticuerpos remanentes xxx cruzan la línea de precipitina para reaccionar con el antígeno de la cubeta adyacente que contiene determinante y dan origen a un espolón sobre la línea de precipitina.
(c) Cruce de Líneas formadas por antígenos no relacionados.

Figura 5-5

Inmunodifusión radial simple: relación entre concentración de antígeno y tamaño del anillo de precipitina formado.

El antígeno con mayor concentración xxx difunde más desde la cubeta antes de alcanzar el nivel que produce la precipitación con el anticuerpo en proporciones cercanas a las óptimas.

Figura 5-6

Medida de la concentración de xxx en suero por inmunodifusión radial simple.

El diámetro de los patrones permite trazar una curva de calibración y puede determinarse la concentración de xxx del suero incógnita según: T1 - suero de paciente con mieloma xxx ; 15 mg/ml ; T2 - suero de paciente con hipogammaglobulinemia; 2.6 mg/ml ; T3 - suero normal; 9,6 mg/ml .
(Cortesía del Dr. F C Hay).

Se han desarrollado variaciones de la técnica que combinan la electroforesis con inmunoprecipitación, en las cuales el movimiento en un campo eléctrico permite que el antígeno haga contacto con el anticuerpo.
Puede aplicarse la contrainmunoelectroforesis a antígenos que migran hacia el polo positivo del agar (fig. 5-9).
Esta técnica cualitativa es mucho más rápida y considerablemente más sensible que la doble difusión (Ouchterlony) y se emplea en la detección del antígeno o el anticuerpo de la hepatitis B, los anticuerpos DNA en el LES (p. 285), los autoanticuerpos contra los antígenos nucleares solubles en la enfermedad mixta del tejido conectivo y las precipitinas del Aspergillus en los casos de la aspergilosis broncopulmonar alérgica.
La electroforesis en cohete es un método cuantitativo que incluye la electroforesis del antígeno en un gel que contiene el anticuerpo.
El arco de precipitación tiene el aspecto de un cohete cuya longitud está relacionada con la concentración de antígeno (fig. 5-10).
Al igual que la contrainmunoelectroforesis, éste es un método rápido, aunque también el antígeno debe moverse hacia el polo positivo de la electroforesis; es por lo tanto apropiado para proteínas tales como la albúmina, la transferrina y la ceruloplasmina pero las inmunoglobulinas se cuantifican con mayor propiedad por inmunodifusión radial simple.
Una importante variación del sistema en cohete, la inmunoelectroforesis cruzada o bidimensional de Laurell, incluye una separación electroforética preliminar de una mezcla de antígenos en una dirección perpendicular a la de la etapa final "en cohete" (fig. 5-11).
De esta manera pueden cuantificarse cada uno de varios antígenos en una mezcla.
Un ejemplo es la estimación del grado de conversión del tercer componente del complemento ( xxx ) a la forma inactiva xxx (véase Fig. 1-10) que puede aparecer en el suero de pacientes con LES activo o el Líquido sinovial de las articulaciones afectadas en la artritis reumatoidea activa para dar dos ejemplos (fig. 5-11b).
En la figura 5-11c se observa con claridad la notable capacidad de la técnica para resolver una compleja mezcla de antígenos.

Figura 5-8

Principales clases de inmunoglobulina humana demostradas por análisis inmunoelectroforético de suero humano utilizando antisuero de conejo en la cubeta.

Se indica la posición de las principales fracciones electroforéticas de globulina.
Pueden reconocerse tres de las cinco principales clases de inmunoglobulina: inmunoglobulina xxx , inmunoglobulina xxx , e inmunoglobulina xxx .
El arco de precipitina de la inmunoglobulina xxx se extiende desde la fuente de la región xxx hacia el rango de movilidad xxx globulina, lo que refleja la gran heterogeneidad de la población de anticuerpo en lo que respecta a composición de aminoácidos y carga neta.

Precipitación indirecta de complejos 

Por varias razones, el agregado de anticuerpo a un antígeno puede dar origen a Un complejo soluble Si se desea analizar el complejo para determinar los antígenos que contiene, puede captarse el complejo de la solución al cambiar su solubilidad o por agregado de un reactivo precipitante anti-inmunoglobulina como se observa en la figura 5-12.
En la figura 5-13 se muestra un ejemplo de cómo se utiliza esta técnica para caracterizar la cadena principal de las moléculas del CMH de clase I humanas.

Aglutinación por anticuerpos 

Mientras que el enlace cruzado de antígenos proteicos multivalentes con anticuerpos lleva a la precipitación, el enlace cruzado de células o partículas voluminosas con anticuerpos dirigidos contra los antígenos de superficie lleva a la aglutinación.

Dado que la mayor parte de las células presentan carga eléctrica, se requiere un número razonable de enlaces con el anticuerpo entre dos células antes de superar la repulsión mutua.
Así, puede ser difícil obtener la aglutinación de células que presenten pocos determinantes a menos que se utilicen métodos especiales tales como tratamiento ulterior con un reactivo antiglobulina.
De forma similar, la mayor avidez del anticuerpo xxx multivalente relativo a xxx (véase p. 78) hace que aquél resulte más efectivo como agente aglutinante comparando cada molécula (fig. 5-14).

Figura 5-9

Contrainmunoelectroforesis.

El anticuerpo retrocede en el gel ante la electroforesis por endósmosis; un antígeno con carga negativa al pH utilizado se moverá hacia el polo positivo y precipitará ante el contacto con el anticuerpo.

Figura 5-l0

Electroforesis en cohete se realiza la electroforesis del antígeno, en este caso albúmina sérica humana, en gel que contiene el anticuerpo.

La distancia desde la cubeta de origen hasta el frente del arco, en forma de cohete está relacionada con la concentración del antígeno.
En el ejemplo la albúmina sérica humana se encuentra en concentraciones relativas de izquierda a derecha: 3,2,1.

Las reacciones de aglutinación se emplean para identificar bacterias y para tipificar eritrocitos; se han empleado con leucocitos y plaquetas y aun con espermatozoides en ciertos casos de infertilidad masculina debida a aglutininas espermáticas.
Por su sensibilidad y conveniencia se ha extendido la determinación a la identificación de anticuerpos contra antígenos solubles que han sido recubiertos artificialmente con varios tipos de partículas.
Han sido muy utilizados los eritrocitos que pueden recubrirse con proteínas luego de modificar su superficie con ácido tánico o cloruro de cromo o por uso directo de agentes bifuncionales de entrecruzamiento como la bisdiazobencidina.
En la actualidad se están utilizando los eritrocitos de pavo de mayor tamaño y sedimentación rápida.
Por lo general las determinaciones se realizan en cubetas de plaquetas plásticas de aglutinación donde pueden observarse los patrones de sedimentación de las células en el fondo de la copa (fig. 5-15); esto provee un indicador más sensible que la agrumación macroscópica.

Partículas inertes como la bentonita y el látex de poliestireno han sido recubiertas también con antígenos para establecer reacciones de aglutinación en particular las empleadas para detectar los factores reumatoideos (fig. 5-16).

Purificación de los antígenos y los anticuerpos por cromatografía de afinidad 

El principio es simple y de muy amplia difusión.
Se fija el antígeno o el anticuerpo por sus grupos amino libres a partículas de sefarosa activadas con bromuro de cianógeno.
Por ejemplo, puede utilizarse un anticuerpo insolubilizado para sacar por absorción a su superficie al correspondiente antígeno de la solución en que se encuentra como componente de una mezcla compleja.
El residuo innecesario se elimina por lavado y el ligando requerido se libera del absorbente al que está unido por afinidad, por disrupción de los enlaces antígeno-anticuerpo al modificar el pH o agregar iones cuotrópicos como el tiocianato (fig. 5-17).
Del mismo modo, puede emplearse un antígeno inmunosorbente para captar por absorción un anticuerpo de una mezcla que luego puede purificarse por efusión.
EL efecto potencialmente lesivo del agente de efusión puede evitarse haciendo correr el antisuero por una columna de afinidad preparada para tener un enlace relativamente débil con el anticuerpo a purificar; en estas circunstancias, el anticuerpo retarda su velocidad de flujo en lugar de ligarse con firmeza.
Si se separa en bandas discretas una mezcla proteica por método isoeléctrico, un enlace individual puede ser utilizado para purificar por afinidad anticuerpos específicos en un antisuero policlonal; esto es muy útil cuando el suministro de antígeno es limitado.

RESPUESTA INMUNITARIA ADQUIRIDA 

Producción de efectores 

La síntesis de anticuerpos humorales 

Detección y recuento de las células productoras de anticuerpo 

Inmunofluorescencia 

Las células que contienen anticuerpo dentro de su citoplasma pueden identificarse por medio de la técnica "sandwich" (véase fig. 5-24).
Por ejemplo, una célula productora de anticuerpos contra el toxoide tetánico, si es tratado previamente con el antígeno se unirá al anticuerpo anti-tetánico marcado con fluoresceína y luego podrá ser visualizado a través del microscopio de fluorescencia.

Técnicas de placa 

Las células secretoras de anticuerpo pueden contarse por dilución en un medio en el cual el anticuerpo formado por cada célula individual produce un efecto rápidamente visible.
En una de las técnicas más utilizadas, desarrollada a partir del método original de Jerne y Nordin, se suspenden células de un animal inmunizado con eritrocitos de carnero junto con un exceso de glóbulos rojos de carnero y complemento en una cámara poco profunda formada entre dos portaobjetos.
Al incubar las células productoras de anticuerpo liberan sus inmunoglobulinas que recubren a los eritrocitos circundantes.
Entonces el complemento causará la lisis de las células recubiertas y se observa una placa libre de eritrocitos alrededor de cada célula formadora de anticuerpo (fig. 7-1).
Las placas directas obtenidas de esta manera revelan productores de xxx fundamentalmente, ya que este anticuerpo es muy hemolítico.
Para demostrar la presencia de linfocitos sintetizadores de xxx es necesario aumentar la unión entre el complemento y el complejo eritrocito-anticuerpo xxx agregando un suero anti-xxx de conejo; así se desarrollan las placas indirectas y pueden utilizarse para hacer el recuento de las células productoras de anticuerpo en las diferentes subclases de inmunoglobulina, siempre que se cuente con el antisuero de conejo adecuado.
El método puede extenderse a otras determinaciones si se recubre un antígeno tal como el polisacárido de neumococo sobre el glóbulo rojo, o si se acoplan grupos de haptenos a la superficie del eritrocito.

En la modificación "Elispot", la suspensión de células formadoras de anticuerpo se incuba sobre un disco de antígeno inmovilizado.
El anticuerpo secretado es capturado en su sitio y visualiza después de ser eliminadas las células, por tratamiento con anti-xxx marcado con peroxidasa, y se desarrolla el color al ser incorporado el sustrato en gel distribuido sobre el piso del disco.
La difusión limitada del producto de la reacción de color en el gel provee de una serie de puntos macroscópicos que pueden contarse con facilidad (fig. 7-2).

Síntesis de proteína 

En el linfocito formador de anticuerpo normal se observa un rápido metabolismo de cadenas livianas que se encuentran en ligero exceso.
En muchos linfocitos de mieloma ocurren controles defectuosos y puede verse un exceso de producción de cadenas livianas o aun la supresión total de la síntesis de cadenas pesadas.

Pueden formarse puentes disulfuro intercatenarios mientras las cadenas pesadas aún están unidas a los ribosomas (fig. 7-3) pero la secuencia en que se generan los intermediarios varía con la naturaleza de la inmunoglobulina.
Por medio de las técnicas de "pulso y búsqueda" con aminoácidos reactivos se determinó que la construcción tanto de las cadenas pesadas como de las livianas proceden en forma continua desde el extremo N-terminal.
Además, el aislamiento del RNAm de cada tipo de cadena ha mostrado que presentan el tamaño adecuado para permitir la síntesis de los péptidos completos.
Por lo tanto la evidencia confirma la tesis actual que postula que las regiones mensajeras para las regiones variables y constantes están unidas antes de salir del núcleo.

Los mecanismos diferenciales de unión proveen además de una explicación racional para la coexpresión de xxx e xxx superficiales con regiones V idénticas en una misma célula (fig. 7-4), y para la variación de producción de receptor xxx ligado a membrana a xxx secretor en el linfocito formador de anticuerpo (fig. 7-5).

Figura 7-1

Técnica de placa de Jerne para el recuento de las células formadoras de anticuerpo (modificación de Cunningham).

(a) Se muestra la técnica directa para las células que sintetizan hemolisina xxx .
La técnica indirecta para visualizar las células productoras de hemolisinas xxx requiere del agregado de anti-xxx al sistema.
La diferencia entre las placas obtenidas por los métodos directo e indirecto da como resultado la cantidad de placas xxx .
El ensayo de placa inversa cuenta el total de las células productoras de xxx al capturar la inmunoglobulina secretada sobre eritrocitos recubiertos por anti-xxx .
Pueden realizarse pruebas de placas múltiples por medio de una modificación que emplea placas de microtitulación. 
(b) Fotografía de placas que se visualizan como zonas circulares coloreadas (algunas de ellas se han señalado con flechas) al ser iluminadas sobre fondo oscuro.
Varían en tamaño de acuerdo con la afinidad del anticuerpo y la velocidad de la secreción por parte de la célula formadora de anticuerpo.
(Cortesía del Sr. C Shapland, la Sra. P Hutchings y el Dr. D Male).

En los linfocitos B individuales se produce cambio de clase 

La síntesis de anticuerpos pertenecientes a las diferentes clases ocurre a velocidades distintas.
Por lo general se produce una respuesta de xxx temprana que tiende a caer con rapidez.
La síntesis de anticuerpo xxx alcanza su máximo en un período de tiempo más prolongado.
Ante un contacto secundario con el antígeno, el curso de tiempo para la respuesta xxx es similar a la del contacto primario.
Por el contrario, la síntesis de anticuerpos xxx se acelera con rapidez hasta alcanzar un título mucho mayor y se produce un descenso relativamente lento en los niveles séricos de anticuerpo (fig. 7-6).
Es probable que lo mismo valga para xxx y en cierto sentido estas dos clases de inmunoglobulina proveen la principal defensa inmediata contra futuras penetraciones de antígenos extraños.

Existen evidencias que sostienen que las células individuales pueden pasar de la producción de xxx a la de xxx .
Varios días después de la inmunización con flagelos de Salmonella se demostró que linfocitos aislados captados en cultivos de microgota producían tanto anticuerpos inmovilizantes xxx como xxx .
En otro experimento se demostró que el contacto antigénico de receptores irradiados que recibían cantidades relativamente pequeñas de células linfoides, producían focos esplénicos de células, cada uno de los cuales sintetizaba anticuerpos de diferente clase de cadena pesada con un único idiotipo.
El idiotipo común sugiere que cada foco deriva de una única célula precursora cuya progenie puede formar anticuerpos de diferente clase.

En la mayor parte de las clases la síntesis de anticuerpo muestra considerable dependencia de la cooperación T en cuanto a que las respuestas en animales T-deprimidos son notablemente deficientes; esto sucede en las respuestas de anticuerpo xxx y parte de las xxx de ratón.
Los antígenos T-independientes tales como el activador policlonal, la endotoxina xxx , inducen la síntesis de xxx con algo de xxx .
En apariencia, ocurre inmunopotenciación por el coadyuvante completo de Freund, una emulsión de agua en aceite que contiene antígeno en la fase acuosa y una suspensión de bacilos muertos de tuberculosis en la fase oleosa (p.

222), al menos en parte por activación de los linfocitos T helper que estimulan la producción de anticuerpo en las clases T-dependientes.
Se ha demostrado en la práctica la predicción de que la respuesta a los antígenos T-independientes (por ejemplo polisacárido de neumococo, p 120) no es potenciada por el adyuvante de Freund; además, como ya se mencionó, estos antígenos evocan en principio a los anticuerpos xxx y una memoria inmunológica mal definida, como es el caso de antígenos T-dependientes inyectados en huéspedes deficientes de linfocitos T por haber sido timectomizados al nacer.

Así, al menos en roedores, el cambio de xxx a xxx y otras clases parece encontrarse sobre todo bajo control de los linfocitos T presumiblemente mediado por factores solubles como ya se sugirió con anterioridad (p. 123), aunque a menudo es difícil establecer si un agente está causando por sí mismo la variación o se trata de un promotor particularmente eficaz para el crecimiento de la clase de xxx que resulta de la variación.
Si se examina la estimulación por LPS de pequeños linfocitos B positivos para xxx de superficie, como se vio con anterioridad, por sí solo el mitógeno no específico evoca la síntesis de xxx .
Luego del agregado de interleuquina-4 ( IL-4 ) al sistema, aumenta la producción de xxx e xxx mientras que el interferón xxx estimula la secreción de xxx a concentraciones que inhiben los efectos de IL-4.
La IL-5 promueve la maduración sin afectar la clase de xxx .
El concepto de que IL-4 era un factor de variación de xxx partía fundamentalmente de resultados obtenidos con un linfoma dado, pero sin datos confirmatorios utilizando linfocitos B normales.
Merece mayor crédito el aumento de 5-10 veces obtenido en la producción de xxx cuando se introdujo el factor transformador de crecimiento ( xxx ) en el sistema LPS y la demostración de una variación de xxx superficial negativo a positivo en las células de las placas de Peyer así tratadas.
Es significativo que el xxx indujo la formación de transcriptos estériles compuestos por un exon 5' derivado de secuencias de línea bacteriana anteriores en la región a de variación (véase fig. 7-7) unida al gen xxx de la clase xxx .
Los exones 5' contienen codones de detención en el marco abierto de lectura del gen xxx por lo que no pueden codificar proteínas grandes.
En la actualidad se considera esto como una instancia de un fenómeno más general en el cual las transcripciones estériles de un gen CH están asociadas con una variación hacia esa clase (fig. 7-7).
Tal vez estos transcriptos faciliten de alguna manera la acción de la recombinasa o reflejen un acceso facilitado para esa enzima de esa región particular de variación.

Bajo la influencia de la recombinasa, un segmento de gen VDJ dado es transferido de xxx a un gen alternativo de región constante por medio de las secuencias especializadas de la región de variación (fig. 7-7) razón por la cual generará anticuerpos de la misma especificidad pero de diferente clase.

Figura 7-2

sistema Elispot (según la técnica ELISA) para el recuento de las células formadoras de anticuerpo. 

El dibujo muestra puntos formados por células de hibridoma formando auto-anticuerpos contra la tiroglobulina revelados por anti xxx ligada a la fosfatasa alcalina (cortesía de P Hutchings).
Se agregan cantidades crecientes de células de hibridoma en las dos cubetas superiores y en la cubeta inferior izquierda que muestran el correspondiente aumento de la cantidad de Elispots.
La cubeta inferior derecha sirve como control y contiene un hibridoma de especificidad irrelevante.

Figura 7-3

Síntesis de inmunoglobulina xxx de ratón.

A medida que se van completando las cadenas H, pueden formar enlaces cruzados de manera espontánea las cadenas peptídicas adyacentes por sus regiones constantes.
Se supone que las cadenas livianas favorecen la apertura de las cadenas terminales del ribosoma al formar la molécula xxx .
La combinación con otra cadena liviana produce la inmunoglobulina completa xxx (basado en Askonas B A ' & ' Williamson A R &lsqb;1986&rsqb; Biochem , J, 109, 637).
El orden en que se forman los puentes disulfuro intercatenarios varía en las distintas inmunoglobulinas de acuerdo con las fuerzas relativas de los enlaces según se ha determinado por la susceptibilidad a la reducción.
Se cree que el receptor xxx de superficie se inserta por sus secuencias hidrófobas en la membrana del retículo endoplasmático a medida que es sintetizado.

Figura 7-4

Los receptores xxx e xxx de la superficie de la membrana de especificidad idéntica aparecen sobre la misma célula por escisión del compuesto de transición primaria del RNA.

A los efectos de simplificar el esquema se ha omitido la secuencia guía. 
xxx intrones.

Figura 7-5

Mecanismos de escisión para la modificación de la membrana a la forma secretora de xxx . 

La secuencia hidrófoba codificada por los exones M-M que fijan al receptor xxx a la membrana es escindida en la forma secretora (también aquí se omite la secuencia guía a los efectos de simplificar el esquema).

La especificidad del anticuerpo secretado por la progenie, ¿es la misma que la de la inmunoglobulina superficial del progenitor clonal?

A menudo la respuesta es sí, con frecuencia es parcial y en ocasiones es no.
Como esto es bastante críptico, merece una revisión.
La hipótesis de selección clonal predice una respuesta afirmativa a la pregunta dado que postula que cada linfocito está programado para producir un único anticuerpo al que ubica en su superficie como receptor para captar al antígeno, y como consecuencia el linfocito está selectivamente preparado para la formación de anticuerpo; como solo conoce la forma de producir un anticuerpo, la especificidad debería ser idéntica al receptor original de superficie.

En apariencia, el concepto un linfocito-un anticuerpo es adecuado 

Por medio de las técnicas de inmunofluorescencia, las moléculas de inmunoglobulina individuales pueden colorearse para las cadenas xxx , pero no para ambas, y en el conejo heterocigoto, por el marcador alotípico materno o el paterno, pero nunca para los dos en forma simultánea ( exclusión alélica ).
Además, los tumores de células plasmáticas sólo producen una proteína de mieloma.
Restricciones similares se aplican a la coloración de xxx de superficie sobre los linfocitos B.

Figura 7-6

Síntesis de clases xxx e xxx de anticuerpo en las respuestas primaria y secundaria al antígeno.

La capacidad de un pequeño porcentaje de linfocitos de fijar antígenos específicos como las células de carnero (formando rosetas) o la flagelina radioactiva de Salmonella sugiere que estas inmunoglobulinas de superficie pueden comportarse como anticuerpos.
Este enlace puede bloquearse por medio de suero anti-inmunoglobulina.
Humphrey ha demostrado que el porcentaje de células que fijan antígeno está aumentado en animales imprimados y disminuido de animales tolerantes.

Figura 7-7

La modificación de clase con el fin de producir anticuerpos de especificidad idéntica pero diferente isotipo de inmunoglobulina (en el ejemplo de xxx a xxx ) se logra por un proceso mediado por una recombinasa que utiliza las secuencias especializadas de modificación y conduce a la pérdida del asa de DNA que interviene xxx .

Aún no se ha establecido el papel de la transcripción de xxx que siempre acompaña a este fenómeno.

Raros ejemplos de clones mutantes que expresan un gen 5 isotípico de la cadena pesada original sugieren que en ocasiones pueden ocurrir modificaciones de los genes de la región constante sobre la cromátide hermana.

Cuando un antígeno soluble Como la flagelina polimerizada se une a una célula especifica provoca la formación de placas y de manto en la xxx de superficie del mismo modo que un suero anti xxx (véase p. 32).
Si se colorean las células cubiertas por antígeno con anti-xxx fluorescente, se encuentra toda la xxx en el capuchón ( cap ) y no queda nada en el resto de la superficie del linfocito, es decir, cuando un antígeno reacciona con un linfocito, todas las moléculas de xxx en la superficie celular se combinan con el antígeno, &rsqb;o que demuestra que presentan especificidad similar.
El mismo resultado se observa si se utiliza un anti-idiotipo para la formación del capuchón ( cap ), es decir, todas las moléculas de xxx de la superficie de un linfocito dado tienen el mismo idiotipo.

El anticuerpo de superficie no parece estar relacionado con la especificidad de la inmunoglobulina secretada 

Si la producción de un anticuerpo dado depende de su representación como xxx de superficie en el pool de linfocitos B sensibles al antígeno, la selección previa de linfocitos positivos y negativos para esa especificidad debería tener profundos efectos sobre la reactividad subsecuente, concepto que ha sido demostrado.

La eliminación de linfocitos antígeno-específicos por reacción con sus receptores de superficie impide la capacidad posterior de esa población de responder al antígeno (fig. 7-8).
Del mismo modo, un elevado porcentaje de linfocitos aislados en virtud de su enlace específico a un hapteno como el DNF, secretará anti-DNF cuando es estimulado bajo condiciones adecuadas in vitro .
Puede demostrarse además que las xxx de superficie y citoplasmáticas de las células secretoras comparten un idiotipo común.
Se colorea primero la xxx de superficie de los linfocitos vivos a 4 C con anti-idiotipo marcado con fluoresceína y luego se fija con glutaraldehído y se colorea el citoplasma de las células citocentrifugadas con anti-idiotipo conjugado con rodamina.
Se demuestra el concepto por medio de la doble coloración.
La pregunta inicial no puede ser contestada en forma inequívoca con la respuesta afirmativa, por causa de la mutación somática.

Figura 7-8

La eliminación de linfocitos con receptores de superficie para el antígeno quita la subsecuente capacidad de responder a ese antígeno.

(a) Linfocitos imprimados (verde) eliminados por reacción entre la xxx de superficie de los linfocitos y el antígeno insoluble que forma la columna.

Para control se emplean perlas recubiertas por ovoalbúmina.
(b) Los linfocitos imprimados (verde) destruidos por radiación se eliminan al unirse al antígeno radiactivo con la xxx de superficie.

Para control, antígeno no marcado (TGAL).
ASB = albúmina sérica bovina; OVA = ovoalbúmina; TGAL = polilisina con cadenas laterales de polialanina que presentan puntas de tirosina y alanina ubicadas al azar.

Los linfocitos B con variación de clase son sometidos a elevados índices de mutación luego de la respuesta inicial 

En el capítulo 3 se planteó este concepto al tratar la generación de la diversidad.
El índice de mutación normal de la región V es del orden de 10 5/par de bases/división celular pero aumenta hasta 10 3/par de bases/generación en linfocitos B como consecuencia de la estimulación antigénica.
En la figura 7-9 se ilustra este proceso graficando la acumulación de mutaciones somáticas en la estructura de anticuerpo inmunodominante VH/VK durante la respuesta inmunitaria a la feniloxazolona.
A medida que pasa el tiempo y ante estimulaciones sucesivas se observa que el índice de mutaciones aumenta en forma notable, y en este contexto es de destacar que las mutaciones que se producen dentro de las asas hipervariables determinantes de complementariedad o adyacentes a ellas, pueden dar origen a células secretoras de anticuerpo con una afinidad de combinación diferente a la de la célula madre original.
En forma aleatoria, algunas de las células hijas muladas tendrán mayor afinidad por el antígeno, algunas presentarán la misma O menor y tal vez algunas carezcan de ella.
En forma similar, las mutaciones en las regiones de marco pueden ser "silenciosas", o, si perturban la capacidad de la molécula de plegarse en forma adecuada, darán origen a moléculas no funcionales.
Es importante destacar que las proporciones de linfocitos B del centro germinativo con mutaciones "silenciosas" son elevadas al comienzo de la respuesta inmunitaria pero descienden en forma notable a medida que transcurre el tiempo, lo que sugiere que la diversificación temprana es seguida por una expansión preferencial de clones que expresan las mutaciones que mejoran sus posibilidades de reaccionar y ser estimuladas por el antígeno.

Afinidad de los anticuerpos 

Efecto de la dosis de antígeno 

Si los demás factores se mantienen, la fuerza de unión de un antígeno por el anticuerpo receptor de superficie receptor de un linfocito B estará determinada por la habitual constante de afinidad de la reacción: xxx y los reactantes se comportarán de acuerdo con la Ley de Acción de Masas (véase p. 77) Es de suponer que cuando un número adecuado de moléculas de antígeno se une a los anticuerpos receptores de la superficie celular, el linfocito será estimulado para desarrollarse a un clon productor de anticuerpo.

Cuando se encuentran pequeñas cantidades de antígeno, sólo aquellos linfocitos con anticuerpos receptores de elevada afinidad serán capaces de ligar antígeno suficiente para que se produzca la estimulación, por lo que sus células hijas también producirán anticuerpo de gran afinidad.
Si se considera la ecuación de equilibrio del antígeno-anticuerpo se verá que a medida que aumenta la concentración de antígeno, aun anticuerpos con afinidad relativamente baja ligarán más antígeno; por lo tanto, a dosis elevadas de antígeno los linfocitos con anticuerpos receptores de menor afinidad serán también estimulados, y como se advierte en la figura 7-10, éstos son más abundantes que los que poseen receptores de elevada afinidad.
Además, es posible que las células con mayor afinidad fijen tanto antígeno que se transformen en tolerantes (véase p. 177).
En suma, las bajas cantidades de antígeno producen anticuerpos de elevada afinidad, mientras que las concentraciones altas de antígeno darán origen a un antisuero con afinidad baja a moderada.

Maduración de la afinidad 

Además de ser más repentinas y más notables, las respuestas secundarias tienden a presentar mayor afinidad.
Se cree que esta maduración de la afinidad luego de la estimulación primaria se debe a dos razones principales.
Primero, una vez que se inicia la respuesta primaria y que la concentración del antígeno declina a niveles bajos, sólo las células con afinidad cada vez mayor podrán fijar suficiente antígeno para mantener la proliferación.
Segundo, en esta etapa las células mutan con mucha rapidez en los centros germinativos y cualquier mutante con mayor afinidad se unirá mejor al antígeno sobre las células dendríticas foliculares y será seleccionado por su persistente expansión clonal.
El aumento de la mutación somática que ocurre pari passu con la maduración de la afinidad coincide con este análisis y se opone al argumento que postula que los clones de alta afinidad se originan sin mutación a partir de muy pequeñas cantidades de precursores en la población preinmune.
La modificación de la especificidad de anticuerpo por mutaciones puntuales permite la diversificación gradual sobre la cual puede actuar la selección positiva de la afinidad durante la expansión clonal; por otra parte, otros mecanismos tales como la conversión genética que produce cambios importantes, destruirán más probablemente la estructura ligadora del antígeno.

Es interesante notar que las respuestas a los antígenos timo-independientes que han desarrollado poco su memoria con muy escasas mutaciones no presentan este fenómeno de maduración de la afinidad.
En general, la capacidad de los linfocitos T helper de facilitar las respuestas ante antígenos activadores no poliméricos, no policlonales, de inducir la proliferación clonal expansiva, de efectuar la variación de clase y, por último, de generar respuestas finas con mayor afinidad han provisto al organismo de respuestas inmunitarias mayores, mejores y más flexibles.

RESPUESTA INMUNITARIA ADQUIRIDA 

Desarrollo 

Desarrollo de los linfocitos T 

La célula madre ( stem cell ) hematopoyética pluripotente da origen a los elementos figurados de la sangre 

La hematopoyesis se origina en el saco vitelino en forma temprana, pero a medida que avanza la embriogénesis esta función pasa a ser realizada por el hígado fetal y finalmente por la médula ósea donde permanece durante el resto de la vida.
Se ha demostrado que la célula madre hematopoyética que da origen a los elementos figurados de la sangre (fig. 9-1) es pluripotente, que siembra otros órganos y que se renueva a través de la creación de otras célula madre.
Por lo tanto, un animal puede estar protegido por completo contra los efectos letales de dosis elevadas de radiación por la inyección de células de la médula ósea que puedan repoblar sus sistemas linfoides y mieloides.
Las células madre se diferencian dentro del microambiente de células sésiles del estroma que producen diversos factores de crecimiento, como xxx , etc .
La importancia de esta interacción entre células madre indiferenciadas y el microambiente que guía su diferenciación se demuestra con claridad por estudios realizados con ratones homocigotos para mutaciones de los loci W o SI los cuales, entre otros defectos, presentan anemia macrocítica severa.
Los ratones W/W carecen de progenitores mieloides y pueden ser restablecidos por medio de la inyección de células madre de médula ósea normal, mientras que Sl/SI, con células madre normales pero un microambiente del estroma defectuoso, pueden corregirse por medio del trasplante de un fragmento de bazo normal (fig. 9-2).

Se han realizado numerosos intentos para aislar poblaciones altamente purificadas de células madre hematopoyéticas.
En el ratón, la célula más probable es la que presenta el siguiente fenotipo de superficie: alta expresión del CMH, baja positividad para xxx , positivo con claridad para xxx (reconocido por un anticuerpo monoclonal que reacciona con las células madre) y para la molécula de adherencia xxx , y negativo para xxx (marcadores para linfocitos B, macrófagos, granulocitos y linfocitos T citotóxicos, respectivamente).
Los bajos niveles de expresión de xxx pueden tener implicaciones en la etiología del SIDA.
Es notable que menos de 100 de estas células puedan prevenir la muerte de un animal irradiado con dosis letales.

El timo proporciona un medio para la diferenciación de los linfocitos T 

La glándula se organiza en una serie de lóbulos según agrupamientos de células epiteliales derivadas embriológicamente de un divertículo de células epiteliales del endodermo digestivo de la tercera bolsa faríngea y que forma zonas corticales y medulares bien definidas (fig. 9-3).
Los anticuerpos monoclonales formados contra células epiteliales tímicas revelan seis patrones de tinción por inmunofluorescencia, los que muestran marcada coherencia de especie.
Existe expresión antigénica compartida entre los epitelios subcapsular, perivascular y medular y antígenos diferentes en las células epiteliales corticales, una de las cuales es el receptor de xxx .
Este marco celular provee el microambiente para la diferenciación de los linfocitos T y produce una serie de hormonas peptídicas de las cuales cuatro han sido bien caracterizadas y estudiadas en su secuencia: timulina, xxx y xxx limosina, y timopoyetina (y su pentapéptido activo xxx ).
En apariencia estas hormonas son capaces de promover la aparición de marcadores de diferenciación de linfocitos T y varias funciones de linfocitos T sobre cultivos con células de médula ósea in vitro , pero aunque aparentemente actúan sobre los numerosos pasos de la diferenciación dentro del timo, aún no se cuenta con los detalles acerca de la forma en que operan y se integran in VIVO .

Las grandes células epiteliales especializadas de la corteza externa se conocen como células "nodrizas" dado que cada una de ellas puede asociarse con grandes cantidades de linfocitos que parecen estar ubicadas dentro de su citoplasma.
Las células epiteliales de la corteza profunda presentan procesos dendríticos ramificados ricos en CMH de clase II.
Se conectan por medio de desmosomas para formar una red por la cual deben pasar los linfocitos corticales en su camino hacia la médula (fig. 9-3) Los linfocitos corticales presentan un denso empaquetamiento si se los compara con los de la médula, muchos se encuentran en división y gran número está muriendo, según se evidencia por sus núcleos picnóticos.
En su camino hacia la médula los linfocitos pasan por un cordón de macrófagos "centinela" en la unión corticomedular.
En la médula se encuentran numerosas células dendríticas interdigitantes derivadas de la médula ósea y las células epiteliales presentan procesos más anchos que sus contrapartidas corticales y expresan altos niveles de CMH de clases I y II.
Agregados enredados y posiblemente degenerados de células epiteliales forman dos característicos corpúsculos de Hassall, tan conocidos por los histopatólogos.

Figura 9-1

Célula madre ( stem cell ) hematopoyética multipotencial y su progenie

Aún resta descubrir la compleja relación que existe con el sistema nervioso: el timo presenta rica inervación con fibras adrenérgicas y colinérgicas, mientras que dos neurotransmisores oxitocina, vasopresina y neurofisina son sintetizados de forma endógena por las células epiteliales subcapsulares, perivasculares y medulares y por las células "nodrizas".
Un estado de estrés agudo conduce a una pérdida rápida de timocitos corticales y a un incremento de las células epiteliales que expresan marcadores corticales y medulares: se cree que son células madre epiteliales intratímicas.
La destrucción de timocitos corticales se debe, al menos en parte, a la acción citolítica de los esteroides, y la invulnerabilidad relativa de los linfocitos medulares se atribuye a que poseen una xxx hidroxil esferoide deshidrogenasa.
La diferente naturaleza de los dos compartimientos principales de la glándula es enfatizada por la atrofia selectiva inducida por numerosos agentes: así, el blanco primario de la organotina es el timocito cortical inmaduro, la dioxina interactúa con un receptor sobre las células epiteliales corticales, mientras que el agente inmunosupresor ciclosporina A provoca atrofia de todos los elementos medulares, con lo que bloquea la diferenciación de timocitos corticales en medulares con consecuencias aún desconocidas.

Figura 9-2

La hematopoyesis requiere de células madre pluripotentes de médula ósea normales que se diferencien en un microambiente normal.

(El locus W codifica el oncogén c-kit, un receptor de membrana para tirosina quinasa sobre la célula madre; se supone que el locus S1 codifica el ligando para este receptor).

Figura 9-3

Características celulares de un lóbulo tímico.

En general se considera que el timo involuciona con la edad, pero esto puede ser relativo, dado que se encuentran timos relativamente grandes en autopsias de adultos, mientras que es posible que la involución de otros se deba al estrés asociado con la enfermedad previa a la muerte.

Las células madre ( stem cells ) de la médula ósea se convierten en linfocitos T inmunocompetentes en el timo 

La evidencia proviene de experimentos realizados sobre la reconstitución de huéspedes irradiados.
Un animal irradiado se restablece a través de trasplantes de médula ósea con la inmediata restitución de precursores de granulocitos; en el largo plazo también se produce la reconstitución de los linfocitos T y B destruidos por la irradiación.
Sin embargo, si el animal es timectomizado antes de la irradiación, las células de la médula ósea no reconstituyen la población de linfocitos T (fig. 9-4).

En los días 11-12 de vida del embrión de ratón las células madre linfoblastoides provenientes de la médula ósea comienzan a colonizar la periferia del rudimento de timo epitelial.
Si se elimina el timo en esta etapa y se lo incuba en cultivo orgánico se genera una amplia variedad de linfocitos T maduros.
Esto no se observa si se cultivan timos de 10 días, lo que demuestra que los colonizadores linfoblastoides clan origen a la progenie de pequeños linfocitos inmunocompetentes.

La diferenciación se acompaña de cambios en los marcadores de superficie 

El protimocito que penetra en el timo atraído por la timotaxina, un factor quimiotáctico, se colorea positivamente para la enzima TdT (fig. 9-5) que se cree que está involucrada en la inserción de las secuencias de nucleótidos de la región N-terminal de los segmentos D y J de la región variable para aumentar la diversidad de los receptores de los linfocitos T (véase p. 69).
Pronto los timocitos corticales expresan los marcadores de superficie xxx y xxx y, bajo la influencia de xxx y tal vez por compromiso de xxx con xxx sobre las células del estroma se activa el gen codificador de xxx .

Esto a su vez induce la expresión del receptor de xxx completo y se inicia una masiva expansión autocrina de los timocitos corticales.
A continuación aparecen xxx y xxx junto con xxx el invariable complejo transductor de señal del receptor del linfocito T y luego das células comienzan a expresar las cadenas variables del receptor del linfocito T. En esta etapa las células no se colorean para xxx o xxx de superficie, los marcadores de subtipos helper/inductor y citotóxico/supresor respectivamente, pero luego, las células que han reorganizado sus genes para el receptor de linfocitos xxx pasan a ser dobles positivas para xxx .
Por último las células atraviesan &rsqb;a unión corticomedular hacia la médula donde se segregan los marcadores xxx y xxx a la vez que se diferencian en poblaciones inmunocompetentes separadas de precursores de linfocitos T helper y linfocitos T citotóxicos (fig. 9-5).

Figura 9-4

Maduración de células madre de médula ósea bajo la influencia del timo para transformarse en linfocitos inmunocompetentes capaces de producir reacciones inmunitarias mediadas por células.

La radiación con rayos X (X) destruye la capacidad de los linfocitos de huésped de montar una respuesta inmunitaria celular pero las células madre de la médula ósea inyectada pueden transformarse en inmunocompetentes y restablecer la respuesta (1) a menos que se elimine el timo (2), en cuyo caso sólo son efectivos los linfocitos ya inmunocompetentes (3).
Las células madre pluripotentes de médula ósea también restablecen los niveles de otros elementos figurados de la sangre (eritrocitos, plaquetas, neutrófilos, monocitos) que de otra manera descienden en forma dramática luego de la irradiación con rayos X, y este tratamiento es crucial en los casos en que la exposición accidental o terapéutica a rayos X u otros agentes anti-mitóticos lesiona las células hematopoyéticas.

Aún se tienen dudas sobre la estirpe precisa de las células NK.
Expresan los marcadores xxx que en general están restringidos a dos linfocitos T contienen receptores de xxx que inducen su proliferación y son capaces de producir xxx .
Sus genes V para el receptor T no están reorganizados pero deben estar relacionados de alguna manera con los linfocitos T.

Reorganización del receptor 

La reorganización de los genes de las regiones V, D, J y C requerida para generar el receptor del linfocito T (p. 65) aún no se ha producido en &rsqb;a etapa de protimocito.
El gen xxx para TdT y el gen activador de recombinasa se transcriben en la etapa de pre-T y alrededor del día 15, pueden detectarse células con xxx en el timo de ratón seguido alrededor del día 19 por la aparición de células xxx (fig. 9-6).
Los ratones que expresan transgenes xxx reorganizados no reorganizan otros segmentos de genes lo que indica que una vez que la cédula expresa una combinación determinada de V, D, J suprime la reorganización de los genes xxx de la cromátide hermana (debe recordarse que cada célula contiene dos cromosomas para cada acúmulo xxx , uno proveniente de cada progenitor).
De este modo cada célula expresa sólo un único receptor xxx y el proceso por e&rsqb; cual los genes homólogos de la cromátide hermana se suprimen se denomina exclusión alélica (véase p 176).
Construcciones transgenéticas con xxx más cortas, que carecen de una región "silenciadora" de configuración CIS presentan una importante acción inhibidora sobre la generación de clones de linfocitos xxx mientras que aumenta la cantidad de células xxx .

A diferencia de lo que ocurre en el Ser humano, en el ratón los linfocitos xxx predominan en asociación con células epiteliales.
Una característica notable de las células que abandonan el timo fetal es la restricción de la utilización de genes V.
Así casi toda la primera ola de células xxx fetales expresan los mismos genes V y colonizan la piel; la segunda ola utiliza la misma combinación de genes xxx pero un par xxx diferente y ellos siembran los órganos reproductores femeninos (fig. 9-7).
En la vida adulta existe una diversidad mucho mayor de receptores debido a un alto grado de variación de la unión (véase p. 66) aunque las células intraepiteliales del intestino y las del tejido linfoide encapsulado forman dos grupos diferentes en lo que respecta al uso de genes V.
Por otra parte los linfocitos xxx son mucho más heterogéneos en la expresión del gen V. 

Los linfocitos son seleccionados en forma positiva para la restricción del CMH propio en el timo 

La capacidad de los linfocitos T para reconocer péptidos antigénicos asociados con el CMH propio se desarrolla el timo.
Si un animal xxx es sensibilizado contra un antígeno los linfocitos T imprimados pueden reconocer ese antígeno sobre células presentadoras de antígeno del haplotipo xxx , es decir, son capaces de utilizar cualquiera de dos haplotipos de los progenitores como elemento restrictivo de reconocimiento.
Sin embargo si se utilizan células de médula ósea del xxx para reconstituir un xxx irradiado timectomizado previamente y al que se implanta un timo xxx , los linfocitos T imprimados con posterioridad solo pueden reconocer antígenos en el contexto de xxx pero no de xxx (Fig. 9-8).
De este modo el fenotipo del timo es el que imprime la restricción xxx sobre los linfocitos T en diferenciación.

En la figura 9-8 se observa que la incubación del implante de timo con desoxiguanosina, que destruye las células de estirpe de macrófagos y dendríticas, no tiene efecto en el entrenamiento linfocitario, lo que sugiere que esta función es cumplida por las células epiteliales.

Estas células son ricas en moléculas del xxx de superficie y el concepto actuad sostiene que dos receptores dobles positivos ( xxx ) de los linfocitos T que cargan receptores que reconocen el CMH propio sobre las células epiteliales son seleccionadoS para diferenciarse en células positivas xxx .
La evidencia de esto proviene de ratones transgénicos en dos cuales los genes adecuados se introducen de manera artificial en huevos fertilizadoS.

Dado que se trata de un área muy activa, es interesante citar algunos ejemplos experimentales.

Figura 9-5

Diferenciación de linfocitos T dentro de la glándula del timo.

Los números se refieren a la designación CS.
TCR1 , receptores xxx , TCR2, receptores xxx .

TdT desoxinucleotidil transferasa terminal.

Un estudio muy sofisticado parte de un clon de linfocitos T citotóxicos obtenido por hembras xxx cruzadas con machos de la misma cepa.
El clon reconoce el antígeno del macho, xxx , y se observa en asociación con las moléculas del CMH propios xxx , es decir, reacciona con el complejo xxx .
Se introducen entonces los clones xxx que forman el receptor de linfocito T de este clon como transgenes en ratones con inmunodeficiencia combinada severa (SCID) que carecen de la capacidad para reorganizar sus propios genes receptores de región variable de Línea bacteriana; de esta forma el único receptor de linfocito T que podrá ser expresado será el del transgén, siempre que se trabaje con hembras en lugar de machos, en los cuales el clon sería eliminado por autorreactividad.
Si las hembras SCID transgénicas contienen el haplotipo original xxx (por ejemplo híbridos xxx entre haplotipos xxx ) el receptor anti xxx estará ampliamente expresado en los linfocitos precursores citotóxicos xxx (cuadro 9-1a), mientras que las transgénicas xxx que carecen de xxx producen sólo timocitos xxx dobles sin linfocitos simples xxx .
Así cuando las células xxx expresan su transgén TCR sólo se diferencian a linfocitos inmunocompetentes xxx si entran en contacto con células epiteliales tímicas del haplotipo de CMH reconocidas por su receptor.
Se dice que estos timocitos de autorreconocimiento tienen selección positiva .

Figura 9-6

Aparición de linfocitos T positivos para receptores xxx maduros en el timo a medida que transcurre el tiempo.

(Datos reproducidos de Tonegawa y col (1989) Progress in Immunology 7, 243.
Melchers F. y (eds.) Springer Verlag, Berlin).

En otro ejemplo, los genes que codifican un receptor xxx de un clon de linfocitos T helper ( xxx ) que responde a citocromo c de polilla asociado con la molécula xxx de clase II (recuérdese que xxx tiene una cadena a y una p) son transferidos a ratones xxx . xxx expresa la molécula de I-E sobre la superficie de las células presentadoras de antígeno, pero xxx no.

En el estudio, la frecuencia de linfocitos xxx circulantes con el receptor xxx era 10 veces mayor en el xxx en relación con las cepas xxx , considerando nuevamente la selección positiva de timocitos dobles positivos que reconocen su propio CMH tímico.
La selección positiva sólo ocurre en ratones manipulados para expresar I-E sobre sus células epiteliales corticales, en lugar de las medulares, lo que demuestra que este paso de la diferenciación es afectado antes de que los timocitos en desarrollo alcancen la médula.

INMUNIDAD CONTRA LA INFECCIÓN 

Profilaxis e inmunodeficiencia 

El control de la infección se intenta por muchos caminos.
En Gran Bretaña se ha logrado romper la cadena de infección de la rabia y la psitacosis por medio del control de la importación de perros y loros, respectivamente.
Los avances realizados en la salud pública -agua corriente, sistemas cloacales, educación en la higiene personal- previenen la diseminación del cólera y de muchas otras enfermedades.
Cuando fallan estas medidas, se depende de la inducción de la inmunidad.

Inmunidad adquirida pasivamente 

Es posible establecer una protección temporaria contra la infección por medio de la administración de anticuerpos preformados provenientes de otro individuo de la misma especie o de especies diferentes.
A medida que los anticuerpos adquiridos se utilizan por combinación con el antígeno o son catabolizados por la vía normal, esta protección poco a poco se pierde.

Las globulinas equinas que contienen anticuerpos antitoxina tetánica y antitoxina diftérica han sido utilizadas de forma extensa como profilaxis, pero en la actualidad esta práctica es más restringida debido a la complicación que representa la enfermedad del suero que se desarrolla en respuesta a la proteína extraña.

Esto ocurre con mayor frecuencia en sujetos ya sensibilizados por contacto previo con la globulina equina; por lo tanto, a los individuos a quienes se les han aplicado anticuerpos antitetánicos de caballo (por ejemplo para la protección inmediata luego de sufrir una herida en espacios abiertos) se les sugiere iniciar un curso de inmunización activa a fin de obviar la necesidad de inyecciones posteriores de proteína equina en cualquier emergencia posterior.

Anticuerpos cedidos por la madre 

En los primeros meses de vida, mientras se está desarrollando lentamente el sistema linfoide propio del bebé, éste obtiene su protección por anticuerpos derivados de la madre y adquiridos por transferencia placentaria y absorción intestinal de las inmunoglobulinas del calostro.
La principal inmunoglobulina de la leche es la xxx secretora que no es absorbida por el bebé pero permanece en el intestino para proteger las superficies mucosas.
Es notable que los anticuerpos xxx estén dirigidos contra antígenos bacterianos y virales que a menudo se encuentran en el intestino, y se presume que las células productoras de xxx , en respuesta a antígenos del intestino, migran y colonizan el tejido mamario (como parte del sistema inmunitario MALT; p. 112) y los anticuerpos producidos aparecen en la leche.

Concentrados de xxx globulina humana 

Los preparados concentrados de xxx globulina humana de adultos resultan valiosos para modificar los efectos de la varicela y el sarampión, en particular en individuos con respuestas inmunitarias defectuosas, como los recién nacidos prematuros, los niños con inmunodeficiencia primaria o desnutrición proteica o los pacientes en tratamiento con esteroides.
Cuando existen contactos con casos de hepatitis infecciosa y viruela también debe administrarse protección con gammaglobulina, en especial cuando en el último caso el material derive del suero de individuos vacunados unas semanas antes.

Debe preferirse el uso de inmunoglobulina antitetánica humana en lugar de la equina que puede causar reacciones del suero.

Los preparados aislados de xxx globulina tienden a formar pequeños agregados de forma espontánea y éstos pueden producir severas reacciones anafilácticas cuando se administran por vía intravenosa debido a su capacidad de agregar las plaquetas y de activar el complemento y generar anafilotoxinas xxx y xxx .
Por esta razón el material siempre se inyecta por vía intramuscular.
Existen preparados libres de agregados, y los concentrados aislados con títulos de anticuerpo elevados contra microorganismos seleccionados, como vacuna, Herpes zoster , tétanos y quizá rubéola, serían muy bienvenidos.
Esta necesidad podría satisfacerse si se lograra producir anticuerpos monoclonales humanos a demanda.

Cultivo de anticuerpos específicos 

Las técnicas para producir anticuerpos monoclonales humanos con especificidades predeterminadas aún dejan mucho que desear pero mejoran de forma permanente.
Se emplea la tecnología de DNA recombinante para obtener por ingeniería genética los anticuerpos de muy elevada afinidad.
Se describen diferentes enfoques de la producción de anticuerpos que tienden a evitar la necesidad de la intervención de un sistema inmunitario huésped tales como las construcciones de cadena única xxx y los anticuerpos de dominio único xxx .
Dado que estos últimos son muy pequeños se cree que podrían alcanzar los receptores celulares de los virus ubicados en el fondo de los canales proteicos inaccesibles al xxx de un anticuerpo intacto.

Debería ser posible generar afinidades elevadas a partir de estos anticuerpos de dominio único y, en un experimento, las mutaciones al azar de cuatro residuos centrales del xxx de antilisozima dieron origen a un mutante, dentro de un gran número de ellos, con mayor afinidad que el original.

Vacunación 

Inmunidad de poblaciones 

En el caso del tétanos, la inmunización activa es beneficiosa para el individuo pero no para la comunidad, dado que no elimina el microorganismo formado en las heces de los animales domésticos y que persiste en el suelo bajo la forma de esporos resistentes.
Cuando una enfermedad depende de la transmisión humana la inmunidad de sólo una proporción de la población puede ayudar a toda la comunidad si conlleva a un descenso del índice de reproducción (es decir, de la cantidad de casos producidos por cada individuo infectado) a un valor menor de uno; en estas circunstancias la enfermedad desaparecerá, como lo atestigua, por ejemplo, la desaparición de la difteria en comunidades en las cuales alrededor del 75% de los niños han sido inmunizados (Fig.

11-1).
Por el contrario, se han observado brotes focales de poliomielitis en comunidades que se oponen a la inmunización por razones religiosas.

Figura 11-1

Notificación de la difteria en Inglaterra y Gales por población de 100000 personas que muestra un espectacular descenso luego de la inmunización.

(Reproducción de Immunisation por G Dick, 1978, Update Books; con autorización del autor y los editores).

Consideraciones estratégicas 

EL objeto de la vacunación consiste en brindar inmunidad efectiva por medio del establecimiento de niveles adecuados de anticuerpo y una población imprimada de linfocitos que puedan expandirse con rapidez ante un contacto renovado con el antígeno y así proveer de protección contra la infección.
En ocasiones como ocurre con la poliomielitis, se requiere un titulo sanguíneo elevado de anticuerpo; en enfermedades micobacterianas tales como la TBC resulta más efectiva una inmunidad mediada por células activadora de macrófagos, mientras que en el caso de la infección por virus influenza es probable que desempeñen un papel importante los linfocitos T citotóxicos.
El sitio de la respuesta inmunitaria evocada por la vacunación también puede ser importante.
Por ejemplo, en el cólera los anticuerpos deben localizarse en la luz del intestino para inhibir la adherencia y la colonización de la pared intestinal.

La erradicación del agente infeccioso no siempre es la meta más práctica.
En el caso del paludismo, la forma hematógena libera moléculas que desencadenan TNF y otras citoquinas provenientes de monocitos y la secreción de estos mediadores es responsable de los efectos desagradables de la enfermedad.
Por lo tanto, una respuesta de anticuerpos dirigida hacia estos antígenos liberados con epitopes de estructura conservada puede representar una estrategia más realista que intentar atrapar al mismo parásito más evasivo captador de antígeno.
En estas condiciones puede ser más aceptable la convivencia con el parásito.

Además de la capacidad de engendrar inmunidad efectiva deben satisfacerse ciertas condiciones mundanas, pero igualmente cruciales, al buscar una vacuna exitosa.

Los antígenos deben poder obtenerse con facilidad, el preparado debe ser estable en condiciones climáticas extremas, de preferencia sin requerir refrigeración y la vacuna debe ser económica y por cierto, segura.
Es obvio que el primer contacto con el antígeno durante la vacunación no debe ser lesivo y la maniobra intenta evitar los efectos patógenos de la infección mientras se mantienen los inmunógenos protectores.

Microorganismos muertos como vacunas 

La forma más simple de destruir la capacidad de los microorganismos de causar la enfermedad manteniendo su constitución antigénica es prevenir su replicación matándolos de manera adecuada.
Los helmintos parasitarios, y en menor grado los protozoarios son muy difíciles de criar a granel a fin de manufacturar vacunas con microorganismos muertos.
Este problema no se plantea con muchas bacterias y virus y, en estos casos, los microorganismos inactivados por lo general han dado origen a antígenos seguros para la inmunización.

Ejemplos de ello son las vacunas contra la liebre tifoidea (en combinación con las relativamente inefectivas paratifoideas A y B), el cólera y la poliomielitis con virus muertos (Salk).
El éxito de la vacuna Salk fue oscurecido en parte por un ligero aumento en la incidencia de decesos por poliomielitis en 1960-61 (fig. 11-2), aunque en la actualidad este hecho ha sido atribuido a la pobre antigenicidad de una de las tres cepas diferentes de virus utilizados, y las vacunas de hoy son mucho más potentes.
Deben adoptarse medidas para asegurar que no se destruyan antígenos protectores importantes en el proceso de inactivación.
Durante la producción de una vacuna temprana con virus muertos contra el sarampión se inactivó el antígeno de fusión que permite la diseminación celular del virus; como consecuencia de ello se obtuvo inmunidad incompleta, por lo que el individuo era susceptible al desarrollo de complicaciones inmunopatológicas ante la infección natural posterior.
Los peligros de la inmunidad incompleta son especialmente preocupantes en sitios en los que el sarampión es endémico y la respuesta inmunitaria está debilitada por la desnutrición proteica.

Dado que la corrección amplia de esta deficiencia en la dieta es poco probable en un futuro cercano, debe considerarse si la estimulación no específica por acción de drogas inmunopotenciadoras u hormonas símicas en el momento de la vacunación podría brindar una solución factible.

Esta propuesta de suplementar una respuesta inmunitaria adaptativa deficiente con algún tratamiento sinérgico ha aparecido en otros contextos.
Así, el antibiótico polimixina B es demasiado tóxico para su uso normal; sin embargo, si se eliminan ciertos grupos terminales, la molécula pierde su toxicidad pero aún retiene su capacidad de actuar sobre la pared exterior de las bacterias gramnegativas, por lo que permite que anticuerpos potencialmente líticos y el complemento lleguen hasta las membranas internas de la bacteria, de otro modo inaccesibles.
Otro fenómeno curioso que puede ser explotado es la observación de que la ameba Entamoeba histolytica , resistente a la lisis por acción del anticuerpo y el complemento, presenta una susceptibilidad muy aumentada si se trata con un inhibidor proteico no tóxico en otros aspectos.

Los microorganismos vivos atenuados presentan muchas ventajas como vacunas 

El objetivo de la atenuación es producir un microorganismo modificado que simule el comportamiento natural del microbio original sin causar enfermedad significativa.
En muchos casos la inmunidad conferida por vacunas con gérmenes muertos, incluso cuando son administradas con un adyuvante (véase más adelante), a menudo es inferior a la que resulta de la infección por microorganismos vivos.
Esto en parte se debe a que la replicación de los microorganismos vivos confronta al huésped con una dosis mayor y más sostenida del antígeno y a que, con los virus brotantes, se requieren células infectadas para el establecimiento de una buena memoria de linfocitos T citotóxicos.
Otra ventaja significativa de la utilización de microorganismos vivos es que la respuesta inmunitaria se produce en su mayor parte en el sitio de la infección natural.
Esto es bien ilustrado por la respuesta de xxx nasofaríngea ante la inmunización con vacuna contra la poliomielitis.
A diferencia de la poca efectividad de la inyección parenteral de vacuna con gérmenes muertos, la administración intranasal provocó una buena respuesta local de anticuerpos; empero, mientras que ésta declinaba luego de transcurridos unos 2 meses, la inmunización oral con virus vivos atenuados estableció un nivel de anticuerpos xxx persistentemente elevado (fig. 11-3).

Figura 11-2

Notificación de parálisis por poliomielitis en Inglaterra y Gales que muestra los efectos beneficiosos de la inmunización comunitaria con vacunas de virus muertos y vivos.

(Reproducida de Immunisation por G Dick, 1978, Update Books con autorización del autor y los editores).

Métodos clásicos de atenuación 

El objetivo de la atenuación. la producción de un microorganismo capaz de causar sólo una forma muy leve de la enfermedad natural, puede obtenerse también utilizando cepas que son virulentas para otras especies pero avirulentas para el ser humano.
El mejor ejemplo de esto lo constituye la notable demostración de Jenner de que la viruela vacuna protegía contra la viruela humana.
Desde entonces, un esfuerzo global realizado por la Organización Mundial de la Salud combinando vacunación amplia y métodos selectivos de control epidemiológico. ha erradicado la enfermedad humana.

Figura 11-3

Respuesta local de xxx a la vacuna contra la poliomielitis.

La síntesis local del anticuerpo secretor está confinada a los sitios anatómicos específicos que han sido estimulados de manera directa por contacto con el antígeno.
Datos obtenidos de Ogra P L y col &lsqb;1975&rsqb;.
En Viral Immunology and Immunopathology , p 67.
Notkins A L &lsqb;comp&rsqb; .
Academic Press, Nueva York).

La atenuación en sí puede obtenerse por medio de la modificación de las condiciones en las cuales crece el microorganismo.
Pasteur logró la producción de formas vivas avirulentas de bacilos del cólera y del ántrax en pollos por medio de artificios tales como cultivos a mayor temperatura y bajo condiciones de anaerobiosis, y pudo conferir inmunidad por infección con microorganismos atenuados.
Una cepa virulenta de Mycobacterium tuberculosis se atenuó por casualidad en 1908, cuando Calmette y Guérin, del Instituto Pasteur en Lille, agregaron bilis al medio de cultivo con la finalidad de obtener un crecimiento disperso.
Después de 13 años de cultivo en medios que contenían bilis la cepa permanecía atenuada y se utilizó con éxito en la vacunación de niños contra la tuberculosis.
El mismo microorganismo, BCG (Bacilo, Calmette, Guérin), se utiliza ampliamente hoy en día para la inmunización de individuos tuberculina-negativos; además, confiere un grado razonable de protección contra el Mycobacterium leprae siempre que la respuesta a los antígenos de reacción cruzada del grupo y no haya sido subvertida por exposición a especies supresoras de microbacterias en el medio local.
La atenuación por adaptación al frío del virus influenza y otros virus respiratorios parece promisoria; dos microorganismos pueden crecer a temperaturas menores (32-34.º C) en el tracto respiratorio superior, pero no producen enfermedad clínica por su incapacidad de replicarse en el tracto respiratorio inferior (37.º C).

Atenuación por tecnología de DNA recombinante 

La recombinación genética se está usando para desarrollar varias cepas atenuadas de virus tales como el de la influenza con virulencia menor en el hombre y algunos con mayor índice de multiplicación en huevos (lo que permite que las nuevas cepas endémicas de la influenza se adapten para la rápida producción de vacuna).
El potencial es muy importante.

Es probable que el tropismo de los microorganismos atenuados hacia el sitio en que ocurre la infección natural se explote en un futuro cercano para establecer inmunidad intestinal contra la fiebre tifoidea y el cólera utilizando formas atenuadas de cepas de Salmonella y de Vibrio cholerae en las cuales los genes de virulencia se hayan identificado y modificado por ingeniería genética.

Vectores microbianos para otros genes 

Un subterfugio ingenioso consiste en utilizar un virus como transportador de genes provenientes de otro virus en particular uno que no pueda cultivarse con éxito, o que sea potencialmente peligroso.
Virus con DNA grandes tales como el de la vacuna pueden actuar como transportadores de uno o mas genes extraños al mismo tiempo que retienen su infectividad para animales y células cultivadas.
Las proteínas codificadas por estos genes se expresan de manera adecuada con respecto a la glucosilación y la secreción y se procesan por presentación del CMH por las células infectadas, lo que da origen a reacciones efectivas de inmunidad mediada por células y humoral.
Un ejemplo de una construcción en la cual la vacuna es un vector para un gen extraño insertado se describe en la figura 11-4.

Una amplia variedad de genes ha sido expresada por medio de vectores de virus de la vacuna y se ha demostrado que los productos de los genes codificadores de las proteínas de la cubierta virar. como la hemaglutinina del virus influenza, la glucoprotema del virus de la estomatitis vesicular, la xxx del HIV-I y la glucoproteína D del virus del herpes simple, podían ser procesados de forma correcta e insertados en la membrana plasmática de células infectadas.
El antígeno de superficie de la hepatitis (HBsAg) fue secretado por células infectadas por virus de vacuna recombinantes bajo la forma característica de partículas de 22 nm .

Esto representa un importante enfoque del problema y ha sido posible proteger a chimpancés contra los efectos clínicos del virus de la hepatitis B, mientras que ratones inoculados con la hemaglutinina recombinante de la influenza generaron linfocitos T citotóxicos y resultaron protegidos contra infecciones por virus influenza.
Se obtuvieron títulos espectaculares de anticuerpos neutralizantes por medio de una recombinación con el gen codificador de la glucoproteína del virus de la rabia y protegieron a los animales contra los efectos intracerebrales severos.
Ha sido posible incluso crear un vector con dos inserciones.

Ningún sistema está libre de inconvenientes y algunos recombinantes crecen poco in vivo .
Además, los individuos inmunodeficientes tienen dificultades para eliminar los virus, aunque la resistencia de ratones desnudos carentes de linfocitos T a 10 8 unidades formadoras de colonias de vacunas recombinantes que expresan el gen IL-2 sugiere la posibilidad de que exista una solución al problema.
También se presenta la objeción de que una vacuna virar que produce efectos colaterales ocasionales pero severos no debería utilizarse en un mundo libre de viruela.
Es posible que se desarrollen cepas menos virulentas, pero en todo caso el tema en cuestión es si la vacuna recombinante causa más problemas que la enfermedad para la cual se busca la vacuna.
Por supuesto, no existen problemas en su uso veterinariO y se han obtenido resultados excelentes en ganado vacuno.

Aunque se basen en los mismos principios que los utilizados para el virus de la vacuna, es posible que sea más fácil aceptar virus híbridos de la poliomielitis como vacunas potenciales, y genes del HIV, del virus de la hepatitis A y de la enfermedad del pie y la boca han sido insertados en el genoma del virus de la poliomielitis y se demostró que producen anticuerpos neutralizantes.

Figura 11-4

Construcción de recombinantes de virus de la vacuna que expresan un gen extraño.

El gen en cuestión, por ejemplo el codificador del antígeno de superficie de la hepatitis xxx , se inserta primero en un vector adecuado para que se ubique en las adyacencias de un promotor de vacuna y flanqueando las secuencias de DNA de la vacuna (en este caso la timidina quinasa-TK ) que determina el sitio de recombinación con el virus.
Se replica el plásmido y luego se utiliza para transfectar células que se infectan simultáneamente con vacuna.
La recombinación homóloga inserta el promotor más el gen extraño en el genoma viral y el recombinante resultante TK es seleccionado por siembra en placas virales resistentes a 5-bromodesoxiuridina (BUdR).
TK no es esencial para el crecimiento virar.
(Reproducida de B Moss &lsqb;1985&rsqb; Immunology Today 6, 243, con autorización).

Se ha pensado en utilizar BCG como vehículo de los antígenos requeridos para evocar la inmunidad de linfocitos T mediada por xxx .
El microorganismo es avirulento, presenta baja frecuencia de complicaciones serias, puede administrarse en cualquier momento después del nacimiento, posee fuertes propiedades adyuvantes, confiere IMC de larga duración luego de una única inyección y es muy económico.
El desarrollo de vectores de dos vías que pueden replicarse en E. coli como plásmidos y en microbacterias como fagos ha permitido que se introduzca DNA extraño en cepas de vacuna de M. smegmatis y de BCG.
Se han creado grandes expectativas en este campo: la incorporación de un gen de resistencia a la kanamicina dentro del plásmido proporciona un marcador selectivo para la bacteria transformada, mientras que la inclusión de una secuencia de señal permite la secreción de una proteína recombinante.

Es posible que la vía oral de vacunación pueda aplicarse no sólo al establecimiento de inmunidad en la mucosa del estómago sino también para proveer protección sistémica.
Por ejemplo, Salmonella typhimurium no sólo invade la mucosa que tapiza el intestino sino que también infecta células del sistema de fagocitos mononucleares en todo el organismo, por lo que estimula la producción de anticuerpos humorales y secretores además de inmunidad mediada por células.

Puede lograrse que la Salmonella atenuada exprese proteínas de Shigella , cólera, paludismo, esporozoítos, etc., y es muy posible que puedan considerarse vacunas orales en potencia.
Así, la construcción antigénica del circunsporozoíto administrada por vía oral a ratones inhibe el desarrollo del parásito en sus etapas hepáticas; dado que esto se produjo en ausencia de anticuerpo, se considera posible que la IMC resulte efectiva.

Restricciones en el uso de vacunas atenuadas 

Las vacunas atenuadas contra la poliomielitis (Sabin), el sarampión y la rubéola han obtenido aceptación general.
Sin embargo, con ciertas vacunas existe un riesgo muy pequeño, aunque real, de que se desarrollen complicaciones tales como la encefalitis que puede aparecer después de la inmunización contra el sarampión; no obstante, debe considerarse que la infección natural conlleva un riesgo de encefalitis mucho mayor (c 1:2.000).
Si se utilizan vacunas de virus vivos existe cierta posibilidad de que el ácido nucleico pueda ser incorporado al genoma del huésped o de que haya reversión a una forma virulenta, aunque esto es poco probable si la cepa atenuada contiene varias mutaciones.

Otra desventaja de las cepas atenuadas es la dificultad y el costo que implica mantener condiciones adecuadas de almacenamiento en Frío, sobre todo en lugares aislados.

En enfermedades tales como la hepatitis virar y el cáncer los peligros asociados con las vacunas de virus vivos hacen impensable su uso.
Nunca se subrayará lo suficiente que los riesgos de complicación deben sopesarse contra el riesgo esperado de contraer la enfermedad con sus propias complicaciones.
Cuando éstas son mínimas puede preferirse evitar una vacunación general y confiar en un curso natural apoyado, en caso necesario, por inmunización pasiva en las localidades situadas alrededor de los brotes aislados de la enfermedad infecciosa.

Es importante reconocer a los niños con inmunodeficiencia antes de inyectar microorganismos vivos; un niño con reactividad disminuida de linfocitos T puede ser superado por el BCG y morir.
Tal vez esto sea poco probable. pero se dice que en un país en particular no existían adultos con deficiencia de células T.
¿Cuál es la razón?
Todos los niños habían sido inmunizados con BCG vivos como parte de un programa de salud comunitario.
Aún queda por determinar hasta qué punto los niños con deficiencias parciales se encuentran en situación de riesgo.
Se desaconseja además, administrar vacunas de virus vivos a pacientes en tratamiento con esteroides, agentes inmunosupresores o radioterapia y quienes padezcan enfermedades malignas tales como linfomas y leucemia; deben incluirse también las mujeres embarazadas por la vulnerabilidad del feto.

En un capítulo anterior se trató la influencia desviatoria del anticuerpo xxx derivado de la madre.
La inyección de anticuerpo xxx (monoclonal) preformado en el momento de la inmunización con una vacuna contra el paludismo parece haber superado este inconveniente en ratones jóvenes, pero aún queda por determinar si esto puede transformarse en una estrategia práctica.
Los resultados preliminares sugieren que los niños de 4-6 meses de edad pueden ser o convertirse por inhalación de vacuna contra el sarampión en aerosol, que se presume que evita el anticuerpo materno; esto tendrá singular importancia en zonas endémicas para el sarampión, donde se requieren condiciones muy precisas con inmunización convencional a medida que desaparece el anticuerpo adquirido de forma pasiva.

TRASPLANTES 

Rechazo de los injertos 

Durante mucho tiempo, uno de los objetivos de la medicina fue el reemplazo de los órganos enfermos por un trasplante de tejido sano, que se vio frustrado en gran parte por la reacción del organismo que rechazaba los implantes provenientes de otros individuos.
Antes de tratar la naturaleza y las implicaciones de este fenómeno de rechazo, es útil definir los términos utilizados en los trasplantes entre individuos y especies: 

Autotrasplante o autoinjerto : tejido reimplantado en el donante original.

Isotrasplante o isoinjerto: implante entre individuos singeneicos (es decir de constitución genética idéntica) tales como los gemelos idénticos o ratones de la misma cepa pura.

Alotrasplante o aloinjerto (término antiguo, homotrasplante): implante entre individuos alogénicos (es decir, miembros de la misma especie pero diferente constitución genética), por ejemplo de hombre a hombre y de una cepa de ratón a otra.

Xenotrasplante o xeroinjerto (heterotrasplante): implante entre individuos xenogénicos (es decir, individuos de diferente especie), por ejemplo de cerdo a ser humano.

Se ha estudiado con mayor intensidad la reacción de alotrasplante, aunque en algún momento se espera poder utilizar otras especies.
El procedimiento más frecuente de alotrasplante es probablemente la transfusión sanguínea, en la cual son bien conocidas las desafortunadas consecuencias de la incompatibilidad.
Se ha analizado con atención el rechazo de implantes sólidos tales como la piel y conviene describir la secuencia de procesos.
Por ejemplo, en ratones, el aloimplante de piel se asienta y se vasculariza en pocos días.
Entre el tercero y el noveno día disminuye en forma gradual la circulación y se incrementa la infiltración del lecho del implante con linfocitos y monocitos, pero muy pocas células plasmáticas.
Comienza a verse a simple vista la necrosis y al cabo de uno o dos días se escara el implante por completo (fig. 13-1).

Indicios de que el rechazo es inmunológico 

Reacciones de primero y segundo tipo 

Si la reacción tiene una base inmunológica, debería esperarse que el segundo contacto con el antígeno represente un proceso más explosivo que el primero y sin duda el rechazo de un segundo implante proveniente del mismo donante está muy acelerado.
La vascularización inicial es pobre y puede no producirse.
Se observa una invasión muy rápida por leucocitos polimorfonucleares y células linfoides con inclusión de las células plasmáticas.
Transcurridos tres o cuatro días se observa trombosis y destrucción celular aguda.

Figura 13-1

Rechazo de injerto de piel CBA por ratón de cepa A.

(a) 10 días después del trasplante áreas descoloridas producidas por destrucción del epitelio y deshidratación de la dermis expuesta. 
(b) 13 días después del injerto: la superficie costrosa indica destrucción total del injerto.
(Cortesía del Prof. L Brent).

Figura 13-2

El rechazo del injerto induce memoria que es específica y puede ser transferida por los linfocitos T

En el experimento 1, un receptor de cepa A de linfocitos T provenientes de otro ratón de cepa A que había rechazado un injerto de cepa B, dará rechazo acelerado (es decir de 2º tipo) de un injerto B.
Los experimentos 2 y 3 muestran la especificidad del fenómeno respecto al tercero de cepa C, no relacionado desde el punto de vista genético.

Especificidad 

No todos los aloinjertos subsecuentes sufren el rechazo de segundo tipo sino sólo los derivados del donante original o de una cepa relacionada.
Los injertos de donantes no relacionados se rechazan igual que los rechazos de primer tipo.

Papel del linfocito 

Los animales timectomizados al nacer presentan dificultad para rechazar los injertos de piel pero su capacidad se restablece por inyección de linfocitos provenientes de un donante normal singénico lo que sugiere que están involucrados los linfocitos T.
El receptor de linfocitos T provenientes de un donante que ya ha rechazado un injerto producirá un rechazo acelerado de otro posterior del mismo tipo (fig. 13-2) lo que demuestra que las células linfoides están imprimadas y retienen la memoria del primer contacto con los antígenos del injerto.

Figura 13-3

Herencia de genes que controlan los antígenos de trasplante.

A representa un gen que expresa el antígeno A y B el correspondiente gen alélico, en el mismo locus genético.

Las cepas puras son homocigotos para A/A y B/B, respectivamente.
Dado que los genes son co-dominantes, un animal con genoma A/B expresará ambos antígenos, se hará tolerante a ellos y por lo tanto aceptará injertos de donantes A o B.
La ilustración muestra que para cada gen que controla una especificidad de antígeno de trasplante tres cuartos de la generación F2 aceptará un injerto de piel de un progenitor.
Para n genes la fracción es (3/4)".
Si los animales F1 A/B vuelven a cruzarse con un progenitor A/A, la mitad de la progenie será A/A y la mitad A/B; sólo éste último aceptará injertos B. 

Producción de anticuerpos 

Luego del rechazo pueden reconocerse anticuerpos humorales con especificidad para el donante del injerto.

Se positivizan las pruebas de hemaglutinación en los ratones, donde los eritrocitos transportan antígenos de trasplante; en el ser humano se detectan linfocitotoxinas.
A menudo una prueba en placa de Jerne que utiliza timocitos de la cepa del donante en lugar de eritrocitos de carnero demuestra la presencia de células formadoras de anticuerpo en los tejidos linfoides de animales trasplantados.

Control genético de los antígenos de trasplante 

La especificidad de los antígenos involucrados en el rechazo del trasplante se encuentra bajo control genético.
Individuos idénticos desde el punto de vista genético tales como ratones de una cepa pura o gemelos univitelinos presentan antígenos de trasplante idénticos y los trasplantes pueden intercambiarse entre ellos con entera libertad.
La segregación mendeliana de los genes que controlan a estos antígenos se ha dilucidado por medio de experimentos de entrecruzamiento entre ratones de diferentes cepas puras.
Dado que estos ratones se aparean dentro de una cepa determinada y siempre aceptan injertos provenientes de sus semejantes deben ser homocigotas para los genes de trasplante.
Si se consideran dos de estas cepas A y B con genes alélicos que difieren en un locus en cada caso los genes materno y paterno serán idénticos con constitución genética de por ejemplo A/A y B/B respectivamente (por convención, los genes se expresan en letra itálica y los antígenos que codifican en letras de tipo normal).
Por cruzamiento de las cepas A y B se obtiene una primera generación (F1) de constitución A/B.
Todos los ratones F1 aceptarán injertos de alguno de los progenitores lo que demuestra que son tolerantes para A y B. Si se entrecruza la generación F1 debería esperarse una distribución promedio de genotipos para la F2 del tipo de la que se presenta en la figura 13-3; sólo 1 de 4 carecería de genes A y por lo tanto rechazaría injertos de A por falta de tolerancia, y 1 de 4 rechazaría injertos de B por la misma razón.
Por lo tanto para cada locus 3 de 4 miembros de &rsqb;a generación F2 aceptará los injertos provenientes de las cepas progenitoras.
Si se extiende el análisis y en lugar de un locus con un par de genes alélicos hubieran loci la fracción de los miembros de la generación F2 que aceptará los injertos provenientes de las cepas progenitores será de (3/4)n.
De esta manera puede obtenerse un número estimado de la cantidad de loci que controlan los antígenos de trasplante.

En el ratón se han establecido unos 40 de tales loci, pero como se vio con anterioridad predomina el locus complejo denominado xxx en el sentido que controla los antígenos de trasplante fuertes que provocan reacciones intensas de alotrasplante, y en capítulos previos se ha estudiado con detalle la estructura (véase fig. 3-18, p 60) y la biología de este sitio principal de histocompatibilidad .
Los antígenos no xxx o menores de trasplante tales como el xxx masculino son reconocidos como péptidos procesados asociados con las moléculas del CMH sobre la superficie celular por los linfocitos T pero no en forma inmediata por los linfocitos B.
No debe confundirse el término "menor" con que estos antígenos no dan origen a serios problemas de rechazo, aunque con mayor lentitud que el CMH.

Otras consecuencias de la incompatibilidad xxx 

Las diferencias del CMH de clase II producen una reacción linfocitaria mixta (RLM) 

Cuando los linfocitos provenientes de cepas de ratones con haplotipo clase II diferente se cultivan juntos, se produce la transformación y la mitosis de las células blásticas (RML), y los linfocitos T de cada población linfocitaria reaccionan contra los determinantes del CMH de clase II en la superficie de la otra población.
Para obtener la "RLM de una vía", los linfocitos estimuladores se transforman en incapaces de responder por tratamiento con mitomicina C o por rayos X, y luego se agregan a los linfocitos capaces de responder provenientes del otro donante.
Las células que responden pertenecen sobre todo a una población de linfocitos T xxx positivos y son estimulados por los determinantes de clase II presentes en la mayoría de los linfocitos B, los macrófagos y en especial sobre las células dendríticas presentadoras de antígeno.
Por lo tanto, la RML es inhibida por antisueros contra los determinantes de clase II sobre las células estimuladoras.

Linfólisis mediada por células (LMC) 

La inclusión de los antígenos del CMH de clase II en el proceso de provocación del rechazo del trasplante ha despertado un interés renovado por el descubrimiento del fenómeno de LMC desarrollado como posible prueba de histocompatibilidad.
Se ilustra el principio en la figura 13-4.
En suma, las células capaces de generar respuesta activada por la RLM inducida por clase II contribuyen a la generación de linfocitos T citotóxicos dirigidos contra los determinantes xxx ; en esencia, el reconocimiento de las moléculas de clase II ayuda a generar electores contra las moléculas de clase II, de alguna manera recuerda al sistema hapteno-transportador en lo referido a la colaboración, donde clase II equivale al transportador y clase I equivale al hapteno (véase p. 120).

Reacción de injerto versus huésped (i.v.h.) 

Cuando se transfieren linfocitos T competentes desde un donante a un receptor incapaz de rechazarlos, las células trasplantadas sobreviven y tienen tiempo para reconocer a los antígenos del huésped y generan una respuesta inmunitaria contra ellas.
En lugar de la reacción normal del trasplante establecida desde el huésped contra el injerto, se produce la reacción inversa, denominada de injerto contra huésped (i.v.h.).

En el roedor joven puede producirse inhibición del crecimiento (enanismo), aumento de tamaño del bazo y anemia hemolítica (debida a la producción de anticuerpos contra los eritrocitos).
En el ser humano, se observa fiebre, anemia, pérdida ponderal, prurito, diarrea y esplenomegalia, y se cree que los principales mediadores de la patología son las citoquinas, en especial TNF.

Cuanto más fuerte sea la diferencia del antígeno de trasplante, más severa será la reacción.

Cuando el donante y el receptor difieren en los locus xxx o xxx , las consecuencias pueden ser fatales, aunque debe tenerse en cuenta que las reacciones contra antígenos de trasplante dominantes menores, o la combinación de algunos de ellos, pueden ser también ser difíciles de controlar.

Figura 13-4

Linfólisis mediada por células la generación de linfocitos T xxx citotóxicos para trasplantes xxx .

Los linfocitos T helper responden a xxx sobre las células de trasplante por proliferación (reacción linfocitaria mixta) y tienen entonces capacidad para ayudar a que se transformen en efectores citotóxicos a los precursores de linfocitos T específicos para los antígenos xxx y xxx del trasplante.

En la figura 13-5 se ilustran dos situaciones posibles que conducen a reacciones i.v.h..
En el ser humano ésta puede originarse en sujetos enérgicos desde el punto de vista inmunitario que reciben trasplantes de médula ósea, p. ej. por inmunodeficiencia combinada (p.

227), por aplasta de eritrocitos luego de accidentes con irradiación o como una posible forma de terapéutica del cáncer.
Los linfocitos T competentes de la sangre o presentes en órganos trasplantados a pacientes inmunosuprimidos pueden generar reacciones i.v.h.: del mismo modo las células maternas pueden atravesar la placenta, aunque en este caso aún no se dispone de evidencias referidas a enfermedades causadas por este mecanismo en el ser humano.

Mecanismos del rechazo de los injertos 

Rechazo mediado por linfocitos 

Gran parte de los trabajos realizados sobre rechazo de aloinjertos han estudiado trasplantes de piel o tumores sólidos debido a que su destino puede seguirse con relativa facilidad.
En estos casos poco se aporta al concepto de que los anticuerpos humorales son el instrumento de destrucción del injerto aunque, como se verá más adelante, esto no siempre ocurre con trasplantes de otros órganos, tales como el riñón.

Mientras que la transferencia pasiva de suero proveniente de un animal que ha rechazado un injerto de piel por lo general no acelera el rechazo de un trasplante similar en el animal receptor, la inyección de células linfoides (en particular pequeños linfocitos recirculantes) es efectiva en acortar la supervivencia del implante (véase fig. 13-2).

Figura 13-5

Reacción de injerto vs. huésped.

Cuando se inoculan linfocitos T competentes en un huésped incapaz de reaccionar contra ellos las células injertadas pueden reaccionar contra los antígenos localizados sobre las células del huésped a las que reconocen como extrañas.
La reacción que sucede puede ser fatal.
Dos de las muchas situaciones posibles se ilustran: (a) el híbrido AB recibe células de un progenitor (BB) que son toleradas pero reacciona contra el antígeno en las células huésped; (b) un receptor AA irradiado con rayos X cuya inmunidad ha sido restablecida con linfocitos BB no puede reaccionar contra el trasplante y se produce una reacción i.v.h.

Sería consistente la existencia de un papel importante de las células Iinfoides en el rechazo de primer tipo con la histología de la reacción temprana que muestra infiltración por células mononucleadas con muy pocos polimorfonucleares o células plasmáticas (fig. 13-6).
El dramático efecto de la timectomía neonatal sobre la prolongación de los Implantes de piel, como ya se mencionó, y la larga supervivencia de los trasplantes en niños con deficiencias tímicas implican la acción de los linfocitos T en estas reacciones.
En el pollo, el rechazo del homoimplante y la reactividad i.v.h. están influenciadas por la timectomía neonatal pero no por la bursectomía.
Se han obtenido evidencias más directas de estudios in vitro que muestran que los linfocitos T obtenidos de ratones que rechazan un aloinjerto son capaces de matar células blanco portadoras de los antígenos del injerto in vitro .

Trabajos recientes referidos a la importancia de las células xxx murinas y xxx humanas como efectoras han generado dudas, tal vez en forma equívoca, sobre el papel de los Linfocitos citotóxicos en el rechazo de trasplantes in vivo ; aunque en ocasiones las células xxx tienen potencial citotóxico para los blancos de clase II, en general están asociadas a la actividad adyuvante, en este caso en particular para los precursores de linfocitos T citotóxicos, y con la producción de linfoquinas que median las reacciones de hipersensibilidad retardada.
El interrogante que se presenta es si actúan para estimular el acceso de linfocitos T citotóxicos a sus blancos.
Se sabe que el xxx produce una regulación ascendente de la expresión del antígeno sobre la célula blanco del trasplante para aumentar su vulnerabilidad para los linfocitos citotóxicos xxx .

Figura 13-6

Rechazo agudo temprano de aloinjerto renal humano 10 días después del trasplante, que muestra infiltración celular densa del intersticio por células mononucleadas (coloración con pironina) (Cortesía del Prof. K Porter).

Los individuos normales presentan una frecuencia muy elevada de células alorreactivas (es decir células que reaccionan con aloinjertos) que se cree es responsable de la intensidad del rechazo de CMH incompatible.
Cabe preguntarse si estos linfocitos T alorreactivos sólo reconocen las moléculas de CMH o responden a CMH extraños asociados con diferentes péptidos derivados de las proteínas citoplasmáticas del injerto por los procesamientos normales.
En apariencia ocurre que dos linfocitos T citotóxicos alorreactivos específicos para xxx expresados sobre el tumor murino xxx son incapaces de lisar células humanas transfectadas y capaces de expresar las moléculas xxx a menos que se agregue al sistema un extracto de proteínas citoplasmáticas xxx clivadas a péptidos por bromuro de cianógeno.
Por lo tanto, los auto-péptidos expresados con el CMH de clase I son responsables del alorreconocimiento.

El papel de los anticuerpos humorales 

Durante mucho tiempo se ha sabido que células alogénicas aisladas tales como los linfocitos pueden ser destruidas por reacciones citotóxicas (tipo 11) que involucran anticuerpos humorales.
Sin embargo, aunque la experiencia previa con implantes de piel y de tumores sólidos sugiere que no son fácilmente susceptibles a la acción de anticuerpos citotóxicos, en la actualidad queda claro que esto no es aplicable a todos los tipos de trasplante de órganos.
Se ilustra mejor este concepto si se consideran las diferentes formas por las cuales pueden ser rechazados los aloinjertos de riñón.

Rechazo hiperagudo en los minutos posteriores al trasplante: se caracteriza por la sedimentación de eritrocitos y microtrombos en el glomérulo, ocurre en individuos con anticuerpos humorales preexistentes ya sea por incompatibilidad de grupo sanguíneo o por presensibilización al CMH de clase I por transfusión sanguínea.

Rechazo agudo temprano se produce hasta 10 días después del trasplante, y se caracteriza por presentar un infiltrado celular denso (fig. 13-6) y ruptura de los capilares peritubulares; se asemeja a una reacción de hipersensibilidad celular que involucra a los linfocitos T, compuestos probablemente por el ataque de citotóxicos xxx sobre las células del injerto cuya expresión de antígeno CMH ha sido regulada en forma ascendente por interferón xxx .

Rechazo agudo tardío que aparece del día 11 en adelante en pacientes suprimidos por prednisona y azatioprina, es causado probablemente por la unión de inmunoglobulina (anticuerpo) y complemento a las arteriolas y capilares glomerulares donde pueden visualizarse por técnicas de inmunofluorescencia.
Estos depósitos de inmunoglobulina sobre las paredes vasculares inducen la agregación plaquetaria en los capilares glomerulares que conduce a la insuficiencia renal aguda (fig. 13-7).
También debe considerarse la posibilidad de la producción de lesiones a las células recubiertas por anticuerpos a través de citotoxicidad celular dependiente de anticuerpo.

Figura 13-7

Rechazo tardío agudo de alotrasplante renal humano que muestra agregación plaquetaria en un capilar glomerular inducido por depósito de anticuerpo sobre la pared vascular (micrografía electrónica) (Cortesía del Prof. K Porter) 

Rechazo tardío y retardo asociado a depósitos subendoteliales de inmunoglobulina y xxx sobre las membranas basales glomerulares que en ocasiones pueden ser una expresión de un trastorno subyacente por complejo inmune (que en principio requeriría el trasplante) o si no de formación de complejo con antígenos solubles derivados del riñón trasplantado.

Por lo tanto, la complejidad de la acción y la interacción de los factores celulares y humorales en el rechazo del trasplante es considerable; en la figura 13-8 se intenta resumir los mecanismos involucrados postulados.

En determinadas circunstancias los anticuerpos pueden proteger un trasplante de la destrucción, este fenómeno importante de estimulación será considerado más adelante.

ENFERMEDADES AUTOINMUNES 

Patogenia, diagnóstico y tratamiento 

Mecanismos patogénicos de las enfermedades autoinmunes 

Se mencionó que a pesar de ciertas excepciones como por ejemplo el infarto de miocardio o la lesión testicular, la liberación traumática de constituyentes orgánicos en general no genera la formación de anticuerpos.
La destrucción de tejido tiroideo por dosis terapéuticas de lodo radiactivo no inicia autoinmunidad tiroidea, ni las lesiones hepáticas de la cirrosis alcohólica resultan en la síntesis de anticuerpos mitocondriales, para dar dos ejemplos.
A continuación se tratará la evidencia que lleva en forma directa a preguntar si la autoinmunidad, cualquiera sea su origen, juega un papel patogénico fundamental en la producción de lesiones tisulares en el grupo de enfermedades denominadas "autoinmunes".

Efectos de los anticuerpos humorales 

Sangre 

Los anticuerpos eritrocitarios juegan un papel en la destrucción de los glóbulos rojos en la anemia hemolítica autoinmune.
Los eritrocitos normales recubiertos por anticuerpo eluido de eritrocitos Coombs-positivos presentan una vida media reducida luego de ser reinyectados en el sujeto normal.
Los anticuerpos plaquetarios son responsables en apariencia de la púrpura trombocitopénica idiopática (PTI).
Cuando se administra xxx proveniente del suero de un paciente a un individuo normal provoca la depresión del recuento de plaquetas y puede captarse el principio activo por absorción sobre plaquetas.
La trombocitopenia neonatal transitoria que puede observarse en lactantes hijos de madres con PTI se explica si se considera el pasaje transplacentario de anticuerpos xxx a la criatura.

Algunos niños con inmunodeficiencia asociada a recuentos muy bajos de glóbulos blancos presentan un factor linfocitotóxico sérico que requiere al complemento para su activación.
La linfopenia que se observa en pacientes con LES y artritis reumatoidea también puede ser un resultado directo del anticuerpo, dado que se ha informado la existencia de anticuerpos no aglutinantes recubriendo los leucocitos en estos casos.

Tiroides 

Anticuerpos citotóxicos 

El suero de pacientes con enfermedad de Hashimoto es citotóxico para células tiroideas humanas cultivadas en monocapa luego de la dispersión con tripsina.
Esta es una clásica reacción de anticuerpo mediada por complemento dirigida contra un antígeno de superficie celular idéntico al antígeno de peroxidasa tiroidea revelado por coloración intracitoplasmática de secciones de tiroides con sueros de pacientes con enfermedad de Hashimoto.
Es curioso que este antígeno sólo se expresa en la porción apical de las células foliculares en contacto con el coloide por lo que en condiciones normales no resulta accesible al anticuerpo circulante.

Esto explica por qué fragmentos simples de tiroides con folículos intactos no se ven afectados por la incubación en presencia de un medio que contiene anticuerpo citotóxico y complemento, y además por qué no hay evidencia de niños nacidos de madres enfermas de Hashimoto con función tiroidea defectuosa a pesar de la presencia de anticuerpo en su suero.
Sin embargo, pueden detectarse anticuerpos en la superficie interior de los folículos tiroideos en tejidos tomados de pacientes con tiroiditis autoinmune, y parece ser necesario postular que sólo se produce la lesión tiroidea cuando hay colaboración con otros factores tales como depósito de inmunocomplejos, efectores de linfocitos T sensibilizados o mecanismos que generan una reversión de la polaridad de las células epiteliales.

Anticuerpos estimulantes del tiroides 

Bajo ciertas circunstancias, los anticuerpos contra la superficie de una célula pueden estimular más que destruir (véase sensibilidad tipo V: Cap. 10).
Esto parece ocurrir en la tirotoxicosis (enfermedad de Graves o de Basedow).
Por mucho tiempo se ha contado con evidencias indirectas que sugieren la existencia de un enlace entre procesos autoinmunes y esta enfermedad: se detectan anticuerpos tiroideos en hasta el 85% de pacientes tirotóxicos y por histología se determina que la mayoría de las glándulas eliminadas por cirugía muestran grados variables de tiroiditis y formación local de anticuerpos además de la característica hiperplasia de células acinares; se encuentra tirotoxicosis con indebida frecuencia en familias de pacientes con tiroiditis de Hashimoto; existe una asociación con autoinmunidad gástrica resultante en un 30% de pacientes con anticuerpos gástricos y hasta un 10% con anemia perniciosa.
El enlace directo se obtuvo con el descubrimiento de Adams y Purves sobre la actividad estimuladora del tiroides en el suero de pacientes tirotóxicos.
Por medio de un nuevo bioensayo encontraron que el suero produce una estimulación de la glándula tiroides en el animal receptor de curso considerablemente prolongado comparado con el de la hormona estimulante del tiroides fisiológico (TSH) de la hipófisis; además se demostró que esto se debía a la presencia de anticuerpos estimulantes del tiroides (en inglés TSAb).
Estos anticuerpos son capaces de bloquear la unión de TSH a las membranas tiroideas y parecen actuar de igual manera que TSH, se cree que por estimulación de receptores idénticos (véase fig. 12-20).
Ambos operan a través del sistema de la adenil ciclasa según indica el efecto potenciador de la teofilina, y producen cambios similares en la morfología ultraestructural de la célula tiroidea, pero es un "experimento de transferencia pasiva" de aparición natural que liga a TSAb en forma más directa con la patogenia de la enfermedad de Graves.
Cuando TSAb proveniente de una madre tirotóxica atraviesa la placenta se asocia con la producción de hipertiroidismo neonatal (fig. 15-1), que se resuelve luego de unas pocas semanas, a medida que es catabolizada la xxx materna.

Existe una buena correlación entre el título de TSAb y la severidad del hipertiroidismo.
Dado que TSAb actúa con independencia del eje hipofisario-tiroideo, la captación de lodo por la glándula no es afectada por la administración de tiroxina o triiodotironina, mientras que en condiciones normales esto causaría inhibición por retroalimentación y supresión de la captación; en esto se basa una importante prueba diagnóstica para la tirotoxicosis.

Existen razones para creer que el aumento de tamaño del tiroides en este trastorno se debe a la acción de anticuerpos que reaccionan con un receptor de "crecimiento" y estimulan directamente la división celular a diferencia de la hiperactividad metabólica, (fig. 15-2a).
Por el contrario, los sueros de pacientes con mixedema primario contienen anticuerpos capaces de bloquear la acción mitógena de la TSH (fig. 15-2b) por lo cual previenen la regeneración de folículos característica del bocio aumentado de tamaño de la enfermedad de Hashimoto.
Puede establecerse entonces que existe considerable diversidad en la respuesta autoinmune contra el tiroides que lleva a la destrucción de tejido, la estimulación metabólica, la promoción del crecimiento o la inhibición mitótica que en combinaciones diferentes son responsables de la variedad de formas en las cuales se presenta la enfermedad tiroidea autoinmune (fig. 15-3).

Estómago 

Los auto-anticuerpos contra el factor intrínseco, un producto de secreción de la mucosa gástrica se demostraron primero en pacientes con anemia perniciosa por administración oral de factor intrínseco, vitamina B12 y el suero de un paciente con esa enfermedad.
Se demostró que el suero prevenía la mediación del factor intrínseco en la absorción de B12 por el organismo, y por estudios posteriores se determinó que el principio activo era un anticuerpo.
En apariencia, los anticuerpos circulantes no son capaces de neutralizar la actividad fisiológica del factor intrínseco; un paciente inmunizado por vía parenteral con factor intrínseco de cerdo en adyuvante completo de Freund presentó niveles elevados de anticuerpos séricos y buenas respuestas cutáneas celulares pero aún absorbía bien B12, cuando era alimentado con factor intrínseco de cerdo.
Estos datos implican que los anticuerpos tienen que encontrarse dentro de la luz del tracto gastrointestinal para presentar efectividad biológica, y pueden identificarse en el jugo gástrico de estos pacientes, sintetizados por células plasmáticas en las lesiones de gastritis.

Mientras que no se dispone de evidencias de que los anticuerpos capaces de bloquear el receptor de gastrina en las células parietales puedan provocar destrucción tisular, se postula que contribuyen a la hipocloridia y la acloridia que se observan en la anemia perniciosa y la gastritis atrófica, en especial cuando quedan células parietales residuales en la mucosa gástrica.
Se piensa que algunos casos de úlcera gástrica pueden ser el resultado de la estimulación de la secreción ácida por activación a través de anticuerpos de los receptores de histamina, y se espera que estudios futuros avalen esta postura.

Figura 15-1

Tirotoxicosis neonatal

(a) Los auto-anticuerpos que estimulan el tiroides por los receptores de TSH son xxx y atraviesan la placenta. 
(b) Por lo tanto, la madre tirotóxica da a luz un bebé con hiperactividad tiroidea que evoluciona favorablemente en forma espontánea a medida que cataboliza la xxx materna. 
(Fotografía cedida por el Dr. A MacGregor).

Figura 15-2

Auto-anticuerpos que afectan el crecimiento del tiroides.

(a) Anticuerpos estimulantes en la enfermedad de Graves con bocio y en bocio coloidal autoinmune que se determinan por el incremento de las células que entran en la fase de síntesis (S) de DNA del ciclo celular en fragmentos de tiroides tratados con xxx del suero del paciente.
Los valores de p se relacionan con diferencias entre los valores obtenidos y la xxx normal.
(b) Anticuerpos bloqueantes en mixedema primario revelados por la capacidad que posee la xxx del paciente para inhibir la estimulación del crecimiento producida por FSH.
Cada punto representa el valor para el suero de un paciente.
(Datos de Drexhage H A, Bottazzo, G F, Doniach D, Bitensky L y Chayen J &lsqb;1980&rsqb; Lancet ii , 287 y de &lsqb;1981&rsqb; Nature 289, 594).

Esperma 

En algunos varones infértiles, los anticuerpos aglutinantes producen la agregación de los espermatozoides e interfieren en su penetración al moco cervical.

Figura 15-3

Relaciones de diferentes respuestas auto-alérgicas con el espectro circular de los trastornos tiroideos autoinmunes.

Las respuestas que incluyen tiroglobulina y el antígeno microvelloso de superficie microsomal de peroxidasa tiroidea conduce a la destrucción tisular mientras que otros auto-anticuerpos pueden estimular o bloquear la actividad metabólica o la división de las células tiroideas.
Algunos investigadores escoceses utilizan el término "Hashitoxicosis" para describir una glándula con tiroiditis de Hashimoto y tirotoxicosis simultáneas (cortesía del Prof. D Doniach y del Dr. G F Bottazzo).

Membrana basal glomerular ( mbg ) 

En el caso de la enfermedad inmunitaria renal los modelos experimentales precedieron al descubrimiento de lesiones paralelas en el ser humano.
La inyección de preparados xxx heterólogos de reacción cruzada en adyuvante completo de Freund produce glomerulonefritis en ovejas y otros modelos animales de experimentación.

Pueden captarse los anticuerpos anti-xxx por coloración por inmunofluorescencia de biopsias provenientes de animales nefríticos con anti-xxx .
Los anticuerpos son absorbidos en su mayor parte, si no todos, por el riñón in vivo pero aparecen en el suero con la nefrectomía y pueden transferir en forma pasiva la enfermedad a otro animal de la misma especie.

Una situación análoga se produce en el hombre en ciertos casos de glomerulonefritis, en particular los asociados con hemorragia pulmonar (síndrome de Goodpasture).
La biopsia renal del paciente muestra depósito lineal de xxx y xxx a lo largo de la membrana basal de dos capilares glomerulares (fig. 12-13a).
Luego de la nefrectomía, pueden detectarse anticuerpos anti-xxx en el suero.
Lerner y col eluyeron el anticuerpo xxx de un riñón enfermo y lo inyectaron en un mono ardilla.
El anticuerpo se fijó con rapidez a la xxx del animal receptor y produjo una nefritis fatal (fig. 15-4).
De allí se sacaron las conclusiones referidas a que la lesión en el ser humano es el resultado directo del ataque de la xxx por estos anticuerpos fijadores de complemento.
Las alteraciones pulmonares que se observan en el síndrome de Goodpasture se atribuyen a reacciones cruzadas con algunos de los anticuerpos anti-xxx .

Músculo 

La debilidad muscular transitoria que se observa en una proporción de bebés nacidos de madres con miastenia gravis recuerda la trombocitopenia neonatal y el hipotiroidismo y sin duda es compatible con el pasaje transplacentario de una xxx capaz de inhibir la transmisión neuromuscular.
Este concepto está avalado por los consistentes hallazgos de anticuerpos anti-receptores musculares para acetilcolina en pacientes con miastenia y la depleción de estos receptores en las placas motoras terminales.
Además, pueden inducirse los síntomas de miastenia en animales por inyección de anticuerpos monoclonales contra los receptores de acetilcolina o por inmunización activa con los mismos receptores purificados.
Sin embargo, la mayoría de los bebés con madres miasténicas no presentan trastorno muscular y se infiere que se protegen a sí mismos por producción de anticuerpos dirigidos contra los idiotipos de los auto-anticuerpos maternos.

El cuadro 15-1 resume estos efectos patógenos directos sobre los auto-anticuerpos humorales.

Figura 15-4

Transferencia pasiva de glomerulonefritis a un mono ardilla por inyección de anticuerpos anti-membrana basal glomerular ( anti-xxx aislados por elución ácida del riñón de un paciente con síndrome de Goodpasture (según Lerner R A, Glascock R J y Dixon F J &lsqb;1967&rsqb; J Exp Med 126, 989).

Efecto de los complejos 

Lupus eritematoso sistémico (LES) 

En los sitios en los cuales se forman auto-anticuerpos contra componentes solubles de acceso permanente. es posible que se formen complejos que pueden dar origen a lesiones similares a las que ocurren en la enfermedad del suero, en especial cuando defectos de los componentes tempranos clásicos del complemento previenen una depuración efectiva (véase p. 247).
En el LES, los complejos de DNA y otros antígenos nucleares, junto a las inmunoglobulinas y el complemento pueden ser detectados por coloración inmunofluorescente de biopsias de riñón provenientes de pacientes con disfunción renal evidente.

El patrón de coloración con anti-xxx o anti-xxx fluorescente es punteado o moteado (fig. 12-13b), en marcado contraste con el patrón lineal producido por los anticuerpos anti-xxx en el síndrome de Goodpasture (fig. 12-13a; p. 264).
Los compleJos crecen en tamaño y se transforman en grandes agregados visibles con el microscopio electrónico como cúmulos amorfos sobre la cara epitelial de la membrana basal glomerular (fig. 15-5).
Durante la fase activa de la enfermedad disminuyen los niveles séricos del complemento a medida que se ven afectados los componentes por los agregados inmunes en el riñón y la circulación.

No han tenido mucho éxito los intentos efectuados para detectar auto-antígenos en los complejos circulantes; por lo general la mayor parte de los constituyentes identificables son inmunoglobulinas y componentes del complemento.
Si bien en forma negativa, esto constituye una evidencia consecuente con la posibilidad de que los anti-idiotipos pudieran perpetuar un estado autoinmunitario una vez iniciado (es decir, actúa como auto-antígeno sustituto) y genera complejos circulantes idiotipo-anti-idiotipo.

Tabla 15-1

Efectos patogénicos directos de los anticuerpos humorales

Estudios por inmunofluorescencia realizados sobre biopsias de piel de pacientes con la enfermedad relacionada lupus eritematoso discoide también revelan la presencia de inmunocomplejos.

Artritis reumatoidea 

Evidencias morfológicas de actividad inmunitaria 

Los cambios articulares en la artritis reumatoidea se producen en esencia por crecimiento maligno de las células sinoviales formando un pannus que recubre y destruye al cartílago y al hueso (fig. 15-6a-f).
La membrana sinovial que rodea y mantiene el espacio articular se transforma en intensamente celular como consecuencia de una considerable hiperreactividad inmunitaria que se evidencia por la presencia de grandes cantidades de linfocitos T, en su mayor parte xxx en distintas etapas de activación, por lo general asociados a células dendríticas y macrófagos; con frecuencia se observan cúmulos de células plasináticas y en ocasiones se encuentran folículos secundarios con centros germinativos, como si la sinovial se hubiera transformado en un ganglio linfático activo (fig. 15-6g, h e i).
Se ha estimado que la síntesis de inmunoglobulinas por parte del tejido sinovial es similar al de un ganglio linfático estimulado.
Existe expresión diseminada de HLA-DR (clase II): son positivos los linfocitos T y B, las células dendríticas y de recubrimiento sinovial y los macrófagos, lo cual indica una acción activa (fig. 15-6k).
Se postula que esta importante reactividad inmunitaria provee de un intenso estímulo a las células de recubrimiento sinovial que sufren transformación a pannus invasivo, el cual provoca erosión articular por liberación de mediadores destructivos.

¿Qué provoca esta actividad inmunitaria?

Desde el punto de vista del linfocito T, aún no se han identificado antígenos convencionales.
Ciertos informes refieren el aislamiento de líneas de linfocitos T provenientes de las articulaciones que son conducidos por líneas de linfocitos B transformadas por virus EB, derivados de la misma población linfocitaria.
Esto sugiere quizá cuatro posibilidades principales para las especificidades de la línea de linfocitos T: idiotipo anti-linfocito B, y CMH de clase II autorreactivo ya sea solo o asociado con proteína EBV o proteínas de estrés (shock térmico) procesadas.
La síntesis de estas últimas aumenta en respuesta a insultos externos tales como aumento de la temperatura, infección viral, estimulación de citoquinas, o aun los procesos activos intraarticulares.
Si bien no será difícil determinar estas cuestiones, hasta el momento no se dispone de la información requerida.
Queda la posibilidad de que existan cantidades significativas de linfocitos T que reaccionan con determinantes autólogos de clase II portadores de péptidos derivados de proteínas de estrés.

Auto-sensibilización de xxx 

Si se considera el mecanismo explicado en la figura 14-10 (p. 296), estos linfocitos T podrían estimular a los linfocitos B articulares para que expresen los péptidos de proteína de estrés que han reaccionado con auto-antígenos portadores de determinantes repetidos; en particular, en el contexto de la artritis reumatoidea, éstas podrían ser las regiones xxx múltiples de xxx dentro de un complejo inmune soluble.
Esto llevaría a la síntesis de auto-anticuerpos contra la región xxx de xxx conocida como anti-globulinas o factores reumatoideos.
Son el sello distintivo de la enfermedad, y se demuestran en casi todos los pacientes con artritis reumatoidea.
La mayoría tiene antiglobulinas xxx que reaccionan en las clásicas pruebas de aglutinación de látex y de eritrocitos de carnero (cuadro 14-2: nota 7), y tanto ellos como los pacientes "sero-negativos" que no reaccionan con estas pruebas se ha demostrado que poseen títulos elevados de antiglobulinas xxx detectables por inmunoensayo de fase sólida (véase p. 97; fig. 15-7).

Figura 15-5

Biopsia renal de un paciente con LES con glomerulonefritis severa por inmunocomplejos y proteinuria.

La micrografía electrónica muestra engrosamiento irregular de las paredes capilares glomerulares por complejos subepiteliales (a) y subendoliales (b).
La región mesangial muestra abundantes complejos (probablemente fagocitados)(cortesía del Dr. A Leatham).

Dado que la auto-sensibilización contra xxx es una característica casi universal de la enfermedad, la mayor parte de las células plasmáticas sinoviales deberían sintetizar anti-globulinas.
Pero en realidad sólo una cantidad relativamente menor de las células plasmáticas (10-20%) fijan xxx marcado con fluoresceína, ya sea como material agregado por calor (fig. 15-6j) o como inmunocomplejos (el factor reumatoideo es un anticuerpo con baja afinidad y sólo se observa un buen enlace cuando se utiliza xxx multivalente como antígeno).
Sin embargo, debe considerarse una característica particular y exclusiva de las antiglobulinas xxx dado que ambas son antígeno y anticuerpo al mismo tiempo, tienen capacidad de auto-asociación (fig. 15-8b) y de esta forma se esconden la mayoría de las valencias libres de la antiglobulina.

Munthe y Natvig supusieron que la destrucción de las regiones xxx por acción de la pepsina liberaría estos sitios de unión ocultos (fig. 15-8c), y observaron que hasta el 40-70% de las células plasmáticas de la sinovial presentaban especificidad anti-xxx luego del tratamiento con esta enzima.

Los agregados de xxx , presumiblemente producto de estas células plasmáticas, pueden detectarse con regularidad en los tejidos sinoviales y en el líquido articular donde dan origen a típicas reacciones inflamatorias agudas con exudados líquidos.
Al ser analizados se determina que están compuestos casi con exclusividad por inmunoglobulinas y complemento mientras que una proporción importante de la xxx se encuentra presente como antiglobulina auto-asociada, según puede observarse al ligarlo a un inmunoabsorbente xxx luego del tratamiento con pepsina.

Glucosilación anormal 

Uno de los acontecimientos más recientes en el estudio de la artritis reumatoidea ha sido el descubrimiento de que la xxx de los pacientes presenta glucosilación anormal.
Los dos dominios xxx de la región xxx están separados (véase p. 52) por dos azúcares unidos por asparagina con la estructura general que se muestra en la figura 15-9a.
Los brazos 1,3 de cada azúcar proveen un puente entre los dominios mientras que los azúcares 1,6 están dirigidos hacia la superficie proteica donde está ubicada la terminal ácido siálico-galactosa en una cavidad especial similar a la lectina (fig. 15-9b y c).

Algunas cadenas terminan en N-acetil-glucosamina y carecen de los azúcares terminales ácido siálico-galactosa.
En individuos normales, alrededor del 14% de los grupos azúcar xxx de xxx carecen de la galactosa terminal en ambas cadenas.
Lo extraordinario radica en que el porcentaje de azúcares que carecen por completo de galactosa en la xxx de los pacientes con artritis reumatoidea siempre es mayor que en los controles v puede alcanzar el 60% (fig. 15-10).
1 El xxx puede tener auto-antigenicidad aumentada.
2 Los complejos xxx autoasociados (fig. 15-8) se mantendrían unidos con mayor fuerza si las terminales de ácido siálico-galactosa del azúcar xxx de uno de los xxx coincide con la cavidad de lectina del xxx que queda vacante por la falta de galactosa en el azúcar xxx . 


Producción de daño tisular 

Según se explica en el epígrafe de la figura 15-8, los complejos pueden estabilizarse por medio de las moléculas de enlace xxx multivalentes, el factor reumatoideo xxx y xxx , y cuando se encuentran en el espacio articular pueden iniciar una reacción de Arthus que lleva a un influjo de polimorfonucleares con los cuales reaccionan con el fin de liberar enzimas lisosomales. 
Éstas incluyen proteinasas neutras y colagenasa capaces de dañar al cartílago articular por degradación de proteoglucanos y fibrillas de colágeno.
Si los complejos están adheridos al cartílago se produce mayor daño dado que los polimorfonucleares se fijan pero son incapaces de fagocitarlo ("fagocitosis frustrada"); en consecuencia se liberan las hidrolasas lisosomales hacia el exterior de la célula, al espacio entre la célula y el cartílago, donde están protegidas de los inhibidores de enzimas tales como la xxx macroglobulina.

Los agregados también pueden estimular las células similares a los macrófagos del recubrimiento sinovial, ya sea en forma directa por sus receptores de superficie o en forma indirecta por fagocitosis y resistencia a la digestión intracelular.
La liberación de linfoquinas tales como xxx y xxx de los linfocitos T activados provee de estimuladores más potentes de los macrófagos.

Las células sinoviales activadas crecen como pannus maligno (cobertura) por encima del cartílago (fig. 15-7d) y en el margen de este tejido de granulación en crecimiento puede observarse degradación (fig. 15-7e), casi con certeza producto de la liberación de enzimas, de intermediarios de oxígeno reactivo y en especial de interleuquina-1 , xxx .
Los macrófagos activa dos también secretan activadores de plasminógeno, y la plasmina formada como consecuencia, activa una colagenasa latente producida por las células sinoviales.

Puede producirse sensibilización al colágeno parcialmente degradado y esto puede conducir de manera secundaria a la amplificación de la lesión.
Los productos secretados por el macrófago estimulado son capaces de activar a los condrocitos y así exacerbar la degradación del cartílago y a los osteoclastos para aumentar la resorción ósea que es una complicación adicional de la enfermedad severa.
Los nódulos subcutáneos son granulomas (fig. 15-6m y n) formados posiblemente por producción local de antiglobulinas insolubles auto-asociadas.

Figura 15-6

Artritis reumatoidea (AR).

(a) Manos de un paciente con AR crónica que muestran las clásicas deformidades en cuello de cisne.
(b) Representación en diagrama de una articulación diartródica que muestra erosiones óseas y cartilaginosas por debajo del pannus derivado de la membrana sinovial.
(c) Articulación interfalángica proximal que muestra marcada erosión ósea v erosión marginal del cartílago.
(d) Pannus temprano de tejido de granulación que crece sobre la rótula. 
(e) Histología del pannus que muestra clara erosión del hueso y el cartílago en el margen celular. 
(f) Histología del pannus coloreado para esterase no específica de macrófagos; nótense los largos procesos dendríticos coloreados.
(g) Células inflamatorias crónicas en las capas más profundas de la sinovia en AR.
(h) Una sinovia hipervellosa con folículos secundarios bien formados con centros germinativos (se visualizan rara vez). 
(i) Vista con gran aumento de una zona sinovial enferma que muestra colecciones de células plasmáticas clásicas.
(j) Células plasmáticas aisladas del tejido sinovial de un paciente coloreado al mismo tiempo para xxx (con xxx marcado con fluoresceína) y factor reumatoideo (con xxx agregado marcado con rodamina).
Dos de las cuatro células plasmáticas positivas para xxx parecen sintetizar factores reumatoideos.
(k) Sinovial reumatoidea que muestra grandes cantidades de células coloreadas con anti xxx ( anti-clase II).
(l) Sinovial reumatoidea que muestra células accesorias positivas para clase II (verde) en íntimo contacto con linfocitos T xxx positivos (naranja). 
(m) Grandes nódulos reumatoideos en el antebrazo.
(n) Aspecto granulomatoso del nódulo reumatoideo con zonas necróticas centrales rodeadas por células epitelioides, macrófagos y linfocitos diseminados.
A menudo se demuestra la presencia de células plasmáticas productoras de factor reumatoideo y es posible que la lesión represente una respuesta a la formación de complejos insolubles anti-xxx .
( Cedido gentilmente por Dr. D Isenberg).

Fig. 15-7.
Antiglobulinas xxx e xxx determinadas por radioensayo en tubo en pacientes con artritis reumatoidea seropositiva (o) y seronegativa.

Las líneas punteadas indican los límites de confianza del 95% (media + 2 DS) del grupo normal.
(De Nineham L, Hay F C y Roitt I M &lsqb;1976&rsqb; J Clin Path, 29, 1121).

Figura 15-8

Complejos auto-asociados de antiglobulinas xxx y la exposición de sitios de unión escondidos por la pepsina.

Aunque de baja afinidad relativa, la fuerza del enlace es incrementada por el efecto bonificador de adosamiento mutuo y, además, estos complejos en la articulación pueden ser estabilizados por inmunoglobulina xxx las cuales tienen sitios polivalentes de enlace para xxx .

Figura 15-9

Los azúcares xxx y su papel en la formación de puentes entre los dominios xxx .

(a) Estructura típica de cada azúcar ligado a N.
Algunas cadenas carecen de galactosa-ácido siálico terminal.
(b) Posición de las cadenas de azúcar en los dominios xxx .
(c) Estructura de las regiones xxx y la asociación entre galactosa-ácido siálico terminal en el brazo 1,6 y la superficie proteica.
Los brazos 1,3, uno de los cuales debe carecer de galactosa, forman un puente entre los dos dominios.
( xxx N-acetilglucosamina; Man, manosa; Gal, galactosa; xxx , ácido siálico; Fuc, fucosa).

La reumatología se vio estimulada por el descubrir miento hecho hace unos pocos años de que una elevada proporción de pacientes con artritis reumatoidea presentan títulos altos de anticuerpos circulantes contra un antígeno nuclear que se encuentra en el virus EB transformado pero que está ausente en los linfocitos normales.
Esto condujo al hallazgo de que los linfocitos T de estos pacientes son deficientes en el control mediado por interferón de la transformación por virus EB de los linfocitos B.
Si bien es poco probable que esto signifique un papel etiológico principal para la infección por virus EB, podría representar una alteración regulatoria básica que resulte en un manejo defectuoso de algún microorganismo iniciador y es posible que permita la emergencia de linfocitos T autorreactivos.
En la figura 15-11 se muestra una revisión de la patogenia y la etiología de la enfermedad, pero debe recordarse que hay muchas otras hipótesis posibles, que incluyen reacciones cruzadas entre microorganismos e xxx y la generación de anti-xxx de linfocitos T, responsables del desarrollo de la auto-sensibilización de xxx .

Vasculitis sistémica 

Estos trastornos se han ligado por tradición a la patogenia mediada por inmunocomplejos pero en realidad aún se desconocen los verdaderos factores causales de las lesiones vasculares.
Con frecuencia se asocian a la presencia de anticuerpos circulantes contra componentes del gránulo polimorfonuclear primario, una serina proteasa III en el caso de la granulomatosis de Wegener y una mieloperoxidasa en algunos pacientes con poliarteritis nodosa.


Bioquímica del rendimiento atlético.

La biología celular del músculo ayuda a entender el rendimiento de los atletas e indica el camino a seguir para mejorar sus posibilidades.

"¡Preparados!"
Un silencio absoluto reina en el estadio mientras 60.000 pare de ojos se clavan en ocho de los hombres más ligeros del planeta.
Nos encontramos en Sevilla.
Es el 22 de agosto de 1999 y se celebra la final de 100 metros de los campeonatos mundiales de atletismo. 

El pistoletazo de salida resuena en el aire cálido del atardecer.
Ruge la muchedumbre cuando los deportistas corren por sus calles.
Tan sólo 9,80 segundos después el ganador cruza la meta.
Ese día lo fue Maurice Greene, un atleta de 25 años de Los Angeles.

¿Por qué es Maurice Greene, y no Bruny Surin de Canadá, que acabó segundo, el hombre más veloz del planeta?
Después de todo, ambos han entrenado sin descanso durante años, manteniendo un régimen ascético basado en ejercicio, reposo, una dieta estricta y poco más.
La respuesta, compleja, comporta un sinnúmero de pequeños detalles, como la actitud mental de los atletas el día de la competición o incluso el diseño de sus zapatillas deportivas.
Pero en las carreras cortas, al depender de la potencia, el factor fisiológico resulta determinante:
las fibras musculares de las piernas de Greene, en particular sus muslos, desarrollan, en el breve intervalo de la carrera, una potencia ligeramente mayor que la de sus contrincantes.

Los hallazgos recientes de diversos laboratorios, incluido el nuestro, han ampliado el conocimiento sobre la adaptación del músculo al ejercicio, o a la falta del mismo, y sobre el grado de alteración que puede sufrir para acometer pruebas diversas, como el esfuerzo prolongado del maratón o el ímpetu de los 100 metros.
Esta información nos ayuda a comprender por qué triunfó Greene y a conocer mejor las posibilidades del común de la gente. 
También arroja luz sobre la cuestión permanente de si los corredores, los nadadores, los ciclistas y los esquiadores de elite nacen diferentes del resto de los mortales o si el tesón y el entrenamiento convierten a cualquiera en un campeón. 

El músculo, el tejido más abundante del cuerpo humano, destaca por su versátil adaptación.
Un entrenamiento intenso con pesas puede duplicar o triplicar el tamaño del músculo;
por contra, la falta de ejercicio -durante un viaje espacial, por ejemplo- puede encogerlo un 20 por ciento en dos semanas.
Los fenómenos bioquímicos y biomecánicos subyacentes bajo esa plasticidad son sumamente complejos, pero decenios de investigación nos han permitido elaborar una imagen bastante completa de la respuesta muscular al entrenamiento atlético.

Un músculo es un haz de células, fibras que se mantienen juntas merced al tejido conjuntivo.
Cada fibra de músculo esquelético consta de una membrana, muchos núcleos dispersos donde se alojan los genes e instalados justo bajo la membrana a lo largo de la fibra, y miles de haces internos, denominados miofibrillas, que constituyen el citoplasma celular.
Las fibras musculares mayores y más largas miden 30 centímetros de longitud y de 0,05 a 0,15 milímetros de anchura; contienen varios millares de núcleos.

Las miofibrillas llenan el interior de la fibra muscular, tienen la misma longitud que ésta y son la parte responsable de la contracción de la célula en respuesta a los impulsos nerviosos.
Las células neuromotoras se extienden desde la médula espinal hasta un grupo de fibras, integrando una unidad motora.
En los músculos de las piernas, una neurona motora controla, "inerva", de cientos a mil fibras musculares.
Por contra, cuando se requiere una precisión finísima, como en el control de un dedo, el globo ocular o la laringe, una neurona motora controla sólo una o, a lo más, unas cuantas fibras musculares. 

La contracción propiamente dicha de la miofibrilla se logra merced a unas microunidades, los sarcómeros; 
éstos se unen entre sí por sus extremos para constituir una miofibrilla.
En el interior del sarcómero distinguimos la miosina y la actina, dos proteínas filamentosas cuya interacción provoca la contracción.
Durante la contracción un sarcómero se acorta como un telescopio retráctil:
los filamentos de actina, situados en los extremos de un filamento de miosina, se deslizan hacia el centro de ésta.

Posee la molécula de miosina un componente, conocido por cadena pesada, que determina las características funcionales de la fibra muscular.

En un adulto, existen tres isoformas o variedades de cadenas pesadas.
Son la I, IIa y IIx, apelativos que reciben asimismo las fibras que las contienen.
A las fibras del tipo I se las conoce también por fibras lentas; a los tipos IIa y IIx, por fibras rápidas.
Hay en ello una razón de peso.

La máxima velocidad de contracción de una fibra del tipo I es unas diez veces menor que la de una fibra del tipo IIx.
La velocidad de las fibras del tipo IIa se encuentra a medio camino entre las dos anteriores.

Figura 1

LAS PIERNAS DE BRIAN LEWIS, corredor de los 100 metros (página anterior), albergan una proporción de fibras musculares rápidas mayor que las de un corredor de fondo u otro atleta que fíe en la resistencia.
La celeridad de contracción de las fibras rápidas IIx decuplica la de las fibras lentas del tipo I;
entre ambas se sitúa la velocidad de contracción de las fibras del tipo IIa.

La materia muscular

Las velocidades de contracción de las fibras difieren en atención a la forma en que éstas descomponen la molécula de adenosín trifosfato en la región de la cadena pesada para obtener la energía necesaria en la contracción.
Las fibras lentas desarrollan un metabolismo aeróbico, bastante eficiente;
mientras que las fibras rápidas dependen más del metabolismo anaeróbico.
Así, las fibras lentas cumplen un papel excelente en ejercicios de resistencia, carreras largas, ciclismo y natación;
las fibras rápidas son esenciales para suministrar potencia (caso de la halterofilia y las carreras cortas).

Si nos fijamos por ejemplo en el músculo cuádriceps del muslo de un adulto sano observaremos un número aproximadamente igual de fibras lentas y rápidas.
Pero las variaciones interindividuales son notables.

Nosotros hemos observado personas con un porcentaje de fibras lentas en el cuádriceps de sólo un 19 por ciento y otras de hasta un 95 por ciento.
El sujeto con un 95 por ciento de las fibras lentas podría, a buen seguro, llegar a ser un maratoniano consumado, pero no tendría nada que hacer en las carreras cortas;
lo opuesto sería cierto para una persona con un 19 por ciento de las fibras lentas. 

Amén de los tres tipos de fibras enumerados, existen híbridos que portan dos isoformas de miosina.
Las fibras híbridas siguen una escala continua que va de las que están casi totalmente compuestas por la isoforma lenta a fibras donde predomina casi por entero la rápida.
En cualquier caso, las características funcionales de la fibra se asemejan a las del tipo de isoforma dominante.

La miosina es una proteína insólita.
Llama la atención que, tras la comparación entre las isoformas de distintos mamíferos, se hallara que hubiera poca variación de una especie a otra.
La miosina lenta (tipo I) en ratas es mucho más parecida a la isoforma lenta en humanos que a la propia miosina rápida en ratas. 
Se refuerza así la idea de que la presión evolutiva ha mantenido a las isoformas de miosina funcionalmente distintas, presión que ha conservado isoformas particulares que surgieron hace millones de años.

Incluso las criaturas más antiguas y primitivas contaban con isoformas de miosina no muy diferentes de las nuestras. 

Figura 2

EL MÚSCULO consta de células repletas de miofibrillas, haces formados por unidades contráctiles denominadas sarcómeros.
Los componentes principales de los sarcómeros son la actina y la miosina, dos proteínas filamentosas.
A medida que los sarcómeros se contraen y se relajan, eses moléculas se deslizan una sobre otra.

Espesor muscular

Las fibras musculares no pueden dividirse para formar otras nuevas.
Con la edad se van perdiendo fibras musculares, que nunca se recuperar n.
Por tanto, el músculo sólo aumentará de masa cuando sus fibras componentes adquieran mayor espesor.

Espesor que depende de la creación de miofibrillas adicionales.
El estrés mecánico que produce el ejercicio sobre los tendones y otras estructuras conectadas al músculo pone en marcha proteínas de señal que activan los genes responsables de la síntesis muscular de más proteínas contráctiles.
Estas proteínas, en su mayoría miosina y actina, se van necesitando a medida que la fibra va produciendo grandes cantidades de ulteriores miofibrillas.

Se requieren más núcleos para la síntesis y mantenimiento de más proteína y para conservar cierta proporción entre volumen celular y número de núcleos.
Sabemos que las fibras musculares contienen múltiples núcleos, pero los del interior de las fibras no pueden dividirse;
los nuevos núcleos provienen de células satélite (o células madre).
Dispersas entre los múltiples núcleos en la superficie de la fibra de un músculo esquelético, las células satélite están separadas de las musculares. 
Las células satélite poseen un núcleo solo y pueden replicarse por división.
Tras fusionarse con la fibra muscular, sirven de fuente de abastecimiento de nuevos núcleos para la fibra en crecimiento.

Las células satélite proliferan en respuesta al desgaste debido al ejercicio.
Se supone que tamaña actividad provoca microfisuras en las fibras.
El área dañada atrae a las células satélite, que se incorporan al tejido muscular y comienzan a producir proteínas para rellenar el espacio.
A medida que se van multiplicando las células satélite, unas persisten en su estado y otras se incorporan en el tejido muscular.
Los núcleos de éstas se tornan indistinguibles del resto de los núcleos de la célula.
Con tales núcleos supernumerarios, la fibra sintetizará más proteínas y creará nuevas miofibrillas.

Para fabricar proteínas, las células ejecutan el programa cifrado en los genes, donde se indica el orden de engarce de los aminoácidos componentes;
dicho de otro modo, especifica qué proteína se sintetizar. 
El proceso a través del cual la información pasa del núcleo celular al citoplasma, donde se producirá la proteína, comienza con la transcripción.

Acontece ésta en el núcleo, cuando la información de un gen (codificada en ADN) se copia en una molécula de ARN mensajero.
Luego, el ARNm porta esta información a los ribosomas, fuera del núcleo.
Los ribosomas ensamblan aminoácidos en proteínas -en actina o en una de las isoformas de miosina- de acuerdo con las especificaciones del ARNm.

A este último proceso se le llama traducción. 
Por "expresión" del gen se entiende el mecanismo entero de síntesis de la proteína a partir del gen en cuestión.

En el estudio del músculo esquelético importan dos aspectos fundamentales, que atañen directamente al rendimiento atlético.
Uno se centra en la hipertrofia muscular causada por el ejercicio y demás estímulos; 
el otro contempla la conversión de las fibras musculares de un tipo en otro por obra de dicho ejercicio.

En ambos frentes se han registrado notables avances, con la participación, entre otros, de los redactores de este articulo.

Hemos de remontarnos a comienzos de los años sesenta, cuando A. J. Buller y John Carew Eccles, de la Universidad Nacional Australiana en Canberra, y el equipo encabezado por Michael Bárány, del Instituto de Enfermedades Musculares en Nueva York, realizaron una serie de estudios en animales en los que convertían fibras musculares rápidas en fibras lentas, y a la inversa. 
Emplearon varios métodos, sobre todo el de inervación cruzada.
Conmutaron un nervio que controlaba un músculo lento con otro asociado a un músculo rápido, de suerte que cada uno controlara el tipo de fibra opuesto.

Provocaron también la estimulación eléctrica de los músculos durante períodos prolongados o, para obtener el efecto opuesto, cortaron el nervio motor.

Durante los años setenta y ochenta, la investigación se propuso demostrar que la capacidad de una fibra para cambiar de tamaño y tipo se daba también en humanos.
Un ejemplo, extremo, de este efecto de plasticidad muscular se produce en las personas que han sufrido una grave lesión de la médula espinal con la paraplejía consiguiente. 
La falta de impulsos nerviosos y la ausencia de actividad muscular provocan una merma extensa de tejido, acorde con lo esperado.
Menos imaginado resulta el cambio espectacular que experimenta el tipo de fibra.
Estos pacientes paralizados sufren una disminución aguda de la cantidad relativa de la isoforma lenta de miosina y un drástico incremento de la cantidad de isoformas rápidas de ésta.

Se ha demostrado que muchos de estos pacientes, después de cinco a diez años de parálisis, apenas tienen miosina lenta en el músculo vasto lateral, que es parte del cuádriceps del muslo; toda la miosina allí presente es del tipo rápido.
Recuérdese que en un adulto sano la mitad de las fibras son lentas y la otra mitad rápidas.
Nosotros propusimos que, para mantener la expresión de la isoforma lenta de la miosina, se requería la estimulación nerviosa del músculo mediante activación eléctrica.
Así, la electroestimulación muscular 0 el ejercicio inducido eléctricamente podían reintroducir, hasta cierto punto, la miosina lenta en los músculos paralizados.

Conversión del músculo 

La transformación de las fibras musculares no se limita al restablecimiento de músculos paralizados y otros casos extremos.
Cuando los músculos sanos se someten a cargas pesadas de forma repetida, como en los programas de entrenamiento con pesas, el número de fibras rápidas IIx desciende y se convierten en fibras rápidas IIa.
En estas fibras el núcleo deja de expresar el gen IIx y comienza a expresar el IIa.
Tras un mes de entrenamiento, todas las fibras musculares IIx acabar n transformadas en fibras IIa.
Al propio tiempo, las fibras aumentan su producción de proteínas y volumen.

A comienzos de los años noventa Geoffrey Goldspink propaso que el gen IIx constituía una especie de normalización por defecto.
Esta hipótesis se ha ratificado en estudios que han demostrado que las personas sedentarias tienen mayor nivel de miosina IIx en sus músculos que las activas y "en forma".
Agréguese a ello la correlación positiva entre miosina IIa y actividad muscular que se ha descubierto en investigaciones complementarias.

¿Qué ocurre cuando se abandona el ejercicio? 
Las fibras IIa se reconvierten en IIx, aunque no en la forma esperada.
Para elucidar los mecanismos de la transformación, se tomaron muestras musculares del músculo vasto lateral de nueve varones daneses jóvenes y sedentarios.

Después, se sometieron a un entrenamiento fuerte de resistencia, centrado en el cuádriceps, durante tres meses; a cuyo término se realizó otra biopsia muscular. 
Transcurrido ese período, se cortó el entrenamiento de resistencia y volvieron a su vida sedentaria.
A los tres meses de inactividad (de acuerdo con su proceder previo al entrenamiento) se les volvió a realizar una tercera y última biopsia.

Como era de esperar, la cantidad relativa de la isoforma IIx de miosina rápida en el músculo vasto lateral cayó de un 9 por ciento a un 2 por ciento, de media, en el período de entrenamiento.
Se contaba con que durante el período de inactividad la cantidad relativa de la isoforma IIx volvería simplemente al nivel previo al entrenamiento, el 9 por ciento.
Para nuestra sorpresa, la cantidad relativa de miosina IIx alcanzó un valor medio del 18 por ciento.

No se continuaron las biopsias tras estos tres meses, pero sospechamos que la miosina IIx volvió a su valor inicial del 9 por ciento algunos meses después.

Desconocemos el motivo de este fenómeno de sobreexpresión de la isoforma IIx de la miosina rápida.

Lo que no nos impide extraer algunas conclusiones prácticas. 
En concreto, si los corredores de distancias cortas quieren aumentar la cantidad relativa de las fibras más rápidas de sus músculos, lo mejor sería empezar por hacer desaparecer las fibras rápidas mediante un entrenamiento intenso, para moderar luego éste a la espera de que las fibras más rápidas se dupliquen.
Así, sería aconsejable que programasen un período de entrenamiento moderado, o de disminución gradual, en vistas a una competición importante.

De hecho, muchos velocistas ya aplican este régimen por propia experiencia, sin conocer la fisiología subyacente. 

Figura 3

CIERTOS RESULTADOS EXPERIMENTALES INESPERADOS tienen aplicaciones prácticas para el atleta. 
Tal como se esperaba, la concentración de miosina rápida IIx decayó durante el entrenamiento de resistencia. 
Pero cuando se abandonó el ejercicio, en vez de volver de nuevo a los niveles de partida, la cantidad relativa de IIx duplicó estos valores tras tres meses de inactividad. 
¿Qué significado encierra ello para un velocista? 
Puesto que en su caso la miosina IIx es decisiva, deberá programar un período de entrenamiento moderado antes de una competición.

¿Lento a rápido?

La conversión de las fibras rápidas IIa en IIx (y a la inversa) es una consecuencia natural del entrenamiento y del cese del ejercicio.

Pero,
¿a qué se debe la conversión de las fibras lentas de tipo I en fibras rápidas de tipo II?
Los resultados a este respecto son más sombríos. 

Muchos experimentos realizados a lo largo de los últimos veinte años no aportaron pruebas de la conversión de fibras lentas en rápidas, o viceversa.
Pero a comienzos de los años noventa nuestros resultados indicaban que un ejercicio intenso puede convertir fibras lentas en fibras rápidas IIa.

En un estudio de tres meses, centrado en velocistas de distancias cortas, se combinó el entrenamiento de resistencia intensa con carreras (ejercicios básicos en el ciclo anual de entrenamientos de un "sprinter").
Por su parte, el equipo encabezado por Mona Esbörnsson, del Instituto Karolinska, obtuvo datos similares en un estudio con doce personas que no eran atletas de elite.
Estos resultados respaldan la tesis de que un programa de entrenamiento intenso con levantamiento de pesas, unido a otras formas de ejercicio anaeróbico, convierte no sólo las fibras IIx en IIa, sino también las fibras del tipo I en IIa.

Si una clase de ejercicio convierte fibras I en fibras IIa,
¿habrá otra que transforme IIa en I? 
No ha quedado demostrado.
Lo cierto es que los atletas de competiciones de resistencia -carreras de fondo, natación, ciclismo y otras- tienen muchas fibras lentas del tipo I (hasta un 95 por ciento) en las principales masas musculares, las de las piernas por ejemplo.
Se ignora si esos deportistas nacieron con tamaño porcentaje de fibras I y vencieron en las disciplinas en que aprovecharon esa característica congénita o si la proporción muscular de fibras del tipo I aumentó con el entrenamiento prolongado.
Sabemos, no obstante, que, si las fibras rápidas del tipo IIa se transforman en lentas de tipo I, el tiempo re querido para esta conversión es muy superior al que requiere la conversión de fibras IIx en IIa.

Quizá los deportistas maratonianos han nacido diferentes del resto de la gente.
Pudiera ser también que los velocistas presentaran peculiaridades congénitas: 
éstos, en contraste con los corredores de distancias largas, se beneficiarían de un porcentaje pequeño de fibras del tipo I. 
Lo que no significa que un aspirante a velocista con demasiadas fibras del tipo I deba retirarse, pues el entrenamiento de resistencia hipertrofia las fibras del tipo II el doble que las fibras del tipo I.
Así, el entrenamiento con pesas aumenta la superficie ocupada por las fibras rápidas sin modificar la proporción muscular entre fibras lentas y rápidas.
Las características funcionales del músculo dependen de las superficies relativas ocupadas por las fibras lentas en la sección muscular.

Cuanto mayor sea la superficie relativa ocupada por las fibras rápidas, tanto más veloz será el músculo.
Por tanto, un "sprinter" puede modificar las características de sus músculos, si los ejercita intensamente, lo que aumenta la superficie relativa de fibras rápidas.

En 1988, el grupo encabezado por Michael Sjöström, de la Universidad de Umea en Suecia, demostró que, en cortes histológicos del músculo, las superficies medias de las secciones transversales de los tres tipos principales de fibras eran casi idénticas en los músculos vastos laterales de un grupo de corredores de maratón.
En estos individuos la superficie media de las secciones de tipo I, IIa y IIx era, respectivamente, de 4800, 4500 y 4600 micrómetros cuadrados.

Por contra, para un grupo de velocistas de distancias cortas la superficie media de las secciones de estas fibras era, respectivamente, de 500O, 7300 y S900 micrómetros cuadrados.
Nosotros hemos recogido resultados similares.

Las transformaciones difíciles de obtener por entrenamiento, así las de fibras IIa en fibras I, se lograr n muy pronto a través de la manipulación genética.
Se estudia la expresión de los genes de miosina presentes en el genoma humano, que, sin embargo, no se expresan naturalmente. 
Esos genes son vestigios archivados de ciertos tipos de miosina que debieron dotar, a nuestros antepasados mamíferos, de un tejido muscular muy rápido, utilísimo para escapar de los depredadores.

Las vacunas que insertarán genes artificiales en los núcleos de las células musculares, serán, a buen seguro, las sustancias estimuladoras (dopantes) del futuro. 
A lo largo de la historia del deporte hubo siempre quien abusara de los estimulantes.
El Comité Olímpico Internacional y otros organismos han venido prohibiendo tales compuestos, con pruebas de control y sanciones.
La invención de nuevas sustancias impone la invención de nuevas pruebas de detección.
En un futuro próximo, cuando los atletas utilicen terapias génicas, estaremos en una situación inédita, en la que quizá resultará imposible identificar como foráneos los elementos genéticos y las proteínas cifradas por ellos en las células musculares.

En la mayoría de los países desarrollados la terapia génica constituye, por razones múltiples, un campo intenso de investigación.
En lugar de tratar las enfermedades musculares con medicamentos, se espera ofrecer al organismo los instrumentos para fabricar las proteínas necesarias para su curación.
Se trata de estrategias viables, al menos en teoría, desde el momento en que se cosecharon los primeros éxitos en la obtención de copias artificiales de genes humanos.
Tales genes pueden introducirse en el cuerpo humano donde, en muchos casos, sustituyen un gen defectuoso.

Igual que nuestros genes, los artificiales son de ADN. 
Se pueden administrar al cuerpo de varias maneras.
Si suponemos que el gen determina una de las muchas proteínas u hormonas de señalización que estimulan el crecimiento del músculo, el camino directo sería inyectar el ADN en el músculo.
Las fibras musculares captarían el ADN y lo incorporarían en su acervo génico. 

Pero ese método no resulta muy eficaz.
Se prefiere el empleo de virus que transporten la carga génica hasta el núcleo celular.
A grandes rasgos, un virus no es más que un conjunto de genes empaquetados en una cápside proteica capaz de engarzarse a una célula e inyectar los genes.
Se sustituyen genes propios del virus por el gen artificial, que pasará sin dificultad a las células del cuerpo.

Mas, a diferencia de la inyección directa de ADN, la carga génica artificial no penetra sólo en las fibras musculares, sino también en otras, de manera singular en las células sanguíneas y hepáticas.
La expresión del gen artificial en un tipo celular distinto del pretendido comportará, sin duda, efectos secundarios. 
Una sobreexpresión del gen en el músculo cardíaco podría producir una cardiomegalia, con riesgo de insuficiencia cardíaca.
Para obviar ese problema se trabaja en otra línea, que comporta extraer del paciente los tipos celulares específicos, incorporar el gen artificial en el laboratorio y reintroducir las células en el organismo.

Si, como es de temer, los atletas abusaran de tales técnicas, las autoridades deportivas se ver n en graves apuros para descubrir el fraude, pues los genes artificiales producir n proteínas idénticas a las naturales.

Bastará una inyección, y no se detectará el ADN introducido a menos que se conozca el genoma de las células modificadas.
Por eso los laboratorios encargados de los análisis deberán contar de antemano con muestras del tejido.
Pero,
¿se prestarán los atletas a una biopsia antes de la competición?
No parece previsible y macho tememos que el dopaje génico escape a todo control.

Figura 4

LOS SPRINTERS Y LOS CORREDORES DE MARATÓN manifiestan diferencias notables en la musculatura de sus piernas.
Las fibras rápidas se apoyan en el metabolismo anaeróbico; las fibras lentas dependen del metabolismo aeróbico.
Por eso, las fibras lentas importan en deportes de resistencia y las rápidas en la velocidad en distancias cortas y la halterofilia.
El velocista es Brian Lewis; el maratoniano, Khalid Khannouchi.

ENVEJECIMIENTO MUSCULAR

Con la edad nuestros músculos se debilitan y pierden ligereza.
¿A qué atribuirlo?

El envejecimiento comporta muchos cambios de los músculos esqueléticos.
El que llama más la atención es la pérdida de masa, que comienza a los 25 años. 

Llegados a los 50, hay ya un 10 por ciento menos; a los 80 se ha perdido un 50 por ciento aproximadamente.
Esta merma asociada a la edad viene causada por la pérdida de fibras musculares.
Los ejercicios de fuerza pueden frenar la pérdida de masa del músculo en conjunto, al aumentar el volumen de las fibras, si bien no evita la disminución de su número.

Antes de atrofiarse, las fibras cambian de forma y aspecto. 
En un corte histológico, las fibras musculares de los jóvenes se distinguen por su carácter angular; 
en los ancianos suelen ser más redondeadas y en casos extremos con forma de plátano.
El envejecimiento parece, además, estimular la "agrupación en tipos": 
en los músculos de individuos jóvenes y de mediana edad las fibras rápidas y lentas se alternan siguiendo el patrón de un tablero de ajedrez, mientras que en el músculo senil las fibras se agregan formando grupos de células lentas o rápidas (este fenómeno se manifiesta también en gente joven que padece ciertas enfermedades del sistema neuromotor).

Algunos fisiólogos atribuyen la agrupación en tipos del músculo envejecido a un proceso completo en el que los nervios que controlan el músculo cambian de una fibra muscular a otra.
La unidad motora comprende todas las fibras musculares controladas, o "inervadas", por un único nervio motor que parte de la médula espinal. 
Al envejecer, algunos de estos nervios motores "mueren". 
Las fibras musculares del nervio dejan de recibir estímulos, se atrofien y mueren, a menos que sean reinervadas por otro nervio motor vecino.

Si una fibra muscular se reinerva con un nervio de un tipo distinto de unidad motora - así, si una fibra muscular rápida se reinerva con un nervio de fibras lentas - la fibra se quedará con señales en conflicto.

Atendiendo a su desarrollo, se trata de una fibra rápida; 
pero recibe estímulos que conduce a un patrón de activación propio de una fibra lenta.
A la larga, parece que este cambio en la estimulación transforma la fibra rápida en fibra lenta (o viceversa, en el caso opuesto). 

El envejecimiento es más pernicioso para las fibras rápidas, que se atrofien a un ritmo mayor de lo que lo hacen las lentas.
Se venia creyendo que el cambio gradual en la distribución de las fibras lentas y rápidas favorecía a las fibras lentas.
Esto explicaría por qué un chico de 10 años deja atrás a su septuagenario abuelo en una carrera de 100 metros lisos, mientras que éste le gana en una carrera de fondo de 10.000 metros. 

Se trata, sin embargo, de una hipótesis controvertida, pues no ha podido demostrarse el aumento previsto de la cantidad relativa de fibras lentas.
En un estudio reciente, abordamos el problema desde otro enfoque.
Convencimos a un grupo de doce ancianos con una edad media de 88 años para que nos cediesen una biopsia de su músculo vasto lateral (que está situado en la parte frontal del muslo y es uno de los músculos esqueléticos humanos mejor estudiados). 
A partir de estas muestras, y operando con agujas finas bajo el microscopio, separamos las fibras y determinamos la composición de isoformas de miosina de 2300.

Sabemos que todos los humanos no tienen sólo fibras lentas y rápidas puras, sino también fibras que contienen a la vez las isoformas de miosina lenta y IIa (rápida) o las dos isoformas rápidas (IIa y IIx).
En el músculo vasto lateral joven estas fibras hibrides son escasas:
menos del 5 por ciento de las fibras contiene a la vez la miosina lenta I y la isoforma de miosina rápida IIa.
En nuestros ancianos, un tercio de todas las fibras examinadas contenía ambas isoformas de miosina.
Esta fibra híbrida era el tipo predominante en los músculos muy seniles.

La cuestión de si el músculo senil tiene más fibras lentas no puede dirimirse con un simple si o no.
No se trata de un cambio en la proporción entre fibras lentas y rápidas, sino de un obscurecimiento de la frontera entre fibras lentas y rápidas.
En el músculo muy senil un tercio de las fibras no son estrictamente ni lentas ni rápidas. sino algo intermedio.

Figura a

EL LEVANTAMIENTO DE PESAS limita la pérdida de masa muscular que acompaña al envejecimiento.
Pero nada puede evitar los cambios en la forma y la distribución de los diferentes tipos de fibra (músculo joven, superior; músculo senil, inferior).

Un nuevo mundo feliz

¿Cómo será el atletismo en la era de la potenciación del rendimiento por métodos genéticos?
Imaginemos la final de 100 metros lisos de los Juegos Olímpicos del año 2012.
Tras las semifinales, los corredores de apuestas ya habían arriesgado por el corredor de la calle cuatro, John Doeson, quien ya en octavos de final rompió en tres centésimas de segundo el récord mundial vigente.
En cuartos de final batió la plusmarca en quince centésimas. 
En semifinales rebajó el récord mundial a unos increíbles 8,94 segundos, y cruzó la meta más de diez metros por delante del segundo.
Los periodistas comentaban en televisión que se estaba asistiendo a "algo fuera de este mundo".

No del todo, pero casi.
En 2012 la terapia génica es una técnica médica muy difundida.
Doce meses antes de las olimpiadas, un médico planteó a Doeson una propuesta capaz de tentar a cualquier velocista. 
¿Qué le parecía si lograra que sus células musculares expresaran la isoforma IIb de miosina? 
En condiciones normales, esta isoforma no se expresa en el hombre, pero el gen está en los músculos del esqueleto, listo para actuar.

Esta isoforma conferiría a las fibras musculares unas celerísimas características funcionales.
La presentan la rata y otros micromamíferos que necesitan huir como balas para eludir a los depredadores.
La isoforma IIb se contrae mucho más deprisa que las fibras IIx o IIa y, por ende, genera mayor potencia.
Aunque Doeson no acababa de entender de qué le estaban hablando, sí retuvo las palabras "velocidad" y "energía".

El médico prosiguió.
El gen expresa un factor de transcripción, proteína que a su vez activa el gen de la isoforma IIb de la miosina.
Al factor de marras, descubierto hacía poco, lo llamaban velocifina. 
Levantando un minivial ante la mirada de Doeson, declaró: 
"Este es el ADN del gen artificial de la velocifina. 

Con sólo unas partículas en tus cuádriceps, poplíteos y glúteos, tus músculos empezar n a producir velocifina, que activará el gen de la miosina IIb.
De aquí a tres meses, tus músculos contendrán una buena proporción de fibras IIb, condición que te permitirá batir el récord de los 100 metros lisos con facilidad.
Tus músculos continuar n sintetizando velocifina durante años sin más inyecciones.
Y sin una biopsia muscular de los cuádriceps, los poplíteos o los glúteos, nadie sabrá de esta modificación genética." Doeson se encamina ahora a la pista.
Recuerda que el médico le había asegurado que era un tratamiento exento de efectos secundarios. 
De momento, todo perfecto.
Se sitúa en la cuarta calle.
"Preparados.
Listos.
BANG!".
Un par de segundos después, Doeson saca ya dos metros al pelotón, ventaja que aumenta con el paso de los segundos.
Sus zancadas son visiblemente más poderosas y frecuentes que las del resto.
Se siente bien al pasar los 30, 40 y 50 metros.
De repente, en los 65 metros, siente una punzada en el poplíteo.

En los 80 metros la punzada se convierte en un dolor insoportable cuando contrae su músculo poplíteo.
Una décima de segundo después, el tendón de la rótula de Doeson cede, incapaz de resistir las fuerzas brutales generadas por su músculo cuádriceps.

El tendón de la rótula arranca parte del hueso de la tibia, que se quiebra, y el cuádriceps entero sale disparado por todo lo largo del hueso fémur.
Doeson cae, su carrera deportiva se acabó.

A medida que la genética comience a introducirse en la praxis médica se producir n cambios profundos en los deportes, no siempre beneficiosos.
La sociedad debe preguntarse si las nuevas plusmarcas y otros laureles atléticos son en realidad una simple continuación del empeño por demostrar lo que nuestra especie puede dar de sí. 


Autismo precoz

La investigación reciente sobre las causas de esta enfermedad se centra en los genes que controlan el desarrollo del cerebro.

Desde hace más de medio siglo la ciencia lucha por desentrañar el origen y la naturaleza del autismo, enfermedad del comportamiento muy compleja y caracterizada por síntomas diversos que suele manifestarse antes de que el niño haya cumplido los tres años.
Los niños autistas se muestran incapaces de interpretar los estados emocionales de los otros; no reconocen la ira, ni la pena ni el engaño. 
Limitados en su capacidad de expresión oral, les cuesta iniciar o mantener una conversación.
Se les ve a menudo obsesionados con un tema, actividad o gesto. 

Estos comportamientos cerrados plantean problemas preocupantes. 
¿Cómo puede el niño estar en el aula, si es imposible convencerle de que deje de golpear su cabeza en el pupitre? ¿Cómo puede trabar amistad con otros si su único interés se centra, por ejemplo, en los calendarios? Cuando los niños autistas sufren, además, retraso mental -como ocurre en la mayoría de los casos- el pronóstico se agrava.
Con una terapéutica intensa del comportamiento mejoran, pero los síntomas que presentan impiden que puedan vivir a su aire, aun cuando tengan un cociente intelectual normal.

Mi interés por las causas del autismo es reciente. 
Empezó por puro azar.
En mis estudios embriológicos había abordado diversos defectos del cerebro.
Pero en 1994 asistí a una reunión científica sobre investigaciones en defectos innatos donde se presentó un estudio ejemplar.
Dos especialistas en oftalmología pediátrica, Marylin T. Miller y Kerstin Strömland, describieron los resultados sorprendentes de un trabajo sobre problemas de motilidad ocular en víctimas de la talidomida, el fármaco de las náuseas matinales en las jóvenes embarazadas, desencadenante de una epidemia de defectos congénitos en los años sesenta.
Los sujetos del estudio eran adultos que, en su etapa de gestación, habían estado expuestos al fármaco.
Miller y Strömland aportaron una observación que se les había escapado a otros investigadores: un 5 por ciento de las víctimas de la talidomida padecían autismo, con una frecuencia 30 veces superior a la habitual en el resto de la población general.

Al oír los resultados, noté como una revelación que excitó todo mi cuerpo.
En el empeño por descubrir las causas del autismo, la investigación había venido insistiendo en la determinación del inicio de la patología.
Se había centrado en los momentos tardíos de la gestación o en los inmediatos al alumbramiento para relacionarlos con el origen, aunque no se disponía de ningún dato que apoyase una u otra hipótesis.
La relación con la talidomida introdujo de repente una luz nueva en el tema.
Sugería que el autismo se origina en las primeras semanas del embarazo, cuando el cerebro del embrión y el resto de su sistema nervioso comienzan a desarrollarse.
El trabajo de Miller y Strömland me convenció de que el misterio del autismo podría resolverse en cuestión de poco tiempo.

Factores genéticos

Dieciséis de cada 10.000 niños nacen con autismo o con alguna afección afín.

Desde que se describió la enfermedad en 1943, se han dado grandes pasos en la definición de los síntomas, pero seguimos sin conocer sus bases biológicas, cuyo desentrañamiento permitiría identificar los factores de riesgo y diseñar nuevas terapias.

Con el examen de la herencia del autismo se ha demostrado que se presenta en familias, aunque no de una manera clara.
Los hermanos de autistas tienen una probabilidad de un 3 a un 8 por ciento de padecer ellos la enfermedad.
Es decir, una proporción superior al 0,16 por ciento que representa el riesgo en la población general.
Aunque, también, proporción muy por debajo de la probabilidad del 50 por ciento, que caracterizaría a una enfermedad genética causada por una mutación dominante única (en que un gen defectuoso heredado de un solo progenitor basta para causar la afección), e incluso del 25 por ciento, que caracterizaría a una mutación recesiva única (en que una copia del gen defectuoso debe heredarse de cada progenitor).

Los resultados hereditarios encajan mejor dentro de los modelos que operan con variantes de genes diversos.
Para complicar las cosas, los parientes de autistas pueden presentar algunos síntomas, aunque no el cuadro completo que justifique el diagnóstico de la enfermedad.
Estos familiares podrían portar algunas variantes de los genes ligadas al autismo, sin llegar a la expresión plena de los factores genéticos.

Ciertos estudios de gemelos realizados en el Reino Unido confirman que el autismo tiene un componente hereditario, al par que sugieren la intervención complementaria del entorno. 
Si los factores fueran los únicos implicados, los gemelos monocigóticos (idénticos), que comparten los mismos genes, deberían tener también una probabilidad del 100 por ciento de coincidir en el diagnóstico.
No sucede así.
Cuando uno de los gemelos es autista, el otro sólo tiene una probabilidad del 60 por ciento de que se le diagnostique la misma afección.
Ese segundo gemelo muestra, sin embargo, una probabilidad del 86 por ciento de desarrollar algunos de los síntomas del autismo.
De las cifras se desprende la existencia de factores modificadores de la predisposición genética.

Embriología del autismo

Se han identificado ya diversos factores de riesgo ambiental. 
La exposición en el útero a la rubéola o a compuestos causantes de defectos congénitos (el etanol o el ácido valproico, por ejemplo) aumenta la probabilidad de autismo.
Mayor probabilidad presentan también los afectados por fenilcetonuria o esclerosis tuberosa, patologías genéticas.
Pese a todo, ningún factor de ésos se persona con la suficiente frecuencia para atribuirle la responsabilidad exclusiva.

Además, la mayoría de tales exposiciones a enfermedades o productos tóxicos afectarían por igual a los gemelos, a los dos.
El entorno ha de participar de una manera mucho más sutil de lo descubierto hasta ahora.
Se ignora por qué la combinación de factores induce en unos casos los síntomas y no en otros.
Esa disparidad dificulta sobremanera la búsqueda de las causas del autismo.

En 1994 Miller y Strömland añadieron un nuevo factor ambiental: la exposición a la talidomida en el útero.
Todas las personas del estudio -suecos adultos nacidos entre finales de los cincuenta y principios de los sesenta- presentaban algunas de las malformaciones que dieron triste fama a la talidomida.
Brazos y piernas malformados, orejas y pulgares deformes, si no ausentes, y disfunciones neurológicas en ojos y músculos faciales.
Por embriología sabemos qué órganos se desarrollan en cada etapa del embarazo; podemos, pues, señalar los días exactos en que se induce una malformación: se atenta contra el pulgar al vigésimo segundo día de la concepción, contra el oído externo entre el vigésimo y el trigésimo, contra brazos y piernas entre los días 25 y 35.

El estudio de Miller y Strömland resultaba para mí apasionante en un punto: la mayoría de las víctimas de la talidomida con autismo mostraban anomalías en las orejas, no así en brazos o piernas.
Esa regularidad evidenciaba que habían padecido la agresión en una fase precoz de la gestación -entre 20 y 24 días después de la concepción- antes de que muchas madres supieran que estaban embarazadas.

No hay mejor pista para saber qué le pasó a un embrión que conocer cuándo pasó.
En el caso del autismo inducido por talidomida, el período crítico es bastante más precoz que lo que se creía.
Antes de la cuarta semana de gestación se forman muy pocas neuronas; además, en su mayoría son las motoras de los nervios craneales, que gobiernan los músculos de los ojos, oído, cara, mandíbulas, garganta y lengua. 
El soma de estas neuronas se encuentra en el tronco cerebral, la región situada entre la médula y el resto del cerebro.
Puesto que estas neuronas motoras se desarrollan al mismo tiempo que el oído externo, podría predecirse que las víctimas de la talidomida con autismo sufrirían también disfunciones de los nervios craneales.
Miller y Strömland confirmaron la predicción; observaron que todos los sujetos autistas padecían anomalías en movimientos oculares, expresión facial o en ambos. 

Por pura lógica había que preguntarse entonces si los casos de autismo debidos a la talidomida se parecían o no a aquellos otros cuya causa se desconocía.
Salvo en su comportamiento, los autistas acostumbran tener un aspecto normal, incluso atractivo.
También su talla es normal, con una cabeza entre normal y un poco grande.

Sin embargo, los escasos estudios que han analizado rasgos ajenos al comportamiento nos han revelado la existencia de anomalías físicas y nerviosas poco importantes, aunque idénticas a las vinculadas al autismo relacionado con la talidomida.
Por ejemplo, ciertas malformaciones menores del oído externo -en particular la rotación posterior, con la zona superior de la oreja que se inclina hacia atrás más de 15 grados- se presentan más a menudo en niños autistas que en niños con un desarrollo normal, niños con retraso mental o hermanos de niños autistas.
Las disfunciones de los movimientos oculares se habían asociado con el autismo antes del estudio de la talidomida.
La ausencia de expresión facial constituye uno de los comportamientos utilizados para el diagnóstico de la enfermedad. 

Neurobiología del autismo

¿Se debían a cambios de la función de los nervios craneales todos los síntomas del autismo?
Probablemente no.
Resultaba más verosímil que las disfunciones nerviosas de los autistas reflejasen una lesión precoz del cerebro que no sólo interesara los nervios craneales, sino que produjera también efectos secundarios en fases más avanzadas del desarrollo cerebral. 
Es decir, la lesión del tronco cerebral podría entorpecer el desarrollo y tendido adecuado de redes nerviosas de otras regiones cerebrales, incluidas las que intervienen en funciones de nivel superior, como el habla, dando lugar a los síntomas de una conducta autista.
O quizá las malformaciones del oído y las disfunciones de los nervios craneales fueran sólo efectos secundarios de una lesión oculta.
Cualquiera que fuera la situación real, los trastornos de los autistas de causa desconocida eran bastante parecidas a las que presentaban las víctimas de talidomida con autismo.
En conclusión, muchos casos de autismo, si no todos, se iniciaban en una fase precoz de la gestación.

A la región del cerebro afectada según el estudio de la talidomida -el tronco cerebral- se había prestado escasa atención en la investigación de daños congénitos del cerebro.
No sólo del autismo.
Solemos asociar el tronco cerebral con funciones básicas, desde la respiración hasta la deglución, pasando por el equilibrio, la coordinación motora y otros. 
Muchos aspectos del comportamiento alterados en el autismo -piénsese en el lenguaje, la planificación y la interpretación de claves sociales- están, así se supone, controlados por regiones superiores del cerebro, como la corteza cerebral y el hipocampo.

También parece que ciertos síntomas habituales en el autismo -falta de expresión facial, hipersensibilidad al tacto y al sonido, y trastornos del sueño- se originan en las regiones del cerebro asociadas con funciones básicas. 
Además, la alteración más frecuente del cerebro autista no es un cambio en el prosencéfalo, sino una reducción en el número de neuronas del cerebelo, un centro magno de procesamiento del postencéfalo con funciones importantes en el control del movimiento muscular. 

La confusión reinante en torno a las regiones cerebrales implicadas en el autismo podría deberse a la falta de seguridad sobre el lugar donde se controlan las funciones.
Así, el grupo dirigido por Eric Courchesne ha demostrado que se activan partes del cerebelo durante la realización de ciertas tareas que requieren un procesamiento cognitivo de alto nivel.

Otra dificultad reside en la propia complejidad de los síntomas del autismo.
Si se pudiera comprobar el valor diagnóstico de algunas disfunciones elementales del comportamiento autista, avanzaríamos con paso más firme en la búsqueda del origen nervioso de las mismas. 

En 1995 nuestro grupo prosiguió el estudio sobre la talidomida examinando el tronco encefálico de una autista. 
Las muestras de tejido procedían de la autopsia de una joven con autismo de causa desconocida; había fallecido en los años setenta, pero afortunadamente se conservaron muestras de tejido cerebral.

Cuando examinamos el tronco encefálico, nos sorprendió la ausencia casi absoluta de dos estructuras: el núcleo facial, que controla los músculos de la expresión facial, y la oliva superior, que es una estación de relé de la información auditiva.
Ambas estructuras surgen del mismo segmento del tubo neural del embrión, el órgano que originará el sistema nervioso central.
Tras el recuento de las neuronas faciales vimos que sólo había 400, mientras que en un cerebro normal ascenderían a unas 9000.

En su conjunto el cerebro de la mujer tenía un tamaño normal; de hecho pesaba algo más que un cerebro medio. 
Planteé la hipótesis de que el tronco encefálico carecería sólo de las neuronas específicas ya identificadas, es decir, las del núcleo facial y la oliva superior.
Para someter a prueba la idea, decidí medir las distancias entre varios puntos de referencia neuroanatómicos. 
Contradijo la hipótesis.
Aunque de un lado al otro las medidas eran normales, las medidas anteroposteriores resultaban asombrosamente menores en el tronco cerebral de la joven autista.
Era como si se le hubiese rebanado una banda de tejido y las dos piezas restantes se hubiesen unido sin dejar rastro del hueco.

Volví a sentir por segunda vez el chispazo de luz. 
Mi excitación no la había provocado un resultado inesperado, sino la clara conciencia de que había observado antes ese tipo de acortamiento, en un artículo donde aparecían fotografías de cerebros anormales de ratón.

Cuando localicé por fin el artículo, entre los montones de papeles apilados en el suelo de mi despacho, comprobé que la correspondencia entre el cerebro de la chica y los de los ratones que se describían en el artículo era más estrecha que lo que yo recordaba.
En ambos casos se apreciaba un acortamiento del tronco encefálico, un núcleo facial menor de lo normal y la ausencia de una oliva superior.
Otros rasgos de los ratones se relacionaban sin ambigüedad con anomalías asociadas al autismo; tenían malformaciones en el oído y carecían de una de las estructuras cerebrales que controlan el movimiento del ojo.

¿Qué era lo que había alterado el cerebro de los ratones? No fue la talidomida, ni ningún otro factor ambiental vinculado al autismo.
Se había suprimido la función de un gen.
Se trataba de ratones transgénicos "knockout", en los que se había eliminado la expresión del gen Hoxa1 con la intención de conocer el papel del mismo en los primeros momentos del desarrollo. 
¿Nos hallábamos ante uno de los genes implicados en el autismo?
La bibliografía parecía respaldar esa vía de investigación en el autismo.
El estudio sobre ratones "knockout" demostraba que el Hoxa1 desempeñaba un papel central en el desarrollo del tronco cerebral.

Laboratorios de Salt Lake City y Londres habían estudiado cepas diferentes de ratones "knockout" con resultados semejantes.
Descubrieron que el gen operaba en el tronco cerebral cuando se formaban las primeras neuronas, esto es, en el mismo período al que Miller y Strömland atribuían la acción patógena autista de la talidomida.
El gen Hoxa1 determina un factor de transcripción, proteína que modula la actividad de otros genes.
Además, el Hoxa1 no interviene en ningún tejido después de la embriogénesis precoz.
Si un gen persiste activo durante toda la vida, lo que es frecuente, su función alterada traerá problemas, que aumentarán con la edad del individuo.

Pero un gen activo sólo durante el desarrollo constituye un candidato mejor para explicar una alteración congénita como el autismo, que parece permanecer estable después de la infancia.

El Hoxa1 es un gen "muy conservador", lo que significa que la secuencia de los nucleótidos de su ADN apenas ha cambiado en el curso de la evolución. 
Se atribuye esa propiedad a los genes que son fundamentales para la supervivencia; si bien sufren mutaciones como los demás genes, la mayoría de los cambios operados resultan letales y, por tanto, no pasan a la generación siguiente.
Asimismo, muchos otros genes aparecen en formas diversas; por ejemplo, los que determinan el color de los ojos o el grupo sanguíneo. 
No ocurre así con los genes muy conservados, de los que no suele haber alelos polimórficos, o variantes alélicas.
Si nadie había descubierto una variante del Hoxa1 , tras examinar muchas especies de mamíferos, no correríamos nosotros mejor suerte buscándole una al autismo.
Mas, por otro lado, parecía probable que, si el alelo variante se encontrase, sería a buen seguro una de las espoletas del desarrollo de la enfermedad.

Apuntando al HOXA1 

El gen correspondiente en humanos se llama HOXA1 y reside en el cromosoma 7.
Bastante pequeño en su tamaño, contiene dos regiones codificadoras de proteína, o exones, junto con regiones que regulan el nivel de producción de proteína o carecen de función alguna.
Las desviaciones de la secuencia normal, en cualquier parte de un gen, repercuten en su función, pero en su mayoría las variaciones desencadenantes de enfermedades se encuentran en las regiones que cifran la molécula de proteína. 
Comenzamos, pues, la búsqueda de alelos variantes, centrándonos en los exones del HOXA1 .
Tomamos muestras de sangre de autistas y de individuos sanos, extrajimos el ADN y rastreamos la presencia de desviaciones que se apartaran de las secuencias normales de los nucleótidos. 

Una buena noticia.
Cazamos dos alelos variantes del HOXA1 .
Uno tenía una desviación menor de la secuencia de un exón, lo que significaba que la proteína codificada por el gen variante difería ligeramente de la proteína cifrada por el gen normal. 
Investigamos de forma exhaustiva el alelo recién descubierto, determinando su incidencia en grupos de personas para establecer si intervenía en el origen del autismo. (El otro alelo variante es más difícil de estudiar porque arrastra cambios en la estructura física del ADN del gen.)

Vimos que la frecuencia de aparición de alelos variantes en autistas era significativamente mayor que en sus familiares exentos de esa patología y que en otros individuos no emparentados sin autismo.
Las diferencias eran mucho mayores que lo que cabía esperar por azar.

Y una mala noticia.
Como habían predicho los estudios con familias, el HOXA1 es un gen más entre los implicados en el espectro de las disfunciones autistas.
Además, el alelo que examinamos se expresa de manera variable, vale decir, su presencia no garantiza la presencia de autismo.
De acuerdo con los datos provisionales, el alelo variante ocurre en un 20 por ciento de la gente que no padece autismo y en un 40 por ciento de quienes lo sufren. 
El alelo viene a duplicar el riesgo.
Pero en el 60 por ciento de las personas autistas está el alelo, prueba de la implicación de otros factores genéticos en el desarrollo de la enfermedad.

Para descubrir de qué factores se trata, debemos seguir buscando otras variantes en el HOXA1 ; no se olvide que la mayoría de las enfermedades genéticas dependen de la acción de muchos alelos diferentes que se desvían de un mismo gen.
Las variantes de otros genes comprometidos en las primeras fases del desarrollo pueden predisponer al autismo.
Nosotros hemos descubierto ya una variante alélica del HOXB1 , un gen del cromosoma 17 que parte de la misma fuente ancestral que el HOXA1 y cumple funciones semejantes en el desarrollo del tronco cerebral, cuyo efecto en el autismo no parece determinante.
Otros investigan regiones del cromosoma 15 y una zona del cromosoma 17 distinta.
Aunque se atiende sobre todo a los alelos que aumentan el riesgo, otros alelos parecen rebajarlo.
Estos podrían explicar la expresión variable del espectro de las afecciones relacionadas con el autismo. 

Cualquier avance, por mínimo que sea, en las bases genéticas del autismo revestiría un gran valor. 
Piénsese en la posibilidad consiguiente de transferir alelos asociados con el autismo del hombre a ratones e introducir en ellos los cambios adecuados para hacerlos genéticamente susceptibles a la enfermedad.
Al exponer estos ratones a sustancias sospechosas de aumentar el riesgo, podríamos abordar la interacción entre factores ambientales y acervo génico, y quizá compilar una lista de sustancias a evitar por la mujer durante las primeras fases de la gestación. 
Y lo que es más importante, al examinar el desarrollo de estos ratones alterados por ingeniería genética, se podrían obtener más datos sobre las lesiones cerebrales presentes en el autismo.
Conocidas las alteraciones cerebrales de los autistas, podrían sugerirse medicamentos y otros medios terapéuticos para aliviar los efectos de las lesiones.

El diseño de un test genético para el autismo -semejante a las pruebas usuales para la fibrosis quística, anemia falciforme u otras- es una cuestión ardua.
Al haber, según todos los indicios, tantos genes implicados en la afección, no puede predecirse con exactitud la probabilidad de que el niño nazca autista examinando sólo una o dos variantes alélicas de los padres.
Podrían desarrollarse pruebas, sin embargo, para hermanos de autistas, que a menudo tienen el temor de que sus hijos hereden la patología. 
Podría buscarse un protocolo de pruebas determinantes de factores de riesgo bien establecidos, lo mismo en el familiar autista que en su hermano sano.
Si la persona con autismo presenta varios alelos de alto riesgo, pero su hermano no, podría asegurarse al menos que ni éste ni su progenie se hallarían sujetos a riesgo.

La búsqueda de las causas del autismo constituye una carrera de obstáculos.
Pero cada factor de riesgo que identifiquemos correrá un poco el velo del misterio.
Y lo que importa más, los nuevos datos generarán nuevas hipótesis.
Igual que los resultados de la talidomida nos llevaron hasta el tronco cerebral y el gen HOXA1 , así también la genética del desarrollo, el estudio del comportamiento, la aplicación de las técnicas de formación de imágenes al cerebro y otros medios nos traerán, cabe esperar, nuevas ráfagas de luz, que redundarán, a la postre, en alivio del terrible sufrimiento producido por la enfermedad.


Estructura y funciones de las células del sistema nervioso 

El cerebro es el órgano que mueve los músculos.

Esta afirmación puede parecer un poco simple, pero, en última instancia, el movimiento -o más precisamente, la conducta- constituye la función principal del sistema nervioso.
Para que los movimientos sean eficaces, el cerebro debe saber lo que ocurre en su entorno.
De este modo, el organismo contiene células especializadas en la detección de los acontecimientos del entorno y células especializadas en la producción de movimientos.
Naturalmente, los animales complejos, como nosotros mismos, no reaccionan automáticamente ante los acontecimientos ambientales; nuestros cerebros son lo suficientemente flexibles como para poder comportarnos de formas diferentes de acuerdo no sólo con las circunstancias presentes sino también con las experimentadas en nuestro pasado.
Además de percibir y actuar, somos capaces de recordar y decidir.
Todas estas capacidades son posibles gracias a los miles de millones de células que se encuentran en nuestro sistema nervioso.

Este capítulo describe la estructura y las funciones más importantes de las células del sistema nervioso.
La información, ya sea en forma de luz, ondas sonoras, olores, gustos, o contacto con objetos, es captada del entorno mediante células especializadas denominadas neuronas sensoriales .
Los movimientos tienen lugar mediante la contracción de los músculos, controlados a su vez por neuronas motoras .

(El término motor alude a movimiento y no a un motor mecánico.) Y entre las neuronas sensoriales y las motoras se hallan todas las otras neuronas que llevan a cabo las funciones de percepción. aprendizaje, recuerdo, decisión y control de las conductas complejas.

¿Cuántas neuronas hay en el sistema nervioso humano?

Podemos hacer una estimación de entre 100.000 millones y 1 billón (es decir, entre 1011 y 1012), pero todavía nadie las ha contado.

Para comprender cómo el sistema nervioso controla la conducta, debemos empezar por comprender sus partes -las células que lo componen-.
Como este capítulo trata sólo de las células, no es necesario estar familiarizado con la estructura del sistema nervioso, que se describirá en el capítulo 4.
Sin embargo, necesitamos saber que el sistema nervioso tiene dos componentes básicos, el sistema nervioso central y el sistema nervioso periférico.
El sistema nervioso central (SNC) está constituido por las partes del sistema nervioso que se sitúan dentro de la cavidad craneal y del canal vertebral: el encéfalo y la médula espinal.

El sistema nervioso periférico (SNP) se encuentra fuera de estas cavidades óseas y consiste en los nervios y en algunos órganos sensoriales.

Células del sistema nervioso 

La primera parte de este capítulo está dedicada a describir las células más importantes del sistema nervioso- las neuronas y las células de soporte -y a la barrera hematoencefálica, que las aísla químicamente del resto del organismo.

LAS NEURONAS 

Estructura básica 

La neurona (célula nerviosa) es la unidad elemental de procesamiento y transmisión de la información en el sistema nervioso.
Hay neuronas de diferentes formas y variedades, dependiendo del tipo de tarea especializada que llevan a cabo.
1 cuerpo celular o soma;
2 dendritas
3 axón: y
4 botones terminales.


Soma 

El soma (cuerpo celular) contiene el núcleo y la mayor parte de la maquinaria que mantiene los procesos vitales de la célula (véase la figura 2.1).
Su forma varía considerablemente en los diferentes tipos de neuronas.

Dendritas 

Dendron significa árbol en griego, y las dendritas de la neurona se parecen mucho a los árboles (véase la figura 2.1).
Las neuronas «conversan» entre sí, y las dendritas actúan como importantes receptores de estos mensajes.
Las informaciones que pasan de una neurona a otra se transmiten a través de la sinapsis que es una unión entre los botones terminales (descritos más adelante) de la neurona emisora y una porción de la membrana somática o dendrítica de la célula receptora. (La palabra sinapsis deriva de la griega sunaptein que significa «conexión».) En una sinapsis la comunicación tiene lugar en una sola dirección: desde el botón terminal a la membrana de la otra célula.

Figura

Principales estructuras o regiones de una neurona multipolar 

Axón 

El axón es un tubo largo y delgados a menudo recubierto por una vaina de mielina (la vaina de mielina se describe más adelante).
Lleva información desde el cuerpo celular hasta los botones terminales (véase la figura 2.1).
El mensaje que lleva recibe el nombre de potencial de acción .

Ésta es una importante función que describiremos con mayor detalle más adelante en este capítulo.

Por ahora, es suficiente decir que se trata de un breve acontecimiento eléctrico/químico que se inicia en el extremo del axón próximo al cuerpo celular y que viaja hacia los botones terminales.
El potencial de acción es como un pulso breve; en un determinado axón, tiene siempre el mismo tamaño y duración.
Cuando alcanza un punto en el que el axón se ramifica, se divide pero no disminuye su tamaño.
Cada rama recibe un potencial de acción con toda su fuerza .

Al igual que las dendritas, los axones y sus ramificaciones pueden tener formas diferentes.
De hecho, los tres principales tipos de neuronas se clasifican de acuerdo con el modo como sus axones y dendritas salen del soma.

Figura

Neuronas. a) Neurona bipolar; se encuentra principalmente en los sistemas sensoriales (por ejemplo, de visión v audición). b) Neurona unipolar; se encuentra en el sistema somato-sensorial (tacto, dolar y similares) 

La neurona representada en la figura 2.1 corresponde al tipo más común que se encuentra en el sistema nervioso central; es una neurona multipolar .
En este tipo de neuronas la membrana somática da lugar a un axón y a los troncos de muchos árboles dendríticos.
Por su parte, las neuronas bipolares dan lugar a un axón y a un árbol dendrítico, en lugares opuestos del soma (véase la figura 2.2a).
Estas neuronas generalmente son sensoriales, es decir, sus dendritas detectan acontecimientos que ocurren en el entorno y envían información acerca de estos al sistema nervioso central.

El tercer tipo de células nerviosas corresponde a la neurona unipolar .
Tiene un único proceso que sale del soma y se divide enseguida en dos ramas (véase la figura 2.2b).
Las neuronas unipolares, al igual que las bipolares, transmiten información desde el entorno hasta el SNC.
Las arborizaciones (ramificaciones semejantes a las ramas de los árboles) alejadas del SNC son dendritas; las que se hallan en el SNC acaban en botones terminales.
Las dendritas de la mayoría de las neuronas unipolares detectan tacto, cambios de temperatura y otros acontecimientos sensoriales que afectan a la piel.

Los botones terminales 

La mayoría de los axones se dividen y ramifican muchas veces.
En los extremos de las ramificaciones finas se encuentran unos pequeños engrosamientos denominados botones terminales, los cuales tienen una función muy especial: cuando un potencial de acción que viaja por el axón alcanza los botones terminales , éstos secretan una sustancia química denominada sustancia transmisora , también llamada neurotransmisor .
Esta sustancia química (hay muchos tipos diferentes en el SNC) afecta a la neurona receptora.
Su efecto consiste en excitar o inhibir a esta última, ayudándola a determinar si enviará un mensaje a lo largo del axón hasta las células con las cuales se comunica.
Los detalles de este proceso serán descritos más adelante en este mismo capítulo y en el capítulo 3.

Una neurona individual recibe información de los botones terminales de los axones de otras neuronas -y los botones terminales de su axón forman sinapsis con otras neuronas-.

Una neurona puede recibir información de docenas, o, incluso, de cientos de otras neuronas, cada una de las cuales puede formar muchas conexiones sinápticas con ella.
La figura 2.3 ilustra cómo son estas conexiones.
Se puede observar que los botones terminales pueden formar sinapsis sobre la membrana de las dendritas o del soma (véase la figura 2.3).

Figura

Visión general de las conexiones sinápticas entre neuronas 

Estructura interna 

La figura 2.4 ilustra la estructura interna de una neurona multipolar típica (véase la figura 2.4).
La membrana define sus límites.
Consiste en una doble capa de moléculas lipídicas (de tipo graso).
Flotando en ella se encuentran diferentes tipos de moléculas proteicas que tienen funciones especiales.

Algunas detectan sustancias del exterior de la célula (tales como hormonas) y transmiten información al interior de ésta acerca de la presencia de estas sustancias.
Otras controlan el acceso al interior de la célula, permitiendo la entrada de algunas sustancias e impidiendo el paso de otras.
Y aún otras actúan como bombas, empujando activamente a ciertas moléculas hacia el interior o el exterior de la célula.
Dado que la membrana de la neurona es especialmente importante para la transmisión de información, más adelante, en este mismo capítulo, discutiremos sus características con mayor detalle.

El núcleo de la célula es redondo u ovalado y está rodeado por la membrana nuclear.

En su interior se localizan el nucléolo y los cromosomas.
El nucléolo fabrica ribosomas , que son pequeñas estructuras relacionadas con la síntesis de proteínas.
Los cromosomas son largas hebras de ácido desoxirribonucleico (ADN) que contienen la información genética de&rsqb; organismo.
La activación de porciones determinadas de los Cromosomas ( genes ) origina la síntesis de otra molécula compleja, el ácido ribonucleico mensajero (ARNm).
El ARNm atraviesa la membrana nuclear y se liga a los ribosomas, donde da lugar a la producción de proteínas específicas.

Figura

Principales estructuras internas de una neurona multipolar 

Las proteínas son elementos particularmente importantes para las funciones celulares.
Además de tener una función estructural. actúan como enzimas , los cuales dirigen los procesos químicos de las células mediante el control de las reacciones químicas.
Los enzimas son moléculas proteicas especiales que actúan como catalizadores: es decir, hacen que se produzca una reacción química, pero ellas mismas no se convierten en parte del producto final.
Dado que las células tienen los elementos necesarios para sintetizar una enorme variedad de compuestos, los enzimas específicos que se hallen presentes en cada caso son los principales determinantes de los compuestos que realmente se producen.

Más aún, hay algunos enzimas que rompen moléculas, y otros que son capaces de unirlas; los que se hallan presentes en una región particular de la célula determinan entonces qué moléculas permanecen intactas.
Por ejemplo, xxx .

En esta reacción reversible, la concentración relativa de los enzimas X e Y determina si predominará el complejo AB o sus constituyentes, A y B. El enzima X da lugar a la unión de A y B: el enzima Y da lugar a la ruptura de AB. (Para que se lleven a cabo estas reacciones se necesita energía.) 

La mayor parte de la célula está formada por el citoplasma.
El citoplasma es complejo y varía considerablemente en los diferentes tipos de células. pero puede ser caracterizado como una sustancia de tipo gelatinoso, semilíquida, que llena el espacio delimitado por la membrana.
El citoplasma no es estático e inerte; se mueve y fluye.
Contiene pequeñas estructuras especializadas, de la misma manera que el cuerpo posee órganos especializados.
Las más importantes se describen a continuación.

Las mitocondrias tienen forma de cuentas ovaladas y constan de una doble membrana.
La membrana interna está arrugada, y las arrugas forman una serie de repisas (crestas) que llenan el interior de la cuenta.
Las mitocondrias juegan un papel vital en la economía de la célula: muchas de las etapas bioquímicas involucradas en la obtención de energía a partir de la degradación de nutrientes tienen lugar en sus crestas.
La mayoría de los biólogos celulares creen que hace mucho tiempo las mitocondrias eran organismos vivos libres, que "infectaron" a las células más grandes.
Dado que las mitocondrias podían extraer energía de forma más eficiente que las células de mayor tamaño, resultaron ser útiles a estas últimas y eventualmente se convirtieron en parte permanente de ellas.
La célula proporciona nutrientes a las mitocondrias , y las mitocondrias, a su vez, proporcionan a la célula una molécula especial - adenosin trifosfato (ATP)- que utiliza como su fuente de energía inmediata.

El retículo endoplasmático , que actúa como una cisterna de almacenamiento y como un canal para transportar sustancias químicas a través del citoplasma, aparece en dos formas: rugoso y liso.
Ambas consisten en capas paralelas de membrana, organizadas en pares, del mismo tipo que la que rodea a la célula.
El retículo endoplasmático rugoso contiene ribosomas.

Las proteínas producidas por los ribosomas ligados al retículo endoplasmático rugoso están destinadas a ser transportadas al exterior de la célula o a ser utilizadas en la membrana.
Distribuidos por el citoplasma hay también ribosomas libres, los cuales parecen producir aquellas proteínas destinadas a ser utilizadas en el interior de la célula.
El retículo endoplasmático liso está relacionado con el transporte de sustancias a través del citoplasma y proporciona canales para la selección de moléculas involucradas en diferentes procesos celulares.
Las moléculas lipídicas (de tipo graso) se producen aquí.

El aparato de Golgi es un tipo especial de retículo endoplasmático.
Algunas moléculas complejas, constituidas por moléculas más sencillas, se ensamblan en él.
También tiene la función de envolver o empaquetar.
Por ejemplo, las células secretoras (como las que liberan hormonas) empaquetan sustancias en una membrana producida por el aparato de Golgi.
Cuando la célula secreta sus productos, se sirve de un proceso llamado exocitosis (exo, «fuera»; cyto, «célula»; -osis, «proceso»).

Explicado de forma breve, los contenedores migran hacia la membrana externa de la célula, se funden con ella, y se rompen, vertiendo el producto en el fluido que rodea a la célula.
Tal como veremos más adelante, las neuronas son células secretoras; se comunican entre sí por medio de sustancias que son secretadas de esta forma.
Por ello, describiremos el proceso de exocitosis con mayor detalle en el capítulo 3.
El aparato de Golgi produce también lisosomas , pequeños sacos que contienen enzimas que degradan sustancias que ya no son necesarias para la célula.
Los productos residuales son entonces reciclados o bien excretados fuera de la misma.

Distribuidos por toda la célula hay neurofilamentos y microtúbulos.
Los neurofilamentos están formados por largas fibras proteicas similares a las que proporcionan la fuerza motriz de los músculos.

Situados justo por debajo de la membrana, dan a las células su forma particular y controlan la localización de las proteínas de la membrana.
Los microtúbulos son más gruesos y largos que los neurofilamentos, y consisten en fascículos de filamentos dispuestos alrededor de una oquedad central.

Transportan sustancias desde un lugar a otro de la célula.

Los axones pueden ser extremadamente largos en relación a su diámetro y al tamaño del soma.
Por ejemplo, en una persona, el axón más largo se extiende desde el dedo gordo del pie a una región localizada en la base del cerebro.
Dado que algunas de las sustancias que necesitan los botones terminales sólo se pueden producir en el soma, debe existir un sistema que las transporte rápida y eficazmente a través del axoplasma (es decir, el citoplasma del axón).
Este sistema recibe el nombre de transporte axoplasmático , proceso activo por el cual los materiales son propulsados a lo largo de los microtúbulos que recorren el axón.
El transporte axoplasmático rápido mueve materiales desde el soma hacia los botones terminales a una velocidad de varios cientos de milímetros por día.

(En un adulto de estatura media, el tiempo que tardarían en llegar las sustancias desde el cerebro al dedo gordo del pie sería de algo menos de una semana.) El transporte axoplasmático lento tiene una velocidad de tan sólo el uno por ciento de la del transporte rápido, pero tiene lugar en ambas direcciones.

LAS CÉLULAS DE SOPORTE 

Las neuronas suponen tan sólo alrededor de la mitad del volumen del SNC.
El resto está formado por diferentes tipos de células de soporte.
Dado que las neuronas tienen una tasa metabólica muy elevada pero no son capaces de almacenar nutrientes, éstos les deben ser suministrados constantemente, al igual que el oxígeno, o de lo contrario morirían con rapidez.
A diferencia de otras células del organismo, las neuronas no pueden ser reemplazadas cuando mueren; nunca tendremos más neuronas que las que tenemos al nacer.
Por ello, el papel de las células que dan soporte y protección a las neuronas es muy importante para nuestra existencia.

Glía 

Las células de soporte más importantes en el sistema nervioso central son las que constituyen la neuroglía , o «pegamento nervioso».
La glía (también llamada células gliales ), en efecto, mantiene unido al SNC, pero hace mucho más que esto.
Las neuronas son células con una existencia muy protegida: las células gliales las amortiguan física y químicamente en relación con el resto del organismo.
Estas células rodean a las neuronas y las mantienen fijas en su lugar, controlando el suministro de algunas de las sustancias químicas que necesitan para intercambiar mensajes con otras neuronas; aíslan a las neuronas entre sí. y de esta forma evitan que los mensajes neurales se mezclen; e incluso actúan como «amas de casa», destruyendo y eliminando los restos de las neuronas que han muerto debido a alguna lesión o por envejecimiento.

Hay varios tipos de células gliales, cada una de las cuales juega un papel especial en el SNC.
Los dos tipos más importantes son los astrocitos y la oligodendroglia . 
Astrocito significa célula en forma de estrella, y es un nombre que describe perfectamente la forma de estas células.
Los astrocitos (o astroglía ) proporcionan soporte físico a las neuronas y limpian los desechos del cerebro.
Producen algunas sustancias químicas que las neuronas necesitan para llevar a cabo sus funciones.

Además, ayudan a controlar la composición química del fluido que rodea a las neuronas, captando o liberando activamente sustancias cuya concentración debe mantenerse dentro de unos niveles críticos.

Algunos de los procesos de los astrocitos (brazos de la estrella) envuelven vasos sanguíneos, otros se arrollan alrededor de algunas partes de las neuronas, de tal manera que las membranas somática y dendrítica quedan ampliamente rodeadas por estas células. (véase la figura 2.5).
Esta disposición sugirió al histólogo italiano Camillo Golgi (1844-1926) que los astrocitos suministran nutrientes a las neuronas desde los capilares, deshaciéndose también de los productos de desecho (Golgi, 1903).
Pensó que los nutrientes pasaban de los capilares al citoplasma de los astrocitos y desde allí a las neuronas, mientras que los productos de desecho seguían la ruta opuesta.
Tal como muestra la figura 2.5, esta hipótesis es factible, aunque no ha sido confirmada.

Figura

Estructura y localización de los astrocitos cuyos procesos rodean a los capilares X a las neuronas del sistema nervioso central 

Además de su posible papel en el transporte de sustancias químicas a las neuronas, los astrocitos actúan como una matriz que mantiene fijas a las neuronas en su lugar.

También rodean y aíslan a las sinapsis, minimizando así, aparentemente, la dispersión de las sustancias transmisoras que son liberadas desde los botones terminales.
Por tanto, los astrocitos proporcionan a las sinapsis una especie de cabina de aislamiento, manteniendo así la privacidad de las conversaciones entre las neuronas.

Las neuronas mueren ocasionalmente por razones desconocidas o también como consecuencia de lesiones cefálicas, infecciones o apoplejías.
Algunos tipos de astrocitos asumen entonces la tarea de limpieza de los desechos.
Estas células son capaces de viajar por todo el SNC; extienden y retraen sus procesos ( pseudópodos , o «pies falsos»), y se deslizan de forma similar a como lo hacen las amebas.
Cuando entran en contacto con un fragmento de desecho proveniente de una neurona muerta, avanzan sobre el mismo, y finalmente lo engullen y lo digieren.
Llamamos a este proceso fagocitosis ( phagein , «comer»; kutos , «célula»).
Si la cantidad de tejido destruido que debe ser eliminado es considerable, los astrocitos se dividirán y producirán suficientes células nuevas como para realizar la tarea.
Una vez eliminado el tejido lesionado, se formará un entramado de astrocitos que rellenará el espacio vacío, y un tipo especializado de astrocitos formará tejido cicatrizante, sellando así el área.

Figura

Oligodendrocito; forma la mielina que rodea a muchos axones en el sistema nervioso central. Cada célula forma un segmento de mielina en varios axones adyacentes 

Los oligodendrocitos se hallan sólo en el SNC, y su función principal es la de proporcionar soporte a los axones y producir la vaina de mielina , que aísla a la mayoría de los axones entre sí. (Algunos axones no están mielinizados y no tienen esta vaina.) 
La mielina está formada por un 80 por ciento de lípidos y un 20 por ciento de proteínas, y es producida por los oligodendrocitos, que forman como un tubo que rodea al axón.
Este tubo no constituye una lámina continua, sino que consiste más bien en una serie de segmentos, cada uno de ellos de aproximadamente I mm de longitud, entre los cuales existe una pequeña ( 1-2 Rm) porción de axón no recubierto.
(Un micrómetro de forma abreviada um, es una millonésima de metro, o una milésima de milímetro.) Cada una de las porciones descubiertas del axón se denomina nódulo de Ranvier , en referencia a su descubridor.
El axón mielinizado (o mielínico), por tanto, parece un collar de cuentas alargadas. (En realidad, las cuentas son muy alargadas, ya que su longitud es aproximadamente 80 veces su anchura.) 

Un único oligodendrocito forma varios segmentos de mielina.
Durante el desarrollo del SNC, los oligodendrocitos generan procesos que tienen una forma parecida a los remos de una canoa.
Cada uno de estos procesos se arrolla muchas veces alrededor de un segmento de axón, y al hacerlo, va produciendo capas de mielina.
Cada «remo», entonces, se convierte en un segmento de la vaina de mielina del axón (véase la figura 2.6).

Los axones no mielinizados (o amielínicos) del SNC no están realmente desnudos; también están recubiertos por oligodendrocitos.
Sin embargo, en este caso las células gliales no fabrican mielina; simplemente emiten un proceso que rodea holgadamente al axón y lo mantienen fijo en su lugar.

Células de Schwann 

En el SNC los oligodendrocitos dan soporte a los axones y producen mielina.
En el SNP las células de Schwann llevan a cabo las mismas funciones.
La mayoría de los axones del SNP son mielínicos.
La vaina de mielina está dividida en segmentos, al igual que en el SNC; cada segmento consiste en una única célula de Schwann, arrollada múltiples veces alrededor del axón.
En el SNC los oligodendrocitos emiten muchos procesos en forma de remo que se arrollan alrededor de muchos axones.
En el SNP una célula de Schwann provee de mielina sólo a un axón, y toda la célula de Schwann -no sólo una parte de ella- rodea al axón (véase la figura 2. 7).

Las células de Schwann también se diferencian de sus homólogas en el SNC, los oligodendrocitos, en un aspecto importante.
Si un nervio periférico (que consiste en un fascículo de muchos axones mielínicos, recubierto de una lámina de tejido conectivo resistente y elástico) es dañado, las células de Schwann ayudan a la digestión de los axones muertos o moribundos (véase la figura 2.8).
Seguidamente, estas mismas células se disponen formando una serie de cilindros que actúan como guías para que los axones vuelvan crecer.
Las porciones distales de los axones rotos mueren, pero del muñón de cada axón partido crecen brotes, que se propagan en todas las direcciones.
Si uno de estos brotes encuentra el cilindro formado por una célula de Schwann, rápidamente crece a través del tubo (a una velocidad de más de 3-4 mm al día), mientras que los otros brotes, no productivos, se marchitan y desaparecen.
Si los extremos seccionados del nervio todavía se hallan suficientemente cerca entre sí, los axones restablecerán las conexiones con los órganos musculares y sensoriales que inervaban previamente.

Figura

Formación de la mielina. Durante el desarrollo, un proceso de un oligodendrocito, o en el sistema nervioso periférico una célula de Schwann entera, se enrolla repetida y estrechamente alrededor de un axón individual, formando un segmento de vaina de mielina 

Por otro lado, si una sección del nervio está demasiado dañada como para poder ser reparada, los axones no serán capaces de encontrar su camino hacia los lugares originales de inervación.
En estos casos, la neurocirugía puede coser entre sí los extremos seccionados del nervio, siempre y cuando éste no haya sufrido mucho daño. (los nervios son flexibles y pueden estirarse ligeramente).
Si se ha perdido un trozo demasiado grande, y si se trataba de un nervio importante (que controlaba, por ejemplo, los músculos de la mano), se puede tomar un trozo de nervio de aproximadamente el mismo tamaño de otra parte del cuerpo.
Dado que una misma área de tejido puede estar inervada por muchos nervios que se solapan, los neurocirujanos son capaces de encontrar fácilmente una rama de nervio que el paciente puede perder sin problemas.
Usando un microscopio especial e instrumentos muy delicados, el cirujano injerta este trozo de nervio en el dañado.
Naturalmente, los axones del trozo de nervio cortado y transplantado mueren, pero los tubos producidos por las células de Schwann guían a los brotes del nervio dañado y les ayudan a encontrar su camino hacia los músculos de la mano.

Figura

Microfotografía mediante microscopio de barrido del extremo seccionado de un nervio periférico 

Desafortunadamente, las células gliales del SNC no son tan cooperativas como las células de soporte del SNP.
Si se daña algún axon del encéfalo o de la médula espinal, se formarán nuevos brotes, como en el SNP.
Sin embargo, estos brotes de axones se encuentran con tejido cicatrizante producido por los astrocitos, y no pueden atravesar esta barrera.
Incluso si pudieran atravesarla, los axones no restablecerían sus conexiones originales sin una guía similar a la que proporcionan las células de Schwann en el SNP.
Durante el desarrollo, los axones tienen dos formas de crecimiento.
La primera da lugar a su alargamiento, lo que permite que alcancen su objetivo, el cual puede estar tan lejos como el otro extremo del encéfalo o de la médula espinal.
Las células de Schwann proporcionan esa señal a los axones dañados.
La segunda modalidad hace que los axones dejen de crecer y empiecen a emitir botones terminales, cuando ya han alcanzado su objetivo.
Liuzzi y Lasek (1987) observaron que incluso cuando los astrocitos no producían tejido cicatrizante, parecían producir una señal química que inducía a los axones en regeneración a que iniciasen la segunda modalidad de crecimiento: detener su expansión longitudinal y empezar a emitir botones terminales.
Por lo tanto, la diferencia existente entre las propiedades regenerativas del SNC y el SNP se debe a diferencias en las características de las células de soporte, y no a diferencias en las neuronas.

LA BARRERA HEMATOENCEFÁLICA 

Hace más de cien años, Paul Ehrlich descubrió que si se inyecta un colorante azul en el torrente sanguíneo de un animal, todos los tejidos excepto el cerebro y la médula espinal quedarán teñidos de azul.
Sin embargo, si el mismo colorante se inyecta en los ventrículos cerebrales, el color azul se expande por todo el SNC (Bradbury, 1979).
Este experimento demuestra que existe una barrera entre la sangre y el fluido que rodea las células del cerebro -la barrera hematoencefálica -.

Algunas sustancias son capaces de atravesar la barrera hematoencefálica, mientras que otras no.
Por lo tanto. es una barrera selectivamente permeable ( per «a través» meare «pasar»).
En la mayor parte del organismo las células que revisten los capilares no se hallan unidas entre sí de una forma absolutamente hermética.

Entre ellas existen pequeñas aberturas que permiten el intercambio libre de la mayoría de las sustancias entre el plasma sanguíneo y el líquido del exterior de los vasos sanguíneos que rodea a las células.
En el sistema nervioso central los capilares no tienen estas aberturas, y es por ello que muchas sustancias no pueden dejar la sangre.

Las sustancias capaces de disolverse en los lípidos atraviesan fácilmente los capilares, ya que simplemente se disuelven en las membranas de las células que los revisten.
Otras sustancias, tales como la glucosa (el combustible principal del sistema nervioso central), deben ser transportadas de forma activa a través de las paredes de los capilares, mediante proteínas especiales.

Los mensajes que son enviados de un lugar a otro del sistema nervioso conllevan un movimiento de sustancias a través de las membranas de las neuronas.
Si la composición del fluido que baña las neuronas cambia incluso débilmente, la transmisión de estos mensajes se verá interrumpida.
Así, si este fluido no estuviera regulado con precisión, el cerebro no podría funcionar con normalidad.
Es muy probable que la presencia de la barrera hematoencefálica facilite la regulación de su composición.

La barrera hematoencefálica no es uniforme en todo el sistema nervioso.
Hay varios lugares en donde es relativamente permeable, permitiendo que algunas sustancias que en otros lugares no pueden atravesarla puedan pasar aquí libremente.
Por ejemplo, el área postrema es una parte del encéfalo que controla los vómitos.
En ella, la barrera hematoencefálica es mucho más débil, lo que incrementa la sensibilidad de esta región a las sustancias tóxicas que se hallan en la sangre.
Un veneno que entra en el sistema circulatorio desde el estómago puede así estimular esta área para iniciar los vómitos.
Si el organismo tiene suerte, el veneno puede ser expulsado del estómago antes de que cause demasiados daños.

4 Estructura del sistema nervioso 

El objetivo de la investigación en neurociencia es comprender el funcionamiento del cerebro.
Para ello es necesario estar familiarizado con la estructura básica del sistema nervioso.
El número de términos introducidos en este capítulo se ha reducido a un mínimo (pero, como se verá, aun así este mínimo sigue siendo elevado).
Con la información que aporta este capítulo, el aprendizaje de la materia presentada en los posteriores no debería plantear ningún problema.

Características básicas del sistema nervioso 

Antes de empezar a describir el sistema nervioso, nos gustaría hacer referencia a los términos que utilizaremos para ello.
La anatomía macroscópica del cerebro fue descrita hace ya mucho tiempo, dándosele un nombre a todo aquello que puede observarse sin ayuda del microscopio.
Los primeros anatomistas dieron nombre a la mayoría de las estructuras cerebrales teniendo en cuenta su similitud con objetos corrientes: amígdala o «almendra»; hipocampo o «caballo de mar»; genu o «rodilla»; córtex o «corteza»; pons o «puente»; uncus o «gancho», son sólo algunos ejemplos.
A lo largo de este libro iremos explicando el significado de los términos anatómicos a medida que los vayamos introduciendo, ya que ello hace que resulten más fácilmente memorizables.
Por ejemplo, el hecho de saber que la palabra córtex significa «corteza» (como la corteza de un árbol) nos ayudará a recordar que el córtex es la capa más externa del cerebro.

Cuando se describen las características de una estructura tan compleja como el cerebro, necesitamos utilizar términos que denoten direcciones.
Las direcciones en el sistema nervioso se describen normalmente con respecto al neuroeje , una línea imaginaria trazada a lo largo de la médula espinal hasta la parte frontal del cerebro.
Por razones de simplicidad, pondremos como ejemplo a un animal que tenga el neuroeje recto.
En la figura 4.1 se representan un cocodrilo y un individuo humano.
Desde luego, este cocodrilo mantiene una estructura lineal; podemos dibujar una línea recta que empiece en su entrecejo y continúe por el centro de la médula espinal (véase la figura 4.1).
El extremo frontal es anterior y la cola es posterior .
También se emplean los términos rostral (hacia el rostro) y caudal (hacia la cola), especialmente cuando nos referimos específicamente al encéfalo.
La zona superior de la cabeza y de la espalda forman parte de la superficie dorsal ; la superficie ventral (delantera) mira hacia el suelo.
Estas direcciones son algo más complicadas en los humanos; por el hecho de caminar erguidos, nuestro neuroeje está curvado, de manera que la parte superior de la cabeza se sitúa perpendicular a la espalda.
La visión frontal del cocodrilo y del humano ilustran los términos lateral y medial , hacia los lados y hacia la línea media, respectivamente (véase la figura 4.1).

Otros dos términos útiles son ipsilateral y contralateral .
El término ipsilateral se refiere a las estructuras del mismo lado del cuerpo.
Así, si decimos que el bulbo olfatorio envía axones al hemisferio ipsilateral, queremos decir que el bulbo olfatorio izquierdo envía axones al hemisferio izquierdo y que el bulbo olfatorio derecho los envía al hemisferio derecho.
El término contralateral se refiere a las estructuras situadas en el lado contrario del cuerpo.
Si decimos que una determinada región de la corteza cerebral izquierda controla los movimientos de la mano contralateral , queremos decir que esta región controla los movimientos de la mano derecha.

Figura

Visión lateral y frontal de un cocodrilo y de un humano mostrando los términos utilizados para hacer referencia a las direcciones anatómicas 

Para observar lo que hay en el interior del sistema nervioso tenemos que abrirlo, y para obtener información sobre lo que en él hallamos tenemos que seccionarlo de una manera sistemática.
La figura 4.2 muestra un sistema nervioso humano.
Podemos seccionar el sistema nervioso de tres modos diferentes: 

1 En sentido transversal, como si fuera un salchichón, lo cual nos permite obtener secciones transversales (también llamadas secciones frontales ). 
2 En sentido paralelo al suelo, lo cual nos permite obtener secciones horizontales 
3 En sentido perpendicular al suelo y paralelo al neuroeje, lo cual nos permite obtener secciones sagitales . 

Obsérvese que, debido a nuestra postura erecta, las secciones transversales de la médula espinal son, de hecho, paralelas al suelo (véase la figura 4.2).

Figura

Planos de sección aplicados al sistema nervioso central 

Figura

Relación del cerebro y la médula espinal con la cabeza y el cuello 

UNA VISIÓN GENERAL 

El sistema nervioso está formado por el encéfalo y la médula espinal, que componen el sistema nervioso central (SNC). así como por los nervios craneales, los nervios espinales y los ganglios periféricos, que constituyen el sistema nervioso periférico (SNP).
El SNC está envuelto en estructuras óseas: el cerebro está cubierto por el cráneo, y la médula espinal, por la columna vertebral.

La figura 4.3 ilustra la relación del cerebro y la médula espinal con la cabeza y el cuello en los humanos.

El lector no debe preocuparse si los nombres que hay en la misma no le resultan todavía familiares; las estructuras correspondientes se describirán más tarde (véase la figura 4.3).
El cerebro es una gran masa de neuronas, glía y otras células de sostén.
Es el órgano más protegido del cuerpo, situado en el interior de un gran óseo duro y flotando en líquido cefalorraquídeo.
Recibe un copioso aporte sanguíneo y está protegido químicamente por la barrera hematoencefálica.

EL APORTE SANGUÍNEO 

El cerebro recibe aproximadamente el 20 por ciento del fluido sanguíneo del corazón, lo cual tiene lugar de manera continua.
Otras partes del organismo, como los músculos esqueléticos y el sistema digestivo, reciben cantidades variables de sangre en función de sus necesidades, relativas a las de otras regiones.
El cerebro no puede almacenar combustible (principalmente glucosa), ni puede extraer energía temporalmente sin la presencia de oxígeno, como hacen los músculos; por eso resulta esencial que tenga un aporte consistente de sangre.
Una interrupción del flujo sanguíneo cerebral agota la mayor parte del oxígeno disuelto en el cerebro; una interrupción de 6 segundos produce inconsciencia.
En pocos minutos se produce un daño permanente.

La circulación sanguínea corporal se dirige desde las arterias a las arteriolas y a los capilares; a su vez.
Los capilares drenan en las vénulas que confluyen para formar venas.
Estas devuelven la sangre al corazón, donde empieza nuevamente el proceso.
La figura 4.4 muestra una visión de la cara basal del cerebro y de su irrigación sanguínea (la médula espinal ha sido separada, así como la mitad del cerebelo y del lóbulo temporal izquierdo).
Dos conjuntos principales de arterias irrigan el cerebro: las arterias vertebrales se encargan de la porción caudal, y las arterias carótidas internas , de la porción rostral (véase la figura 4.4).
Puede observarse que la irrigación sanguínea es bastante peculiar: las principales arterias se unen y luego se separan de nuevo.

Normalmente, la sangre de las arterias caudales y rostrales no suele mezclarse mucho y, en el caso de la irrigación rostral, tampoco suele haber mucha mezcla entre la sangre de los lados derecho e izquierdo.
Pero si un vaso sanguíneo se obtura (por ejemplo, a causa de un coágulo), el flujo sanguíneo puede seguir rutas alternativas, reduciéndose así la probabilidad de que cese la irrigación sanguínea y de que, como consecuencia de ello, se lesione el tejido cerebral.

LAS MENINGES 

Todo el sistema nervioso (cerebro, médula espinal, nervios craneales y espinales, y ganglios autonómicos) está cubierto por tejido conectivo.
Las láminas protectoras que rodean al cerebro y a la médula se denominan meninges .
Éstas constan de las tres capas que se muestran en la figura 4.5.
La capa más externa es gruesa, dura y flexible, pero no maleable; se llama duramadre .
La capa intermedia de las meninges, la membrana aracnoides , debe su nombre a la apariencia de tela de araña de las trabéculas aracnoideas . que sobresalen de ella (del griego arachné que significa «araña»; trabécula significa «sendero»).
La membrana aracnoides es blanda y esponjosa, y se sitúa bajo la duramadre.

Íntimamente unida al cerebro y a la médula espinal, y recubriendo todas las circunvoluciones de su superficie, se halla la piamadre .
Los vasos sanguíneos más pequeños del cerebro y de la médula están contenidos en esta capa.
Entre la piamadre y la membrana aracnoides se sitúa el espacio subaracnoideo .
Este espacio está lleno de un líquido llamado líquido cefalorraquídeo (LCR) (véase la figura 4.5).

El sistema nervioso periférico (SNP) está cubierto por dos capas de meninges.
La capa intermedia (membrana aracnoides), con el LCR asociado a ella, cubre sólo el cerebro y la médula espinal.
Fuera del sistema nervioso central (SNC), las capas externa e interna (duramadre y piamadre) se funden y forman una lámina que cubre los nervios espinales y craneales y los ganglios autonómicos.

Figura

Aporte sanguíneo arterial del cerebro, visto desde la cara basal 

Figura

Las meninges: duramadre, membrana aracnoides y piamadre 

En la primera edición de este libro dijimos que ignorábamos por qué las capas más externa e interna de las meninges se llamaban «madre».
Hemos recibido una carta de un historiador médico del Departamento de Anatomía de la UCLA (Universidad de California en Los Ángeles) explicándonos el origen de este nombre. (A veces vale la pena proclamar la propia ignorancia.) Un médico persa del siglo X, Ali ibn Abbas, utilizaba el término árabe al umm para referirse a las meninges.
Este término significa literalmente «madre», pero se utilizaba para designar cualquier material envolvente porque el árabe carecía de un término específico para la palabra membrana .
La membrana dura era denominada al umm aldjafiya , y la membrana blanda interior, al umm al rigiga .
Cuando los escritos de Ali ibn Abbas se tradujeron al latín en el siglo XI, el traductor, que probablemente no estaba familiarizado con la estructura de las meninges, tradujo literalmente la palabra al umm .
Se refirió entonces a las membranas como «dura madre» y «pía madre» (pía en el sentido de «delicada»), en lugar de utilizar una palabra latina más apropiada.

EL SISTEMA VENTRICULAR Y LA PRODUCCIÓN DE LCR 

El cerebro es muy blando y tiene una consistencia gelatinosa.
El cerebro humano, con su considerable peso (aproximadamente 1.400 gramos) y su delicada constitución requiere de una gran protección frente a los golpes.
Ni siquiera puede soportar su propio peso; resulta difícil extraer y manipular el cerebro fresco de un sujeto recientemente fallecido sin dañarlo.

Por suerte, el cerebro intacto de una persona viva está bien protegido.
Flota en el baño de LCR contenido en el espacio subaracnoideo.
Dado que está completamente inmerso en líquido, su peso neto se reduce aproximadamente a unos 80 gramos: así, la presión sobre su base se halla considerablemente disminuida.
El LCR que rodea al cerebro y a la médula espinal reduce también el impacto de los golpes sobre el sistema nervioso central que ocasionarían los movimientos bruscos de la cabeza.

El cerebro contiene una serie de cámaras huecas e interconectadas llamadas ventrículos , llenas de LCR (véase la figura 4.6).
Las cámaras de mayor tamaño son los ventrículos laterales , que están conectados con el tercer ventrículo.
El tercer ventrículo está localizado en la línea media del cerebro: sus paredes dividen las áreas circundantes en mitades simétricas.
Un puente de tejido neural llamado masa intermedia atraviesa la línea media del tercer ventrículo y sirve como un punto útil de referencia.
El acueducto cerebral es un largo tubo que conecta el tercer ventrículo con el cuarto ventrículo .
Los ventrículos laterales constituyen el primero y segundo ventrículos, aunque nunca se denominan de este modo (véase la figura 4.6).

El líquido cefalorraquídeo (LCR) se extrae de la sangre y tiene una composición parecida a la del plasma sanguíneo.
El LCR se forma en un tejido especial con un riego sanguíneo especialmente rico llamado el plexo coroideo , situado en los cuatro ventrículos.
La circulación de LCR empieza en los ventrículos laterales, fluye hacia el tercer ventrículo, y luego, a través del acueducto cerebral, hacia el cuarto ventrículo.
De ahí fluye a través de un conjunto de orificios hacia el espacio subaracnoideo, que envuelve todo el sistema nervioso central.

Finalmente, el LCR es reabsorbido por la circulación sanguínea.
El volumen total de LCR es de aproximadamente 125 mililitros (ml), y su vida media (el tiempo que tarda la mitad del LCR del sistema ventricular en ser reemplazado por LCR fresco) es de unas 3 horas.
Por consiguiente, el plexo coroideo produce esa cantidad varias veces al día.

Figura

El sistema ventricular encefálico 

Ocasionalmente, el flujo de LCR se interrumpe en algún punto de su vía de paso.
Por ejemplo, un tumor del mesencéfalo puede provocar la oclusión del acueducto cerebral, bloqueando el flujo de LCR.
Esta oclusión ocasiona un gran incremento de la presión de los ventrículos, debido a que los plexos coroideos continúan produciendo LCR.
Las paredes de los ventrículos se expanden y producen una alteración conocida como hidrocefalia (literalmente «agua en la cabeza»).
Si la obstrucción persiste, y si no se hace nada para revertir el aumento de la presión intracerebral, los vasos sanguíneos se verán ocluidos, produciéndose una lesión permanente (y quizás fatal) en el cerebro.
Por suerte, el cirujano puede operar a la persona, haciendo un agujero en el cráneo e insertando un tubo de plástico en uno de los ventrículos.
Luego, el tubo se coloca debajo de la piel y se conecta a una válvula reductora de la presión que se implanta en la cavidad abdominal.
Cuando la presión de los ventrículos llega a ser excesiva, la válvula hace posible que el LCR fluya al abdomen, desde donde es reabsorbido por el flujo sanguíneo.

Resumen intermedio 

Los anatomistas han adoptado una terminología para describir la localización de las diferentes partes del cuerpo. Anterior significa en dirección a la cabeza, posterior en dirección a la cola, lateral hacia el costado, medial hacia la línea media, dorsal hacia la espalda y ventral hacia la superficie frontal del cuerpo.
En el caso especial del sistema nervioso, rostral significa hacia el rostro (o nariz) y caudal hacia la cola. Ipsilateral significa «el mismo lado» y contralateral significa «el lado contrario».
Una sección transversal (o frontal) secciona el sistema nervioso como si fuera un salchichón, una sección horizontal lo corta paralelo al suelo, y una sección sagital lo hace de manera perpendicular al suelo, paralelo al neuroeje.

El sistema nervioso central está formado por el encéfalo y la médula espinal, y el sistema nervioso periféricos por los nervios espinales y craneales, y por los ganglios periféricos.
El SNC está cubierto por las meninges: duramadres aracnoides y piamadre.
El espacio situado bajo la membrana aracnoides está lleno de líquido cefalorraquídeo, en el cual flota el cerebro.
El SNP está cubierto sólo por la duramadre y la piamadre.
El líquido cefalorraquídeo se produce en los plexos coroideos de los ventrículos laterales. tercero y cuarto.
Fluye desde los ventrículos laterales hacia el tercer ventrículo, por el acueducto cerebral hacia e&rsqb; cuarto ventrículo y luego hacia el espacio subaracnoideo y finalmente, vuelve a la circulación sanguínea.
Si el flujo de LCR se bloquea, debido a un tumor o a cualquier otra obstrucción, aparece hidrocefalia: un aumento del volumen de los ventrículos, con el consiguiente daño cerebral.

El sistema nervioso central 

Aunque el encéfalo es extremadamente complicado. si entendemos las características básicas de su desarrollo resulta más fácil aprender y recordar la localización de las estructuras más importantes que lo forman.
Con esta finalidad en mente, introduciremos aquí esas características en el contexto del desarrollo del encéfalo.

DESARROLLO DEL SISTEMA NERVIOSO CENTRAL 

El sistema nervioso central inicia su existencia en una etapa temprana del desarrollo embrionario como un tubo hueco, manteniendo esta forma básica incluso después de su pleno desarrollo.
Durante el mismo, algunas partes del tubo se elongan, se invaginan y se doblan y el tejido que rodea al tubo se va engrosando.
Las células que darán lugar a las neuronas se hallan en la superficie interna del tubo.
Estas células se dividen y producen neuronas.
Las cuales migrarán en dirección radial, alejándose del centro.
Su localización final viene guiada por lactores tanto físicos como químicos.
La guía física es aportada por células gliales de orientación radial; las neuronas recién formadas migran a lo largo de los procesos de esas células.
La guía química atrae a tipos específicos de neuronas hacia localizaciones particulares, donde acaban situándose (véase la figura 4.7).

En las etapas iniciales de su propio desarrollo el sistema nervioso central contiene tres cámaras interconectadas.

Estas cámaras se convertirán en los ventrículos, y el tejido que las rodea, en las tres partes principales del encéfalo: el prosencéfalo (o cerebro anterior ), el mesencéfalo (o cerebro medio ) y el rombencéfalo (o cerebro posterior ) (véase la figura 4.8a).
Más adelante, la cámara rostral se divide en tres cámaras separadas, que se convertirán en los dos ventrículos laterales y en el tercer ventrículo.
La región que rodea a los ventrículos laterales se convertirá en el telencéfalo («encéfalo terminal»), y la región que rodea al tercer ventrículo se convertirá en el diencéfalo («interencéfalo») (véase la figura 4.8b).
En su forma final, la cámara situada en el interior del mesencéfalo («encéfalo medio») se estrecha, formando el acueducto cerebral y desarrollándose dos estructuras en el rombencéfalo: el metencéfalo («encéfalo de detrás») y el mielencéfalo («encéfalo de la médula») (véase la figura 4.8c).

Figura

Sección transversal del sistema nervioso en un estadio temprano del desarrollo. Las células gliales de orientación radial contribuyen a guiar la migración de neuronas recién formadas 

Figura

Esquema del desarrollo cerebral mostrando la relación del cerebro con los ventrículos. Los dibujos a y b muestran secciones de un cerebro en desarrollo; el dibujo C Constituye una visión lateral. a) etapa inicial del desarrollo; b) etapa intermedia del desarrollo; c) etapa al avanzada del desarrollo, alrededor del nacimiento 

Figura

Subdivisiones anatómicas del cerebro 


División principal
- Prosencéfalo
- Mesencéfalo
- Rombencéfalo


Ventrículo
- Lateral
- Tercero
- Acueducto cerebral
- Cuarto


Subdivisión
- Telencéfalo
- Diencéfalo
- Mesencéfalo
- Metencéfalo
- Mieléncefalo


Principales estructuras
- Corteza cerebral
- Ganglios basales
- Sistema límbico
- Tálamo
- Hipotálamo
- Téctum
- Tegméntum
- Cerebelo
- Protuberancia
- Bulbo raquídeo

La tabla 4.1 resume los términos que han sido introducidos y menciona algunas de las principales estructuras de cada una de las subdivisiones del encéfalo.
Estas estructuras serán descritas en el resto del capítulo (véase la tabla 4.1).

6 La visión 

Como vimos en el capítulo 4, el cerebro realiza dos funciones principales: controla el movimiento de los músculos. produciendo conductas útiles, y regula el medio interno del organismo.
Para realizar ambas tareas necesita estar informado no sólo sobre el medio externo sino también sobre el propio cuerpo.
Toda esta información es captada por los sistemas sensoriales.
Este capítulo y el siguiente tratan del modo en que los órganos de los sentidos detectan cambios en el medio externo y del modo en que el cerebro interpreta las señales neurales producidas en estos órganos.

Generalmente se dice que tenemos cinco sentidos: vista, oído, olfato, gusto y tacto.
En realidad, tenemos más de cinco, pero incluso los expertos no están de acuerdo en cómo deberían trazarse las fronteras entre las diversas categorías de todos ellos.
Desde luego, deberíamos añadir el sentido vestibular: además de suministrarnos información auditiva, el oído interno nos proporciona información sobre la orientación y el movimiento de la cabeza.
El sentido del tacto (o más exactamente, la somestesia ) detecta cambios en la presión, el calor, el frío, la vibración y la posición de los miembros y Se los eventos que dañan a los tejidos (es decir, que producen dolor).
Todo el mundo está de acuerdo en que podemos detectar estos estímulos: la cuestión es si son o no detectados por sentidos separados. diferentes entre sí.

Este capítulo trata de la visión.
La modalidad sensorial que ha recibido mayor atención por parte de psicólogos, anatomistas y fisiólogos.
El porqué de esta atención deriva de la fascinante complejidad del órgano sensorial de la visión y de la proporción relativamente grande del cerebro que está dedicada al análisis de la información visual.
Otra razón, con toda seguridad, es la importancia que la visión tiene para nosotros mismos como individuos.
La fascinación natural hacia tan rica fuente de información sobre el mundo conlleva la curiosidad de saber cómo trabaja esta modalidad sensorial.
El capítulo 7 trata del resto de modalidades sensoriales: la audición.

La somestesia, el sentido vestibular, el gusto y el olfato.

El estímulo 

Como todos sabemos, los ojos detectan la presencia de la luz.

Para los seres humanos, la luz es una estrecha banda del espectro de la radiación electromagnética.
Cuando esta radiación tiene una longitud de onda comprendida entre 380 y 760 nm (un nanómetro, nm, es una milmillonésima parte del metro) resulta visible para nosotros (véase la lámina 6.1).
Otros animales pueden detectar rangos diferentes de radiación electromagnética.
Por ejemplo, las abejas pueden detectar diferencias en la radiación ultravioleta reflejada por flores que a nosotros nos parecen blancas.
El rango de longitudes de onda que denominamos luz , no es cualitativamente diferente del resto del espectro electromagnético; es simplemente la parte del continuum que los humanos podemos ver.

El color de la luz que percibimos viene determinado por tres dimensiones: el color , la saturación y la luminosidad .
La luz viaja a una velocidad constante de aproximadamente 300.000 km por segundo.
Así, si la frecuencia de oscilación de la onda varía, la distancia entre los picos de las ondas variará de forma similar, pero de modo inverso.
Las oscilaciones más lentas implican ondas de longitud más larga, y las ondas más rápidas, longitudes de onda más cortas.
Las longitudes de onda determinan la primera de las tres dimensiones perceptuales de la luz: el color .
El espectro visible muestra el rango de colores que nuestros ojos pueden detectar.

La luz puede variar también en intensidad, lo que corresponde a su segunda dimensión perceptual: la luminosidad .
Si aumenta la intensidad de la radiación electromagnética, la luminosidad aparente también aumenta.
La tercera dimensión, la saturación hace referencia a la pureza relativa de la luz que se percibe.
Si toda la radiación es de una misma longitud de onda, el color que se percibe es puro, o totalmente saturado.
Contrariamente, si la radiación contiene todas las longitudes de onda, no produce sensación de color -parece luz blanca-.
Los colores con cantidades intermedias de saturación consisten en mezclas de longitudes de onda diferentes.
En la lámina 6.2 se observan algunas muestras, todas con el mismo color pero con diferentes niveles de luminosidad y de saturación (véase la lámina 6.2).

Anatomía del sistema visual 

Para que un individuo pueda ver, una imagen debe enfocarse en la retina, la capa interna del ojo.
Esta imagen origina cambios en la actividad eléctrica de millones de neuronas de la retina, lo que resulta en mensajes que se envían por el nervio óptico al resto del cerebro. 
(Decimos «el resto» porque la retina es, en realidad, parte del cerebro; ella y el nervio óptico se hallan en el sistema nervioso central y no en el periférico).
Esta sección describe la anatomía de los ojos, los fotorreceptores de la retina que detectan la presencia de la luz, y las conexiones entre la retina y el cerebro.

Figura

Músculos extraoculares que mueven los ojos 

LOS OJOS 

Los ojos están suspendidos en las órbitas , cavidades óseas de la parte frontal del cráneo.
Se sostienen en su sitio y se mueven mediante seis músculos extraoculares sujetos a la rígida y blanca capa externa de cada uno de ellos denominada esclerótica (véase la figura 6.1).
Normalmente, no podemos mirar detrás de los globos oculares y ver esos músculos porque su unión con los ojos está oculta por la conjuntiva .
Esta membrana mucosa forra el párpado y se pliega hacia atrás sujetándose al globo ocular (por eso, las lentes de contacto, si se desplazan de su sitio sobre la córnea, no se «caen detrás del ojo»).

La lámina 6.3 ilustra la anatomía externa e interna del globo ocular (véase la lámina 6.3).

Los ojos realizan tres tipos de movimientos: de convergencia, sacádicos y de búsqueda.
Los movimientos de convergencia son movimientos cooperativos que mantienen la fijación de ambos ojos sobre el mismo objetivo -o, más exactamente, que mantienen la imagen del objeto diana en partes correspondientes de ambas retinas-.
Si situamos un dedo frente a nosotros, lo miramos, y luego nos lo acercamos al rostro, los ojos realizarán movimientos convergentes hacia la nariz.
Si miramos entonces un objeto del otro lado de la habitación, nuestros ojos girarán hacia el exterior, y veremos una doble imagen borrosa del dedo.

Cuando examinamos la escena que hay frente a nosotros, la mirada no vaga lenta y continuamente recorriendo su contenido.
En lugar de ello, los ojos realizan movimientos sacádicos espasmódicos -movemos la mirada bruscamente de un punto a otro-.
Cuando leemos una línea de este libro, nuestros ojos se detienen varias veces, moviéndose muy rápidamente entre cada parada.
No podemos controlar conscientemente la velocidad de esos movimientos; durante cada sacada los ojos se mueven tan rápidamente como pueden.
Solo cuando realizamos movimientos de búsqueda -como seguir a un dedo con la mirada mientras lo movemos de un lado a otro- podemos hacer que los ojos se muevan más lentamente.

La capa externa de la mayor parte del ojo, la esclerótica, es opaca y no deja pasar la luz.
Sin embargo, la córnea, la capa externa de la parte frontal del mismo, es transparente y sí la deja pasar.
La cantidad de luz que entra en el ojo es regulada por el tamaño de la pupila, que es una abertura del iris, el aro de músculos pigmentados situado detrás de la córnea.
El cristalino, que está inmediatamente detrás del iris, consiste en una serie de capas como las de una cebolla?
transparentes.
Su forma puede alterase por la contracción de la musculatura ciliar.

Estos cambios de forma permiten al ojo enfocar imágenes de objetos próximos o alejados -un proceso denominado acomodación -.

Después de pasar por el cristalino, la luz atraviesa el interior del ojo, que contiene el humor vítreo.
El humor vítreo («líquido vidrioso») es una sustancia clara, gelatinosa, que proporciona al ojo su volumen.

Después de atravesarla, la luz se proyecta sobre la retina , la capa interna de la parte posterior del ojo.
En ella se localizan las células receptoras, los bastones y los conos (denominados así por su forma), conocidos en su conjunto como fotorreceptores .
La retina humana tiene aproximadamente 120 millones de bastones y 6 millones de conos.
A pesar de que hay muchos más bastones, los conos nos proporcionan la mayor parte de la información sobre nuestro entorno.
En concreto, son responsables de la visión diurna.
Nos suministran información sobre los detalles del entorno: por ello son los responsables de la visión más nítida, o agudeza (de acus , que significa «aguja»).
La fóvea , región central de la retina, media nuestra visión más aguda y sólo tiene conos.
Los conos son también los responsables de la visión del color -la capacidad para discriminar luces de diferentes longitudes de onda-.
Aunque los bastones no detectan los diferentes colores y proporcionan una visión poco aguda, son más sensibles a la luz.
En un ambiente débilmente iluminado usamos la visión de los bastones; por ello, con luz débil somos ciegos al color y carecemos de visión foveal.
Si estamos en el exterior en una noche oscura y miramos directamente una luz débil y distante (es decir, centrando su imagen en la fóvea) ésta desaparece.

Otra característica de la retina es el disco óptico , lugar donde los axones que llevan la información visual se reúnen y salen del ojo formando parte del nervio óptico.
El disco óptico origina un punto ciego , porque en él no se localizan receptores.
Normalmente no percibimos los puntos ciegos, pero su presencia puede ser demostrada.
Podemos localizarlos realizando el ejercicio descrito en la figura 6.2.

Un examen más minucioso de la retina muestra que ésta consiste en varias capas de cuerpos celulares de neuronas, sus axones y dendritas, y los fotorreceptores.
La lámina 6.4 muestra una sección transversal de la retina de un primate, la cual está dividida en tres capas principales: la capa fotorreceptora, la capa de células bipolares y la capa de células ganglionares.

Obsérvese que los fotorreceptores están en la parte posterior de la retina; para llegar a ellos, la luz tiene que pasar a través de esas capas superpuestas.
Afortunadamente son transparentes (véase la lámina 6.4).

Prueba pura localizar el punto ciego.
Se cierra el ojo izquierdo, y se mira al signo + con el ojo derecho.
A continuación desplazamos la hoja acercándola y alejándola de nosotros.
Cuando esté alrededor de 20 cm de distancia de la cara, el círculo negro desaparece, porque su imagen se proyecta entonces en el punto ciego del ojo derecho.

Los fotorreceptores forman sinapsis con las células bipolares , neuronas cuyos dos brazos conectan las capas más superficial y más profunda de la retina.
Sucesivamente, estas neuronas forman sinapsis con las células ganglionares , neuronas cuyos axones viajan por el nervio óptico (segundo par craneal), llevando la información visual al cerebro.

Además, la retina tiene células horizontales y células amacrinas ; ambos tipos transmiten información en dirección paralela a la superficie de la retina, combinando así los mensajes de los fotorreceptores adyacentes (véase la lámina 6.4).

Los FOTORRECEPTORES 

La figura 6.3 muestra el dibujo de un bastón y de un cono.

Obsérvese que cada fotorreceptor consiste en un segmento externo conectado por un cilio a un segmento interno, que contiene el núcleo (véase la figura 6.3).
El segmento externo tiene varios cientos de lamellae , finas láminas de membrana. ( Lamella es el diminutivo de lámina , «lámina fina».) 

El primer paso en la cadena de sucesos que conducen a la percepción visual implica una sustancia química especial denominada fotopigmento. (Los pasos de este proceso fueron descubiertos por George Wald, que recibió el premio Nobel por su trabajo en 1967.) Los fotopigmentos son moléculas especiales embebidas en la membrana de las láminas; un único bastón humano tiene aproximadamente 10 millones de ellas.
Las moléculas tienen dos componentes: una opsina (una proteína) y retinal (un lípido).
Hay varios tipos de opsinas; por ejemplo, el fotopigmento de los bastones humanos, la rodopsina , consiste en opsina de los bastones más retinal. ( Rhod en realidad se refiere al rhodon griego, «rosa», y no a bastón .
Antes de desteñirse por la acción de la luz.
La rodopsina tiene un color rosado.) El retinal se sintetiza a partir de la vitamina A, lo que explica que se diga que las zanahorias, ricas en esta vitamina F son buenas para la vista.

Figura

Fotorreceptores 

Si se expone a la luz una molécula de rodopsina, se rompe en sus dos constituyentes: la opsina de los bastones y el retinal.
Cuando esto sucede, la opsina cambia su color rosado por un amarillo pálido; por ello decimos que la luz destiñe al fotopigmento.
La escisión de éste origina un cambio en el potencial de membrana del fotorreceptor, denominado potencial receptor , que modifica la tasa a la que aquél libera su sustancia transmisora.
La membrana del fotorreceptor es diferente a la de otras neuronas -tiene canales iónicos que están normalmente abiertos -.
Estos canales admiten cationes: por ello, el potencial de reposo de la membrana está menos polarizado que el de otras neuronas y el potencial receptor consiste en una hiperpolarización .
Así, a diferencia de otras neuronas, los fotorreceptores liberan continuamente su neurotransmisor.
Cuando la luz hace que una molécula de fotopigmento se escinda, los canales de cationes de la membrana externa del fotorreceptor se cierran.
De este modo, como los cationes ya no pueden seguir entrando en la célula, la membrana se polariza aún más, y se deja de liberar sustancia transmisora (véase la figura 6.4).

En la retina de los vertebrados, los fotorreceptores proporcionan inputs a las células bipolares y a las horizontales.
La lámina 6.5 muestra el circuito neural desde un fotorreceptor a una célula ganglionar.
Este circuito está muy simplificado y omite las células horizontales y las amacrinas.
Los dos primeros tipos de células del circuito -fotorreceptores y células bipolares- no producen potenciales de acción.
En vez de ello, la liberación de sustancia transmisora está regulada por el valor de su potencial de membrana; las despolarizaciones aumentan la liberación, y las hiperpolarizaciones la disminuyen.
Los círculos indican lo que se vería en la pantalla de un osciloscopio cuando se registran cambios en los potenciales de membrana de las células en respuesta a un punto de luz que incide en el fotorreceptor.

El efecto hiperpolarizante de la luz en las membranas de los fotorreceptores se muestra en el esquema de la derecha.
La hiperpolarización reduce la liberación de sustancia transmisora por el fotorreceptor.

Como la sustancia transmisora normalmente hiperpolariza las dendritas de la célula bipolar, una reducción en su liberación hace que la membrana de la célula bipolar se despolarice .
Así, la luz hiperpolariza el fotorreceptor y despolariza la célula bipolar (véase la lámina 6.5).
Esta despolarización hace que la célula bipolar libere más sustancia transmisora, lo cual despolariza la membrana de la célula ganglionar, originando un aumento de su tasa de actividad.
Por todo ello, la luz que incide en el fotorreceptor causa excitación de la célula ganglionar.

El circuito que se muestra en la lámina 6.5 muestra una célula ganglionar cuya tasa de actividad aumenta en respuesta a la luz.
Tal como veremos, otras células ganglionares disminuyen su tasa de actividad en respuesta a la luz.
Estas neuronas están conectadas a células bipolares que forman diferentes tipos de sinapsis con los fotorreceptores.
Las funciones de estos dos tipos de circuitos se explican más adelante, en una sección denominada «Codificación de la información visual en la retina».

Figura

La transducción. Una explicación hipotética de la producción de potenciales receptores en los fotorreceptores 

Figura

Microfotografía de una sección del núcleo geniculado lateral derecho de un mono rhesus (teñido con violeta de cresilo) 

CONEXIONES ENTRE LOS OJOS Y EL CEREBRO 

Los axones de las células ganglionares de la retina llevan información al resto del cerebro.
Ascienden a través de los nervios ópticos y alcanzan el núcleo geniculado lateral dorsal del tálamo.
Este núcleo se llama así por su semejanza con una rodilla doblada (genu significa «rodilla»).
Tiene seis canas de neuronas, cada una de las cuales recibe información de un solo ojo.
Las neuronas de las dos capas internas tienen cuerpos celulares más grandes que las de las cuatro capas externas.
Por esta razón, las dos capas internas se denominan capas magnocelulares , y las cuatro externas, capas parvocelulares (parvo hace referencia al tamaño pequeño de las células).
Tal como veremos más adelante, estos dos conjuntos de capas pertenecen a sistemas diferentes, que son responsables del análisis de diferentes tipos de información visual.

Asimismo, reciben inputs de diferentes tipos de células ganglionares retinianas (véase la figura 6.5).

Las neuronas del núcleo geniculado lateral dorsal envían sus axones a través de las radiaciones ópticas a la corteza visual primaria -la región que rodea a la cisura calcarina ( calcarina significa en forma de espuela)-, una cisura horizontal localizada en el lóbulo occipital posterior y medial.
La corteza visual primaria se denomina con frecuencia corteza estriada , porque tiene una capa de células que se tiñe de un color oscuro ( estrías ) (véase la figura 6.6).

La lámina 6.6 muestra un diagrama de una sección horizontal del cerebro humano.
Los nervios ópticos se juntan en la base del cerebro para formar el quiasma óptico , estructura en forma de X ( khiasma significa «cruce»).
En él, los axones de las células ganglionares que llevan la información de la mitad interna de la retina (lado nasal) cruzan a través del quiasma y ascienden hacia el núcleo geniculado lateral dorsal del lado contrario del cerebro.
Los axones de la mitad externa de la retina (lado temporal) permanecen en el mismo lado del cerebro.
El cristalino invierte la imagen del mundo proyectada sobre la retina (y de forma similar invierte izquierda y derecha).
De este modo. como los axones de la mitad nasal de la retina cruzan al otro lado del cerebro, cada hemisferio recibe información de la parte contralateral (lado contrario) de la escena visual.
Es decir, si una persona mira hacia el frente, el hemisferio derecho recibe información del lado izquierdo del campo visual, y el hemisferio izquierdo recibe información del lado derecho (véase la lámina 6.6).

Figura

Microfotografía de una sección de la corte a estriada de un mono macaco rhesus. El borde de esta corte a se indica con flechas 

Además de la vía primaria retino-genículo-cortical, las fibras de la retina dan lugar a otras vías distintas.
Por ejemplo, una vía que va al hipotálamo sincroniza los ciclos de actividad de un animal según el ritmo de las 24 horas del día y la noche. (Estudiaremos este sistema en el capítulo 9).

Otras vías, especialmente las que viajan al téctum óptico y los núcleos pretectales, coordinan los movimientos de los ojos, controlan los músculos del iris y del cristalino, y ayudan a dirigir nuestra atención a movimientos repentinos de la periferia del campo visual.

8 Control del movimiento 

En los capítulos anteriores hemos descrito la naturaleza de la comunicación neural, la estructura básica del sistema nervioso y la fisiología de la percepción.

Ahora es el momento de considerar la función esencial del sistema nervioso: el control de la conducta.
El cerebro es el órgano que mueve los músculos.
Hace también otras muchas cosas, pero todas ellas secundarias en relación con el movimiento de nuestro cuerpo.
Este capítulo trata de los principios de la contracción muscular, de algunos circuitos reflejos de la médula espinal y del modo como el cerebro inicia las conductas.
El resto del libro describe la fisiología de categorías particulares de conducta y las vías mediante las que nuestro comportamiento puede ser modificado por la experiencia.

Los músculos 

Los mamíferos tienen tres tipos de músculos: el esquelético, el liso y el cardíaco.

EL MÚSCULO ESQUELÉTICO 

Los músculos esqueléticos son los que hacen posible nuestro movimiento (o el de nuestros esqueletos), siendo así los responsables de nuestra conducta.
La mayoría están ligados a los huesos, a cada uno de sus extremos, causando su movimiento al contraerse.

(Una excepción son los músculos de los ojos y también algunos músculos abdominales, los cuales están unidos a los huesos únicamente en uno de sus extremos.) Los músculos se fijan a los huesos a través de los tendones, tiras fuertes de tejido conectivo.

Los músculos esqueléticos pueden producir una gran variedad de movimientos, pero nos referiremos principalmente a dos tipos: la flexión y la extensión.
La contracción de un músculo flexor produce flexión , por ejemplo, el doblar una pierna.
La extensión , que es el movimiento contrario, se produce por contracción de los músculos extensores.
Estos son los denominados músculos antigravitatorios , los que usamos para mantenernos de pie.
Cuando un animal cuadrúpedo levanta una pata, el movimiento es de flexión.

Apoyándola otra vez sobre el suelo realiza un movimiento de extensión.
A menudo, algunas personas dicen que «doblan» sus músculos.
Este es un uso incorrecto del término.
Los músculos se contraen ; los miembros se flexionan .
Los culturistas exhiben los músculos de sus brazos contrayendo simultáneamente los músculos flexores y extensores de esa extremidad.

ANATOMÍA 

En la figura 8.1 se muestra la estructura detallada de un músculo esquelético.
Tal como se puede observar, se compone de dos tipos de fibras musculares.
Las fibras musculares extrafusales son inervadas por axones de las motoneuronas alfa .
La contracción de estas fibras proporciona la fuerza motriz del músculo.
Las fibras musculares intrafusales son órganos sensoriales especializados que están inervados por dos axones, uno sensorial y otro motor.
Estos órganos se denominan también husos musculares debido a su forma.
De hecho, la palabra latina fusus significa «huso»; por tanto, las fibras musculares intrafusales están localizadas dentro de los husos, mientras que las extrafusales se hallan fuera de ellos.

La región central ( cápsula ) de la fibra muscular intrafusal contiene terminales sensibles a la extensión de la fibra muscular.
En realidad existen dos tipos de fibras musculares intrafusales, aunque para simplificar sólo mostraremos un tipo de ellas.
El axón eferente de las motoneuronas gamma hace que la fibra muscular intrafusal se contraiga; pero esta contracción aporta una cantidad insignificante de fuerza.
Como veremos más adelante, su función consiste en modificar la sensibilidad a la extensión de la terminal aferente de la fibra.

Un único axón mielínico de una motoneurona alfa inerva varias fibras musculares extrafusales.
En los primates, el número de fibras musculares inervadas por un único axón varía considerablemente dependiendo de la precisión con que el músculo puede ser controlado.
En los músculos que mueven los dedos o los ojos, la proporción puede ser menor de uno a diez; en los músculos que mueven las piernas, puede ir de uno a varios centenares.
Una motoneurona alfa, su axón y las fibras musculares extrafusales asociadas constituyen una unidad motora .

Una fibra muscular individual consiste en un haz de miofibrillas , cada una de las cuales está constituida a su vez por hebras solapadas de actina y miosina.

Las pequeñas protusiones de los filamentos de miosina ( puentes de cruzamiento de miosina ) son los elementos móviles que interactúan con los filamentos de actina produciendo las contracciones musculares (véase la figura 8.1).
Las regiones donde se solapan los filamentos de actina y miosina presentan franjas oscuras, o estrías ; razón por la cual a los músculos esqueléticos se les llama también a menudo músculos estriados .

Bases físicas de la contracción muscular 

La sinapsis entre el botón terminal de una neurona eferente y la membrana de una fibra muscular se denomina unión neuromuscular .
Los botones terminales de las neuronas sinaptan en las placas motoras , estructuras localizadas en las ranuras que hay a lo largo de la superficie de las fibras musculares.
Cuando un axón emite potenciales de acción, los botones terminales liberan acetilcolina, originando una despolarización de la membrana postsináptica ( potencial de placa terminal ).
Los potenciales de placa terminal son mucho mayores que los potenciales excitatorios postsinápticos de las sinapsis entre neuronas; un potencial de placa terminal provoca siempre la activación de la fibra muscular, propagándose el potencial en toda su longitud.
Este potencial de acción induce una contracción, o sacudida , de la fibra muscular.

La despolarización de una fibra muscular origina la apertura de canales de calcio dependientes del voltaje, permitiendo la entrada de iones de calcio en el citoplasma.
Esto inicia la contracción.
El calcio actúa como un cofactor que permite a las miofibrillas extraer energía del ATP presente en el citoplasma.
El proceso es el siguiente: los puentes de cruzamiento de la miosina se ligan alternativamente a las hebras de actina, se inclinan en una dirección, se separan por sí mismos, se inclinan hacia atrás y se vuelven a unir a la actina, pero esta vez en un punto más lejano de la hebra, y así sucesivamente.
Es decir, es como si los puentes de cruzamiento de la miosina «remasen» a lo largo de los filamentos de actina.
En la figura 8.2. se ilustra esta secuencia, que tiene como resultado el acortamiento de la fibra muscular (véase la figura 8.2).

Un único impulso de una motoneurona produce una única sacudida de una fibra muscular.
Los efectos físicos de esta respuesta son considerablemente más prolongados que los del potencial de acción, debido a la elasticidad del músculo y al tiempo requerido para eliminar el calcio de las células. 
(Al igual que el sodio, el calcio es extraído activamente de las células mediante una bomba situada en la membrana.)
En la figura 8.3. se muestra cómo los efectos físicos de una serie de potenciales de acción se pueden solapar, causando una contracción sostenida de la fibra muscular.

Una unidad motora de un músculo de la pierna de un gato puede elevar hasta 100 gramos de peso, lo que atestigua la considerable fuerza que tiene el mecanismo de contracción (véase la figura 8.3) 

Figura

Anatomía del músculo esquelético 

Como el lector sabe por propia experiencia, la contracción muscular no es un fenómeno de todo o nada, como lo son las sacudidas de las fibras musculares constituyentes.
Obviamente, la fuerza de la contracción muscular viene determinada por la tasa media de descarga de varias unidades motoras.
Si en un momento dado se activan muchas de estas unidades, la contracción será vigorosa.
Si sólo se activan unas pocas.
La contracción será débil.

Feedback sensorial desde los músculos 

Tal como hemos visto, las fibras musculares intrafusales tienen terminales sensibles a la extensión.
Estas fibras están dispuestas en paralelo respecto a las fibras musculares extrafusales.
De esta forma, las fibras intrafusales se estiran cuando el músculo se alarga y se relajan cuando el músculo se acorta.
Así, a pesar de que estas neuronas aferentes son receptores de extensión , sirven como detectores de longitud muscular .
Esta distinción es importante.

Los receptores de extensión se localizan también en los tendones, en los llamados órganos tendinosos de Golgi (OTG).
Estos receptores detectan la cantidad total de extensión que el músculo, a través de los tendones, ejerce sobre los huesos a los cuales está ligado.
Los receptores de extensión del órgano tendinoso de Golgi codifican el grado de extensión por su tasa de respuesta.
No responden a la longitud del músculo sino a la fuerza de estiramiento del mismo.
Por el contrario, los receptores de las fibras musculares intrafusales detectan la longitud del músculo, no su tensión.

Figura

Mecanismo de la contracción muscular. a) Localización de los puentes de cruzamiento de miosina. b) Los puentes de cruzamiento de miosina «remando», lo cual hace que los filamentos de actina y de miosina se muevan uno con respecto al otro 

Figura

Potenciales de acción y contracciones. Una sucesión rápida de potenciales de acción puede hacer que una fibra muscular produzca una contracción sostenida.Cada punto representa un potencial de acción individual 

La figura 8.4. muestra la respuesta de axones aferentes de los husos musculares y de un órgano tendinoso de Golgi frente a varios tipos de movimientos.
El apartado a muestra los efectos de la elongación pasiva de los músculos, el tipo de movimiento que podríamos observar en nuestro antebrazo si, manteniéndolo absolutamente relajado, fuese lentamente extendido por otra persona que lo estuviera sosteniendo.
La tasa de descarga de un tipo de neurona aferente del huso muscular (HMI) aumenta, mientras que la actividad de la fibra aferente del órgano tendinoso de Golgi no varía (véase la figura 8.4a).
En la misma figura, apartado b, se muestran los resultados cuando el brazo se deja caer rápidamente; obsérvese que el segundo tipo de neuronas aferentes de los husos musculares (HM2) responden con ráfagas rápidas de impulsos.
Esta fibra, pues, señala los cambios rápidos en la longitud muscular (véase la figura 8.4b).
En la figura 8.4c se muestra lo que pasaría si se colgara repentinamente un peso de la mano mientras el antebrazo se mantuviese paralelo al suelo.
Las neuronas que inervan los HMI y HM, (especialmente las de los HM2, que responden a cambios rápidos en la longitud del músculo) se activan muy brevemente debido a que el brazo baja durante un tiempo muy corto pero vuelve otra vez a su posición original.
El órgano tendinoso de Golgi, que controla la fuerza de la contracción, se activa en proporción a la tensión del músculo, por lo que aumenta su tasa de respuesta en cuanto se sitúa el peso sobre la mano (véase la figura 8.4c).

Figura

Efecto de los movimientos del brazo sobre la tasa de descarga de los axones aferentes de los músculos y tendones. a) Extensión pasiva lenta del brazo. b) Extensión rápida del brazo. c) Adición de un peso sobre un brazo mantenido en posición horizontal. HM1 y HM2 son dos tipos de husos musculares; OTG es una fibra aferente procedente del órgano tendinoso de Golgi 

EL MÚSCULO LISO 

En nuestro cuerpo hay dos tipos de músculos lisos , ambos controlados por el sistema nervioso autónomo.
Los músculos lisos multiunidades se localizan en las grandes arterias, alrededor de los folículos pilosos (donde producen la piloerección ), y en el ojo (donde controlan la acomodación del cristalino y la dilatación de la pupila).
Este tipo de músculo liso está normalmente inactivo, pero se contrae en respuesta a la estimulación neural o a determinadas hormonas.
En contraste, los músculos lisos de una sola unidad se contraen normalmente de forma rítmica.

Algunas de estas células producen espontáneamente potenciales marcapasos , que pueden ser considerados como potenciales postsinápticos excitatorios autoiniciados.
Estos potenciales lentos provocan potenciales de acción, los cuales se propagan por las fibras musculares lisas adyacentes, causando una onda de contracción muscular.
El nervio eferente (y varias hormonas) puede modular la tasa rítmica, aumentándola o disminuyéndola.
Los músculos lisos de una sola unidad se localizan principalmente en el sistema gastrointestinal, en el útero y en los pequeños vasos sanguíneos.

EL MÚSCULO CARDÍACO 

Como su nombre indica, el músculo cardíaco se encuentra en el corazón.
Este tipo de músculo tiene el aspecto de un músculo estriado pero funciona como el músculo liso de una sola unidad.
El corazón late regularmente, incluso cuando está denervado.
La actividad neural y determinadas hormonas (especialmente las catecolaminas) modulan la tasa cardíaca.
Hay un grupo de células rítmicamente activas que se sitúan en el marcapasos del corazón y que son las que inician las contracciones musculares que constituyen los latidos cardíacos.

Resumen intermedio 

En nuestro cuerpo hay músculos esqueléticos, músculos lisos y el músculo cardíaco.
Los músculos esqueléticos están compuestos por fibras musculares extrafusales, las cuales proporcionan la fuerza de la contracción.
Las motoneuronas alfa establecen sinapsis con estas fibras, controlando su contracción.
La musculatura esquelética también está compuesta por fibras musculares intrafusales, las cuales detectan cambios en la longitud del músculo.
La longitud de la fibra muscular intrafusal y, por tanto, su sensibilidad frente a incrementos en la longitud muscular, está controlada por las motoneuronas gamma.
Además de las fibras musculares intrafusales, los músculos tienen receptores de extensión en los órganos tendinosos de Golgi, situados en sus extremos.

La fuerza de la contracción muscular depende de largas moléculas proteicas denominadas actina y miosina, dispuestas en filas paralelas superpuestas.
Cuando un potencial de acción, iniciado por la sinapsis de la placa motora terminal, provoca la entrada de Ca2+ en la fibra muscular, las miofibrillas extraen energía del ATP y causan una contracción de la fibra muscular, produciendo un movimiento de los puentes de cruzamiento de miosina similar al de «remar».

Los músculos lisos están controlados directamente por el sistema nervioso autónomo, mediante conexiones neurales, e indirectamente por el sistema endocrino.
Los músculos lisos multiunidades se contraen únicamente en respuesta a la estimulación neuronal u hormonal.
Por el contrario, los músculos lisos de una sola unidad se contraen normalmente de forma rítmica, aunque su tasa de contracción está bajo el control del sistema nervioso autónomo.
El músculo cardíaco también se contrae espontáneamente, y su tasa de contracción también está influenciada por el sistema nervioso autónomo.

Control reflejo del movimiento 

Aunque la conducta está controlada por el cerebro, la médula espinal posee un cierto grado de autonomía.

Determinadas clases de estímulos somatosensoriales pueden provocar respuestas rápidas a través de las conexiones neurales localizadas en la médula espinal.

Estos reflejos constituyen el nivel más simple de integración motora.

EL REFLEJO MONOSINÁPTICO DE EXTENSIÓN 

Es fácil demostrar la actividad de la vía neural funcional más simple del cuerpo.
Sentémonos en una superficie lo suficientemente elevada para que las piernas queden colgando y hagamos que alguien nos golpee ligeramente en el tendón patelar, justo por debajo de la rodilla.
Este estímulo provoca la rápida extensión del músculo cuadríceps, en la parte superior del muslo.
Ello provoca la contracción de este músculo, lo que a su vez hace que la pierna dé un puntapié hacia adelante. (Probablemente sólo unos pocos se tomarán la molestia de realizar esta prueba ya que es harto conocida; la mayor parte de los exámenes físicos incluyen la prueba de este reflejo.) El intervalo de tiempo entre el golpe en el tendón y el inicio de la extensión de la pierna es de aproximadamente 50 milisegundos.
Este intervalo es demasiado corto para involucrar al cerebro; en ese caso debería ser considerablemente más largo para que la información sensorial pudiera llegar al mismo y para que la información motora pudiera ser transmitida como respuesta.
Por ejemplo, supongamos que se le pide a una persona que mueva su pierna tan rápidamente como le sea posible después de ser golpeada su rodilla.
Esta vez la respuesta no sería refleja, ya que implicaría mecanismos sensoriales y motores cerebrales.
En este caso, el intervalo entre el estímulo y el inicio de la respuesta sería varias veces mayor que el tiempo requerido por el reflejo patelar.

Obviamente, el reflejo patelar como tal no tiene utilidad; el que un animal dé una patada cuando su rodilla es ligeramente golpeada no le aporta ninguna ventaja selectiva.
A pesar de ello, cuando se presenta un estímulo más natural, la utilidad de este mecanismo es manifiesta.
En la figura 8.5 se muestran los efectos de colocar un peso en la mano de una persona.
Esta vez se ha incluido un esquema de la médula espinal, con sus raíces, para mostrar el circuito neural que constituye el reflejo monosináptico de extensión.
En primer lugar, sigamos el circuito: los impulsos aferentes iniciados en el huso muscular son conducidos a botones terminales de la sustancia gris medular.
Estos botones sinaptan con una motoneurona alfa que inerva las fibras musculares extrafusales del mismo músculo.
A lo largo del trayecto desde el receptor al efector hay solamente una sinapsis -de aquí que el reflejo se denomine monosináptico - (véase la figura 8.5a).

Consideraremos ahora una función útil de este reflejo.
Si se incrementa el peso que la persona sostiene, el antebrazo empieza a descender.
Este movimiento alarga el músculo e incrementa la tasa de respuesta de las neuronas aferentes del huso, cuyos botones terminales estimulan entonces a las motoneuronas alfa, aumentando su tasa de descarga.
En consecuencia, la fuerza de la contracción muscular aumenta y el brazo puede sostener el peso en alto (véase la figura 8.5b).

Otra importante función del reflejo monosináptico de extensión es el control postural.
Para poder mantenernos erguidos, debemos mantener el centro de gravedad por encima de nuestros pies; en caso contrario nos caeríamos.

Cuando nos mantenemos de pie. tendemos a oscilar hacia adelante y hacia atrás. y también lateralmente.
Nuestros sacos vestibulares y nuestro sistema visual tienen un importante papel en el mantenimiento de la postura.
De todas formas, estos dos sistemas reciben la ayuda del reflejo monosináptico de extensión .
Por ejemplo, consideremos lo que le ocurre a una persona cuando empieza a inclinarse hacia adelante.
El gran músculo de la pantorrilla (gemelo o gastrocnemius) se estira, y su extensión provoca una contracción muscular compensatoria que empuja los dedos del pie hacia abajo, restaurando la posición vertical (véase la figura 8.6).

EL SISTEMA MOTOR GAMMA 

Los husos musculares son muy sensibles a los cambios en la longitud del músculo: pequeñas elongaciones de éste son suficientes para provocar un incremento en su tasa de activación.
Algo muy interesante es que este mecanismo de detección es ajustable.

Figura

Reflejo monosináptico de extensión. a) Circuito neural. b) Una función útil 

Figura

Papel del reflejo monosináptico de extensión en el control postural 

Recuérdese que las terminaciones de las fibras musculares intrafusales pueden ser contraídas por la actividad de los axones eferentes que inervan las motoneuronas gamma, cuya tasa de activación determina el grado de contracción.

Cuando los husos musculares están relajados son relativamente insensibles a la extensión.
Pero cuando las motoneuronas gamma se activan, los husos de acortan y entonces se vuelven mucho más sensibles a los cambios en la longitud muscular.
Esta capacidad de ajustar la sensibilidad simplifica el papel del encéfalo en el control del movimiento.
Cuanto mayor sea el control que pueda ejercer la médula espinal, menor número de mensajes deben ser enviados al y desde el cerebro.

Ya vimos anteriormente que los axones aferentes de los husos musculares contribuyen a mantener la posición de las extremidades incluso cuando varía el peso que soportan.
El control eferente de los husos musculares permite que estos detectores de longitud muscular contribuyan también a los cambios en la posición de las extremidades.
Consideremos un único huso muscular.
Cuando su axón eferente está completamente inactivo, el huso está completamente relajado y extendido.
Cuando la tasa de activación del axón eferente aumenta, el huso se vuelve cada vez más corto.
Si de forma simultánea el resto del músculo también se acorta, no habrá estiramiento de la región central que contiene los terminales sensoriales, y el axón aferente no responderá.
Sin embargo, si el huso muscular se contrae más rápidamente que el conjunto del músculo, se producirá una considerable cantidad de actividad aferente.

El sistema motor utiliza este fenómeno de la siguiente forma: cuando el cerebro emite órdenes para mover una extremidad, se activan tanto las motoneuronas alfa como las gamma.
Las motoneuronas alfa inician la contracción muscular.
Si hay poca resistencia, tanto las fibras musculares extrafusales como las intrafusales se contraerán aproximadamente lo mismo, y se observará muy poca actividad en los axones aferentes del huso muscular.
Ahora bien, si la extremidad encuentra resistencia, las fibras musculares intrafusales se acortarán más que las extrafusales, de forma que los axones sensoriales empezarán a activarse y a hacer que el reflejo monosináptico de extensión refuerce la contracción.
Así pues, el cerebro utiliza el sistema motor gamma en el movimiento de las extremidades.
Estableciendo una tasa de descarga en el sistema motor gamma , el encéfalo controla la longitud de los husos musculares e, indirectamente, la longitud de todo el músculo.

REFLEJOS POLISINÁPTICOS 

El reflejo monosináptico de extensión es el único reflejo espinal conocido que involucra una única sinapsis.
Todos los demás son polisinápticos .
Se pueden encontrar ejemplos relativamente sencillos, como la retirada de la pierna en respuesta al dolor, pero también más complejos, como la eyaculación del semen.
Los reflejos espinales no ocurren de forma aislada; normalmente están controlados por el cerebro.
Por ejemplo, en el capítulo 2 se ha descrito cómo la inhibición del cerebro puede evitar que una persona deje caer un ramo de rosas a pesar de que, al pincharse con sus espinas, el estímulo doloroso que recibe en sus dedos active el reflejo de extensión de los mismos.
En esta sección estudiaremos algunos principios generales mediante los cuales operan los reflejos polisinápticos.

Antes de nada, deberíamos decir que los diagramas de los circuitos que hasta aquí hemos utilizado (incluido el que acabamos de ver en la figura 8.5) son excesivamente simples.

Aunque los circuitos reflejos se representan generalmente como una única cadena de neuronas, en realidad, la mayoría de los mismos involucran a miles de ellas.
Cada axón normalmente sinapta con muchas neuronas, y cada neurona recibe sinapsis de muchos axones diferentes.

Como vimos anteriormente, los axones aterentes del órgano tendinoso de Golgi funcionan como detectores de extensión muscular.
Existen dos poblaciones de axones aferentes desde los órganos tendinosos de Golgi, diferencialmente sensibles a la extensión.
Los axones aferentes más sensibles informan al cerebro de la intensidad con que el músculo, está estirando.
Los menos sensibles tienen una función adicional.
Sus botones terminales sinaptan con interneuronas espinales -neuronas localizadas íntegramente en la sustancia gris de la médula espinal que sirven para interconectar con otras neuronas también espinales-.
Estas interneuronas sinaptan con las motoneuronas alfa que inervan al mismo músculo.
Sus botones terminales liberan glicina, lo que origina potenciales postsinápticos inhibitorios en las motoneuronas (véase la figura 8.7).
La función de esta vía refleja consiste en disminuir la fuerza de la contracción muscular cuando hay peligro de lesionar los tendones o los huesos a los cuales están ligados los músculos.
Los levantadores de pesos mejoran su rendimiento si sus órganos tendinosos de Golgi son desactivados mediante inyecciones de un anestésico local, pero corren el riesgo de que el tendón se desgarre o incluso de romper el propio hueso.

Figura

Reflejo inhibitorio polisináptico. El input desde el órgano tendinoso de Golgi puede provocar potenciales inhibitorios postsinápticos en la motoneurona alfa 

El descubrimiento del reflejo del órgano tendinoso de Golgi proporcionó la primera prueba real de inhibición neural, mucho antes de que se conocieran los mecanismos sinápticos.
Un gato descerebrado , cuyo tronco cerebral ha sido seccionado, exhibe el fenómeno conocido como rigidez descerebrada .
Su espalda se arquea y sus patas se estiran rígidamente.
Esta rigidez resulta de la excitación que se origina en la formación reticular caudal, la cual facilita de manera considerable todos los reflejos de extensión, especialmente los de los músculos extensores, incrementando la actividad del sistema motor gamma.

Situada rostralmente a dicha sección troncoencefálica, hay una región inhibitoria de la formación reticular que generalmente contrarresta a la excitatoria.
La transección elimina la influencia inhibitoria, permitiendo únicamente la excitatoria.
Si intentamos flexionar la pata estirada de un gato descerebrado, nos encontramos con una resistencia creciente que desaparece repentinamente, permitiendo la flexión de la pata.
Es algo parecido a lo que ocurre cuando se cierra la cuchilla de una navaja (de ahí el término reflejo de navaja ).
La liberación súbita, por supuesto, está mediada por la activación del reflejo del órgano tendinoso de Golgi.

El reflejo monosináptico de extensión sirve incluso como base de reflejos polisinápticos.
Los músculos están organizados en pares opuestos.
El agonista mueve la extremidad en la dirección estudiada y, dado que los músculos no pueden retroceder, el antagonista mueve la extremidad en la dirección opuesta.
Fijémonos en lo siguiente: cuando se provoca un reflejo de extensión en el músculo agonista, éste se contrae con rapidez, haciendo que el antagonista se alargue.
Podría parecer, pues, que el antagonista se halla ante un estímulo que debiera provocar su reflejo de extensión.
Sin embargo, el antagonista se relaja.
Veamos por qué.

Los axones aferentes de los husos musculares, además de enviar sus botones terminales a la motoneurona alfa y hacia el encéfalo, sinaptan también con interneuronas inhibitorias.
Los botones terminales de estas interneuronas sinaptan a su vez con las motoneuronas alfa que inervan el músculo antagonista (véase la figura 8.8).
Por consiguiente, el reflejo de extensión excita al agonista e inhibe al antagonista , de manera que la extremidad puede moverse en la dirección controlada por el músculo estimulado.

Figura

Reflejos secundarios. La activación del huso muscular provoca la excitación de las motoneuronas alfa del agonista y la inhibición del antagonista 

10 Conducta reproductora 

Las conductas reproductoras constituyen la categoría más importante de conducta social porque sin ellas la mayoría de las especies no sobrevivirían.
Las conductas reproductoras (como el cortejo, el apareamiento, la conducta parental y la mayor parte de las conductas agresivas) constituyen los tipos más destacables de conductas sexualmente dimórficas , es decir, de conductas que difieren en machos y hembras ( di + morfos , «dos formas»).
Como veremos, las hormonas presentes tanto antes como después del nacimiento juegan un papel muy importante en el desarrollo y control de las conductas sexualmente dimórficas.

Este capítulo describe el desarrollo sexual masculino y femenino, y en él se discuten el control neural y hormonal de dos de las conductas sexualmente dimórficas más importantes para la reproducción: la conducta sexual y la conducta maternal.

Desarrollo sexual 

El sexo cromosómico de una persona se determina en el momento de la fertilización.
Sin embargo, este acontecimiento no es más que el primero en una serie de pasos que culminan en el desarrollo de un macho o una hembra.

Esta sección trata de las principales características del desarrollo sexual.

PRODUCCIÓN DE GAMETOS Y FERTILIZACIÓN 

Todas las células del cuerpo (a excepción de los espermatozoides y los óvulos) contienen veintitrés pares de cromosomas, incluido un par de cromosomas sexuales.
La información genética que programa el desarrollo de un ser humano está contenida en el ADN que constituye estos cromosomas.
Nos sentimos orgullosos de nuestra capacidad para miniaturizar circuitos computarizados sobre chips de sílice, pero este logro resulta primitivo si lo comparamos con el anteproyecto genético de un ser humano, demasiado pequeño para poder percibirse a simple vista.

La producción de gametos (óvulos y espermatozoides; gamein significa «casarse») tiene lugar mediante una forma especial de división celular.
Este proceso da lugar a células que contienen un miembro de cada uno de los veintitrés pares de cromosomas.
El desarrollo de un ser humano se inicia en el momento de la fertilización, con la unión de un único espermatozoide con un óvulo, que pasan a compartir sus veintitrés cromosomas individuales para reconstituir los veintitrés pares de cromosomas.

El sexo genético de una persona está determinado por el espermatozoide del padre en el momento de la fertilización.
Veintidós de los pares de cromosomas determinan el desarrollo físico del organismo independientemente de su sexo.
El último, par consiste en dos cromosomas sexuales , los cuales determinarán si el hijo será niño o niña.

Existen dos tipos de cromosomas sexuales: cromosoma X y cromosoma Y .
Las hembras tienen dos cromosomas X (XX); por tanto, todos los óvulos de una mujer contienen un cromosoma X. 
Los hombres tienen un cromosoma X y un cromosoma Y (XY).
Cuando los cromosomas sexuales de un hombre se separan, la mitad de los espermatozoides pasan a tener un cromosoma X y la otra mitad un cromosoma Y. 
Un espermatozoide con el cromosoma Y da lugar a un óvulo fertilizado XY, y, por tanto, a un varón.
Un espermatozoide con el cromosoma X da lugar a un óvulo fertilizado XX y, por consiguiente, a una niña (véase la figura 10.1) 

Figura

Determinación del sexo. El sexo de los hijos depende de si el espermatozoide que fertiliza al óvulo es portador de un cromosoma X o Y

DESARROLLO DE LOS ÓRGANOS SEXUALES 

Los hombres y las mujeres difieren en muchos aspectos: su cuerpo es diferente, tienen partes de su cerebro que también difieren y sus conductas reproductoras son distintas.

¿Están todas estas diferencias codificadas en el minúsculo cromosoma Y, el único pedazo de material genético que distingue a los varones de las hembras?
La respuesta es no.
Las células, tanto de machos como de hembras, contienen la información genética necesaria para desarrollar el cuerpo de cualquiera de los dos sexos.
Es la exposición hormonal, anterior y posterior al nacimiento, la responsable de nuestro dimorfismo sexual.
Lo que controla el cromosoma Y es el desarrollo de la glándula que produce hormonas sexuales masculinas.

Existen tres categorías generales de órganos sexuales: las gónadas, los órganos sexuales internos y los genitales externos.
Las gónadas (testículos u ovarios) son las primeras en desarrollarse.

Las gónadas (del griego gonos , «procreación») tienen una función dual: producen óvulos o espermatozoides y segregan hormonas.

Durante la cuarta semana de desarrollo prenatal, los fetos de ambos sexos son idénticos.
Los dos tienen un par de gónadas no diferenciadas idénticas, que pueden transformarse en testículos o en ovarios.
El factor que controla su desarrollo parece ser un único gen del cromosoma Y llamado SRY.
Este gen produce un enzima llamado factor determinante de los testículos , responsable de que las gónadas indiferenciadas se conviertan en testículos.
Si este gen no está presente, se convertirán en ovarios.
De hecho, si este gen se inserta en uno de los cromosomas X de un embrión de ratón hembra (XX), el animal se desarrollará como macho (Koopman y cols, 1991).

Una vez se han desarrollado las gónadas, se ponen en marcha una serie de acontecimientos que determinarán el sexo del individuo.
Estos acontecimientos están dirigidos por hormonas, las cuales afectan al desarrollo sexual de dos maneras.
Durante el desarrollo prenatal estas hormonas ejercen efectos organizadores , los cuales influyen en el desarrollo de los órganos sexuales y del cerebro de una persona.
Estos efectos son permanentes: una vez iniciada una determinada vía en el curso del desarrollo, ya no puede volverse atrás.
El segundo rol de las hormonas sexuales es su efecto activador .
Estos efectos ocurren en una etapa posterior, cuando los órganos sexuales ya están desarrollados.
Por ejemplo, las hormonas activan la producción de espermatozoides, posibilitan la erección y la eyaculación e inducen la ovulación.
Dado que el cuerpo de machos y hembras adultos ha sido organizado de manera diferente, las hormonas sexuales también tendrán efectos activadores distintos en los dos sexos.

Los órganos sexuales internos son bisexuales ; es decir, todos los embriones contienen los precursores tanto de los órganos sexuales femeninos como de los masculinos.
Sin embargo, durante el tercer mes de gestación sólo uno de estos precursores se desarrolla; el otro desaparece.
El precursor de los órganos sexuales femeninos, que se transforman en las fimbrias y las trompas de Falopio, el útero y los dos tercios internos de la vagina , se llama sistema de Müller .
El precursor de los órganos sexuales masculinos, que se transforma en el epidídimo, el conducto deferente, las vesículas seminales y la próstata, se llama sistema de Wolff (estos sistemas se denominan así en honor a sus descubridores.
Véase la figura 10.2).

El sexo de los órganos sexuales internos de un feto se determina por la presencia o ausencia de las hormonas segregadas por los testículos.
Es decir, si esas hormonas están presentes, se desarrolla el sistema de Wolff.
Si no, se desarrolla el sistema de Müller.
Este último (el sistema femenino) no necesita ningún estímulo hormonal de las gónadas para desarrollarse; simplemente, lo hace.
En cambio, las células del sistema de Wolff (masculino) no se desarrollan si no son estimuladas por una hormona.
Así, los testículos segregan dos tipos de hormonas.
La primera, un péptido llamado hormona inhibidora del sistema de Muller , hace exactamente lo que indica su nombre: impide que el sistema de Muller se desarrolle.
Por consiguiente, tiene un efecto desfeminizante .
La segunda, un grupo de hormonas esteroidales llamadas andrógenos , estimula el desarrollo del sistema de Wolff. (Este tipo de hormonas también tiene un nombre muy apropiado: andros significa «hombre», y gennan , «producir».) Los andrógenos ejercen un efecto masculinizante .

Dos andrógenos diferentes son responsables de la masculinización.
El primero, la testosterona , es segregada por los testículos (y recibe su nombre de éstos).
Un enzima llamado 5- reductasa convierte a la testosterona en otro andrógeno conocido como dihidrotestosterona .

Como se recordará del capítulo 3, las hormonas ejercen sus efectos sobre las células diana estimulando receptores hormonales apropiados.
Así, el precursor de los órganos sexuales internos masculinos (el sistema de Wolff) contiene receptores para los andrógenos que están acoplados a los mecanismos celulares que promueven el crecimiento y la división.
Cuando las moléculas de testosterona se unen a estos receptores, provocan el desarrollo y crecimiento del epidídimo, el conducto deferente y la próstata.

En cambio, las células del sistema de Muller contienen receptores para la hormona inhibidora del sistema de Muller, los cuales de alguna manera impiden el crecimiento y la división.
Por tanto, la hormona inhibidora del sistema de Muller impide el desarrollo de los órganos sexuales internos femeninos.

Los experimentos con animales de laboratorio han demostrado que los ovarios no son necesarios para que se desarrolle el sistema de Muller.
Este hallazgo es responsable del dicho según el cual «el impulso de la naturaleza es crear hembras».

Una anomalía genética demuestra la validez de esta afirmación en humanos.
Las personas con el síndrome de Turner sólo tienen un cromosoma sexual: un cromosoma X (por tanto, en lugar de tener células XX, las tienen XO (O indica la ausencia de un cromosoma).
En la mayoría de los casos, el cromosoma X existente proviene de la madre, lo cual indica que la causa del trastorno radica en un espermatozoide defectuoso (Knebelmann y cols., 1991).
Debido a la ausencia de cromosoma Y, no se desarrollan los testículos.
Además, como son necesarios dos cromosomas X para producir ovarios, tampoco se desarrollan estas glándulas.
Aunque no tienen gónadas, las personas con el síndrome de Turner se transforman en mujeres, con órganos sexuales internos y genitales externos femeninos normales (lo cual demuestra que los fetos no necesitan ovarios para transformarse en femeninos).
Por supuesto, no pueden tener hijos, porque, al carecer de ovarios, no tienen la capacidad de producir óvulos.

Figura

Desarrollo de los órganos sexuales internos 

Los genitales externos son los órganos sexuales visibles e incluyen el pene y el escroto en varones, y los labios vulvares, el clítoris y la parte más externa de la vagina en mujeres.
Como acabamos de ver, los genitales externos no necesitan ser estimulados por hormonas sexuales femeninas para transformarse en femeninos; se desarrollan así de manera natural.
Sin embargo, el desarrollo masculino requiere la presencia de andrógenos (especialmente de dihidrotestosterona) (Josso y cols., 1991).
Por consiguiente, el sexo de los genitales externos de una persona está determinado por la presencia o ausencia de testículos (o, más exactamente, por la presencia o ausencia de los andrógenos segregados por éstos).
Este hecho explica por qué las personas con el síndrome de Turner tienen genitales externos femeninos a pesar de carecer de ovarios (véase la figura 10.3).

Figura

Desarrollo de los genitales externos 

La figura 10.4 resume los factores que controlan el desarrollo de las gónadas, los órganos sexuales internos y los genitales (véase la figura 10.4).

MADURACIÓN SEXUAL 

Los caracteres sexuales primarios incluyen las gónadas, los órganos sexuales internos y los genitales externos.

Estos órganos están presentes en el momento del nacimiento.
Los caracteres sexuales secundarios , como el crecimiento de los pechos y el ensanchamiento de las caderas, o la barba y la voz grave, no aparecen hasta la pubertad.
Sin ver sus genitales, podemos adivinar el sexo de un niño antes de la adolescencia por su corte de pelo o su manera de vestir; los cuerpos de los niños y las niñas son bastante similares.
Sin embargo, durante la pubertad las gónadas son estimuladas para producir sus hormonas, las cuales a su vez causan la maduración sexual de la persona.
El inicio de la pubertad tiene lugar cuando las células del hipotálamo segregan hormonas liberadoras de gonadotropinas (GnRH), que estimulan la producción y liberación de hormonas gonadotropas desde la adenohipófisis.

Las hormonas gonadotropas («dirigidas hacia las gónadas») estimulan a las gónadas para que produzcan sus hormonas, Das cuales son responsables, en último término, de la maduración sexual (véase la figura 10.5).

Figura

Control horizontal de la masculinización y de la de feminización de los órganos sexuales internos y de los genitales externos 

Las dos hormonas gonadotropas son la hormona folículo-estimulante (HFE) y la hormona luteinizante (HL), denominadas así por los efectos que producen en las hembras (producción de folículos ováricos y su posterior luteinización, efectos que se describirán en la siguiente sección de este capítulo).
Sin embargo, las mismas hormonas se producen también en los varones, en los que estimulan a los testículos para que produzcan espermatozoides y segreguen testosterona.
Si se intercambian las glándulas hipofisarias de un macho y de una hembra en ratas, los ovarios y los testículos responden perfectamente a las hormonas segregadas por la nueva glándula (Harris y Jacobsohn, 1951-1952).

En respuesta a las hormonas gonadotropas (generalmente llamadas gonadotropinas), las gónadas segregan hormonas esteroidales sexuales.
Los ovarios producen estradiol , una de las hormonas del tipo conocido como estrógenos .
Los testículos producen principalmente testosterona (un andrógeno).
Ambos tipos de glándulas producen también una pequeña cantidad de las hormonas del otro sexo.
Las hormonas gonadales tienen efectos sobre muchas partes del cuerpo.

Tanto el estradiol como la testosterona inician el cierre de las zonas de crecimiento de los huesos y, por tanto, detienen el crecimiento esquelético.
El estradiol también provoca el desarrollo del pecho, el crecimiento de la mucosa uterina, cambios en la deposición de grasa corporal y maduración de los genitales femeninos.
La testosterona estimula el crecimiento del vello facial, axilar (sobacos) y púbico; hace que la voz se vuelva grave; altera la línea capilar de la cabeza (a menudo causando calvicie en edades posteriores de la vida); estimula el desarrollo muscular, y ocasiona el crecimiento de los genitales.
Esta descripción excluye dos características secundarias femeninas: el pelo axilar y el púbico.
Estas características no se deben a los estrógenos sino a los andrógenos segregados por la corteza de las glándulas suprarrenales.
Incluso un varón castrado (al que le han extirpado los testículos) antes de la pubertad tendrá pelo en las axilas y el pubis, estimulado por sus propios andrógenos suprarrenales.
La tabla 10.1 presenta una lista de las principales hormonas sexuales y de algunos de sus efectos.
Estos efectos serán discutidos posteriormente en este capítulo (véase la tabla 10.1).

La bipotencialidad de muchos de los caracteres sexuales secundarios se mantiene a lo largo de la vida.
Si un hombre es tratado con un estrógeno (por ejemplo, para controlar un tumor dependiente de andrógenos), se le desarrollarán los pechos y su vello facial se volverá más fino y suave.
Sin embargo, su voz seguirá siendo grave, dado que el agrandamiento de su laringe es permanente.
Por el contrario, una mujer que tenga niveles elevados de andrógenos (generalmente debido a tumor que segregue androstenedión) desarrollará barba y su voz se volverá más grave.

Figura

Maduración sexual. La pubertad se inicia cuando el hipotálamo segrega hormonas liberadoras de gonadotropinas 

Tabla

Clasificación de las hormonas sexuales. 


Clase
- Andrógenos
- Estrógenos
- Gestágenos 
- Hormonas hipotalámicas 
- Gonadotropinas 
- Otras hormonas


Hormonas principales en humanos(lugar donde se producen)
- Testosterona (testículos)
- Dihidrotestosterona (producida a partir de la testosterona por de la 5-a-reductasa) 
- Androstenedión (glándula suprarrenal) 
- Estradiol (ovarios)
- Progesterona (ovarios)
- Hormona liberadora de gonadotopinas (hipotálamo) 
- Hormona folículo-estimulante(adenohipófisis)
- Hormona luteinizante(adenohipófisis)
- Prolactina (neurohipófisis)
- Oxitocina (neurohipófisis)


Algunos efectos
- Desarrollo del sistema de Wolff; producción de espermatozoides, crecimiento del vello facial, púbico y axilar; desarrollo muscular; agrandamiento de la laringe; inhibición del crecimiento óseo; impulso sexual en hombres (¿y en mujeres?) 
- Maduración de los genitales externos masculinos 
- En mujeres, crecimiento del vello púbico y facial; menos importantes que la testosterona en varones 
- Maduración de los genitales femeninos; crecimiento de los senos; alteraciones de los depósitos adiposos; crecimiento de la mucosa uterina; inhibición del crecimiento óseo; impulso sexual en mujeres (?) 
- Mantenimiento de la mucosa uterina
- Secreción de gonadotropinas
- Desarrollo de los folículos ováricos
- Ovulación; desarrollo del cuerpo lúteo
- Producción de leche; período refractario masculino (?) 
- Excreción de leche; período refractario masculino (?);
- vinculación social (?) 

Resumen intermedio 

El sexo está determinado por los cromosomas sexuales: el conjunto XX produce una hembra, y el XY produce un varón.

Los varones son resultado de la acción del gen SRY del cromosoma Y que contiene el código para la producción de la proteína determinante de los testículos, responsable de que las gónadas primitivas se transformen en testículos.
Los testículos segregan dos tipos de hormonas que ocasionan el desarrollo de un varón.

La testosterona (un andrógeno) estimula el desarrollo del sistema de Wolff (masculinización), y la hormona inhibidora del sistema de Muller suprime el desarrollo del sistema de Muller (desfeminización).
Los genitales externos se desarrollan a partir de precursores comunes.
En ausencia de hormonas gonadales, los precursores se transforman en la forma femenina; en presencia de andrógenos (principalmente la dihidrotestosterona, que deriva de la testosterona por acción de la S-a-reductasa), se transforman en la forma masculina (masculinización).
Por defecto, el cuerpo es femenino («El impulso de la naturaleza...»); sólo por acción de las hormonas testiculares se convierte en masculino.
La masculinización y la desfeminización se denominan efectos organizadores de las hormonas; los efectos activadores tienen lugar cuando el desarrollo es completo.
Una persona con el síndrome de Turner (XO) no desarrolla gónadas, pero sí órganos sexuales internos y genitales externos femeninos.

La madurez sexual tiene lugar cuando el hipotálamo empieza a segregar la hormona liberadora de gonadotropinas, la cual estimula la secreción de las hormonas folículo-estimulante y luteinizante desde la adenohipófisis.
Estas últimas estimulan las gónadas para que segreguen sus propias hormonas, las cuales causan la maduración de los genitales y el desarrollo de los caracteres sexuales secundarios (efectos activadores).

Control hormonal de la conducta sexual 

Hemos visto que las hormonas son responsables del dimorfismo sexual (diferencias entre machos y hembras) en la estructura del cuerpo y sus órganos.
Las hormonas ejercen efectos organizadores y activadores sobre los órganos sexuales internos, los genitales y los caracteres sexuales secundarios.
Naturalmente, todos estos efectos influyen en la conducta de una persona.
El simple hecho de tener el físico y los genitales de un hombre o de una mujer ejerce un poderoso efecto.
Pero las hormonas no sólo nos dan un cuerpo masculino o femenino; también actúan sobre la conducta mediante su interacción directa con el sistema nervioso.
Los andrógenos presentes durante el desarrollo prenatal afectan el desarrollo del sistema nervioso.

Además, tanto las hormonas sexuales masculinas como las femeninas ejercen efectos activadores sobre el sistema nervioso adulto, influyendo en los procesos fisiológicos y en la conducta.
Esta sección hace referencia a algunos de estos efectos.

CONTROL HORMONAL DE LOS CICLOS REPRODUCTIVOS FEMENINOS 

El ciclo reproductivo de las hembras primates se denomina ciclo menstrual (de mensis , que significa «mes»).
Las hembras de otras especies de mamíferos también tienen ciclos reproductivos, denominados ciclos de estro .
Estro significa «tábano»: cuando una rata hembra está en estro, su estado hormonal la hace actuar de modo distinto a otras ocasiones (y, a su vez, hace que los machos actúen también de manera diferente).
La característica principal que distingue los ciclos menstruales de los ciclos estrales es el crecimiento y la pérdida mensuales de la mucosa uterina.
Las otras características son aproximadamente las mismas (excepto por el hecho de que el ciclo de estro de las ratas dura cuatro días).

Los ciclos menstruales y los de estro consisten en una secuencia de acontecimientos controlados por las secreciones hormonales de la adenohipófisis y los ovarios.
Estas glándulas interactúan entre sí, de manera que las secreciones de una afectan a las de la otra.
El ciclo se inicia con la secreción de gonadotropinas desde la adenohipófisis.
Estas hormonas (especialmente la HFE) estimulan el crecimiento de los folículos ováricos , pequeñas esferas de células epiteliales que rodean a cada óvulo.
Las mujeres producen normalmente un folículo ovárico cada mes; si se producen dos y éstos son fertilizados, se desarrollarán gemelos dizigóticos (fraternos).
A medida que los folículos ováricos maduran. segregan estradiol, que causa el crecimiento de la mucosa uterina en preparación para la implantación del óvulo, en caso de ser fertilizado por un espermatozoide.
El feedback , debido a los niveles crecientes de estradiol, acaba provocando la liberación masiva de HL desde la adenohipófisis (véase la figura 10.6).

Figura

Control neuroendocrino del ciclo menstrual 

El aumento de HL provoca la ovulación el folículo ovárico se rompe, liberando al óvulo.
Bajo la influencia continuada de la HL el folículo ovárico abierto se convierte en el cuerpo lúteo («cuerpo amarillo»), que produce estradiol y progesterona (véase la figura 10.6).
Esta última hormona promueve el embarazo ( gestación ).
Mantiene asimismo la pared uterina e inhibe la producción de otros tolículos ováricos.
Mientras tanto, el óvulo se introduce en una de las trompas de Falopio y empieza su avance hacia el útero.
Si se topa con espermatozoides en su camino por la trompa de Falopio y es fertilizado, empieza a dividirse y varios días más tarde se fija a la pared uterina.
Si el óvulo no es fertilizado, o es fertilizado demasiado tarde como para desarrollarse lo suficiente en el momento en que llega al útero, el cuerpo lúteo deja de producir estradiol y progesterona, y la mucosa de la pared uterina se desprende.
Se inicia la menstruación.

CONTROL HORMONAL DE LA CONDUCTA SEXUAL DE LOS ANIMALES DE LABORATORIO 

Resulta difícil estudiar la interacción entre las hormonas sexuales y el desarrollo del cerebro humano.
Hemos de dirigir nuestra atención hacia dos fuentes de información: los experimentos con animales y varios trastornos del desarrollo ovárico en humanos, que actúan como «experimentos» de la propia naturaleza.
Veamos primero las pruebas obtenidas a partir de la investigación con animales de laboratorio.

Machos 

La conducta sexual masculina es muy variada, aunque las características esenciales de la penetración (introducción del pene en la vagina de la hembra), las sacudidas pélvicas (movimientos rítmicos de las patas traseras, causantes de la fricción de los genitales) y la eyaculación (emisión de semen) son típicas de todos los mamíferos macho.
Los humanos, por supuesto, han inventado todo tipo de conducta sexual copulatoria y no copulatoria.
Por ejemplo, los movimientos pélvicos que provocan la eyaculación pueden ser realizados por la mujer, y el juego sexual puede provocar el orgasmo sin que se llegue a la penetración.

La conducta sexual de las ratas ha sido más estudiada que la de cualquier otro animal de laboratorio.
Cuando una rata macho encuentra a una hembra receptiva, se pasa un tiempo acariciándola con el hocico, y oliendo y lamiendo sus genitales, para luego montarla realizando movimientos pélvicos.
La monta varias veces, logrando la penetración en la mayoría de las ocasiones.

Después de ocho a quince penetraciones, separadas aproximadamente por un minuto (cada una dura alrededor de un cuarto de segundo), eyacula.

Después de la eyaculación, el macho deja de mantener actividad sexual durante un período de tiempo (minutos en la rata).
La mayoría de los mamíferos vuelven a copular una y otra vez, mostrando una pausa más prolongada, denominada período refractario , después de cada eyaculación (este término proviene del latín refringere , «interrumpir»).
En algunos mamíferos ocurre un fenómeno interesante.

Si a un macho «exhausto» debido a la repetida copulación con la misma hembra, se le presenta una nueva hembra, empieza a responder con rapidez (a menudo, tan rápido como en su contacto inicial con la primera hembra).

La sucesiva presentación de nuevas hembras puede mantener su actividad durante períodos prolongados de tiempo.
Es indudable que este fenómeno resulta importante en las especies en las que un único macho insemina a todos los miembros de su harén.
En las especies en las que existe aproximadamente el mismo número de machos que de hembras activos para la reproducción, las actuaciones de este tipo son menos probables.

El fenómeno que acabamos de describir, que también se observa en los gallos, suele llamarse efecto Coolidge .
Se dice que la siguiente historia es verídica, pero no podríamos jurarlo.

(Si no lo es, debería serlo.)

El que fue presidente de Estados Unidos, Calvin Coolidge, y su esposa estaban visitando una granja.
La señora Coolidge preguntó al granjero si la continua y vigorosa actividad sexual del gallinero era obra de un único gallo.
La respuesta fue que sí.

«Debería explicárselo al señor Coolidge», dijo ella.
Luego el presidente le preguntó al granjero si se trataba de una gallina diferente cada vez.
La respuesta fue nuevamente afirmativa.

«Debería decírselo a la señora Coolidge», dijo él.

La conducta sexual de los roedores macho depende de la testosterona, hecho reconocido desde hace tiempo (Bermant y Davidson, 1974).
Si una rata macho es castrada (es decir, se le extirpan los testículos), su actividad sexual acaba por desaparecer.
Sin embargo, la conducta puede restaurarse mediante inyecciones de testosterona.
Más adelante en este mismo capítulo describiremos la base neural de este efecto activador.

Como hemos visto con anterioridad, la testosterona puede convertirse, por acción de la S-o^-reductasa, en dihidrotestosterona, que es responsable de algunos de los efectos masculinizantes que tienen lugar durante el desarrollo prenatal.

La testosterona puede convertirse también en otra hormona: el estradiol.
Este proceso, denominado aromatización , tiene lugar por acción de un enzima denominado aromatasa . (En química, se denominan compuestos aromáticos a los que contienen un determinado anillo de seis carbonos).
Muchas células cerebrales contienen aromatasas; cuando las moléculas de testosterona se introducen en ellas, se convierten en estradiol.
Las moléculas de estradiol se dirigen hacia el núcleo, se unen a receptores para los estrógenos y ponen en marcha sus efectos fisiológicos (véase la figura 10.7).

Al menos algunos de los efectos activadores de la testosterona sobre la conducta sexual masculina se producen por acción de la testosterona aromatizada (es decir, del estradiol) sobre los receptores estrogénicos Bonsall, Clancy y Michael (1992) descubrieron que una droga que bloquea a la aromatasa (el enzima que convierte a la testosterona en estrógeno) reducía (aunque no abolía) la conducta sexual de las ratas macho adultas.

Hay otras hormonas que intervienen en la conducta sexual masculina.
La oxitocina , una hormona producida por la neurohipófisis, contrae los conductos lácteos (induciendo la eyección de leche en hembras lactantes).
También se produce en los machos, en los cuales, obviamente, no interviene en la lactancia.
La oxitocina se libera durante el orgasmo tanto en machos como en hembras, y parece contribuir a las contracciones de la musculatura lisa del sistema eyaculador masculino y de la vagina y el útero (Carter, 1992c).
Los efectos de esta liberación hormonal pueden verse fácilmente en mujeres que están lactando, las cuales a menudo excretan algo de leche durante el orgasmo.
Dado que esta hormona se segrega al final de la copulación, algunos investigadores creen que interviene en la saciedad sexual y, quizás en algunas especies, en la vinculación social entre los miembros de las parejas sexuales (Carter, 1992b).
Como veremos más adelante, la oxitocina también funciona como neurotransmisor o neuromodulador en el encéfalo, donde podría ejercer efectos facilitadores sobre la conducta sexual.

El período refractario que tiene lugar después de la eyaculación podría deberse también a la acción de otra hormona, quizás en conjunción con la oxitocina.
La prolactina , una hormona segregada por la adenohipófisis, estimula la producción de leche por las glándulas mamarias.

Como la oxitocina, la prolactina también se libera en ratas macho después de la eyaculación (Oaknin y cols., 1989).
Además, la prolactina ejerce efectos inhibitorios sobre la conducta sexual masculina.
Por ejemplo, Doherty, Baum y Todd (1986) trasplantaron una glándula hipofisaria a ratas macho, provocando un dramático incremento de los niveles sanguíneos de prolactina.
Aunque la hormona no tuvo ningún efecto en la capacidad de los animales para lograr la erección del pene, produjo una importante disminución de las conductas de monta y de penetración.
Es posible, pues, que tanto la oxitocina como la prolactina que se segregan durante la eyaculación inhiban la posterior actividad sexual durante el período refractario.

Figura

La aromatización. En algunas células los efectos de la testosterona son realizados por un estrógeno. La testosterona es aromatizada en estradiol, el cual activa a receptores estrogénicos del núcleo 

Hembras 

Es habitual describir a las hembras de los mamíferos como participantes pasivos en la copulación.
Es cierto que en muchas especies el papel de la hembra durante la monta y la introducción es meramente el de asumir una postura de exposición de sus genitales al macho.
Esta conducta se denomina lordosis (del griego lordos , que significa «doblado hacia atrás»).
La hembra también suele apartar la cola (si tiene) y mantenerse lo suficientemente rígida para soportar el peso del macho.

Sin embargo, la conducta de una hembra de laboratorio en el inicio de la copulación suele ser muy activa.
Ciertamente, si se intenta la copulación con un roedor hembra que no esté en estro, ésta huirá activamente o rechazará al macho.
Pero si se halla en un estado receptivo, suele acercársele, acariciarle con el hocico, oler sus genitales y mostrar conductas características de su especie.
Por ejemplo, una rata hembra exhibe saltos rápidos y de corta duración, así como movimientos rápidos de las orejas, que los machos encuentran irresistibles (McClintock y Adler, 1978).

La conducta sexual de los roedores hembra depende de las hormonas gonadales presentes durante el estro: estradiol y progesterona.

En las ratas, el estradiol aumenta unas 40 horas antes de que la hembra se vuelva receptiva; y justo antes del inicio de la receptividad, el cuerpo lúteo empieza a segregar grandes cantidades de progesterona (Feder, 1981).
Las ratas ovariectomizadas (ratas a las que se les ha extirpado los ovarios) no son sexualmente receptivas.
Aunque la receptividad sexual puede darse en roedores ovariectomizados mediante la administración únicamente de elevadas dosis de estradiol, el tratamiento más eficaz es el que mimetiza la secuencia normal de hormonas: una pequeña cantidad de estradiol seguida por progesterona.
La progesterona sola resulta ineficaz; por tanto, el estradiol hace posible su eficacia.
El efecto permisivo del estradiol requiere unas 16-24 horas, después de las cuales la inyección de progesterona produce conductas receptivas durante una hora (Takahashi, 1990).

Los mecanismos neurales responsables de estos efectos se describirán más adelante.

La secuencia de estradiol seguida por progesterona tiene tres efectos en las ratas hembra: aumenta su receptividad, su proceptividad y su atractivo.
La receptividad se refiere a su capacidad y disposición a copular (a aceptar los avances de un macho manteniéndose quieta y mostrando lordosis cuando éste intenta montarla).
La proceptividad se refiere al ansia de copular, demostrada por el hecho de buscar un macho y llevar a cabo conductas que tienden a excitar el interés sexual de éste.
El atractivo se refiere a los cambios fisiológicos y conductuales que afectan al macho.

La rata macho (al igual que mucho otros mamíferos macho) muestra su mayor grado de respuesta con las hembras que están en estro (en celo).
Los machos ignoran a las hembras cuyos ovarios han sido extirpados, pero las inyecciones de estradiol y progesterona restauran el atractivo sexual de éstas (y también cambian su conducta hacia el macho).
Los estímulos que excitan el interés sexual de una rata macho incluyen el olor y la conducta de la hembra.
En algunas especies hay también cambios visibles, tales como el entumecimiento de la piel de la región genital en el mono hembra, que afectan a su capacidad de atracción sexual.

EFECTOS ORGANIZADORES DE LOS ANDRóGENOS SOBRE LA CONDUCTA: MASCULINIZACIÓN Y DESFEMINIZACiÓN 

El dicho "El impulso de la naturaleza es crear hembras» se aplica tanto a la conducta sexual como a los órganos sexuales.
Así, si el cerebro de un roedor no es expuesto a andrógenos durante un período crítico del desarrollo, el animal presentará conductas sexuales femeninas de adulto (si se le administra estradiol y progesterona).
Por suerte para los experimentadores, este período crítico tiene lugar poco después del nacimiento en las ratas y en varias otras especies de roedores, que nacen en una condición bastante inmadura.
Así, si una rata macho es castrada inmediatamente después del nacimiento, se le permite llegar a la vida adulta, y luego se le administran inyecciones de estradiol y progesterona, responderá a la presencia de otro macho arqueando su espalda y presentando los cuartos traseros.
En otras palabras, actuará como si fuera una hembra (Blaustein y Olster, 1989).

En cambio, si el cerebro de un roedor es expuesto a andrógenos durante su desarrollo, ocurren dos fenómenos: desfeminización conductual y masculinización conductual.
La desfeminización conductual se refiere a los efectos organizadores de los andrógenos que impiden que el animal muestre conducta sexual femenina de adulto.
Como veremos más adelante este efecto se logra suprimiendo el desarrollo de circuitos neurales que controlan la conducta sexual femenina.
Por ejemplo, si una rata hembra es ovariectomizada, y se le administra una inyección de testosterona inmediatamente después del nacimiento, no responderá a las ratas macho si, de adulta, recibe inyecciones de estradiol y progesterona.

La masculinización conductual se refiere al efecto organizador de los andrógenos que permite a los animales mostrar una conducta sexual masculina de adultos.
Este efecto se logra estimulando el desarrollo de los circuitos neurales que controlan la conducta sexual masculina.
Por ejemplo, si el roedor hembra de nuestro anterior ejemplo recibe testosterona de adulta, en lugar de estradiol y progesterona, realizará conductas de monta e intentará copular con una hembra receptiva (véase Breedlove, 1992, y Carter, 1992b, para referencias a estudios específicos) (véase la figura 10.8).

Figura

Efectos organizadores de la testosterona. Durante el período perinatal, esta hormona masculiniza y desfeminiza la conducta sexual de los roedores

Por supuesto, los dos efectos organizadores de los andrógenos sobre el cerebro (masculinización y desfeminización conductuales) son estimulados por la testosterona, pero algunos de estos efectos involucran a receptores para el estradiol.
Recordemos que el enzima aromatasa, presente en algunas células, convierte a la testosterona en estradiol, el cual estimula a receptores estrogénicos localizados en el núcleo.
Parece ser que la desfeminización conductual se debe en gran parte a la estimulación de receptores estrogénicos, mientras que algunos aspectos de la masculinización conductual se consiguen a través de los efectos directos de la testosterona sobre receptores androgénicos, y otros a través de los efectos indirectos de la testosterona aromatizada sobre receptores estrogénicos (véase Brand y cols., 1991, para referencias específicas).

El lector puede preguntarse por qué no todos los fetos son desfeminizados y al menos parcialmente androgenizados; al fin y al cabo, los fetos son expuestos al estradiol de su madre.
La respuesta a esta pregunta resulta interesante.
La sangre fetal contiene una sustancia llamada a-feto-proteína , que se une al estradiol y lo inactiva (Breedlove,1992).
Esto significa que el único estradiol que ejerce efectos organizadores es el que se produce dentro de la célula por acción de la aromatasa sobre las moléculas de testosterona.

Las investigaciones llevadas a cabo por Moore y sus colaboradores (revisadas por Moore, 1987) indican que algunos de los efectos masculinizadores y desfeminizadores de los andrógenos son indirectos.
Como veremos en una sección posterior, las ratas hembra dedican una considerable cantidad de tiempo a lamer la región genital de sus crías.
Esta conducta es muy útil, porque estimula la urinación y permite que la madre ingiera el agua y los minerales liberados, de tal manera que éstos puedan ser reciclados en su leche.

Moore y sus colaboradores descubrieron que las madres pasan mucho más tiempo lamiendo a sus crías macho, y se preguntaron si esta conducta podría tener efectos sobre la posterior conducta sexual de esas crías.
De hecho, así es.
En primer lugar, los experimentadores hallaron que los andrógenos eran responsables de la presencia de un olor en la orina de las crías macho que resultaba atractivo para sus madres.
Luego hallaron que si destruían la capacidad de las madres para detectar este olor, éstas dejaban de prestar una atención especial a las crías macho (y éstas últimas mostraban una disminución de su conducta sexual cuando eran adultos).

Pero si los experimentadores cepillaban cada día los genitales de las crías macho con un pequeño pincel, estos animales mostraban una conducta sexual normal cuando crecían.
Por tanto, al menos algunos de los efectos organizadores de los andrógenos actúan a través de un intermediario, la madre.

EFECTOS DE LAS FEROMONAS 

Las hormonas transmiten mensajes de una parte del cuerpo (la glándula secretora) a otra (el tejido diana).
Otra clase de sustancias químicas, llamadas feromonas , transmiten mensajes de un animal a otro.

Estas sustancias, al igual que las hormonas, afectan a la conducta reproductora.
Karl son y Luscher (1959) acuñaron este término, a partir de pherein , «transportar», y horman , «excitar».
Las feromonas son liberadas por un animal y afectan a la fisiología o a la conducta de otro.
La mayoría de las feromonas se detectan mediante el olfato, pero algunas son ingeridas o absorbidas por la piel.

Las feromonas pueden afectar a la fisiología o a la conducta reproductora.
Consideremos primero sus efectos sobre la fisiología de la reproducción.
Cuando los ratones hembra viven en grupo, su ciclo estrogénico se lentifica y acaba por detenerse.
Este fenómeno se llama efecto Lee-Boot (van der Lee y Boot, 1955).
Si un grupo de hembras es expuesto al olor de un macho (o a su orina), los animales reinician sus ciclos de estro, y estos ciclos tienden a sincronizarse.
Este fenómeno se conoce como efecto Whiffen (Whitten, l959).
El efecto Vandenbergh (Vandenbergh, Whitsett y Lombardi, 1975) consiste en la aceleración del inicio de la pubertad en un roedor hembra causada por el olor de un macho.

12 Sed e ingesta de líquidos 

Como dijo el fisiólogo francés Claude Bernard (1813-1878), «La constancia del medio interno es una condición necesaria para una vida libre».
Esta conocida cita dice sucintamente qué es lo que los organismos tienen que hacer para ser capaces de sobrevivir en los entornos hostiles a las células vivas que los componen (es decir, vivir una «vida libre»): tienen que regular la naturaleza del fluido interno que baña sus células.

Las características fisiológicas de las células que constituyen nuestros cuerpos evolucionaron hace mucho tiempo, cuando estas células flotaban aún libremente en el océano.
En esencia, lo que ha logrado el proceso evolutivo ha sido la capacidad para producir nuestra propia agua marina que rodea o baña nuestras células, la capacidad para añadir a esta agua marina el oxígeno y los nutrientes que esas células necesitan y para retirar de la misma los desperdicios que, de alguna forma, podrían envenenarlas.
Para desempeñar estas funciones, disponemos de los sistemas digestivo, respiratorio, circulatorio y excretor.
También contamos con las conductas necesarias para hallar e ingerir los alimentos y el agua.

La regulación del fluido que baña nuestras células forma parte de un proceso llamado homeostasis («situación similar»).

Este capítulo trata de los medios a través de los cuales nosotros, los mamíferos, logramos el control homeostático de las características vitales de nuestro fluido extracelular a través de nuestra conducta de ingesta : la ingesta de alimento, de agua y de minerales, tales como el sodio.
Empezaremos por examinar la naturaleza general de los mecanismos reguladores, para luego pasar a considerar nuestras conductas de beber y comer.

La naturaleza de los mecanismos reguladores fisiológicos 

Un mecanismo regulador fisiológico es aquel que mantiene la constancia de algunas características internas del organismo frente a la variabilidad externa, por ejemplo, el mantenimiento de una temperatura corporal constante a pesar de los cambios en la temperatura ambiente.
Un mecanismo regulador tiene cuatro características esenciales: la variable sistema (la característica a ser regulada); un valor fijo establecido (el valor óptimo de la variable sistema); un detector , que controla el valor de la variable sistema; y finalmente un mecanismo corrector , que restaura la variable sistema al valor fijo establecido.

Un ejemplo de un sistema regulador es el de una habitación cuya temperatura sea regulada por un calentador controlado a través de un termostato.
La variable sistema es la temperatura del aire de la habitación y el detector de esta variable es el termostato.
Este dispositivo puede ajustarse de modo que los contactos de un interruptor se cierren cuando la temperatura esté por debajo de un valor preestablecido (el valor fijo establecido).
El cierre de los contactos pone en marcha el mecanismo corrector, las bobinas del calentador (véase la figura 12.1).

Si la habitación se enfría por debajo del valor fijo establecido en el termostato, éste pone en marcha el calentador, el cual caldeará otra vez la habitación.
El aumento de la temperatura de la habitación hará que el termostato apague el calentador.
Este proceso se denomina feedback negativo debido a que la actividad del mecanismo corrector (la producción de calor) retroalimenta al termostato y hace que éste apague el calentador.
El feedback negativo es una característica esencial de todos los sistemas reguladores.

En este capítulo y en el 13 se tratan los sistemas reguladores implicados en las conductas de ingesta: beber y comer.
Estas conductas constituyen mecanismos correctores que restauran los niveles de agua o de nutrientes almacenados en el cuerpo cuando éstos disminuyen.
Debido a la demora entre la ingestión y la recuperación de los niveles normales de los almacenes, las conductas de ingesta están controladas por mecanismos de saciedad , así como por detectores que controlan las variables sistema.
Los mecanismos de saciedad son necesarios a causa de la fisiología de nuestro sistema digestivo.
Por ejemplo, supongamos que realizamos ejercicio físico en un ambiente cálido y seco, perdiendo agua corporal.
Esta pérdida de agua hace que los detectores internos pongan en marcha el mecanismo corrector, la conducta de beber.
Entonces bebemos rápidamente uno o dos vasos de agua y dejamos de beber.
¿Qué es lo que detiene nuestra conducta de ingesta? El agua está aún en nuestro sistema digestivo, todavía no ha podido llegar al fluido circundante de las células, donde se necesita.
Por lo tanto, aunque la conducta de beber se inició por detectores que evalúan la necesidad de agua de nuestro cuerpo, se detuvo por otros medios .

Figura

Ejemplo de un sistema regulador 

Debe existir un mecanismo de saciedad que nos diga, en efecto: «¡deja ya de beber!; esta agua cuando sea absorbida desde el sistema digestivo a la sangre será suficiente para saciar la necesidad de tu cuerpo.» Los mecanismos de saciedad controlan la actividad del mecanismo corrector (en este caso, el beber), pero no las propias variables del sistema.

Cuando bebemos una cantidad suficiente, los mecanismos de saciedad provocan el cese de esta conducta, anticipando el restablecimiento del equilibrio que se producirá posteriormente (véase la figura 12.2).

Figura

Esquema del sistema que controla la conducta de beber. Algunos hechos sobre el equilibrio de fluidos 

Antes de que podamos comprender cómo se lleva a cabo el control fisiológico de la conducta de beber, debemos conocer algo acerca de los compartimentos de fluidos del cuerpo y de las relaciones que se establecen entre ellos.
Además, para entenderlo mejor, debemos conocer también algo acerca de las funciones del riñón.

Los COMPARTIMENTOS DE FLUIDOS DEL CUERPO 

El cuerpo tiene cuatro principales compartimentos de fluidos: uno intracelular y tres extracelulares.
Aproximadamente dos tercios del agua corporal están contenidos en el fluido intracelular , que es la porción fluida del citoplasma de las células.
El resto es fluido extracelular , que incluye el fluido intravascular (el plasma sanguíneo), el líquido cefalorraquídeo y el fluido intersticial .
El término intersticial significa «estar entre»; de hecho, el fluido intersticial es el que está entre nuestras células (es como el «agua marina» que las baña).
Teniendo en cuenta el objetivo de este capítulo, no haremos referencia al líquido cefalorraquídeo, centrándonos en los otros tres compartimentos (véase la figura 12.3).

Figura

Tamaño relativo de los compartimentos de fluidos corporales 

Los compartimentos de fluidos están separados por barreras semipermeables, que permiten el paso de algunas sustancias pero no de otras.
Las paredes de los capilares separan el fluido intravascular (plasma sanguíneo) del fluido intersticial, mientras que las membranas celulares separan el fluido intersticial del fluido intracelular.
El volumen del fluido intracelular está controlado por la concentración de soluto en el fluido intersticial (el término soluto hace referencia a las sustancias sólidas disueltas en una solución).
Normalmente, el fluido intersticial es isotónico (de isos , «igual», y tonos , «tensión») respecto al fluido intracelular.
Es decir, la concentración de las sustancias disueltas en las células y en el fluido intersticial que las baña está equilibrada, de modo que el agua no tiende ni a salir ni a entrar en las mismas.
Si el fluido intersticial sufre pérdidas de agua (se vuelve más concentrado, o hipertónico ), el agua entonces tendrá tendencia a salir hacia el exterior de las células.
En cambio, si aumenta la cantidad de agua en el fluido intersticial (es decir, si se vuelve más diluido, o hipotónico ), el agua entrará en las células.
Cualquiera de estas condiciones daña a las células; una pérdida de agua las priva de su capacidad para llevar a cabo muchas reacciones químicas, y una ganancia de la misma puede provocar la ruptura de sus membranas.
Así pues, la concentración del fluido intersticial tiene que estar regulada de una forma muy precisa.

El volumen del plasma sanguíneo tiene también que regularse estrechamente debido a los mecanismos de funcionamiento del corazón.
Si el volumen de sangre aumenta demasiado, la presión sanguínea puede alcanzar niveles peligrosamente altos.
Por el contrario, si el volumen de sangre disminuye demasiado, el corazón no puede seguir bombeándola de forma efectiva; y si el volumen no se restaura, se producirá una insuficiencia cardíaca.

A esta condición se la denomina hipovolemia, literalmente, «bajo volumen sanguíneo» ( -emia procede de la palabra griega haima , «sangre»).
El sistema vascular del cuerpo puede realizar algunos ajustes ante la pérdida de volumen sanguíneo, como contraer los músculos de las paredes de las venas y las arterias más pequeñas, reduciendo, de este modo, el espacio por donde ha de circular la sangre; pero este mecanismo corrector tiene unos límites definidos.

El volumen de fluido intersticial no necesita regularse de forma tan precisa.
La razón más importante de ello es que, cuando los volúmenes de los fluidos de los compartimentos intracelular e intravascular se mantienen dentro de los límites normales, el volumen de fluido extracelular automáticamente también lo hace.
Únicamente en el caso de ciertas condiciones patológicas -como lesión capilar, insuficiencia cardíaca o niveles muy bajos de proteínas en sangre- el volumen del fluido intersticial se vuelve anormalmente alto. (No hay modo de que sus niveles se vuelvan anormalmente bajos sin que los otros compartimentos de fluidos sufran también alteraciones.) Sin embargo, aunque el volumen del fluido intersticial normalmente no sea un asunto preocupante, su tonicidad (la concentración de soluto) tiene que ser regulada con precisión, ya que esta variable es la que determina si el agua entra o sale de las células.

Como veremos, el fluido intracelular y el volumen sanguíneo están controlados por dos grupos diferentes de receptores.
Un único grupo de ellos podría no ser suficiente, ya que es posible que uno de estos compartimentos de fluidos sufra cambios sin afectar al otro.
Por ejemplo, una pérdida de sangre (obviamente) reduce el volumen del fluido intravascular, pero no tiene efectos sobre el volumen del fluido intracelular.
Por otra parte, la ingesta de un alimento salado aumentará la concentración de soluto del fluido intersticial, provocando la salida de agua de las células. 
Éstas permanecerán deshidratadas incluso después de que el riñón haya conseguido deshacerse del exceso de sodio (y una cantidad de agua para eliminarlo) y haya restaurado el volumen normal de sangre.
De esta forma, el cuerpo necesita un grupo de receptores para medir el volumen sanguíneo y otro para medir el volumen celular.

Del mismo modo que hay dos grupos de receptores, existen dos grupos de mecanismos correctores.
Uno involucra la ingesta y la excreción de agua, y el otro está relacionado con la ingesta y la excreción de sodio.
Obviamente, los riñones son los responsables de excretar tanto el agua como el sodio, mientras que la ingesta de estas sustancias se realiza mediante la conducta de ingerir sal (o alimentos que la contengan) y de beber agua.

En general, bebemos más agua de la que nuestro cuerpo necesita, eliminando el exceso gracias a la acción de los riñones.
De forma parecida, ingerimos más sodio del que necesitamos, siendo también los riñones los responsables de eliminar los excedentes.
De esta forma, si queremos comprender los mecanismos mediante los cuales regulamos el equilibrio hídrico y del sodio, antes tenemos que comprender la función de los riñones y el modo como están controlados.

Los RIÑONES 

Un riñón humano consiste en aproximadamente un millón de unidades funcionales llamadas nefronas .
Cada nefrona extrae fluido de la sangre y lo transporta a través de conductos colectores al uréter .
El uréter, a su vez, conecta el riñón con la vejiga urinaria, donde la orina se almacena hasta que puede ser liberada en el momento conveniente.

(Esta conducta, que los fisiólogos educadamente denominan micción y que deriva de la palabra latina urinate , no tiene absolutamente nada que ver con regulación.
Una vez que la orina está en la vejiga, se encuentra fuera del cuerpo por lo que a los tres compartimentos de fluidos se refiere) (véase la figura 12.4).

Figura

Anatomía del riñón y de una nefrona individual 

Los riñones controlan la cantidad de agua y de sodio que el cuerpo excreta, lo que a su vez determina tanto el volumen como la concentración (tonicidad) del fluido extracelular.

Si bebemos una gran cantidad de agua y tenemos que eliminar el exceso, nuestros riñones envían una gran cantidad de orina a la vejiga.
De forma parecida, si comemos alimentos salados y también tenemos que eliminar el exceso de sodio, los riñones extraen el sodio de nuestra sangre y lo envían a la vejiga.
Sin embargo, si el organismo ha perdido agua a través de la evaporación, los riñones la conservan, produciendo una cantidad pequeña de orina.
Además, si el cuerpo presenta una deficiencia de sodio, los riñones comenzarán a excretar cantidades muy pequeñas de este mineral en la orina.
Las cantidades de sodio y de agua que los riñones excretan se controlan a través de dos hormonas: la aldosterona y la vasopresina.
La excreción de sodio está controlada por la aldosterona , una hormona esteroide secretada por la corteza suprarenal.

Los niveles altos de aldosterona hacen que los riñones retengan sodio en el cuerpo, es decir, ocasionan el paso de cantidades muy pequeñas de sodio a la vejiga.
De esta forma, si el cuerpo contiene demasiado sodio, el nivel de secreción de aldosterona disminuye y el sodio es excretado en la orina.
En el caso de que haya niveles muy bajos de sal, un nivel elevado de aldosterona hará que la sal se conserve (véase la figura 12.5).

La excreción de agua por los riñones se controla mediante una hormona llamada vasopresina .

El nombre de esta hormona no describe su función principal con respecto al control homeostático de los fluidos, ya que ésta es la de instruir a los riñones sobre la cantidad de agua a retener.
Los niveles altos de vasopresina tendrán como consecuencia que los riñones retengan tanta agua como sea posible, excretando únicamente las cantidades necesarias para que el cuerpo pueda deshacerse de los productos de desecho del metabolismo.
El término vasopresina hace referencia a su capacidad, bajo determinadas circunstancias, para provocar la contracción de los vasos sanguíneos.
Un término mucho más adecuado es el de hormona antidiurética (la palabra diuresis proviene de los términos griegos dia , «a través», y ouron , «orina»; así la hormona antidiurética reduce la producción de orina).
No obstante, el nombre de vasopresina es el que los fisiólogos utilizan con más frecuencia.

La vasopresina es una hormona peptídica secretada por la neurohipófisis.
Se produce en los cuerpos celulares de neuronas localizadas en dos núcleos del hipotálamo: el núcleo supraóptico y el núcleo paraventricular .
La hormona se almacena en vesículas y viaja a lo largo de los axones hacia la neurohipófisis, donde se concentra en los botones terminales.
Cuando las neuronas de los núcleos supraóptico y paraventricular se activan, sus botones terminales liberan vasopresina, que entra en el torrente sanguíneo.
De este modo, la producción, el almacenamiento y la liberación de vasopresina son exactamente iguales a los de cualquier otra sustancia transmisora peptídica, excepto en que la vasopresina afecta a receptores situados en otra parte del cuerpo, y no a receptores localizados en una membrana tras un espacio sináptico.

Si bebemos más agua de la que nuestro cuerpo necesita, la neurohipófisis reducirá su secreción de vasopresina, y los riñones excretarán el exceso de agua.
En el caso de que suframos una deshidratación;, la neurohipófisis aumentará la secreción de vasopresina, y los riñones eliminarán una mínima cantidad de agua (véase la figura 12.5).

Figura

Control hormonal del riñón. La aldosterona, secretada por la corteza de la glándula suprarrenal, hace que el riñón retenga el sodio. La vasopresina, secretada por la neurohipófisis, provoca la retención de agua 

La importancia de la vasopresina en la retención de agua está demostrada por la enfermedad producida precisamente por la falta de esta hormona: la diabetes insípida .
Este término significa literalmente «insípido», pues la orina de una persona con este tipo de deficiencia es tan diluida que prácticamente no tiene gusto.
Las personas diabéticas sin tratamiento pueden excretar aproximadamente 25 litros de agua diaria, lo que significa que tendrían que permanecer muy cerca de un lavabo y de una fuente de agua.
El tratamiento, por supuesto, consiste en la administración de vasopresina (mediante un vaporizador nasal), que es absorbida en el torrente sanguíneo.

Hasta el momento, hemos tratado únicamente de variables del sistema y de mecanismos correctores.
Pero ¿qué se sabe sobre los detectores?
Parece ser que algunos de los que controlan la secreción de aldosterona y de vasopresina (y que, por lo tanto, controlan la eliminación de sodio y de agua por los riñones) controlan también la ingesta de agua y de sal.
Los trataremos en la siguiente sección.

Resumen intermedio 

Un sistema regulador implica cuatro características: una variable sistema (la variable a regular), un valor fijo establecido (el valor óptimo de la variable del sistema), un detector para evaluar la variable sistema y un mecanismo corrector para restaurarla.
Los sistemas reguladores fisiológicos, tales como el control de los fluidos y los nutrientes corporales, requieren de un mecanismo de saciedad para anticipar los efectos del mecanismo corrector.
Ello es debido a que los cambios ocasionados por las conductas de comer y de beber ocurren únicamente después de un considerable período de tiempo.

El cuerpo tiene tres principales compartimentos de fluidos: el fluido intracelular, el fluido intersticial y el fluido intravascular.
El sodio y el agua pueden pasar fácilmente entre el fluido intravascular y el intersticial, pero el sodio no puede penetrar a través de la membrana celular.
La concentración de soluto del fluido intersticial tiene que regularse estrechamente.
Si se vuelve hipertónico, las células pierden agua; mientras que si se vuelve hipotónico, ganan agua.
El volumen del fluido intravascular (plasma sanguíneo) también tiene que ser mantenido dentro de unos límites.

Los riñones regulan la eliminación de agua y de sodio; en el proceso de la eliminación de agua, los productos de desecho se eliminan también a través de la orina.
La aldosterona, una hormona esteroide liberada por la corteza adrenal, provoca la retención de sodio.
La vasopresina, una hormona peptídica producida por los núcleos supraóptico y paraventricular, y liberada por la neurohipófisis, provoca la retención de agua.

Conducta de beber y apetito de sal 

Como acabamos de ver, para que nuestros cuerpos puedan funcionar adecuadamente, el volumen de los dos compartimentos de fluidos - el intracelular y el intravascular- tiene que ser regulado.
La mayoría de las veces, ingerimos más agua y más sodio del necesario, siendo los riñones los responsables de eliminar el exceso.
Sin embargo, si los niveles de agua o de sodio son demasiado bajos, se activan los mecanismos correctores -beber agua o ingerir sodio-.
Todos estamos familiarizados con la sensación de sed, la cual tiene lugar cuando necesitamos beber agua.
Sin embargo, un apetito de sal es mucho más raro, debido a que es difícil que las personas no ingiramos la suficiente cantidad de sodio en nuestra dieta. incluso aunque no se añada sal a los alimentos.
No obstante, existen mecanismos destinados a aumentar la ingesta de sodio, aunque rara vez se activen en los miembros de nuestra especie.

Como la conducta de beber puede ser estimulada tanto por la pérdida de agua del compartimento de fluido intracelular como del intravascular, los investigadores, para describir ambas posibilidades, han adoptado los términos sed osmótica y sed volémica (Fitzsimons, 1972, Epstein, 1973).
El término volémico hace referencia a las medidas de volumen del plasma sanguíneo.
El término osmótico , algo más complejo, se tratará en el siguiente apartado.
El término sed significa diferentes cosas en diferentes circunstancias.
Su definición original hace referencia a una sensación que las personas dicen tener cuando están deshidratadas.
En este caso se utiliza en un sentido descriptivo.
Debido a que desconocemos cómo se deben sentir los animales experimentales, la sed significa simplemente una tendencia a buscar el agua y a beberla.

SED OSMÓTICA 

La sed osmótica se produce cuando la tonicidad (concentración de soluto) del fluido intersticial aumenta.
Esto hace que salga agua al exterior de las células, reduciendo su volumen.
El término osmótico hace referencia a que los detectores están realmente respondiendo a (midiendo) las diferencias de concentración del fluido intracelular y el fluido intersticial que los rodea.
El proceso de osmosis se refiere al movimiento de agua a través de una membrana semipermeable, de una región con baja concentración de soluto a una con alta concentración.

Verney (1947) propuso, por vez primera, la hipótesis de que el encéfalo contenía neuronas que respondían a los cambios en la concentración de soluto del fluido intersticial.
Denominó a dichas neuronas osmorreceptores .
Asimismo observó que la infusión de cloruro sódico hipertónico en la arteria carótida de un perro estimulaba la secreción de vasopresina, haciendo que los riñones disminuyeran la excreción de agua.
En la figura 12.ó se muestra cómo la inyección provocó una disminución casi inmediata en la producción de orina (véase la figura 12.6).

Verney sugirió que los osmorreceptores eran neuronas cuya tasa de respuesta se veía afectada por su nivel de hidratación.
Es decir, que si el fluido intersticial que las baña se volviera más concentrado, perderían agua a través del mecanismo de osmosis.

El encogimiento les haría alterar su tasa de descarga, enviando señales a las neuronas que controlan la tasa de secreción de vasopresina (véase la figura 12.7).

Como veremos más adelante, otros estudios más recientes han confirmado que los osmorreceptores existen y que pueden iniciar la conducta de beber, además de la secreción de vasopresina.

Efectos de una inyección de cloruro sódico hipertónico en la arteria carótida de un perro.
La solución hace salir agua de las células del encéfalo y provoca la secreción de vasopresina por la neurohipófisis.
La hormona hace que el agua sea retenida por los riñones, reduciendo el flujo de orina.

Antes de tratar los datos relativos a la existencia y localización de los osmorreceptores, nos proponemos ampliar algo más la explicación sobre las condiciones que provocan la sed osmótica.
Nuestro cuerpo pierde agua continuamente, principalmente a través de la evaporación.
Cada vez que respiramos exponemos al aire las superficies húmedas internas del sistema respiratorio; de esta forma, cada vez que lo hacemos provocamos la pérdida de una pequeña cantidad de agua.
Además, nuestra piel no es completamente impermeable; parte del agua pasa a través de sus capas y se evapora al llegar a la superficie.
La humedad perdida por evaporación es, por supuesto, agua destilada pura. 
(El sudar también hace perder agua; pero como con ella se pierde también sal, produce también una necesidad de sodio.) Cuando perdemos agua por evaporación, la perdemos de todos los compartimentos de fluidos, del intracelular, del intersticial y del intravascular.
Así, una deshidratación normal produce tanto sed osmótica como sed volémica .

Figura

Explicación hipotética del funcionamiento de un osmorreceptor 

En la figura 12.8 se ilustra cómo la pérdida de agua por evaporación agota tanto el compartimento de fluido intracelular como el intravascular.
Para simplificar, únicamente se muestran unas pocas células y se ha exagerado el volumen del fluido intersticial.
El agua se pierde directamente del fluido intersticial, que se vuelve ligeramente más concentrado que el intracelular o que el intravascular.
De esta forma, el agua es extraída tanto de las células como del plasma sanguíneo.
Cuando se ha perdido suficiente agua de las células, se estimula la secreción de vasopresina y disminuye la producción de orina.
Finalmente, la pérdida de agua de las células y del plasma sanguíneo será lo suficientemente grande como para provocar la sed, tanto osmótica como volémica (véase la figura 12.8).

Si la evaporación fuera el único medio de producir alteraciones en la distribución del agua en los tres compartimentos de fluidos, entonces no necesitaríamos dos clases de detectores para estimular la sed; un conjunto de osmorreceptores sería suficiente.
Sin embargo, es posible incurrir en una pérdida de fluido intracelular sin perder fluido intravascular. 
(Y, como veremos en el siguiente apartado, también es posible que ocurra la situación contraria).

El modo más común de provocar una pérdida de fluido intracelular, independiente del intravascular, es comiendo alimentos salados.
La sal se absorbe desde el sistema digestivo al plasma sanguíneo; de esta forma, el plasma se vuelve hipertónico.
Esta condición saca agua del fluido intersticial, lo que hace que este compartimento se vuelva también hipertónico, provocando la salida de agua de las células hacia el espacio intersticial.
Cuando aumenta el volumen del plasma sanguíneo, los riñones empiezan a excretar grandes cantidades tanto de sodio como de agua.
Finalmente, se excreta el exceso de sodio junto con el agua que procede de los fluidos intersticial e intracelular.
El resultado neto es una pérdida de agua de las células. En ningún momento descendió el volumen de plasma sanguíneo ; en realidad, fue temporalmente más alto de lo normal, siendo esto lo que inició la excreción de sodio X de agua por los riñones.

Figura

Pérdida de agua por evaporación 

Como hemos visto anteriormente, Verney había establecido la hipótesis de que el estímulo que causa la sed osmótica es la pérdida de agua de las células del cuerpo.
Fitzsimons (1972) obtuvo pruebas que apoyaban la hipótesis de Verney, demostrando que el estímulo que provoca la sed osmótica no era simplemente un cambio en la concentración de soluto del fluido intersticial, sino el efecto que esos cambios tienen sobre el contenido de agua de las células.

Fitzsimons extirpó los riñones a un grupo de ratas para evitar que éstas pudieran eliminar el agua o cualquiera de las sustancias que les administró.

Después, inyectó a los animales soluciones hipertónicas de sustancias que pueden entrar en las células (tales como glucosa y urea), o de sustancias que no pueden hacerlo (tales como cloruro de sodio, sulfato de sodio y sacarosa -el azúcar común-).
Todas las sustancias que inyectó debían aumentar la concentración de soluto del fluido intersticial.
Sin embargo, únicamente las sustancias que no podían entrar en las células sacaron agua de ellas.
Por ejemplo, un aumento en la concentración de urea en el fluido intersticial no tiene efecto sobre las células porque la urea se difunde libremente hacia su interior, y no se establece un gradiente de concentración.

De hecho, los únicos animales que bebieron en exceso fueron aquellos que recibieron sustancias que no podían penetrar en las células, ocasionando la salida de agua fuera de ellas; así, el estímulo para la sed osmótica es la deshidratación celular.

Además, Fitzsimons halló que una inyección de urea provocaba la conducta de beber, pero en pequeñas cantidades.
La razón de este efecto es que la urea atraviesa lentamente la barrera hematoencefálica.
Por lo tanto, una inyección de urea produce un ligero y temporal aumento de la concentración de soluto del plasma sanguíneo en relación con la del cerebro, lo que hace que el agua pase del cerebro al fluido intravascular.

Así, la urea provoca una deshidratación temporal del cerebro.
El hecho de que la deshidratación del cerebro provoque la conducta de beber, sugiere que los osmorreceptores que producen la sed están localizados en él (volveremos más adelante a retomar esta idea).

Fitzsimons no fue el primero en obtener pruebas de que los osmorreceptores estaban localizados en el cerebro.
Anderson (1953) observó que las inyecciones de una solución salina hipertónica en el hipotálamo rostrolateral provocaban la conducta de beber, pero no la liberación de vasopresina.
En cambio, las inyecciones en regiones hipotalámicas caudales estimulaban la liberación de vasopresina, pero no la sed.
De esta forma, parece ser que la conducta de beber y la secreción de vasopresina en respuesta a la hipertonicidad están mediadas por diferentes grupos de receptores.

La localización de los osmorreceptores que controlan la conducta de beber no está aún bien establecida.
Un extenso estudio de Picotear y Blass (1975) sobre mapas de distribución de osmorreceptores sugirió que los osmorreceptores que estimulan la sed en la rata están localizados en la porción medial del área preóptica lateral.
Estos investigadores observaron que las inyecciones de sacarosa hipertónica en esta región favorecían la conducta de beber.
Además, Blass y Epstein (1971) observaron que cuando se provocaba sed a una rata mediante la administración de una inyección subcutánea de solución salina hipertónica, la conducta de beber podía ser inhibida inyectando agua en el área preóptica.
Probablemente, esa agua eliminaba la señal para la sed en los detectores. (Esta manipulación es similar a la que tendría lugar si calentáramos el termostato en una habitación fría; lo «engañaríamos», haciendo que se apagara la caldera de la calefacción).

Un estudio más reciente no ha conseguido confirmar estos resultados.
Andrews y cols. (1992) utilizaron minibombas especiales para infundir continuamente pequeñas cantidades de soluciones hiper e hipotónicas en el área preóptica de ambos hemisferios.
Estos autores observaron que las dos infusiones tenían efectos variables sobre la conducta de beber de las ratas: algunas veces las infusiones de solución salina hipertónica podían reducir la conducta de beber de los animales y las infusiones de agua podían aumentarla .

Además, varios estudios (por ejemplo, Coburn y Stricker, 1978) han puesto de manifiesto que las ratas aún pueden mostrar sed osmótica tras la lesión del área preóptica lateral.

Entonces, ¿dónde están localizados los osmorreceptores?
La mayoría de investigadores actualmente creen que están localizados más medialmente, en la región que rodea el extremo anteroventral del tercer ventrículo (la AV3V).
Buggy y cols. (1979) observaron que las inyecciones de solución salina hipertónica directamente en la región AV3V provocaban sed, mientras que las inyecciones en el área preóptica lateral, no.
En algunas especies (como el perro), los osmorreceptores podrían estar localizados en un órgano circunventricular especializado, localizado en la zona inmediatamente rostral a la región AV3V.
El encéfalo tiene varios órganos circunventriculares (regiones especializadas con rico aporte sanguíneo, localizadas a lo largo del sistema ventricular).
El lector ya está familiarizado con dos de estos órganos: el área postrema y la glándula pineal (tratados en los capítulos l y 9).
En este capítulo trataremos sobre dos más de ellos: el OVLT y el OSF (remítase a la figura 12.10).

El OVLT ( órgano vasculoso de la lámina terminal ), al igual que los otros órganos circunventriculares, está localizado en el lado sanguíneo de la barrera hematoencefálica.
Esto significa que las sustancias disueltas en la sangre pasan fácilmente al fluido intersticial del interior de este órgano. 
(Pronto podremos valorar la importancia de esta característica.) Thrasher y Keil (1987) observaron que, tras la lesión del OVLT, los perros ya no bebían como respuesta a la inyección de solución salina hipertónica (tampoco mostraron aumento alguno en la secreción de vasopresina).
De todas formas, los estudios de Johnson y sus colegas (véase Johnson y Edwards, 1990) pusieron de manifiesto, en ratas, que las lesiones restringidas del OVLT no abolían la sed osmótica; las lesiones se tenían que extender a otras áreas cerebrales alrededor de la región AV3V para que tuvieran efecto.
En consecuencia, a pesar de que algunos receptores del encéfalo de la rata podrían estar localizados en el OVLT, existen otros localizados en el área preóptica medial.

Tal como ya hemos visto, cuando las ratas reciben una inyección de urea, muestran poca sed osmótica, probablemente porque la urea provoca una deshidratación temporal del cerebro.
Ahora bien, la urea no deshidrata ninguno de los tejidos situados fuera de la barrera hematoencefálica, como el OVLT.
Consecuentemente, Johnson y Edwards podrían tener razón: algunos osmorreceptores quizás estén localizados dentro de la barrera hematoencefálica.

SED VOLÉMICA 

La sed volémica se produce cuando el volumen del plasma sanguíneo (el volumen intravascular) disminuye.
Como hemos visto anteriormente, cuando perdemos agua a través de la evaporación, se pierde de los tres compartimentos de fluidos: el intracelular, el intersticial y el intravascular.
De esta forma, la evaporación produce tanto sed volémica como sed osmótica.
Si la evaporación fuera el único modo mediante el cual nuestro cuerpo perdiera fluidos, no necesitaríamos tener mecanismos reguladores de la conducta de beber controlados por los detectores del volumen de sangre.
Sin embargo, es posible sufrir una pérdida en el volumen intravascular sin que el compartimento intersticial quede afectado.

La pérdida de sangre es la causa más obvia de la sed volémica pura.
Desde la más remota historia, las crónicas de las batallas relatan que los supervivientes heridos gritaban pidiendo agua. 
(Más prosaicamente, el vómito o la diarrea hacen eliminar fluido isotónico del cuerpo, disminuyendo, en consecuencia, el volumen de fluido intravascular.) Así, la sed volémica proporciona una segunda línea de defensa contra la pérdida de agua que la lesión del sistema osmótico podría ocasionar, proporcionando también los medios necesarios para que la pérdida de fluido isotónico provoque la conducta de beber.

Además, debido a que la hipovolemia implica tanto una pérdida de sodio como de agua (es decir, el sodio contenido en el fluido isotónico que se perdió), la sed volémica también conlleva un apetito de sal.

El modo más fácil de producir hipovolemia en animales experimentales como la rata podría consistir en extraerles sangre.
Un procedimiento menos drástico consiste en inyectar un coloide en la cavidad abdominal del animal o bajo la piel distendida de su espalda (Fitzsimons, 1961).
Un coloide es una sustancia parecida a la cola (del griego kolla , «goma») constituida por grandes moléculas que no pueden atravesar &rsqb;as membranas celulares.
De este modo, permanece en la cavidad abdominal o en el espacio bajo la piel.
Como la solución de estas moléculas es hipertónica, saca fluido extracelular fuera del tejido.
El fluido que sale del plasma sanguíneo es isotónico; cuando las moléculas de agua reducen el gradiente de concentración producido por el coloide, arrastran cloruro sódico con ellas.
Inicialmente, el agua proviene del fluido intersticial; pero cuando el volumen de este espacio de fluido disminuye, la presión sanguínea hace que el fluido que procede del plasma sanguíneo empiece a llenar el espacio vacante.
No sale fluido de las células, ya que la concentración de soluto del fluido intersticial permanece estable.
Al cabo de una hora, la neurohipófisis comienza a liberar vasopresina, y el volumen de orina cae.
Aproximadamente al mismo tiempo, el animal comienza a beber, y continúa haciéndolo hasta que la mayor parte del volumen de líquido robado del fluido extracelular ha sido reemplazado.

Explicaremos ahora un experimento específico que ilustra la producción de sed volémica.
Fitzsimons (1961) inyectó un coloide denominado glicol polietileno en la cavidad abdominal de las ratas y, posteriormente, drenó el fluido acumulado, eliminando el cuerpo del coloide junto con el agua y el cloruro de sodio que había extraído del fluido extracelular.
La pérdida de agua provocó una sed considerable; los animales bebieron agua copiosamente.
Uno o dos días después, proporcionó a los animales agua y una solución salina hipertónica del 1,8 %, que las ratas normalmente rehúsan beber.
Esta vez, consumieron ávidamente la solución salina. 
¿Por qué lo hicieron?
El procedimiento había eliminado tanto el agua como la sal, de modo que las ratas necesitaron ambas sustancias para restaurar la normalidad de sus compartimentos de fluidos.
De este modo, la pérdida de sal indujo un fuerte apetito de sal .

¿Qué detectores son los responsables de iniciar la sed volémica y el apetito de sal?
En realidad hay al menos dos grupos de receptores que cumplen esta doble función: uno en los riñones y el otro en el corazón y en los grandes vasos sanguíneos.

Figura

Detección de hipovolemia por el riñón X el sistema renina-angiotensina 

El sistema renina-angiotezisina 

Los riñones contienen células capaces de detectar disminuciones en el flujo de la sangre que les llega.
La causa principal de 13 reducción de flujo sanguíneo es la pérdida de volumen sanguíneo; así, estas células detectan la presencia de hipovolemia.
Cuando el flujo sanguíneo hacia los riñones disminuye, éstas secretan un enzima llamado renina .
Este enzima entra en la sangre, donde cataliza la conversión de una proteína denominada angiotensinógeno en una hormona llamada angiotensina . (En realidad, existen dos formas de angiotensina.
El angiotensinógeno se transforma en angiotensina I, que es convertida rápidamente por un enzima en angiotensina II.
La forma activa es la angiotensina II, que abreviaremos como AII.) 

La angiotensina II tiene numerosos efectos fisiológicos: estimula la corteza suprarrenal para secretar aldosterona, estimula a la neurohipófisis para secretar vasopresina X aumenta la presión sanguínea provocando la contracción de los músculos de las pequeñas arterias.
(Recordemos que la presencia de aldosterona inhibe la secreción de sodio por los riñones, y que la presencia de vasopresina inhibe la eliminación de agua por los mismos.)
Además, la AII tiene dos efectos conductuales: inicia la conducta de beber y (a través de su efecto estimulante sobre la aldosterona) produce apetito de sal.
Por lo tanto, una reducción en el flujo sanguíneo de los riñones provoca una retención tanto de agua como de sodio en el cuerpo, ayuda a compensar sus pérdidas reduciendo el tamaño de los vasos sanguíneos y estimula al animal a encontrar e ingerir agua y sal (véase la figura 12.9).

A pesar que de hace ya bastantes años que la angiotensina está siendo estudiada. sólo recientemente los científicos han desarrollado técnicas de radioinmunoensayo suficientemente sensibles para medir los niveles de AII en pequeñas muestras sanguíneas de un animal.
Van Eekelen y Phillips (1988), en un experimento con ratas, inyectaron continuamente AII en vena y extrajeron periódicamente muestras de sangre de una arteria, analizando las cantidades presentes de AII.
Observaron que los animales, que se podían mover libremente, no bebían hasta que los niveles sanguíneos de AII alcanzaban un nivel de aproximadamente 450 pg/ml. 
(Un picogramo , abreviado pg, corresponde a una billonésima parte de un gramo, o 10-12, El prefijo pico- proviene de la palabra italiana piccolo , «pequeño».) 
Un nivel sanguíneo de AII de 450 pg/ml es el que presenta una rata cuando ha estado privada de agua aproximadamente durante 48 horas.
Así, es posible que la capacidad del sistema renina-angiotensina para provocar la conducta de beber sea importante únicamente en situaciones de emergencia.

Barorreceptores 

El segundo grupo de receptores para la sed volémica se localiza en el corazón.
Hace ya tiempo que los fisiólogos saben que las aurículas del corazón (las partes que reciben la sangre de las venas) contienen neuronas sensoriales que detectan extensión.
Las aurículas se llenan pasivamente con la sangre de retorno de las venas.
Cuanta más sangre haya, más se llenan las aurículas inmediatamente antes de cada contracción del corazón.
Así, cuando desciende el volumen del plasma sanguíneo, los receptores de extensión de las aurículas detectan el cambio.

Estos receptores ocupan la mejor localización posible para poder detectar la presencia de hipovolemia.

Fitzsimons y Moore-Gillon (1980) pusieron de manifiesto que la información procedente de esos receptores puede estimular la sed.
Investigando con perros, situaron un pequeño globo en la vena cava inferior, la vena que trae la sangre de la mayor parte del cuerpo (a excepción de la cabeza y de los brazos) hacia el corazón.
Cuando se hinchaba el globo, se reducía el flujo sanguíneo hacia el corazón, disminuyendo así la cantidad de sangre que entraba en la aurícula derecha.
Al cabo de unos 30 minutos, los perros empezaron a beber.
Este efecto fue también observado incluso cuando los investigadores administraron saralasina , una droga que bloquea los receptores de la angiotensina; en consecuencia, no era producido por la secreción renal de renina.
En otro experimento, Moore-Gillon y Fitzsimons (1982) implantaron un pequeño globo en la unión entre una de las grandes venas de los pulmones y la aurícula izquierda del corazón.

(Extirparon la parte del pulmón irrigada por la vena, de modo que el hecho de hinchar el globo no afectaba al flujo sanguíneo que llegaba al corazón.) 
El hecho de hinchar el globo estimuló directamente a los receptores de extensión de la aurícula izquierda y redujo la ingesta de agua de los animales.

14 Aprendizaje y memoria: mecanismos básicos 

Las experiencias nos cambian; lo que encontramos en nuestro entorno altera nuestra conducta modificando nuestro sistema nervioso.
Como han indicado muchos investigadores, comprender la fisiología de la memoria es el desafío fundamental de la investigación en neurociencia.
El cerebro es complejo, y también lo son el aprendizaje y la memoria.

Aunque los cambios individuales que tienen lugar en las células cerebrales puedan ser relativamente simples, el cerebro consta de billones de neuronas.
Por consiguiente, aislar e identificar los cambios específicos responsables de un tipo concreto de memoria resulta extremadamente difícil.

De manera parecida, aunque los elementos de una determinada tarea de aprendizaje puedan ser simples, sus implicaciones para un organismo pueden resultar muy complejas.
Puede que la conducta que el investigador observa y mide constituya sólo uno de los múltiples cambios resultantes de una determinada experiencia.
Sin embargo, a pesar de estas dificultades, los largos años de trabajo parecen estar dando fruto al fin.

Se han desarrollado enfoques y métodos nuevos a partir de los antiguos, habiéndose producido un progreso real en la comprensión de la anatomía y la fisiología del aprendizaje y el recuerdo.

Naturaleza del aprendizaje 

El aprendizaje se refiere al proceso mediante el cual las experiencias modifican nuestro sistema nervioso y, por consiguiente, nuestra conducta.
Nos referimos a estos cambios como recuerdos .
Aunque pueda resultar práctico describir los recuerdos como si fueran notas que se guardan en archivadores, ciertamente no es este el modo como las experiencias son reflejadas por el cerebro.
Las experiencias no se «almacenan»; sino que cambian el modo de percibir, ejecutar, pensar y planificar.
Y lo hacen cambiando físicamente la estructura del sistema nervioso, alterando los circuitos neurales que participan en la percepción, la ejecución, los pensamientos y la planificación.

La principal función de la capacidad de aprender es desarrollar conductas que se adapten a un entorno cambiante.
La capacidad de aprendizaje nos permite hallar comida cuando tenemos hambre, calor cuando tenemos frío y compañía cuando estamos solos.
También nos permite evitar objetos o situaciones que podrían resultar nocivos para nosotros.

Sin embargo, el hecho de que la función última del aprendizaje sea un cambio útil de la conducta no significa que el aprendizaje se produzca sólo en las regiones cerebrales que controlan el movimiento.
El aprendizaje puede adoptar al menos cuatro formas básicas: aprendizaje perceptivo, aprendizaje estímulo-respuesta, aprendizaje motor y aprendizaje relacional.
Este capítulo discute los tres primeros tipos de aprendizaje, mientras que en el capítulo 15 se hará referencia al aprendizaje relacional.

El aprendizaje perceptivo es la capacidad para aprender a reconocer estímulos vistos con anterioridad y a distinguirlos de otros similares.
La principal función de este tipo de aprendizaje es la identificación y categorización de objetos (incluidos otros miembros de nuestra especie) y situaciones.
A menos que hayamos aprendido a reconocer algo, no podemos aprender cómo comportarnos ante ello (no sacaremos provecho de nuestra experiencia con ello, y aprovecharnos de la experiencia es lo que caracteriza al aprendizaje).

Cada uno de nuestros sistemas sensoriales es capaz de mostrar aprendizajes perceptivos.
Podemos aprender a reconocer objetos por su apariencia visual, por los sonidos que emiten, por la sensación táctil que producen o por su olor.

Podemos reconocer a las personas por la forma de su cara, por los movimientos que hacen al caminar o por el sonido de su voz.

Cuando las oímos hablar, podemos reconocer las palabras que dicen y, quizás, su estado emocional.
Como veremos, el aprendizaje perceptivo parece lograrse mediante cambios en la corteza de asociación sensorial.
Es decir, aprender a reconocer estímulos visuales complejos supone cambios en la corteza de asociación visual, aprender a reconocer estímulos auditivos complejos supone cambios en la corteza de asociación auditiva, etc.
(Los estímulos muy simples, como los cambios de brillo, no requieren la participación de la neocorteza; el aprendizaje que involucra a estos estímulos puede lograse mediante componentes subcorticales de los sistemas sensoriales.) 

El aprendizaje estímulo-respuesta es la capacidad para aprender a realizar un tipo específico de conducta ante un determinado estímulo.
Por tanto, comporta el establecimiento de conexiones entre los circuitos involucrados en la percepción y los involucrados en el movimiento.
Esta conducta puede ser una respuesta automática, como, por ejemplo, un reflejo de defensa o una secuencia complicada de movimientos aprendidos con anterioridad.
El aprendizaje estímulo-respuesta incluye dos categorías principales de aprendizajes que han sido estudiadas por los psicólogos: condicionamiento clásico y condicionamiento instrumental .

El condicionamiento clásico es una forma de aprendizaje en la que un estímulo no significativo adquiere las propiedades de uno significativo.

Implica una asociación entre dos estímulos.
Un estímulo que con anterioridad apenas haya tenido efectos sobre la conducta pasa a ser capaz de evocar una conducta refleja, típica de la especie.
Por ejemplo, cerramos los ojos cuando vemos que a nuestro lado están hinchando mucho un globo.
Esta conducta constituye una reacción defensiva típica de nuestra especie que sirve para proteger los ojos.
Normalmente, esta reacción tiene lugar cuando oímos un ruido intenso, cuando algo se aproxima con rapidez a la cara o cuando nos tocan los ojos o la piel que los rodea; estas respuestas no tenemos que aprenderlas.
Cuando vemos que un globo está muy inflado reaccionamos de ese modo debido a nuestras experiencias anteriores con globos que han explotado.
En algún momento del pasado, probablemente cuando éramos niños, un globo muy hinchado nos explotó cerca de la cara y el impacto provocó de modo reflejo una reacción de defensa.
A través del condicionamiento clásico, el estímulo que precedió al estallido (la visión de un globo muy hinchado) se convirtió en un estímulo capaz de promover la reacción de defensa.

Los nombres asignados a los estímulos y respuestas que constituyen el condicionamiento clásico se indican en la figura 14.1.
Utilizaremos esos términos repetidamente en este capítulo, por lo que vale la pena aprenderlos ahora, si al lector no le resultan ya familiares.
La ráfaga de aire, el estímulo original, se denomina estímulo inconcondicionado (EI): provoca de manera incondicionada la reacción típica de la especie.
La reacción de cerrar los ojos se denomina respuesta incondicionada (RI).
Después de varias experiencias con globos que explotan, la visión del balón inflado, el estímulo condicionado (EC), pasa a producir esta misma reacción que ahora se denomina respuesta condicionada (RC).
La respuesta está condicionada al emparejamiento entre el estímulo condicionado y el estímulo incondicionado (véase la figura 14.1).

El condicionamiento clásico se produce cuando un estímulo neutro es seguido por otro que promueve una respuesta de manera automática.
Hace posible que el organismo aprenda a ejecutar una respuesta típica de la especie bajo nuevas condiciones.
Por tanto, el condicionamiento clásico sirve para preparar a un organismo para un acontecimiento posterior.
Por ejemplo, una señal de aviso puede permitir que el organismo se defienda de algún daño; los estímulos asociados con un posible compañero sexual pueden hacer emitir una respuesta que funcione como manifestación sexual, y los asociados con la comida pueden provocar la secreción de saliva, jugos gástricos e insulina.
Además (e incluso más importante), el condicionamiento clásico hace que algunos estímulos neutros se conviertan en reforzadores o en punitivos condicionados (pero hablaremos de ello más adelante).

¿Qué cambios ocurren en el cerebro cuando tiene lugar un condicionamiento clásico?
La figura 14.2 muestra un circuito neural simplificado que podría explicar este tipo de aprendizaje.
Utilizaremos un ejemplo simple: la respuesta defensiva de parpadeo en el conejo.
Un débil soplo de aire dirigido hacia el ojo puede provocar este reflejo; por tanto, el soplo de aire funciona como EI.
El EC es un tono de 1.000 Hz.
Para simplificar, imaginaremos que el EI es detectado por una única neurona del sistema somatosensorial y el EC por una única neurona del sistema auditivo.
También imaginaremos que la respuesta (el parpadeo) está controlada por una única neurona del sistema motor (véase la figura 14.2).

Veamos ahora cómo funciona el circuito.
Si presentamos un tono de 1.000 Hz, vemos que el animal no muestra ninguna reacción, porque las sinapsis que conectan la neurona sensible al tono con las neuronas del sistema motor son débiles.
Sin embargo, si presentamos un soplo de aire en un ojo, éste parpadea.
Esto ocurre porque el proceso de la evolución ha generado una sinapsis fuerte entre la neurona somatosensorial y la neurona motora que causa el parpadeo (sinapsis S, de «soplo»).
Para establecer un condicionamiento clásico, primero presentamos el tono de 1.000 Hz y luego, casi inmediatamente después, un soplo de aire.
Después de repetir varias veces estos emparejamientos de estímulos, vemos cómo podemos prescindir del soplo de aire; el tono de 1.000 Hz produce por sí solo el parpadeo.

Hace más de cuarenta años, Hebb propuso una regla que podría explicar cómo las neuronas cambian con la experiencia de un modo que puede traducirse en cambios en la conducta (Hebb, 1949).
El principio de Hebb dice que si una sinapsis se activa repetidamente al mismo tiempo que la neurona postsináptica emite potenciales de acción, tendrán lugar una serie de cambios en la estructura o en la neuroquímica de la sinapsis responsables de que esa sinapsis se vea reforzada.
¿Cómo se puede aplicar el principio de Hebb a nuestro circuito?
Si el tono de 1.000 Hz se presenta en primer lugar, la sinapsis débil T (de «tono») se activa.
Si el soplo se presenta inmediatamente después, la sinapsis fuerte S se activa y hace que la motoneurona emita potenciales de acción.
Este hecho refuerza todas las sinapsis con la motoneurona que se hallaban activas en ese momento .
Por supuesto, ello implica a la sinapsis T. Después de varios emparejamientos entre los estímulos, y de varios incrementos de la fuerza sináptica, la sinapsis T llega a ser suficientemente fuerte por sí misma como para hacer que la motoneurona también se active.
El aprendizaje ha tenido lugar (véase la figura 14.2).

Obviamente, el sistema auditivo del conejo tiene más de una neurona, y lo mismo ocurre con el sistema motor.
Las neuronas del sistema auditivo tienen conexiones con todos los tipos de neuronas del sistema motor (con las que controlan los movimientos de las orejas y el hocico, correr, oler, masticar y otras cosas que pueden hacer los conejos).
Pero antes de que tenga lugar el aprendizaje, estas conexiones son débiles; la audición de un tono de 1.000 Hz produce una activación tan pequeña de las neuronas del sistema motor que el animal no ejecuta ninguna respuesta manifiesta.

(Por supuesto, el ruido puede sobresaltar al animal, pero esta respuesta suele desaparecer cuando el tono se ha presentado varias veces.)
De las miles de sinapsis del sistema motor que han sido activadas por el tono de 1.000 Hz, sólo las localizadas en neuronas que acaban de emitir potenciales de acción se verán reforzadas.
Si el EI se acaba de presentar y el animal acaba de parpadear, la mayoría de las células activadas más recientemente serán las que controlen el parpadeo (y sólo las sinapsis con estas células se verán reforzadas).

Cuando Hebb formuló este principio, no era posible determinar si era cierto o falso.
Ahora, finalmente, se han llevado a cabo suficientes progresos en las técnicas de laboratorio como para que se pueda determinar a fuerza de las sinapsis individuales, de manera que los investigadores pueden estudiar las bases fisiológicas del aprendizaje.
Veremos los resultados de algunos de estos enfoques más adelante en este capítulo, en el apartado titulado «Mecanismos de plasticidad sináptica».

El segundo tipo principal de aprendizaje estímulo-respuesta es el condicionamiento instrumental (también denominado condicionamiento operante ).
Mientras que el condicionamiento clásico implica respuestas automáticas, típicas de la especie, el condicionamiento instrumental implica conductas aprendidas.
Y mientras que el condicionamiento clásico involucra la asociación entre dos estímulos, el instrumental requiere la asociación entre una respuesta y un estímulo .
El condicionamiento instrumental es una forma de aprendizaje más flexible y adaptativa.
Permite que un organismo adapte su conducta en función de las consecuencias de la misma.
Es decir, cuando una conducta es seguida por consecuencias favorables, tiende a aparecer con más frecuencia; cuando es seguida por consecuencias desfavorables, tiende a ocurrir con menos frecuencia.
De manera genérica, las «consecuencias favorables» se denominan estímulos reforzantes , mientras que las «consecuencias desfavorables» se denominan estímulos punitivos .
Por ejemplo, una respuesta que permita que un sujeto hambriento encuentre comida será reforzada, y una respuesta que cause dolor será castigada. (A menudo los psicólogos se refieren a estos términos como reforzadores y castigos ).

Veamos en qué consiste el proceso del refuerzo.
De manera resumida, el refuerzo ocasiona cambios en el sistema nervioso de un animal que aumentan la probabilidad de que un determinado estímulo provoque una determinada respuesta.
Por ejemplo, cuando se coloca a una rata hambrienta por primera vez en una jaula operante, no es muy probable que apriete la palanca localizada en una de sus paredes.
Sin embargo, si la aprieta y recibe un trozo de comida inmediatamente después, la probabilidad de que ejecute otra vez esa respuesta aumenta.
Dicho de otro modo, el refuerzo hace que la visión de la palanca constituya el estímulo que favorezca la respuesta de apretarla.
No es correcto decir simplemente que aumenta la frecuencia de una determinada conducta .
Si no hay palanca, la rata que haya aprendido a apretarla no levantará la pata en el aire.
Se necesita la visión de aquélla para producir la respuesta.
Por tanto, el proceso de refuerzo establece una conexión entre los circuitos neurales involucrados en la percepción (la visión de la palanca) y los involucrados en el movimiento (el acto de apretar la palanca).

Como veremos más adelante en este capítulo, el cerebro tiene un mecanismo de refuerzo que controla este proceso (véase la figura 14.3).

La tercera categoría principal de aprendizaje, el aprendizaje motor , constituye de hecho una forma especial de aprendizaje estímulo-respuesta; por ello, no lo trataremos separadamente en este capítulo.
Para simplificar, podemos considerar al aprendizaje perceptivo como el establecimiento de cambios en los sistemas sensoriales del cerebro, al aprendizaje estímulo-respuesta como el establecimiento de conexiones entre los sistemas sensoriales y los motores, y al aprendizaje motor como el establecimiento de cambios en los sistemas motores.
Pero, de hecho, el aprendizaje motor no puede tener lugar sin la guía sensorial del entorno.
Por tanto, se trata de una forma especial de aprendizaje estímulo-respuesta.
Por ejemplo, la mayoría de las destrezas motoras comportan la interacción con objetos: bicicletas, máquinas tragaperras, agujas de hacer punto, etc.
Incluso las que tienen lugar sin objetos, como los pasos de danza solitarios, requieren un feedback de las articulaciones, los músculos, el aparato vestibular, los ojos o el contacto entre los pies y el suelo.
El aprendizaje motor difiere de otras formas de aprendizajes principalmente en el grado en que se aprenden nuevas formas de conductas: cuanto más nuevas sean éstas, más circuitos neurales del sistema motor cerebral tendrán que ser modificados (véase la figura 14.4).

Las tres formas de aprendizaje que hemos descrito hasta el momento consisten principalmente en cambios en el sistema sensorial, entre un sistema sensorial y el sistema motor o en el sistema motor.
Pero obviamente, el aprendizaje suele ser más complejo que eso.
El cuarto tipo de aprendizaje comporta el aprendizaje de relaciones entre estímulos individuales.
Por ejemplo, una forma de aprendizaje perceptivo algo más compleja afecta a las conexiones entre diferentes áreas de la corteza asociativa.
Cuando oímos el maullido de un gato en la oscuridad, podemos imaginar la apariencia del gato y lo que sentiríamos si le tocáramos el pelo.
Obviamente, los circuitos neurales de la corteza asociación auditiva que reconocen el maullido están de algún modo conectados con los circuitos correspondientes de las cortezas e asociación visual y somatosensorial.
Estas interconexiones también se logran a través del aprendizaje.

El aprendizaje espacial (la percepción e la localización espacial) implica también aprendizaje de las relaciones entre muchos estímulos.
Por ejemplo, pensemos en lo que tenemos que aprender para familiarizarnos con el contenido de una habitación.
En primer lugar, debemos aprender a reconocer cada uno de sus objetos.
Además, tenemos que aprender la localización relativa de esos objetos entre sí.
Por tanto, cuando nos hallamos en un lugar determinado de la habitación, la percepción de esos objetos y de su localización con respecto a nosotros nos indica dónde estamos exactamente.

Otros tipos de aprendizajes relacionales son aún más complejos.
El aprendizaje episódico (recordar secuencias de acontecimientos -episodios- de los que hemos sido testigos) requiere el control no sólo de estímulos individuales, sino también del orden en que ocurren.
El aprendizaje observacional (aprendizaje basado en la visión e imitación de otras personas) requiere que recordemos lo que hace otra persona, la situación en que se lleva a cabo esta conducta y la relación entre los movimientos de la otra persona y los nuestros.
Como veremos en el capítulo 15, un sistema especial que incluye el hipocampo y otras estructuras relacionadas parece llevar a cabo funciones de coordinación necesarias para muchos tipos de aprendizaje en los que la participación de regiones individuales de la corteza cerebral no es suficiente.

Resumen intermedio 

El aprendizaje origina cambios en la manera de percibir, actuar, pensar y sentir.
Esto es posible gracias a los cambios que tienen lugar en el sistema nervioso a nivel de los circuitos responsables de la percepción, del control de los movimientos y de las conexiones entre ambas funciones.

El aprendizaje perceptivo consiste principalmente en cambios en los sistemas perceptivos que hacen posible que reconozcamos estímulos, de forma que podamos responder a ellos i de manera apropiada.
El aprendizaje estímulo-respuesta consiste en conexiones entre los sistemas perceptivos y los sistemas motores.
Las formas más importantes de aprendizaje estímulo-respuesta son el condicionamiento clásico y el instrumental.
El condicionamiento clásico tiene lugar cuando un estimulo neutro es seguido por un estímulo incondicionado (EI) que provoca de manera natural una respuesta incondicionada (RI).
Después de este emparejamiento, el estimulo neutro se convierte en un estímulo condicionado (EC); ahora provoca por sí solo la respuesta incondicionada (RC).

El condicionamiento instrumental se produce cuando una respuesta es seguida por un estímulo reforzante, como el agua para un animal sediento.
El estímulo reforzante aumenta la probabilidad de que otros estímulos presentes en el momento de realizar la respuesta evoquen esa respuesta.
Ambas formas de aprendizaje estímulo-respuesta pueden ocurrir como resultado del fortalecimiento de las conexiones sinápticas, tal como describe el principio de Hebb.

El aprendizaje motor, aunque involucra principalmente cambios en los circuitos neurales que controlan el movimiento, está guiado por estímulos sensoriales; por tanto, constituye de hecho un aprendizaje estímulo-respuesta.
El aprendizaje relacional, la forma más compleja de aprendizaje, será descrito en el capítulo 15, e incluye la capacidad para reconocer objetos por medio de más de una modalidad sensorial, de reconocer la localización relativa de los objetos en el entorno y de recordar la secuencia en que han tenido lugar los acontecimientos durante episodios concretos.

16 Comunicación humana 

Las conductas verbales constituyen una de las formas más importantes de la conducta social humana.
Nuestra evolución cultural ha sido posible gracias a la capacidad para hablar y escuchar, escribir y leer.
El lenguaje permite que nuestros descubrimientos sean acumulativos.
El conocimiento obtenido por una generación puede ser transmitido a la siguiente.

La función básica de la comunicación verbal se manifiesta en los efectos que ejerce sobre las otras personas.

Cuando hablamos con alguien casi siempre esperamos que nuestras palabras induzcan a la otra persona a realizar alguna conducta.
A veces, esta conducta comporta una ventaja obvia para nosotros, como cuando le pedimos a alguien que nos traiga un objeto o que nos ayude a realizar una tarea.
En otras ocasiones, simplemente solicitamos un intercambio social.
Incluso la conversación «vana» no es tan vana, ya que hace que otra persona nos mire y nos responda.

Este capítulo trata de las bases neurales de la conducta verbal: hablar, entender el habla, leer y escribir.

Producción y comprensión del habla: mecanismos cerebrales 

Nuestro conocimiento sobre la fisiología del lenguaje proviene fundamentalmente de la observación de los efectos que tienen las lesiones cerebrales sobre la conducta verbal de las personas.
Aunque los investigadores han estudiado a personas sometidas a cirugía cerebral o que han sufrido lesiones craneales, tumores cerebrales o infecciones, la mayoría de las observaciones se han llevado a cabo con individuos que han sufrido apoplejías, o accidentes cerebrovasculares .
El tipo más frecuente de accidente cerebrovascular está causado por la obstrucción de un vaso sanguíneo.
La interrupción del flujo sanguíneo priva a una región cerebral de riego sanguíneo, lo cual provoca la muerte de las células de esa región.
La figura 16.1 muestra un imagen de TC de una persona que sufrió un accidente cerebrovascular causante de una afasia de Broca (esta alteración se describirá más adelante).
La lesión es la región oscura señalada por la flecha (véase la figura 16.1).

La categoría más importante de trastornos del lenguaje es la afasia , una alteración primaria de la comprensión o de la producción del habla causada por una lesión cerebral.
No todas las alteraciones del habla son afasias.
Para poder clasificar a un paciente de afásico, éste ha de mostrar dificultades en la comprensión, repetición o producción de un habla comprensible, y estas dificultades no deben ser causadas por déficit sensoriales o motores simples, ni por falta de motivación.

Por ejemplo, la incapacidad para hablar causada por la sordera o por la parálisis de los músculos involucrados en el habla no se considera afasia.
Además, el déficit debe hallarse relativamente aislado; es decir, el paciente ha de dar la impresión de ser consciente de lo que pasa a su alrededor y de comprender que otros están intentado comunicarse con él.

LATERALIZACIÓN 

La conducta verbal es una función lateralizada ; la mayoría de las alteraciones del lenguaje ocurren como consecuencia de lesiones en el lado izquierdo del cerebro.

La mejor manera para determinar qué lado del cerebro es dominante para el habla es la de administrar el test de Wada, denominado así en honor a su inventor. (Como vimos en el capítulo 15, esta prueba también se utiliza para evaluar funciones mnésicas.) Un paciente que esté a punto de ser sometido a una cirugía que pudiera invadir un área del habla recibe un anestésico de acción breve en una de las arterias carótidas y, más tarde, cuando los efectos del mismo desaparecen, en la otra.
Este procedimiento anestesia primero un hemisferio cerebral y después el otro; de este modo, en pocos minutos se puede valorar la participación de cada uno de los hemisferios en las funciones verbales.
En más del 95 % de las personas diestras, el hemisferio izquierdo es dominante para el habla.
Es decir, cuando se anestesia este hemisferio el sujeto pierde la capacidad de hablar.
Sin embargo, cuando se anestesia el hemisferio derecho, puede seguir hablando y conversando.
Esta cifra es algo inferior en los sujetos zurdos: aproximadamente el 70 % (Rasmussen y Milner, 1977).
Por ello, a menos que se especifique otra cosa, puede asumirse que las lesiones cerebrales descritas en la primera parte de este capítulo se localizan en el hemisferio izquierdo (dominante para el lenguaje).

¿Por qué uno de los hemisferios está especializado en el habla?
Las funciones perceptivas del hemisferio izquierdo están más especializadas en el análisis de secuencias de estímulos, es decir, donde éstos aparecen uno después del otro.
Las funciones perceptivas del hemisferio derecho están más especializadas en el análisis de aspectos espaciales y de formas y figuras geométricas, donde los elementos se presentan simultáneamente.
El habla es secuencial; consiste en secuencias de palabras, las cuales están compuestas, a su vez, por secuencias de sonidos.
Por eso tiene sentido que el hemisferio izquierdo se haya especializado en su percepción.
Además, tal como vimos en el capítulo 8, el hemisferio izquierdo está involucrado en la ejecución de secuencias movimientos voluntarios.
Quizás este hecho explique la localización de los circuitos neurales involucrados en la producción del habla, así como en su percepción, en este hemisferio.

Aunque los circuitos que están primariamente involucrados en la comprensión y producción del habla se localizan en el hemisferio izquierdo, sería erróneo determinar que el hemisferio derecho no interviene en ello.
El habla no consiste sólo en hablar, también implica tener algo que decir.
De manera similar, escuchar no es sólo oír y reconocer palabras, sino comprender el significado de lo que se está diciendo.

Cuando escuchamos y comprendemos palabras, y cuando hablamos o pensamos sobre nuestras propias percepciones o recuerdos, estamos utilizando otros circuitos neurales además de los que están directamente relacionados con el habla.
Así, estos circuitos juegan también un papel en la conducta verbal.
Por ejemplo, la lesión del hemisferio derecho provoca dificultades para interpretar mapas, percibir ilustraciones espaciales y reconocer formas geométricas complejas.
Las personas con este tipo de lesiones también tienen dificultades para hablar de cosas como mapas y formas geométricas complejas, y para comprender lo que otras personas dicen acerca de ello.
El hemisferio derecho también parece estar involucrado en la organización de una narración, en la selección y conjunción de los elementos sobre los que queremos hablar (Gardner y cols., 1983).
Como vimos en el capítulo 11, el hemisferio derecho interviene en la expresión y el reconocimiento de las emociones mediante el tono de voz.
Y como veremos en este capítulo, también participa en el control de la prosodia , es decir, del ritmo y el énfasis normales del habla.
Por consiguiente, ambos hemisferios cerebrales contribuyen a construir nuestras habilidades lingüísticas.

PRODUCCIÓN DEL HABLA 

Ser capaces de hablar (es decir, de producir un habla con significado) requiere varias habilidades.
En primer lugar, la persona debe tener algo de qué hablar.
Pensemos en lo que eso significa.
Podemos hablar de algo que está ocurriendo en este momento o de algo que ocurrió en el pasado.
En el primer caso, hablamos sobre nuestras percepciones: de cosas que vemos, oímos, sentimos, olemos, etc.
En el segundo caso, estamos hablando sobre nuestros recuerdos acerca de lo que ocurrió en el pasado.
Tanto las percepciones de acontecimientos presentes como los recuerdos de acontecimientos que tuvieron lugar en el pasado involucran mecanismos cerebrales de la región posterior de los hemisferios cerebrales (los lóbulos occipital, temporal y parietal).
Por tanto, esta región es en gran parte responsable de nuestra capacidad para tener algo que decir.

Por supuesto, también podemos hablar de algo que no ha ocurrido nunca.
Es decir, podemos utilizar nuestra imaginación para inventarnos una historia (o decir una mentira).
Se saben muy pocas cosas acerca de los mecanismos neurales responsables de la imaginación, pero parece probable que impliquen la participación de los mecanismos responsables de las percepciones y recuerdos; al fin y al cabo, cuando nos inventamos una historia, debemos basarnos en el conocimiento adquirido originalmente a través de la percepción y que hemos retenido en la memoria.

Suponiendo que una persona tenga algo que decir, el hecho de llegar a hacerlo requiere algunas otras funciones cerebrales.

Como veremos en esta sección, la conversión de percepciones, recuerdos y pensamientos en lenguaje nos hace utilizar mecanismos neurales localizados en los lóbulos frontales.

La lesión de una región de la parte inferior del lóbulo frontal izquierdo (el área de Broca) altera la capacidad para hablar: ocasiona afasia de Broca .
Este trastorno se caracteriza por un habla lenta, laboriosa y poco fluida.
Cuando se intenta mantener una conversación con un sujeto con este tipo de afasia, a la mayoría de las personas les resulta difícil no intentar ayudarle sugiriéndole las palabras que obviamente están intentando pronunciar.
Pero aunque la pronunciación de muchas palabras suele ser defectuosa, las palabras que consiguen pronunciar tienen significado.
La región posterior de los hemisferios cerebrales tiene algo que decir, pero la lesión del lóbulo frontal hace que a estos pacientes les resulte difícil expresar estos pensamientos.

A las personas con afasia de Broca les resulta más fácil utilizar algunos tipos de palabras que otros.
Tienen muchas dificultades para utilizar las palabras cortas con significado gramatical, tales como un, el, algunos, en o sobre .
Estas palabras se denominan palabras funcionales , porque ejercen importantes funciones gramaticales.
Las palabras que sí logran utilizar suelen ser casi siempre palabras con contenido (palabras con significado, como nombres, adjetivos y adverbios, tales como manzana, casa, lanzar o pesado ).
A continuación se presenta una muestra del habla de un hombre con afasia de Broca explicando al examinador por que se halla en el hospital.
Como puede verse, sus palabras tienen significado, pero lo que dice no es gramatical.

Los puntos suspensivos indican pausas prolongadas.

Las personas con afasia de Broca pueden comprender el habla mucho mejor de lo que pueden producirlo.
De hecho, algunos observadores consideran que su comprensión no se halla alterada, pero, como veremos, esto no es del todo cierto.
Broca (1861) sugirió que esta forma de afasia se debía a una lesión en la corteza frontal de asociación, justo por delante de la región de la corteza motora primaria correspondiente a la cara.
Investigaciones posteriores han demostrado que, en esencia, esta apreciación es correcta, y ahora esta región recibe el nombre área de Broca (véase la figura 16.2).

Las lesiones causantes de la afasia de Broca están desde luego centradas en proximidad del área de Broca.
Sin embargo, la lesión restringida a la corteza del área de Broca no parece producir este tipo de afasia la lesión ha de extenderse a otras regiones vecinas del lóbulo frontal y a la sustancia blanca subcortical subyacente (H.
Damasio 1989; Naeser y cols., 1989).

Además, existen pruebas de que las lesiones de los ganglios basales (especialmente de la cabeza del núcleo caudado) también pueden producir una afasia similar a la de Broca (Damasio, Eslinger y Adams, 1984); La lámina 16.1 muestra un gráfico promedio de TEP del flujo sanguíneo regional de un grupo de sujetos que estaban leyendo palabras en voz alta (Leblanc y cols., 1992).
Como puede verse la tarea provoca la activación de regiones subcorticales situadas por debajo del área de Broca (incluida la cabeza del núcleo caudado), aparte de la neocorteza (véase la lámina 16.1).

Recientes estudios en los que se ha utilizado la TEP han mostrado que una lesión funcional puede afectar a regiones mucho más extensas que el área de la lesión tisular primaria.
Por ejemplo, Metter (1991) indica que pequeñas lesiones de los ganglios basales y de la sustancia blanca subcortical pueden causar una disminución del metabolismo de una región bastante extensa de la frontal (incluso cuando la autopsia no muestre la pérdida de neuronas corticales).
Además, las lesiones del lóbulo frontal a menudo causan un descenso del metabolismo de los lóbulos temporal y parietal, presumiblemente porque alteran las conexiones entre estas áreas.
Por tanto, la extensión total de la lesión suele subestimarse si sólo se examinan imágenes de TC o de RM.

Wernicke (1874) sugirió que el área de Broca contiene recuerdos motores (en particular, recuerdos de la secuencia de movimientos musculares requeridos para articular palabras ).
Hablar involucra movimientos rápidos de la lengua, los labios y la mandíbula, los cuales deben estar coordinados entre sí y con los de las cuerdas vocales; por tanto, hablar requiere algunos mecanismos de control motor muy sofisticados.
Obviamente, tiene que haber circuitos neuronales en algún lugar de nuestro cerebro que, cuando son adecuadamente activados, pongan en marcha la ejecución de estas secuencias de movimientos.
Dado que la lesión de la parte caudal inferior del lóbulo frontal izquierdo (incluida el área de Broca) altera la capacidad para articular palabras, esta región es la candidata más plausible para la localización de estos «programas».
El hecho de que esta región esté situada justo delante `de la parte de la corteza motora primaria que controla los músculos utilizados para el habla apoya esta conclusión.

Pero las funciones relacionadas con el habla del lóbulo frontal izquierdo incluyen otras cosas aparte de programar los movimientos utilizados para hablar.
La afasia de Broca es mucho más que un déficit de pronunciación.
En general, las lesiones del área de Broca y de las regiones que la rodean producen principalmente tres tipos de déficit: agramaticalidad, anomia y dificultades de articulación .
Aunque la mayoría de los pacientes con afasia de Broca suelen presentar los tres déficit en algún grado, su gravedad puede variar considerablemente de un sujeto a otro (seguramente porque sus lesiones cerebrales difieren).

La agramaticalidad se refiere a la dificultad del paciente para utilizar construcciones gramaticales.
Este trastorno puede aparecer solo, sin que exista ninguna dificultad para la pronunciación de las palabras (Nadeau 1988).
Como vimos, las personas con afasia de Broca pocas veces utilizan términos funcionales.
Además, tampoco suelen utilizar marcadores gramaticales como -aba , ni auxiliares como he (como en he ido ).
Por algún motivo, en inglés sí que suelen utilizar la terminación -ing , quizás porque este sufijo transforma los verbos en nombres.
Un estudio de Saffran, Schwartz y Marin (1980) ilustra esta dificultad.
Las siguientes citas corresponden a pacientes agramaticales que intentan describir dibujos.

El segundo déficit importante del habla que se observa en la afasia de Broca es la anomia (sin nombre).
Anomia se refiere a la dificultad para hallar la palabra adecuada; dado que todos los afásicos omiten palabras o utilizan términos inadecuados, la anomia constituye de hecho un síntoma primario de todas las formas de afasia.

Sin embargo, como el habla de los sujetos con afasia de Broca es poco fluida, la anomia de estas personas resulta especialmente destacable; su expresión facial y el uso frecuente de sonidos como «eh» nos indica con claridad que estas personas hacen un esfuerzo por encontrar las palabras adecuadas.

La tercera característica importante de la afasia de Broca es la existencia de dificultades articulatorias .
Los pacientes pronuncian mal las palabras, a menudo alterando la secuencia de sonidos.
Por ejemplo, la palabra cuerpo puede pronunciarse «cuepro».
Al contrario que los sujetos con afasia de Wernicke, las personas con afasia de Broca reconocen que su pronunciación es incorrecta y suelen intentar corregirla.

Como dijimos antes, estos tres déficit se observan en combinaciones diferentes en cada paciente, dependiendo de la localización exacta de la lesión y, en cierta medida, de la etapa de recuperación en que se halle.

Aunque no se han descifrado los correlatos anatómicos de cada una de estas alteraciones, podemos caracterizarlas de un modo jerárquico.
En el nivel inferior, más elemental, se halla el control de la secuencia de movimientos de los músculos del habla.
El siguiente nivel es el de la selección de «programas» específicos para cada palabra individual; la afectación de esta capacidad provoca anomia.
Finalmente, el nivel más elevado es la selección de la estructura gramatical, como el orden de las palabras, el uso de términos funcionales y los sufijos.
Presumiblemente, el control de la articulación involucra el área de la cara de la corteza motora primaria y algunas áreas de los ganglios basales; la selección de palabras, su orden y el uso de marcadores gramaticales supone la actividad del área de Broca y de otras regiones de la corteza frontal de asociación.

Hasta el momento hemos descrito la afasia de Broca como un trastorno de la producción del habla.
En una conversación ordinaria, estos afásicos parecen entender todo lo que se les dice.
Parecen irritados y enojados por su incapacidad para expresar bien sus pensamientos, y a menudo hacen gestos para complementar su escaso lenguaje verbal.

La considerable disparidad entre la producción y la comprensión del habla hace que se asuma con demasiada rapidez que su comprensión es normal.
Pero no lo es.

Schwartz, Saffran y Marin (1980) les mostraron a pacientes con afasia de Broca unos pares de dibujos en los que los sujetos y los objetos de la acción estaban invertidos: por ejemplo un caballo que daba coces a una vaca y vaca que daba coces a un caballo, un camión que empujaba un coche y un coche que empujaba un camión, o una bailarina aplaudiendo a un payaso y un payaso aplaudiendo a una bailarina.
Ante cada uno de los pares de dibujos que enseñaban a los pacientes, leía una frase, por ejemplo, «el caballo da coces a la vaca».
La tarea del sujeto consistía en señalar el dibujo adecuado, con lo cual daba a entender si comprendía la construcción agramatical de la frase (véase la figura 16.3).
Su ejecución era muy pobre.

El dibujo correcto del estudio de Schwartz y sus colegas venía especificado por un aspecto particular de la gramática: el orden de las palabras.
La agramaticalidad que acompaña a la afasia de Broca parece alterar la capacidad de los pacientes para utilizar información gramatical, como el orden de las palabras, para decodificar el significado de la frase.
Por ello, su déficit de comprensión es paralelo a su déficit de producción.
Si oyen una frase como «el hombre aplasta al mosquito», entienden que hay un hombre y un mosquito, así como la acción de aplastar.

Obviamente, no resulta nada difícil imaginar quién es el que hace qué a quién.
Pero una frase como «el caballo da coces a la vaca» no ofrece ninguna pista extra; si no se entiende la estructura gramatical de la frase, tampoco se comprende su significado.

Otros experimentos han puesto de manifiesto que las personas con afasia de Broca tienen dificultades para ejecutar una secuencia de órdenes como «coge el círculo rojo y toca el cuadrado verde con él» (Boller y Denis, 1979).

Este hallazgo, junto con los otros síntomas descritos en esta sección, sugieren que una importante función del lóbulo frontal izquierdo es la secuenciación - de movimientos de los músculos del habla (al producir palabras), y de palabras (comprensión y producción del habla gramatical)-.

Trastornos mentales: trastornos por ansiedad, autismo y adicción 

Hasta hace pocos años, los temas que se tratan que este capítulo no hubieran sido discutidos en un libro de fisiología de la conducta.
Se creía que los trastornos por ansiedad y el autismo se aprendían, principalmente debido a errores de los padres en la educación de sus hijos.
A pesar de que siempre hubo al menos algún respaldo a favor de la idea de que las psicosis graves, como la esquizofrenia, tenían una base biológica, otros trastornos mentales eran prácticamente considerados como de origen psicogénico -es decir, producidos por factores «psicológicos -.
Por su parte, la adicción era considerada como una falta de voluntad, provocada por situaciones de desesperación, una sociedad permisiva y (obviamente) por los padres.

Estas tendencias han cambiado (o el péndulo se ha balanceado hacia el otro extremo, si se prefiere la metáfora).
Obviamente, el contexto familiar de una persona, la clase social, el estatus económico y otros factores similares afectan a la probabilidad de que una persona desarrolle un trastorno mental, pudiendo también contribuir a su recuperación.
Pero los factores fisiológicos, tanto los heredados como los que afectan desfavorablemente al desarrollo del cerebro o los que lo dañan, también pueden contribuir de forma muy importante.
Este capítulo se centra en las investigaciones realizadas sobre estos factores fisiológicos.

Trastornos por ansiedad 

Hemos visto que los trastornos afectivos se caracterizan por emociones extremas no realistas: depresión o euforia (manía).
Los trastornos por ansiedad se caracterizan por miedo y ansiedad infundados y no realistas.
En esta sección describiremos dos de estos trastornos que parecen tener causas biológicas: la angustia y el trastorno obsesivo compulsivo.

TRASTORNO DE ANGUSTIA 

Descripción 

Las personas con trastorno de angustia sufren episodios de ataques de ansiedad aguda -períodos de terror agudo y sostenido de duración variable, desde unos pocos segundos a varias horas-.
La incidencia estimada de la angustia afecta del 1 al 2 % de la población (Robbins y cols., 1984).
Las mujeres tienen el doble de probabilidades que los hombres de sufrir este trastorno.
Normalmente, aparece en el inicio de la edad adulta; raramente empieza después de los treinta y cinco años (Woodruff, Guze y Clayton, 1972).

Las crisis de angustia conllevan muchos síntomas físicos, tales como dificultades respiratorias, sudor frío, irregularidades en los latidos cardíacos, vértigo, debilidad y sensaciones de irrealidad.
Las víctimas de crisis de angustia a menudo tienen la sensación de que se van a morir.
La ansiedad es una reacción normal frente a muchas situaciones estresantes de la vida, y ninguno de nosotros está completamente libre de ella.
De hecho, la ansiedad es indudablemente útil para hacernos estar más alerta y tomarnos en serio las cosas importantes.
Sin embargo, la ansiedad que todos sentimos de vez en cuando es, obviamente, diferente del miedo y del terror intensos que experimenta una persona presa de una crisis de angustia.

Entre las diferentes crisis de angustia, muchas personas sufren ansiedad anticipatoria : el miedo a sufrir otra crisis de angustia.
A menudo, este tipo de ansiedad da lugar al desarrollo de un grave trastorno fóbico: la agorafobia ( agora significa «espacio abierto»).
De acuerdo con el Diagnostic and Statistical Manual III-R oficial de la Asociación Americana de Psiquiatría, la agorafobia asociada a las crisis de angustia es un miedo a «estar en lugares o situaciones de los cuales es difícil (o embarazoso) escapar, o en los cuales no se puede conseguir ayuda en el caso de que tenga lugar una crisis de angustia...
Como resultado de este miedo, la persona restringe sus salidas o necesita que alguien la acompañe cuando sale de casa».
La agorafobia puede resultar gravemente incapacitante: algunas personas con este trastorno han permanecido en sus casas o apartamentos durante años, temerosos de aventurarse a salir al exterior.

Posibles causas 

Dado que los síntomas físicos de las crisis de angustia son tan abrumadores, muchos pacientes rechazan la idea de que padecen un trastorno mental, insistiendo en que su problema es médico.
De hecho, pueden estar en lo cierto: numerosas pruebas sugieren que el trastorno de angustia puede tener un origen biológico.
En primer lugar, parece ser hereditario; así, existe una mayor tasa de concordancia para este trastorno entre gemelos monocigóticos que entre dicigóticos (Slater y Shields, 1969), y casi el 30 % de los parientes de primer grado de los sujetos con angustia padecen también este trastorno (Crowe y cols., 1983).
Su distribución en el árbol genealógico de una familia sugiere que está causado por un único gen dominante (Crowe y cols., 1987).

En las personas que tienen antecedentes de trastorno de angustia, las crisis pueden desencadenarse aplicándoles inyecciones de ácido láctico (un producto derivado de la actividad muscular), o haciéndoles respirar aire que contenga una elevada cantidad de dióxido de carbono (Gaffney y cols., 1988; Woods y cols., 1988).
Cowley y Arana (1990) indican que entre el 40 y el 60 % de las personas con dichos antecedentes reaccionarán a una inyección en un determinado momento.
Una inyección de ácido láctico (o de una de sus sales, el lactato sódico) parece producir arousal fisiológico, especialmente en aquellas personas que padecen trastorno de angustia.
Koeningsberg y cols. (1992) administraron inyecciones endovenosas de lactato sódico a sujetos mientras dormían (mediante catéteres previamente insertados).
Observaron que la inyección despertaba a un número significativamente mayor de sujetos que padecían trastorno de angustia que de sujetos normales de control.

Algunos investigadores han sugerido que las personas que padecen crisis de angustia simplemente tienen un sistema nervioso autónomo más reactivo.
Es decir, un acontecimiento estresante les produce una reacción más intensa de la rama simpática del SNA, dando lugar a una respuesta emocional que el paciente interpreta como una crisis médica.
Sin embargo, Roth y cols. (1992) no observaron ninguna evidencia a favor de este aumento de reactividad autonómica.
Estos investigado res hicieron respirar aire que contenía un 5 % de dióxido de carbono a sujetos normales y a sujetos que sufrían trastorno de angustia.
Un 46 % de los sujetos que padecían este trastorno informaron de que sufrieron una crisis de angustia; mientras que ninguno de los sujetos normales padeció alteración alguna Sin embargo, las medidas fisiológicas del arousal autonómico no mostraron diferencias entre ambos grupos de sujetos en cuanto a su reactividad.
Estas medidas mostraron, en cambio, que los sujetos con el trastorno tenían niveles superiores de arousal incluso antes de hace respirar el dióxido de carbono, de forma que se mostraban crónicamente temerosos.

La susceptibilidad a las crisis de angustia inducidas por lactato parece ser, al menos en parte, hereditaria (Balon y cols. (1989) administraron lactato sódico a 45 sujetos normales y observaron que 10 de ellos tuvieron crisis de angustia.
Los investigadores obtuvieron la historia familiar de los sujetos mediante un entrevistador que no sabía cuáles eran los que habían tenido crisis de angustia.
Hallaron que más del 24 % de los parientes de los sujetos que reaccionaron a la inyección con crisis de angustia tenían un historial de trastornos por ansiedad, comparado con menos del 8 % de los sujetos no reactivos.

En varios estudios con TEP se ha medido el flujo sanguíneo cerebral durante las crisis de angustia desencadenadas por una inyección de lactato (Reiman y cols. (1986) observaron que la actividad de la circunvolución parahipocampal aumentaba inmediatamente antes de producirse la crisis de angustia, mientras que la de los polos anteriores de los lóbulos temporales lo hacía durante el propio ataque.
Reiman y cols. (1989) produjeron ansiedad anticipatoria en sujetos normales haciéndoles creer que estaban a punto de recibir una descarga eléctrica muy dolorosa. (Los sujetos recibieron una débil descarga al principio del experimento para que se creyeran lo que les decían los experimentadores, pero en realidad no llegaron a recibir la descarga intensa.) La TEP mostró que la ansiedad producía un incremento de la actividad de los polos temporales también en los sujetos normales (véase la lámina 18.1).

Reiman y sus colaboradores señalan que los estudios con animales de laboratorio también sugieren que los polos temporales están involucrados en las reacciones de ansiedad.
Por ejemplo, la estimulación de esta región en monos produce respuestas autonómicas y expresiones faciales de miedo, y los humanos con epilepsia debida a un foco en los lóbulos temporales anteriores a menudo informan de sentimientos de ansiedad y miedo justo antes de producirse las convulsiones.
Naturalmente, aunque la corteza temporal anterior esté involucrada en una reacción de ansiedad, no tenemos ninguna razón para sospechar que el trastorno de angustia esté causado por una anormalidad en esa región; la reacción podría estar provocada por una anomalía en cualquier otro lugar del cerebro.
Como vimos en el capítulo 11, los lóbulos temporales anteriores envían a la amígdala información relativa a estímulos con una significación emocional -como los estímulos que indican peligro-.
A pesar de que los estudios de neuroimágenes realizados en humanos no han implicado a la amígdala en el trastorno de angustia, dada la importancia de esta estructura en las respuestas emocionales negativas es difícil suponer que no esté relacionada con dicho trastorno.

El tratamiento de los trastornos por ansiedad consiste normalmente en una combinación de terapia de conducta y benzodiacepinas.
Como vimos en el capítulo 3, las benzodiacepinas tienen un intenso efecto ansiolítico («disolvente de la ansiedad»).
El cerebro posee receptores para las benzodiacepinas, que son una parte del complejo receptor GABA.
Cuando un agonista benzodiacepínico se une a su receptor aumenta la sensibilidad del lugar de unión del GABA y produce un efecto ansiolítico.
Por otro lado, cuando un agonista inverso benzodiacepínico ocupa el lugar receptor reduce la sensibilidad del lugar de unión del GABA y aumenta la ansiedad.
De este modo, los trastornos por ansiedad podrían estar causados por una disminución del número de receptores benzodiacepínicos o por la secreción de un neuromodulador que actuara como un agonista inverso en los receptores benzodiacepínicos.

Como vimos en el capítulo 17, las ratas que reciben un medicamento antidepresivo (la clomipramina) a una edad temprana desarrollan posteriormente síntomas de depresión, que pueden ser reducidos mediante un fármaco antidepresivo o por privación de sueño paradójico.
De forma parecida, la administración prenatal de un tranquilizante benzodiacepínico puede hacer que los gatos se vuelvan temerosos.
Marczynski y Urbancic (1988) administraron inyecciones de diacepam (Valium) a gatas gestantes y valoraron la temerosidad de las crías cuando tuvieron un año de edad.
Observaron que los animales mostraban inquietud y ansiedad ante situaciones nuevas.
Este temor puede reducirse mediante una inyección de diacepam.
A continuación, midieron el nivel de receptores benzodiacepínicos en el cerebro de los animales y observaron una disminución en el hipotálamo, la corteza frontal, la corteza parietal anterior y la línea media del tálamo.
Así, la temerosidad parece estar asociada con una disminución en el número de receptores benzodiacepínicos y, presumiblemente, con una menor sensibilidad al agonista benzodiacepínico endógeno, sea éste el que sea.

Otras dos sustancias que han sido implicadas en la angustia son la colecistoquinina y la serotonina.
Como vimos en el capítulo 13, la colecistoquinina (CCK) podría estar relacionada con la saciedad.
Este péptido se sintetiza en células del duodeno y también en neuronas cerebrales, donde se libera conjuntamente con neurotrasmisores como la dopamina.
En una revisión bibliográfica, Bradwejn y cols. (1992) concluyeron que existían claras pruebas a favor de la implicación de la CCK en la ansiedad.
En un estudio de doble ciego, Bradwejn, Koszycki y Meterissian (1990) observaron que una inyección de CCK-4 (una forma de CCK que atraviesa la barrera hematoencefálica) desencadenaba crisis de angustia en sujetos que padecían de ese trastorno, pero no tenía efectos en sujetos normales de control.
Otro estudio realizado con ratas puso de manifiesto que la inyección directa de CCK en la amígdala provocaba reacciones de ansiedad, que podían ser bloqueadas por la inyección de una benzodiacepina (Csonka y cols., 1988).

Como hemos visto anteriormente en este mismo capítulo, la serotonina parece: estar implicada en la depresión.
Las pruebas sugieren que la serotonina también podría estar relacionada con la angustia.
A pesar de que los síntomas de ésta y del trastorno obsesivo compulsivo (descrito en el siguiente apartado) son diferentes, las drogas que actúan como agonistas serotonérgicos han resultado efectivas para el tratamiento de ambos trastornos (Coplan, Gorman y Klein, 1992).
Así pues, las tres sustancias citadas: las benzodiacepinas, la CCK y la serotonina, parecen estar involucradas en la ansiedad.
Su función específica y la forma en que estas tres sustancias pueden interactuar aún nos resultan desconocidas.

TRASTORNO OBSESIVO COMPULSIVO 

Descripción 

Como su nombre indica, las personas con un trastorno obsesivo compulsivo padecen obsesiones (pensamientos que no pueden apartar de sus mentes) y compulsiones (conductas que no pueden abstenerse de realizar).
Las obsesiones se dan en diferentes trastornos mentales, como la esquizofrenia.
Sin embargo, a diferencia de ésta, las personas con trastorno obsesivo compulsivo reconocen que sus pensamientos y sus conductas no tienen sentido y desean desesperadamente librarse de ellos.
Con frecuencia las compulsiones se vuelven progresivamente más absorbentes, hasta que interfieren con la carrera profesional y la vida cotidiana de las personas.

La incidencia del trastorno obsesivo compulsivo es de un 1 o un 2 %.
Las mujeres tienen una probabilidad ligeramente mayor que los varones de sufrir este trastorno.
Al igual que el trastorno de angustia, por lo general el trastorno obsesivo compulsivo se inicia al comienzo de la edad adulta (Robbins y cols, 1984).

Estudios transculturales han puesto de manifiesto que los síntomas de este trastorno son similares en varios grupos raciales y étnicos (Akhtar y cols., 1975; Khanna y Channabasavanna, 1987; Hinjo y cols., 1989).
Las personas que padecen esta enfermedad raramente se casan, quizás debido a su frecuente miedo obsesivo a la suciedad y la contaminación, o a la vergüenza asociada rituales que se ven forzados a realizar, que les hacen evitar los contactos sociales (Turner, Beidel y Nathan, 1985).

La mayoría de las compulsiones pueden ser incluidas en una de las cuatro categorías siguientes: contar, comprobar, limpiar y evitar .
Por ejemplo, los individuos pueden comprobar repetidamente que los quemadores de la cocina estén apagados y que las ventanas y las cerraduras estén cerradas.
Davidson y Neale (1974) narran el caso de una mujer que se lavaba las manos más de quinientas veces al día porque tenía miedo de contaminarse con gérmenes.
Esta conducta persistió incluso cuando sus manos se cubrieron de dolorosas llagas.
Otras personas limpian meticulosamente su apartamento o lavan, secan y tienden sus ropas constantemente.
Algunos sujetos tienen miedo de salir de casa porque temen contaminarse y rehúsan tocar a otros miembros de su familia.
Si accidentalmente se «contaminan», generalmente llevan a cabo largos rituales de purificación (véase la tabla 18.1).

Algunos investigadores creen que las conductas compulsivas observadas en el trastorno obsesivo compulsivo son formas de conductas típicas de la especie (por ejemplo, el aseo personal, limpiar y prestar atención a las fuentes de peligro potencial), que, debido a una disfunción cerebral, se han liberado de los mecanismos normales de control (Wise y Rapoport, 1988).

Posibles causas 

Se están empezando a acumular datos que sugieren que el trastorno obsesivo compulsivo puede tener un origen genético.
Los estudios de familias han puesto de manifiesto que este trastorno está asociado con una enfermedad neurológica que aparece durante la infancia (Pauls y Leckman, 1986; Pauls y cols., 1986).
Esta enfermedad, el síndrome de Tourette , se caracteriza por tics musculares y vocales: muecas faciales, ponerse en cuclillas, marcar el ritmo, dar vueltas rápidas sobre sí mismo, escupir, sorber por la nariz, toser, gruñir o repetir determinadas palabras (especialmente vulgaridades).
Leonard y cols. (1992a, 1992b) observaron que la mayoría de los pacientes con trastorno obsesivo compulsivo presentaba tics, y que muchos pacientes con el síndrome de Tourette mostraban obsesiones y compulsiones.
Ambos grupos de investigadores consideran que los dos trastornos están provocados por las mismas causas subyacentes, que podrían ser el resultado de un único gen dominante.
No está claro por qué algunas personas con este gen defectuoso desarrollan el síndrome de Tourette en la infancia temprana, mientras que otras desarrollan un trastorno obsesivo compulsivo en una edad más avanzada.

Tal como ocurre en la esquizofrenia, no todos los casos de trastorno obsesivo compulsivo tienen un origen genético: en algunas ocasiones esta alteración ocurre después de alguna lesión cerebral de diferente origen, como un trauma durante el nacimiento, una encefalitis o un golpe en la cabeza (Hollander y cols., 1990).
En particular, los síntomas parecen estar asociados con la lesión o disfunción de los ganglios basales, la circunvolución cingulada y la corteza prefrontal.
Varios investigadores han observado que el trastorno obsesivo compulsivo algunas veces va asociado a las coreas de Huntington y de Sydenhams que implican degeneración de los ganglios basales (Swedo y cols., 1989b; Cummings y Cunningham, 1992).

Laplane y cols. (1989) observaron que la lesión de los ganglios basales (provocada por anoxia o por sustancias tóxicas algunas veces producía síntomas obsesivos y compulsivos.

En varios estudios con TEP se han observado incrementos del metabolismo de la glucosa en los lóbulos frontales y en la circunvolución cingulada (Baxtcr y cols., 1987, 1989; Swedo y cols., 1989c; Rubin y cols., 1992).
Mientras que algunos estudios han observado una actividad disminuida en Los ganglios basales (que coincide con los resultados de los estudios de lesión que acabamos de citar), otros han observado un incremento .
Como vimos en el capítulo 11, la corteza prefrontal (concretamente la corteza orbitofrontal) y la corteza cingulada están implicadas en las reacciones emocionales, por lo que no es sorprendente suponer que pudieran estar implicadas también en el trastorno obsesivo compulsivo.
De hecho, algunos pacientes con un grave trastorno obsesivo compulsivo han sido tratados con éxito mediante la destrucción quirúrgica del fascículo del cíngulo, un grupo de axones que conectan las cortezas prefrontal y cingulada con la límbica del lóbulo temporal (Hallantine y cols., 1987).
Obviamente, como la lesión cerebral no es reversible, estas operaciones sólo se realizan en casos graves, después de haberse demostrado que las terapias conductuales y farmacológicas son inefectivas.

Swedo y cols. (1992) midieron, mediante TISP, el flujo sanguíneo regional cerebral de pacientes que sufrían de trastorno obsesivo compulsivo, antes de ser tratados farmacológicamente y un año después del tratamiento.
Estos investigadores observaron una correlación entre el aumento de síntomas y la reducción de actividad en la corteza orbitofrontal.
Estos resultados proporcionan fuertes pruebas a favor de que la corteza prefrontal está implicada de forma importante en este trastorno.

De momento, el tratamiento más eficaz del trastorno obsesivo compulsivo es la terapia, farmacológica.
Hasta ahora, se han identificado tres fármacos eficaces: la clomipramina, la fluoxetina y la fluvoxamina.
Aunque estos fármacos también son antidepresivos eficaces, su acción antidepresiva no parece estar relacionada con su capacidad para aliviar los síntomas del trastorno obsesivo compulsivo.
Por ejemplo, Leonard y cols. (1489) compararon los efectos de la clomipramina y desimipramina (un fármaco antidepresivo) en el tratamiento de los síntomas de niños y adolescentes con trastorno obsesivo compulsivo grave.

Durante tres semanas todos los pacientes recibieron placebo.

Después, durante cinco semanas, la mitad recibieron clomipramina (CMl) y la otra mitad desimipramina (DMI), siguiendo un procedimiento de doble ciego.
Al cabo de este tiempo, los fármacos se intercambiaron.
Como muestra la figura 18.1, la CMI resultó mucho más efectiva; de hecho, los pacientes a los que se cambió de CMI a DMI mostraron un empeoramiento de sus síntomas (véase la figura 18.1).

Todos los fármacos antiobsesivos eficaces bloquean específicamente la recaptación de 5-HT; por tanto, son agonistas serotonérgicos específicos.
Cuando se administra un antagonista serotonérgico a los pacientes, sus síntomas empeoran (Hollander y cols., 1992).
En general, la serotonina tiene un efecto inhibitorio sobre las conductas típicas de una especie, lo cual ha hecho que varios investigadores sugirieran que esos fármacos alivian los síntomas del trastorno obsesivo compulsivo, reduciendo la fuerza de las conductas de aseo, limpieza y evitación de peligro que subyacen a este trastorno.

La importancia de la actividad serotonérgica en la inhibición de las conductas compulsivas se ha subrayado en tres compulsiones interesantes: la tricotilomanía, la onicofagia y la acrodermatitis (dermatitis de las extremidades).

La tricotilomanía consiste en darse estirones del cabello.
Las personas que sufren este trastorno (casi siempre mujeres) a menudo pasan horas por la noche, arrancándose los cabellos uno a uno, y algunas veces comiéndoselos (Rapoport, 1991).
La onicofagia es una conducta compulsiva consistente en comerse las uñas, que, en caso extremo, puede provocar graves lesiones en las puntas de los dedos. (El morderse las uñas de los pies no es raro.) Estudios de doble ciego han puesto de manifiesto que estos dos trastornos pueden tratarse eficazmente con clomipramina, un fármaco utilizado en el tratamiento del trastorno obsesivo compulsivo (Swedo y cols., 1989b; Leonard y cols., 1992).

La acrodermatitis causada por lameduras es una enfermedad de los perros, no de los humanos.
Algunos perros se lamen continuamente alguna parte de su cuerpo, especialmente sus muñecas o tobillos (denominados el carpe y el corvejón ).
Las lameduras hacen caer el pelo y, a menudo, erosionan también la piel.
El trastorno parece tener un origen genético: se observa casi exclusivamente en razas de cierto tamaño como el gran danés, el perro labrador y el pastor alemán, y viene de familia.
En un estudio de doble ciego se observó que la clomipramina reduce esta conducta compulsiva (Rapoport, Ryland y Kriete, 1992).
Al principio, cuando leí el término «doble ciego» en el trabajo de Rapoport y sus colaboradores, me divertí pensando que los investigadores iban con cuidado para que los perros no se dieran cuenta de si les estaban administrando clomipramina o un placebo.

Luego, me dí cuenta de que, por supuesto, eran los propietarios de los perros los que desconocían la sustancia que les estaban administrando.


¿Sobreviviremos?

Dado que huésped y microorganismo patógeno evolucionan a la par, ¿conservará el sistema inmunitario su predominio?

La especie humana ha existido en una forma muy parecida a la actual durante unos 200.000 años.
A lo largo de este tiempo cabe suponer que el sistema inmunitario ha desempeñado un papel decisivo en nuestra capacidad de resistir la exposición a parásitos, bacterias, virus, toxinas y otros factores de riesgo que se hacen fuertes por interacción con la bioquímica de nuestro organismo.

Debe advertirse que esta relación ha sido mutua y permanente, en la que los dos bandos han ido adaptándose por medios muy diversos, desde la guerra declarada hasta la acomodación e incluso la simbiosis.
A medida que la relación ha seguido su curso, ni nosotros ni nuestros cohabitantes nos hemos estancado, hablando en términos evolutivos.
Tan larga carrera debería por sí sola darnos confianza en que nuestra especie seguirá sobreviviendo, al menos en lo que se refiere al mundo microbiano.
Pero tal optimismo podría convertirse fácilmente en tonadilla silbada al atravesar un cementerio, como si no fuera con nosotros.

La sombra de duda la han traído ciertos cambios operados en aspectos clave de las condiciones de vida de la humanidad. 
Nuestro sistema inmunitario se enfrenta hoy a desafíos más terribles que nunca.
Durante quizás un siglo-intervalo insignificante en una perspectiva evolutiva- alrededor del 20 por ciento de la humanidad ha vivido en la moderna sociedad industrial, un entorno artificial que hemos construido, notablemente libre de los parásitos y gérmenes patógenos que desencadenan la respuesta de nuestro sistema inmunitario.
¡Pues muy bien! se dirá acaso. 
Sin embargo, conviene preguntarse por el efecto de subempleo que semejante progreso haya tenido sobre nuestras defensas.

Puede que el sistema inmunitario no disfrute ya del lujo de nuestra época.
El transporte aéreo, el mayor crecimiento de la población y las megaciudades han aumentado muchísimo la facilidad con que las personas quedan expuestas a la acción de los patógenos. 
Por su parte, los microbios no se han dormido sobre sus laureles: continuamente están apareciendo otros nuevos. 
Existe hoy un virus que ataca a las defensas mismas de las que dependemos para sobrevivir: el virus de la inmunodeficiencia humana (VIH).

La bacteria Legionella , la espiroqueta de la enfermedad de Lyme y el virus de la fiebre del valle del Rift son otros ejemplos de la capacidad de la naturaleza para poner radicalmente en jaque a nuestro sistema inmunitario. 
Hasta los viejos gérmenes patógenos se inventan nuevas mañas: ciertas cepas, evolucionadas y drogorresistentes, del bacilo de la tuberculosis acaban de hacer estragos en centros urbanos industriales.
¿Terminarán esas tendencias con el confortable punto muerto al que se había llegado? 
¿Seguirán coexistiendo Homo sapiens y microbios, o se alzará un bando con la victoria?

Una parte importante de la respuesta a esta pregunta la encontramos en la historia evolutiva del sistema inmunitario. 
Hemos de volver la vista atrás algunos cientos de millones de años, hasta la época en que los primeros vertebrados evolucionaron a partir de sus antepasados invertebrados, pues fue entonces cuando apareció el sistema inmunitario.
La investigación descubre un hecho significativo:
se ha recurrido siempre al sistema inmunitario con el fin exclusivo de defender el organismo contra la infección. 

Ningún otro factor externo lo configuró. 
Esta observación vienen hoy a confirmarla esos experimentos de la naturaleza, rarísimos, en los que nacen niños con un sistema inmunitario disfuncional por culpa de la mutación de un gen decisivo.

Si no reciben el tratamiento adecuado, morirán de alguna infección.
Sólo pueden sobrevivir en el ambiente estéril de la burbuja, una cámara aislante.
El mismo experimento natural demuestra que la protección contra el crecimiento aberrante dentro del huésped no afecta al desarrollo del sistema inmunitario. 

En los bebés burbuja y en sus correlatos animales -ratones con un defecto congénito del timo, por ejemplo- no se da una alta incidencia de la mayoría de los tipos de cáncer.

Estos experimentos naturales descartan otras dos funciones que se habían esgrimido.
Algunos investigadores han propuesto que el sistema inmunitario estimula la formación de glóbulos rojos.
Sin embargo, los partícipes del experimento natural siguen produciendo cantidades normales de eritrocitos.
Otros estudiosos han sugerido que el sistema inmunitario puede evitar ciertas formas de esterilidad defendiéndose contra los efectos de los leucocitos paternos.
Pero ocurre que los ratones con un defecto congénito del timo se reproducen perfectamente si se los mantiene en un ambiente semiestéril. 

Estos hallazgos no significan necesariamente que resulte imposible conseguir que el sistema inmunitario ataque a las células cancerosas o facilite la reproducción.
Pero hay que ser realistas:
costará bastante inducirle a que lleve a cabo tareas a las que no se ha ido acostumbrando en el curso de la evolución.

Además de la unicidad de su misión, la consideración de la historia del sistema inmunitario nos revela un segundo rasgo principal:
al parecer, este sistema ha evolucionado siguiendo un proceso de elaboración.

Se ha hecho con ese poder protector mediante la incorporación de diversas defensas que existían en los invertebrados. 
No se limitó a desechar aquellos mecanismos y sustituirlos por algo mejor.

Entre los sistemas perfectamente operativos que protegen a los invertebrados se cuentan los fagocitos deambulantes y las proteínas que flotan libres en los fluidos corporales; ambos pueden adherirse a las bacterias invasoras.
Estas defensas están capacitadas para cumplir la mayoría de las facciones de un sistema inmunitario maduro, si exceptuamos la de establecer una respuesta reforzada contra invasores conocidos de antemano.
Las defensas ancestrales carecen de memoria específica, nota distintiva de un sistema inmunitario propiamente dicho.

Habiendo evolucionado en presencia de esas defensas no adaptativas, y remotas en su origen, el sistema inmunitario incorpora algunos elementos de éstas para sus propios fines.
No es concesión a la fantasía el remontarse hasta aquellas formas ancestrales para descubrir el origen del sistema del complemento o de las células presentadoras de antígenos. 
Se encuentran moléculas tan antiguas como los receptores del complemento en los linfocitos modernos.
Nuestros macrófagos lucen moléculas del complejo principal de histocompatibilidad (MHC), estructuras que proceden de las células fagocíticas de los invertebrados. 

El sistema inmunitario parece haber creado su capacidad de reconocer los antígenos y organizar ataques contra ellos evolucionando desde proteínas defensivas "mudas" (las del complemento) e incorporándolas.
El sistema nervioso siguió un curso evolutivo muy similar.
Se hizo más enrevesado, a buen seguro, para llevar a cabo funciones de mayor complejidad.
Así, su porción más antigua, el tallo cerebral, interviene en las funciones automáticas, como el latido y la respiración;
el cerebelo rige los movimientos motores complejos, mientras que el cerebro funciona como la sede de la consciencia, interviene en la percepción y coordina todo el conjunto.

La semejanza entre los dos sistemas se extiende a la estructura y a la función.
En la levadura, organismo antiquísimo, se reconoce ya un citoesqueleto constituido por una disposición característica de proteínas especializadas.
El citoesqueleto no se limita a servir de sostén pasivo de la estructura celular;
en el caso de nuestras neuronas, por ejemplo, es la maquinaria que mueve, desde el interior celular hasta la sinapsis, las vesículas portadoras de los transmisores; 
en el caso del sistema inmunitario, las células presentadoras de antígenos acuden a un método similar para ingerir el material foráneo, fragmentarlo y presentar sus componentes esenciales al mundo exterior.

Por desgracia, nos es imposible rastrear la mayoría de los pasos que fue dando en su evolución el sistema inmunitario.
Casi todos los avances decisivos acontecieron, tal parece, en una fase muy primitiva de la evolución de los vertebrados, estadio que se halla escasamente representado en el registro fósil y del cual sobreviven pocas especies. 
Hasta los vertebrados más primitivos de los hoy existentes parecen reordenar sus genes codificantes de los receptores de antígenos y poseer células T y B distintas, así como moléculas de MHC.
Así pues, el sistema inmunitario surgió con todos sus pertrechos.

Es una pena que haya persistido un número tan exiguo de experimentos evolutivos de la transición hacia el mundo vertebrado.

Sólo en el caso de las inmunoglobulinas podemos discernir bastantes vestigios de una secuencia evolutiva.

Según cabe presumir, el primer sistema constaría exclusivamente de un gen productor de inmunoglobulina.

Este se reprodujo rápidamente, estableciendo una serie de duplicados, cada uno de los cuales formaría una molécula diferente de inmunoglobulina.
Emergerían después los mecanismos de control que pudieron dirigir la producción de segmentos génicos separados que estarían capacitados para recombinarse.
Hasta los humanos conservan en un rincón de su sistema inmunitario algo de la disposición primitiva: los genes que controlan la síntesis de la cadena ligera lambda de las inmunoglobulinas, una forma de la cual consta de sólo dos miembros.

Las moléculas de inmunoglobulina, las proteínas más características del sistema inmunitario, se combinan con el antígeno para señalar su presencia a otros leucocitos o para iniciar las destructivas reacciones en cadena del complemento.
Tales proteínas pueden estar agrupadas en superfamilias.
Se ha observado que la mosca de la fruta, Drosophila , pone en juego moléculas similares a las de las superfamilias de la inmunoglobulina, si bien no poseen función inmunitaria; 
se trata de moléculas de adhesión celular que dirigen el crecimiento de las neuronas en las larvas.

Ciertas moléculas de interés inmunitario son aún más antiguas.

Fijémonos en las bacterias. que poseen proteínas transportadoras homólogas a las que nuestras propias células emplean para acarrear péptidos hasta las moléculas de MHC.
Una vez aquí, las proteínas cargan los péptidos en el surco de presentación del antígeno. 
El rasgo más notorio del sistema inmunitario es que sus proteínas se han diversificado y especializado hasta un punto sin precedentes, excepto en el sistema nervioso. 
Son contadas las moléculas del sistema inmunitario realmente nuevas.
La selección natural -o, para decirlo con Francis Crick. el "hojalatero"-necesita sólo un número limitado de piezas para su labor componedora. 

La investigación sobre el sistema inmunitario de los peces y los anfibios acaba de descubrir otro interesante superviviente.
Estos vertebrados de sangre fría tienen sistemas inmunitarios que responden con pausada lentitud, equipados con receptores de antígenos poco diversificados; por botón de muestra, el renacuajo de la rana común sólo produce unas 100 moléculas de anticuerpo diferentes. 
Por otro lado, las ranas tienen unos potentes péptidos defensivos, las magaininas, que perforan las paredes de las células bacterianas.
Moléculas que funcionan igual que las magaininas, tales como las escualaminas de los tiburones y las cecropinas sintetizadas por los insectos (y quizá por otras muchas especies), se han encontrado en todo el reino animal... incluso en criaturas de sangre caliente.

A medida que el sistema inmunitario fue ganando en importancia y complejidad, sufrió una serie de reorganizaciones internas con transacciones e intercambios de diverso tipo.
El hecho de que los recursos metabólicos de un organismo sean limitados obliga a esas operaciones de economía.
La inversión en sistemas efectores, filo cortante con el que células asesinas y anticuerpos guillotinan virus y bacterias, ha de equilibrarse con la inversión en sistemas reguladores necesarios para mantener bajo control la empresa entera.
Así, en el sistema inmunitario humano abundan más las células T reguladoras, o CD4 , que las células de cualquier otra clase.

Lo mismo que en un contexto etológico, resulta aquí inevitable apelar al lenguaje del mercado libre. 
Los inmunólogos seguirán los pasos de quienes dedicaron buena parte de su tiempo a valorar la inversión de los ciervos en cornamentas o la ventaja neta que obtiene el colibrí de su búsqueda de flores jóvenes. 
¡ Qué perfecta imagen en miniatura del mercado la que ofrecen los ganglios linfáticos y el bazo!
En los centros germinales de esos órganos, las células B en transformación compiten furiosamente entre sí por la mengua da cantidad de antígeno que les es necesaria para sobrevivir.
Las ganadoras tienen asegurada la recompensa de la replicación masiva.
Las células que no encuentran antígeno sucumben. 

Donde la naturaleza deja de suministrar pruebas que respalden las investigaciones sobre la evolución, los inmunólogos han tomado en préstamo un instrumento de los economistas: la creación de modelos por ordenador.
Franco Celada, del Hospital metropolitano de Enfermedades Articulares de Nueva York, y Philip E. Seiden, del Centro de Investigaciones Thomas J. Watson de IBM, han ideado un autómata celular que remeda los episodios celulares que se suceden en el seno del sistema inmunitario.
Conjuntos de células T , células B y células presentadoras de antígeno, simuladas y convenientemente dotadas de receptores y moléculas de MHC, se exponen (en sentido figurado) a la presencia de antígeno, y se les deja interactuar.
El autómata responde bien a la estimulación antigénica, organizando respuestas primarias y secundarias fáciles de reconocer.

Se obtienen resultados más interesantes cuando se le plantean cuestiones de mayor calado; por ejemplo:
¿cuál es la cuantía óptima de tipos de MHC por individuo?
El programa contesta haciendo un balance entre la ventaja que constituye el poder presentar cada vez más péptido y la desventaja que representa el suprimir un número creciente de células del repertorio de las T .
Recuérdese que, además de presentar el antígeno, la molécula de MHC identifica el tejido como propio.
Por tanto, un aumento de moléculas de MHC significa un crecimiento de la cantidad de autoantígenos;
lo que exige, en consecuencia, que se suprima el número correspondiente de células T para evitar la autoinmunidad. 
Además, la merma en tipos de MHC reduce en última instancia la flexibilidad de la respuesta ante microorganismos invasores.

Tras algunas operaciones, el modelo nos da una cifra comprendida entre cuatro y ocho, en buena coherencia con la observación. 
Aunque no sea el primer modelo matemático ideado sobre el sistema inmunitario, sí es, con mucho, el recurso más manejable y atractivo de cuantos se han desarrollado para explorar estos temas evolutivos.
Su prueba de fuego vendrá cuando intente dar respuesta a cuestiones cuya solución aún se desconoce.

Un problema afín tiene que ver con el surco de presentación del antígeno de las moléculas de MHC.
¿Por qué admite esa hendidura una secuencia de exactamente nueve aminoácidos?
Un bioquímico poco avisado diría que ese tamaño es un puro accidente de la geometría de la molécula de MHC.
Un biólogo evolucionista algo más perspicaz no se contentaría con esa salida.
Aduciría, en cambio, que la longitud del surco tal vez refleje un equilibrio de la presión de selección entre dos necesidades opuestas:
la de conservar en lo posible el repertorio de células T y la de impedir a los parásitos que fabriquen proteínas invisibles para el sistema de las células T .
Por ejemplo, si un surco alojase solamente seis aminoácidos, podrían estar presentes en las proteínas propias casi todos los hexapéptidos posibles, con la consiguiente eliminación de la mayoría de las células T ;
pero si el surco diera acomodo a 14 aminoácidos, los parásitos podrían evolucionar en el sentido de evitar la incorporación en sus proteínas de péptidos capaces de enlazarse. 

Podemos imaginarnos a los vertebrados primitivos ensayando varias longitudes hasta dar con la que mejor encajara.
Se trata, sin embargo, de una posibilidad que queda todavía fuera del alcance de los actuales modelos informáticos, pero se conseguirá sin duda en el futuro.

¿Cómo conciliar esta apremiante necesidad de economía, economía lograda de hecho ya en muchos aspectos, con otros caracteres manifiestamente despilfarradores del sistema inmunitario?
Los leucocitos, pongamos por caso, se agrupan en varias clases repetidas, dotada cada una de un armamentario bastante completo de receptores antigénicos. 

Podemos aventurar, sin disparatar, que la ventaja de esta repetición consiste en que posibilita la especialización. 

Para habérselas debidamente con los diferentes parásitos se necesitan diferentes inmunoglobulinas; a modo de ejemplo, la IgA es la más eficaz contra las lombrices intestinales. 

Cada inmunoglobulina demanda su propio clon de células B .

De manera similar, las células T citotóxicas proporcionan una defensa apropiada contra virus (el de la gripe, por ejemplo) y contra bacterias intracelulares (así, Listeria ).
Y la división tajante en células T y células B refleja, al menos en parte, la ventaja de dar a las primeras la responsabilidad de la autotolerancia y permitir con ello a las segundas prestarse a la hipermutación. 
No resultaría nunca combinarlas unas con otras. 
Imagínese a las células T de una población exoneradas de células autorreactivas en el timo (lo que de hecho ocurre) y generando entonces aquellas, por hipermutación, nuevos receptores (algo privativo de las células B ): ¡acabaría eso en una pérfida autoinmunidad!

Podemos ya empezar a enhebrar algunas soluciones a la cuestión planteada.
Me gustaría hacerlo despejando un posible malentendido.

La explicación que de la evolución del sistema inmunitario he dado hasta aquí podría inducir a pensar que, en buena parte, el proceso se cerró hace mucho. 
La verdad es muy otra.
Puede asegurarse casi con toda certeza que el sistema inmunitario humano evoluciona actualmente de una generación a otra con una celeridad sin precedentes. 
Gran parte de este cambio afecta a los genes polimórficos, genes que expresan muchas formas distintas de la misma molécula de MHC 0 anticuerpo.

Nuestro sistema inmunitario es intensamente polimórfico, mucho más que cualquier otra parte de nuestro organismo. 
Estas moléculas varían de un individuo a otro hasta tal punto que una determinada combinación difícilmente aparece duplicada.
Este estado de cosas refleja la multiplicación de nuestros parásitos, que se reproducen mucho más rápidamente que nosotros y pueden, por ello, evolucionar más deprisa.

En cuanto a otras formas de polimorfismo, por ejemplo el cromatismo alar de las mariposas, tal diversidad viene verosímilmente sostenida por la selección en favor de los heterocigotos; con este nombre se designan los individuos que poseen copias diferentes de cada gen, en nuestro caso los genes que fomentan la variabilidad de las moléculas que se enlazan con el antígeno.
Un individuo disfruta de una clara ventaja si tiene, además de los cuatro a ocho loci génicos antes mencionados, dos genes diferentes en cada locas.
Esa disposición eleva al máximo las posibilidades de engarzarse a un péptido, por lo menos, de cada proteína vírica o bacteriana.

El polimorfismo del MHC reviste mayor complejidad y encierra mayor interés que la simple selección en favor de respuestas inmunitarias frente a los parásitos.
Acontece también el fenómeno contrario:
un proceso vigoroso crea con toda probabilidad una rica variedad de genes cuyos productos suprimen la respuesta inmunitaria.
Esos genes se encargan de amortiguar una reacción inmunitaria que dañe directamente al cuerpo en el transcurso de la respuesta ante un microorganismo patógeno.

Donde mejor puede verse el equilibrio entre los dos tipos de control es en la lepra, enfermedad que aflige a más de un millón de personas en las regiones tropicales.
Desapareció de Europa hace escasos siglos y por causas desconocidas.
Los sujetos infectados con la bacteria de la lepra responden de distintos modos.
Tore Godal, de la Organización Mundial de la Salud, halló que la mayoría de los pacientes se liberan de la infección y sólo les quedan vestigios de reactividad en su sistema inmunitario.
Otros adquieren una infección "tuberculoide", en la que el cuerpo incoa una respuesta de células T vigorosa, aunque sólo parcialmente eficaz.
Los de un tercer grupo desarrollan una condición "lepromatosa", en la que se anula la respuesta de células T .
La piel se llena de bacterias infecciosas, pero la vida puede proseguir con relativa normalidad.
En los casos en que la respuesta no se ha suprimido del todo, la inmunopatología tiende a manifestarse.
El equipo de René de Vries, del Hospital Universitario de Leiden, ha de mostrado con solidez que los genes del MHC, sobre todo, ejercen el control del espectro de respuestas.

Sin duda, la mayoría de estos genes inmunosupresores sobreviven en las sociedades industriales como reliquias de infecciones crónicas del estilo de la lepra.
Con ello nos proporcionan (bien que accidentalmente) una medida de protección contra las enfermedades de autoinmunidad y los trastornos alérgicos. 
El equipo de Donald J. Capra, del Hospital Clínico de la Universidad de Texas en Dallas, dirigió un extenso estudio de diabéticos insulinodependientes que respondieron a una llamada de la radio local.
Hallaron en su muestra muchos menos genes de MHC de cierto tipo ( HLA-DQw 1.2 ) que los que habría sido de esperar por su frecuencia en los individuos sanos de control:
Presumiblemente, tales genes anulan la respuesta inmunitaria que destruye las células beta del páncreas humano.
Se han comprobado observaciones parecidas con otros genes HLA en la artritis reumatoide y la vasculitis.

No parece que el efecto protector baste para explicar la supervivencia de esos genes. 
(Se trata de trastornos raros y que afligen principalmente a una población de edad avanzada.)
Pero sí despiertan éstos el interés de los investigadores.
En el Centro de Investigación sobre el Reuma de Berlín creemos que la protección viene garantizada por citocinas inhibidoras tales como el factor de crecimiento beta transformador.
Indagamos tratamientos que remeden tales efectos genéticos.
Hemos adoptado ese enfoque influidos por el trabajo innovador de Howard L. Weiner, de la Universidad de Harvard, sobre la esclerosis múltiple, publicado a comienzos de este año.

La evolución de los mecanismos inmunosupresores no es más que un ejemplo del condicionamiento recíproco entre hospedadores y parásitos en su desarrollo a lo largo del tiempo; pero hay otros muchos casos.
A decir verdad, cuanto más ahondamos en el conocimiento de las moléculas del sistema inmunitario, más nos sorprenden tales adaptaciones. 
Ciertos virus utilizan moléculas del sistema inmunitario para lograr el acceso a las células hospedadoras.

Esa misma habilidad explotadora que distingue a los gérmenes patógenos puede ser en ocasiones explotada por la especie humana.
El grupo de John P. Tite, de los laboratorios Wellcome, ha definido una secuencia de la invasina bacteriana, una proteína que facilita la invasión del tejido hospedador.
La secuencia se une a integrina, proteína de la superficie de la célula hospedante.
El equipo de Tite espera utilizar esta información para construir moléculas que bloqueen la invasina.
El descubrimiento de que ciertos virus sintetizan proteínas de unión a citocinas ha intrigado a los científicos de la Immunex Corporation de Seattle.
Las proteínas en cuestión refuerzan la virulencia inhibiendo, presumiblemente, la respuesta inmunitaria.
Los expertos de Immunex están construyendo proteínas similares con el fin de controlar la artritis reumatoide, una enfermedad autoinmunitaria.

En el otro extremo del espectro, numerosas secuencias de ADN vírico han logrado integrarse en el genoma humano y nunca se expresan (la mayoría de las variedades del color del pelaje de los hámsters, tan apreciados por algunos, resultan del bloqueo génico operado por esas secuencias víricas).
Las secuencias pueden también emplearse con fines que nada tienen que ver con la replicación vírica;
en este sentido, ciertos genes víricos que determinan superantígenos endógenos se han conservado en el genoma del ratón.

Desde allí, eliminan los productos de otros genes denominados genes V, que rigen la síntesis de proteínas constituidas en sitios de anclaje para los superantígenos bacterianos.

Podemos empezar a verle un sentido a esta coevolución si introducimos el principio de la "agenda compartida", de Richard Dawkins.
El ADN de un virus endógeno latente tiene las mismas necesidades que el ADN de su hospedador: ambos comparten el mismo plan de acción.
No ocurre así con el virus que, tras breve período de latencia, mata a su huésped;
se comparte en este caso muy poca agenda. 
Habría que añadir que la latencia completa (la incorporación en el ADN del anfitrión) es fenómeno desacostumbrado, pues la mayoría de los virus no sintetizan retrotranscriptasa, la enzima necesaria para la incorporación. 

La exigencia de transmisión que lleva el parásito tiende a reforzar los planes compartidos, y sin apenas excepciones ésa ha sido la dirección seguida por el curso evolutivo. 
Sólo las formas de infección nuevas, así los virus de reciente transmisión desde otras especies, traen consigo su propio proyecto.

Rara vez, si alguna, le compensa a un parásito matar a su anfitrión, realidad ésta a la que deberíamos estar profundamente agradecidos.

¿Podemos confiar en que este vivir y dejar vivir, largamente mantenido, persista en el mundo moderno?
Nunca hubo tantas facilidades para mezclar hospedadores y parásitos carentes de experiencia recíproca anterior.
Cada decenio incrementan la población mundial unos mil millones de seres humanos.
Muchos de ellos se apiñan en megaciudades de los países en vías de desarrollo. 

El avión facilita el movimiento de millones de personas alrededor del globo.
El pasado reciente nos ilustra sobre lo que puede suceder cuando las personas se topan con un germen patógeno del que ni la evolución humana ni su sistema inmunitario les protegen; lo recoge gráficamente una vieja canción referida al paludismo:

"¡Golfo de Benín, golfo de Benín, cuán pocos salen de tantos llegados aquí!"

Los cruces y promiscuidad de los siglos de comunicaciones marítimas constituyen una segunda razón por la que el sistema inmunitario debe estar cambiando con especial celeridad.
Robert C. Gallo, del Instituto Nacional del Cáncer, ha defendido convincentemente que el VIH puede hundir sus raíces en un complicado paso, de simios a humanos, de dicho virus de la inmunodeficiencia.
Poca duda cabe de que las epidemias de gripe, que azotan el mundo entero, tienen idéntico origen; podrían iniciarse cuando una nueva variedad del virus salta una barrera interespecífica para propagarse, con los viajeros, por doquier.

Aunque nuestra destreza para manipular el sistema inmunitario se refina día a dio, la mejor defensa natural contra la mayoría de los gérmenes patógenos nuevos y viejos reside en el polimorfismo del sistema inmunitario, que contrarresta la ventaja de que disfrutan los gérmenes patógenos gracias a su capacidad para evolucionar rápidamente. 
Debemos la supervivencia de nuestra especie a la enorme variabilidad del sistema.

¿Cabría contar con una ulterior elaboración de la estructura del sistema u otras defensas radicales?
 
¿Podrían los mecanismos de control alcanzar un nivel de complejidad mayor que los mejorase cualitativamente? 
No; no hemos de confiar en una estrategia de defensa que siguiera semejante evolución.
Todas las estructuras actuales estuvieron ya prefiguradas en formas de vida precedentes. 
Y no hay señal de que vayan a emerger ab ovo estructuras nuevas.

Los crecientes niveles de vida y la consiguiente reducción del impacto de las infecciones sobre la selección natural constituirán las principales fuerzas determinantes del cambio rápido.
Ante el reto que nos plantean los organismos con su variabilidad permanente, es probable que asistamos, en respuesta, a la modificación de la frecuencia relativa de genes polimórficos como los del MHC.

Con el tiempo, se producirá una amplia sustitución de genes.
El aislamiento respecto a la gama entera de los microorganismos patógenos también nos permite conservar la reserva de variabilidad genética que es nuestra defensa principal.
A la espera de su empleo, los polimorfismos siempre están disponibles.

Se trata de adaptaciones para ajustar mejor el sistema a misiones cambiantes en el mundo moderno.
Esas adaptaciones serán en conjunto beneficiosas, nadie lo duda, por más que ignoremos su naturaleza.
Resulta presumible que la pérdida de genes deletéreos se deje sentir más que la adquisición de otros beneficiosos.

Hasta es posible que los genes determinantes del MHC que confieren sensibilidad a la artritis reumatoide o a la esclerosis múltiple vayan desapareciendo gradualmente.

Este proceso tendría poco que ver con cualquier efecto evolutivo que las enfermedades pudiesen producir -al fin y al cabo afligen a individuos de edad avanzada que contaron con la oportunidad de reproducirse.

Es más probable que ocurra por haber sido eliminadas las infecciones que promovieron la selección en pro de la pervivencia de esos genes.

Desde la perspectiva de la sociedad industrial, los genes parecen perjudiciales, aunque prestaron un innegable servicio a nuestros antepasados flagelados por las infecciones.

Sabemos muy poco sobre qué genes determinantes del MHC son los necesarios en una u otra infección.
En consecuencia, toda predicción en este asunto resulta aventurada.
Al comienzo de la Ilíada , atribuye Homero la peste que azota a los aqueos a las flechas disparadas por el enojado Apolo.
¡Ojalá que éste pudiera informarnos del efecto del MHC sobre los blancos elegidos por el protector de la medicina!

Esta tendencia general hacia una menor selección de los genes que confieren resistencia a las infecciones encierra obvios peligros.
A medida que debilitemos nuestras defensas genéticas contra las infecciones de antaño, aumentaremos casi con certeza nuestra sensibilidad a las nuevas.
Sírvanos de consuelo saber que la evolución procede con lentitud, sacando el máximo provecho de los pros por encima de los centras a lo largo de muchas generaciones, sin olvidar, empero, que la evolución humana se mueve ahora más deprisa que nunca.

Contrasta esta tendencia más notoria con lo que sucede a propósito de otras fuerzas evolutivas, que parecen ejercer un efecto bastante menor.
La progresiva y mejor preparación para rastrear el curso de los genes nocivos que mencionábamos al comienzo de este artículo, que nos permite eliminarlos (en detecciones selectivas y con el cumplimiento del consejo genético por parte de las parejas), dejará sentir su efecto en las familias más que en el conjunto de la especie.
Están ya en perspectiva las vacunas anticonceptivas, como las basadas en la hormona del embarazo, la gonadotropina coriónica.

El grupo de G. Pran, del Instituto Nacional de Inmunología de Delhi, ha demostrado la eficacia de esta forma de anticoncepción, pero sólo en mujeres que den la debida respuesta inmunitaria. 

La información genética empleada de modo apropiado puede convertirse en un medio mucho más potente. 

Cuanto mejor dominemos el curso seguido por los factores genéticos destructivos, mejor situados estaremos para desbaratarlos.
Tomemos como ejemplo la artritis reumatoide: 
los estudios realizados entre hermanos gemelos sugieren que los factores genéticos aportan hasta un tercio de la sensibilidad a esa patología, amén de atribuir un papel predominante, dentro de esos factores, al complejo principal de histocompatibilidad.

En la diabetes la situación es similar.
Podemos empezar a descubrir el "tipo" sensible.
Con los años, llegaremos a trazar con mayor exactitud el perfil de la sensibilidad, que incluirá por supuesto el MHC y otros genes, amén de parámetros inmunitarios como el patrón de síntesis de citocinas.
Todo esto podría hacerse en favor de los individuos sanos, para que supieran en qué circunstancias sus respuestas podrían fallar o pasarse de raya, y tomaran así sus precauciones para evitar o prever el riesgo.

¿Ha alcanzado, pues, el sistema inmunitario su cenit tras los escasos cientos de millones de años que ha tardado en desarrollarse?
¿Podrá responder en el futuro a los nuevos desafíos de la evolución?
Estos interrogantes, perfectamente legítimos, carecen de una respuesta segura, por la sencilla razón de que nos hallamos ante una situación inédita, sin parangones. 
Aun cuando conociéramos la frecuencia con que las especies hospedadoras cayeron aniquiladas en el pasado por sus parásitos, de poco nos serviría semejante información para interpretar nuestra situación actual.

Pero hay base para el optimismo.

Jamás se había tenido tanto éxito en la erradicación de una enfermedad como el obtenido con el mero refuerzo de la actividad inmunitaria; estoy pensando en el caso de la viruela, hace menos de 20 años.

Ni tampoco había progresado tanto la medicina ni se había empeñado con tanta firmeza contra las infecciones;
estoy aludiendo a la Organización Mundial de la Salud (y, más en concreto, a sus programas de investigación especializada) y a los centros nacionales especializados como el de Atlanta en Estados Unidos o el de Majadahonda en España.

Por encima de todo, sin embargo, mi confianza la deposito en las fuerzas de la microevolución, en la capacidad del polimorfismo para ir sacando de su vasto acervo defensas nuevas y, por último, en la habilidad de anfitrión y parásito para adoptar una agenda compartida.

Figura 1

UN RETO al sistema inmunitario lo representan el rápido aumento de la población humana, la aglomeración en megaciudades ( la foto recoge una instantánea de un andén en una estación de cercanías londinense )y la existencia de medios de transporte muy desarrollados y veloces.
Estos factores están cambiando nuestra relación con los virus, las bacterias y los parásitos.

Figura 2

LA RÁPIDA DIFUSIÓN DE LA ENFERMEDAD mediante los viajes aéreos y otras modalidades de transporte es un hecho de la vida contemporánea.
Graves y a veces mortíferas enfermedades infecciosas, como las causadas por diversas cepas del virus de la gripe, el sida y nuevos brotes farmacorresistentes de la tuberculosis se expenden con rapidez por todo el mundo, pasando con frecuencia de los países subdesarrollados a los industrializados.

Figura 3

NÚMERO ÓPTIMO de tipos de MHC.
La cifra depende de que haya un equilibrio entre la necesidad de responder a grandes cantidades, quizá muchos millones, de microorganismos invasores y la necesidad de evitar la autoinmunidad.
La observación, así como las simulaciones hechas con ordenador, al estilo de las realizadas por Philip E. Selden, de IBM, de quien proviene la gráfica superior, muestran que el número óptimo de tipos de MHC está entre 4 y 8. 
La ilustración de abajo enseña cómo la molécula MHC presenta el antígeno a las células T coadyuvantes. 
El proceso comienza cuando un macrófago se encuentra con un antígeno. 
El macrófago atrae el antígeno y se lo traga.
Una vez ingerido, la célula lo degrada y lo combina con una molécula de MHC.
El macrófago expone el compuesto antígeno-molécula de MHC en su membrana externa ( abajo, a la derecha ), donde puede ser descubierto por una célula T ; ésta alerta entonces a las células B para que sinteticen anticuerpos que movilicen otros componentes del sistema inmunitario.

Figura 4

LA LEPRA, enfermedad que ya no es endémica en Europa, manifiesta un rango de patologías que es el resultado de la capacidad del sistema inmunitario de autosuspenderse cuando existe el riesgo de que una respuesta suya perjudique al propio individuo. 
En el paciente de la izquierda se han expresado genes que han conducido a una falta de respuesta eficaz. 
En consecuencia, el bacilo de la lepra se multiplica y congrega en las ampollas llenas de líquido que resaltan sobre la piel. 
En el paciente del centro, una eficaz respuesta inmunitaria ha motivado que sólo aparezca una lesión bajo el cuero cabelludo.
La mano de la derecha pertenece a un paciente en el que se organizó una vigorosa defensa inmunitaria, aunque no se eliminaron todas las bacterias.
En estos cases intermedios, la respuesta inmunitaria daña los tejidos.

Figura 5

LA ESTRATEGIA TERAPÉUTICA para tratar la artritis reumatoide parte de una maniobra defensiva que el virus de la viruela efectúa contra la inflamación.
Iníciase una respuesta inflamatoria cuando un mensajero molecular que circula entre células del sistema inmunitario, el llamado factor de la necrosis tumoral (FNT), se incrusta en su receptor. 
Para bloquear la respuesta inflamatoria, los virus alojados en las células que serían destruidas por la reacción inducen la síntesis de receptores señuelos.
Estos se acoplan con el FNT antes de que el mensajero pueda contactar con una célula del sistema inmunitario, respuesta que ha inspirado a los investigadores de la Immunex Corporation de Seattle una terapia para la artritis reumatoide, enfermedad causada por inflamación.
Immunex está desarrollando un FNT señuelo constituido por dos receptores artificiales acoplados a un fragmento de anticuerpo.
Razonan que ese compuesto en Y competirá eficazmente por las moléculas de FNT, abortando con ello la respuesta inflamatoria autoinmune .
Los primeros resultados han sido satisfactorios ( derecha ).
El receptor artificial FNTRs:Fc inhibe la respuesta en células cultivadas a concentraciones muy inferiores a aquellas en que lo hacen otros receptores de FNT, en cuya función se realizó el ensayo. 

Figura 6

NUEVA CEPA desencadenante de tuberculosis. 
De extraordinaria virulencia, se muestra resistente también a la acción farmacológica.
Estas formas alteradas de los microorganismos patógenos tradicionales, y de gérmenes nuevos (por ejemplo, el virus de la inmunodeficiencia humana, o VIH agente del sida, y la Legionella causante de la "enfermedad del legionario"), ponen en un aprieto al sistema inmunitario humano, si no lo arruinan. 

Agendas compartidas y sin compartir

De acuerdo con Richard Dawkins, de la Universidad de Oxford, la relación entre el microorganismo patógeno y su anfitrión va de la absoluta compatibilidad a la total divergencia, a tenor de la fracción de agenda para la supervivencia que compartan.
Cuando el genoma de un patógeno viaja en los gametos de su hospedador, participan ambos del mismo proyecto y así "acuerdan" que el anfitrión se mantenga sano y apto para la reproducción.
Los gametos de muchas especies de mamíferos, incluido el mapache, portan un importante complemento de tales genes víricos, benignos y útiles.
La relación entre el virus de la rabia y su hospedador ilustra el efecto opuesto.
El virus, que se propaga a través de la saliva, tiene poco interés por la supervivencia de su hospedador.
La tendencia histórica de las relaciones entre los humanos y sus patógenos ha sido -cree el autor- hacia la agenda compartida.


Nuevo árbol de la vida

Hace unos diez años aparecían esbozadas las líneas básicas de la evolución. 
Parte de ese esquema, sobrio y elegante, comienza a cuestionarse. 

Va para siglo y medio que Charles Darwin postulara el origen de todas las especies actuales a partir de otro elenco menor, surgido a su vez de otro más restringido, que procedía de otro más exiguo- y así hasta el amanecer de la vida.
A tenor de esa explicación, las relaciones de parentesco entre los organismos, modernos y extintos, podían plasmarse en un árbol genealógico.

La mayoría de los investigadores acepta ese planteamiento. 
Muchos alegan incluso que están perfectamente claros los rasgas generales del árbol, cuya raíz sería una célula, el antepasado común universal de todos los organismos, aparecida hace unos 3500 o 3800 millones de años.
Aunque no resultó fácil ponerse de acuerdo sobre ese guión, se ha convertido en doctrina oficial desde hace poco más de un decenio.

Mas, para sorpresa general, el árbol se tambalea. 
Descubrimientos recientes han empezado a minar los supuestos parentescos primeros, los inmediatos a la raíz.

Figura 1

REPRESENTACIÓN del árbol universal de la vida, según la doctrina mayoritaria

Propone ésta que los primero descendientes del último ancestro común - una célula sin núcleo- dieron lugar a dos grupos de procariotas (células sin núcleo): las bacterias y las arqueas.
Más tarde, a partir de las arqueas surgieron unos organismos cuyas células mostraban una estructura más compleja: los eucariotas.
En el transcurso de su proceso evolutivo, los eucariotas adquirieron, al ingerir y retener en su citoplasma a ciertas bacterias, unos valiosos orgánulos capaces de generar energía, las mitocondrias; en el caso de las plantas, los cloroplastos.

El primer bosquejo

Hasta hace unos 35 años, a nadie se le ocurrió reconstruir un árbol universal.

Desde Aristóteles, en el siglo IV antes de Cristo, hasta los años sesenta del siglo XX, se infería el parentesco entre organismos de su comparación anatómica, fisiológica o ambas.
Con ese sistema se establecieron relaciones genealógicas razonables entre los organismos superiores.
Los análisis de innumerables caracteres revelaban, por ejemplo, que los homínidos compartían antepasado con los primates, precursor que compartía con los simios otro ancestro; éste tenía en común con los prosimios otro antecedente, etcétera.

Pero los microscópicos seres unicelulares no solían aportar una información desbordantes sobre sus relaciones de parentesco.
La situación resultaba especialmente decepcionante, pues los microorganismos fueron los únicos habitantes de la Tierra durante la primera mitad, si no dos tercios, de la historia del planeta; la ausencia de una filogenia clara (árbol genealógico) para los microorganismos generaba en los científicos grandes dudas sobre la naturaleza y la secuencia de sucesos que dieron lugar a las innovaciones radicales en la estructura y función celular.
Por ejemplo, entre el nacimiento de la primera célula y la aparición de hongos, plantas y animales, las células aumentaron en tamaño y complejidad, adquirieron un núcleo y un citoesqueleto (el andamiaje interno) y encontraron un mecanismo que les permitió alimentarse de otras células. 

A mediados de los sesenta, Emile Zuckerkandl y Linus Pauling, del Instituto de Tecnología de California, idearon una estrategia revolucionaria que paliara esa falta de información. 
En vez de ceñirse a los caracteres anatómicos o fisiológicos, se plantearon la posibilidad de utilizar diferencias en genes o proteínas para trazar parentescos y dependencias.

La filogenia molecular, así se llama el método, resulta ser de una lógica implacable.
Los genes, constituidos por secuencias específicas de nucleótidos, son los responsables de la síntesis de proteínas, compuestas por cadenas de aminoácidos.
Pero los genes matan (cambian su secuencia), lo que no pocas veces redunda en alteración de la proteína cifrada.
Las mutaciones genéticas que no dejan sentir su efecto en la función de la proteína o que la mejoran, se acumularán a lo largo del tiempo.
Y así, conforme dos especies van alejándose de su antepasado común, las secuencias de sus genes irán también divergiendo, divergencia que se acrecienta con el transcurso de las generaciones.
Para reconstruir el pasado evolutivo de los organismos - sus árboles filogenéticos- habrá, pues, que atender a las semejanzas y diferencias existentes entre las secuencias de sus genes o proteínas. 

Hace 35 años se lograban las primeras secuenciaciones de aminoácidos de las proteínas; los genes tendrían que esperar.
Los estudios realizados con proteínas, a lo largo de los años sesenta y setenta, demostraron la validez de la filogenia molecular al confirmar, y extender luego, los árboles filogenéticos de los vertebrados, grupo cabalmente conocido.
Refrendaron, además, algunas hipótesis sobre la relación existente entre ciertas bacterias; demostraron, por ejemplo, que las bacterias que producen oxigeno durante la fotosíntesis forman un grupo homogéneo, el de las cianobacterias.

Con el recurso creciente a las secuencias proteicas, Carl R., Woese, de la Universidad de Illinois, fijó la atención en un nuevo parámetro de distancias evolutivas: el ARN ribosómico microsubunitario (« small subunit ribosomal RNA », SSU rRNA ).

Esta molécula, genéticamente determinada, constituye un componente clave de los ribosomas, las «fábricas» celulares que sintetizan las proteínas; en efecto, las células necesitan dicha microsubunidad para vivir.
Así las cosas, Woese pensó - nos hallamos en las postrimerías de los años sesenta- que las variaciones experimentadas por la microsubunidad (o, con mayor exactitud, por los genes que la cifran) serían un excelente indicador del grado de parentesco entre los seres vivos, de las bacterias elementales a los animales complejos.
Por consiguiente, el ARN ribosómico microsubunitario podría desempeñar la función, en palabras de Woese, de «cronómetro molecular universal».

Al principio, los métodos para avanzar en dicho proyecto eran indirectos y laboriosos.
Pero hacia el ocaso de los años setenta, Woese contaba ya con suficientes datos para extraer sus importantes conclusiones.
Desde entonces, los expertos en filogenia que estudian la evolución microbiana, lo mismo- que los interesados en las ramas superiores del árbol, han basado sus pautas de ramificación en el análisis de la secuencia de los genes del ARNr microsubunitario.
Este cúmulo de secuencias de ARNr resultó determinante para forzar, a finales de los ochenta, un acuerdo común sobre la estructura del árbol universal.
Hoy se dispone de secuencias de ARNr de varios millares de especies.

Desde el comienzo, los resultados obtenidos con el ARNr corroboraron algunas intuiciones, al par que produjeron inesperadas sorpresas.
En los años sesenta, los microscopistas habían arribado a la conclusión de que el mundo de lo vivo podía dividirse en dos grandes dominios, Eukarya (eucariotas) y Bacteria (procariotas), en razón de la estructura de sus células.
Los organismos eucariotas (animales, plantas, hongos y muchos seres unicelulares) contenían un núcleo verdadero (un orgánulo donde alojan los cromosomas, intracelular y envuelto por una membrana).
Las células eucariotas presentaban, por su lado, otras características específicas; así, un citoesqueleto, un intrincado sistema de membranas internas y mitocondrias (orgánulos en los que tiene lugar la respiración aeróbica que permite la obtención de energía a partir de los nutrientes).
En las células de las algas y las plantas superiores distinguíanse los cloroplastos (orgánulos en los que se lleva a cabo la fotosíntesis). 

A los procariotas, sinónimo entonces de bacterias, se les consideraban células sencillas, sin núcleo y protegidas por una membrana y una pared externa rígida. 

Los primeros datos obtenidos por Woese confirmaban la distinción entre procariotas y eucariotas al demostrar que las secuencias de los ARNr microsubunitarios bacterianos guardaban entre sí un mayor parecido que con las secuencias de los ARNr eucariotas.
Los hallazgos iniciales a propósito de esa molécula otorgaron también credibilidad a uno de los temas más interesantes de la biología celular evolutiva:
la hipótesis del endosimbionte, avanzada para explicar el mecanismo en cuya virtud las células eucariotas adquirieron mitocondrias y cloroplastos.

A tenor de la hipótesis endosimbionte, en el camino hacia la conversión en eucariotas hubo algunos, entre los primitivos procariotas anaeróbicos (incapaces de utilizar el oxígeno para obtener energía), que perdieron su pared celular.
La membrana, más flexible, que subyacía bajo dicha pared creció y se replegó sobre sí misma.
Fruto de esas remodelaciones surgieron el núcleo y otras membranas intracelulares, al tiempo que la célula adquiría la capacidad de engullir y digerir procariotas vecinos, sin tener que recabar siempre los nutrientes por adsorción de pequeñas moléculas de su entorno.

Llegó un momento en que uno de los descendientes de nuestro eucariota primitivo ingirió células bacterianas del grupo de las proteobacterias alfa, duchas en obtener energía por respiración.
Pero en vez de digerir tales células bacterianas, como si se tratara de mera nutrición, el eucariota estableció una relación mutuamente beneficiosa (simbióntica) con ellas.
El eucariota daba cobijo a las bacterias engullidas, y el «endosimbionte» le proveía de energía extra gracias a la respiración.
Por último, los endosimbiontes perdieron los genes que hasta entonces precisaban para su crecimiento independiente y transfirieron otros al núcleo de la célula hospedadora, terminando por transformarse en mitocondrias.
En un proceso similar, los cloroplastos derivan de cianobacterias engullidas, y retenidas, en cierto momento de su historia por una célula eucariota portadora de mitocondrias.

Las mitocondrias y los cloroplastos de las células eucariotas actuales conservan todavía un escueto número de genes; entre ellos, los que codifican el ARNr microsubunitario.
Por consiguiente, y una vez se dispuso de las herramientas apropiadas a mediados de los años setenta, los investigadores decidieron comprobar si esos genes que cifran el ARNr microsubunitario se habían heredado, respectivamente, de las proteobacterias alfa y de las cianobacterias, en consonancia con lo predicho por la teoría endosimbionte. 
Se demostró que así era.

Pero la paz conseguida se vio turbada por una idea arriesgada. 
Estamos a finales de los setenta.
Woese aseguró entonces que el cuadro de la vida que pivotaba sobre los dos dominios, el de las bacterias ( Bacteria ) y el de los eucariotas ( Eukarya ), debía ceder el paso a otro que incorporase un tercer dominio.

Algunos procariotas adscritos a las bacterias, aunque a éstas se parecieran, en su genética distaban mucho, sostenía Woese.
De entrada, el ARNr respaldaba una divergencia precoz.
De muchas de estas especies se conocía ya su comportamiento peculiar, sin ir más lejos su preferencia por vivir en condiciones ambientales extremas; nadie, sin embargo, había cuestionado su «estatuto bacteriano».

Woese sugería ahora que conformaban un tercer dominio ( Archaea ), no menos distante de las bacterias que éstas de los eucariotas.

Figura 2

LA TEORÍA ENDOSIMBIÓNTICA propone que las mitocondrias se formaron después de que un procariota que había evolucionado hasta convertirse en eucariota primitivo engulló ( a ) y conservó ( b ) una o más células de las proteobacterias alfa

Con el tiempo, la bacteria adquirió la capacidad de vivir en el interior de la célula huésped, a la vez que transfirió algunos de sus genes al núcleo de ésta ( c ), convirtiéndose en una mitocondria.
Más tarde, algunos eucariotas portadores de mitocondrias ingirieron una cianobacteria, que acabó dando lugar a un cloroplasto ( d ).

Figura 3

REPRESENTACIÓN de las relaciones de parentesco existentes entre los ARN ribosómicos ( ARNr ) procedentes de unas 600 especies

Cada línea equivale a la secuencia del ARNr de una especie o un grupo; la mayoría de las líneas corresponde a ARNr cifrados por genes del núcleo, aunque hay también genes de ARNr de cloroplastos y mitocondrias.
Las líneas de los ARNr mitocondriales deben su mayor longitud a la rápida evolución de los genes mitocondriales.
Los árboles construidos a partir de los ARNr carecen de raíz; otra serie de datos sitúan a ésta en el punto coloreado, que corresponde a la parte inferior de los árboles de la figura 1.

Tras la polémica, el acuerdo 

Contra la propuesta de Woese se opuso una feroz resistencia inicial.
Pero la mayoría acabó por convencerse. 
En la aceptación tuvo que ver la estructura global de ciertas moléculas de especies arqueanas, que corroboraba la organización tripartita.
Por citar un dato, las membranas celulares de todas las arqueas contenían lípidos de características únicas y muy diferentes - en sus propiedades físicas y en su constitución química- de los lípidos bacterianos.

También, las proteínas de las arqueas responsables de diversos procesos celulares cruciales presentan una estructura diferente de las proteínas que ejecutan las mismas tareas en las bacterias; así ocurre en los procesos de transcripción y traducción.
Para sintetizar una proteína, la célula copia, o transcribe, el gen correspondiente en una cadena de ARN mensajero.
A continuación, los ribosomas traducen la información del ARN mensajero en una ristra específica de aminoácidos. 
Los bioquímicos descubrieron que la ARN polimerasa de las arqueas, la enzima que lleva a cabo la transcripción, guarda un parecido mayor con la de los eucariotas que con la de las bacterias, y ello no sólo en lo concerniente a la estructura, sino también en la naturaleza de su interacción con el ADN .
Las proteínas arqueanas que forman parte de los ribosomas que traducen el ARN mensajero también se asemejan más a las eucariotas que a las bacterianas. 

Aceptada la tesis de la división de la vida en tres dominios, había que dilucidar de cuál de los dos grupos primitivos - bacterias y arqueas- se originó la primera célula eucariota.
Los estudios sobre el parentesco entre la maquinaria de transcripción y traducción de los eucariotas y las arqueas revelaron que los primeros provenían de éstas.

Semejante conclusión recibió un nuevo espaldarazo en 1989, cuando los grupos liderados por J., Peter Gogarten y Takashi Mikaya utilizaron las secuencias de otros genes en su afán de identificar la raíz del árbol universal. 
La comparación de los ARNr microsubunitarios (« SSU rRNA ») puede decirnos qué organismos están estrechamente emparentados entre sí; mas la técnica en cuestión resulta incapaz de revelar qué grupos son más primitivos y, en consecuencia, más cercanos a la raíz.
El análisis de las secuencias de ADN que cifran dos proteínas celulares esenciales confirmaba que el último ancestro común engendró a bacterias y arqueas; de éstas se ramificaron luego los eucariotas.

Desde 1989, un rosario de descubrimientos han venido cementando la tesis del triple dominio.
En los cinco últimos años se han hecho públicas las secuencias completas del genoma (conjunto de todos los genes) de media docena de arqueas y de más de 15 bacterias.
La comparación de estos genomas ha confirmado la sospecha de que bastantes genes implicados en la transcripción y en la traducción de las arqueas y de los eucariotas son muy similares, así como que estos procesos se desarrollan de forma muy parecida en ambos dominios.
Pese a que las arqueas carecen de núcleo diferenciado bajo ciertas condiciones experimentales sus cromosomas recuerdan a los de los eucariotas:
el ADN forma complejos con unas proteínas muy parecidas a las histonas de éstos, y sus cromosomas pueden adoptar la estructura eucariota de collar de perlas.
Además, en la replicación de tales cromosomas interviene una serie de proteínas muy parecidas a las que participan en los mismos procesos de los eucariotas, pero no en los bacterianos. 

Figura 4

ÁRBOL FILOGENÉTICO de varios grupos de especies, construido a partir del gen que determina la enzima HMGCoA reductasa

En este árbol sumario se aprecia que el gen de la reductasa de Archaeoglobus fulgidus , una típica arquea, proviene de una bacteria, no de un antepasado arqueo.
Se suma tal descubrimiento a los muchos aducidos para reafirmar la idea de que la transferencia génica lateral ha desempeñado un papel decisivo en la evolución de la vida unicelular.
El árbol universal aceptado no tiene en cuenta este factor.

Dudas persistentes

Con esa muchedumbre de datos extraordinarios y congruentes se pergeñó la estructura del árbol genealógico universal, que hoy acepta la mayoría.
A tenor de dicho cuadro, la vida se desdobló primero en bacterias y arqueas.
A continuación, los eucariotas surgieron de un precursor arqueoideo.
Seguidamente, los eucariotas incorporaron genes bacterianos en dos ocasiones, recabando mitocondrias de las proteobacterias alfa, y cloroplastos, de las cianobacterias. 

Con todo, a medida que vamos disponiendo de un número creciente de secuencias completas de genomas, varios grupos (incluido el mío) han detectado algunos hechos que contradicen las ideas en vigor.
Si el árbol aceptado fuese correcto, en las células eucariotas sólo descubriríamos genes de origen bacteriano en el ADN de las mitocondrias y de los cloroplastos, aparte de los que se transfirieron al núcleo desde los precursores alfaproteobacterianos y cianobacterianos de estos orgánulos.
Los genes transferidos serían, por otra parte, los implicados en la respiración o en la fotosíntesis, no los responsables del resto de los procesos celulares, que se habrían heredado de la arquea ancestral.

Pero esa hipótesis no se ha cumplido.
Genes presentes en el núcleo de los eucariotas provienen, con frecuencia, no sólo de las arqueas sino también de las bacterias.
Un buen número de tales genes de origen bacteriano participan en el control de funciones que no tienen nada que ver con la respiración y la fotosíntesis y que son, sin embargo, tan fundamentales para la supervivencia celular como la transcripción y la traducción. 

En el árbol clásico se lee también que los genes de origen bacteriano fueron incorporados por los eucariotas y no por las arqueas.
Pero existen numerosas pruebas de que muchas arqueas portan un cupo nutrido de genes bacterianos.
Mencionemos, como botón de muestra, el caso de Archaeoglobus fulgidus , que reúne todas las características de una arquea (lípidos adecuados en su membrana celular y mecanismos de transcripción y traducción esperados) y, sin embargo, emplea una forma bacteriana de la enzima HMGCoA reductasa para sintetizar los lípidos de la membrana. 
Presenta, además, numerosos genes bacterianos que le ayudan a conseguir energía y nutrientes en los pozos petrolíferos submarinos, uno de sus medios favoritos. 

La explicación razonable de resultados tan contradictorios hay que buscarla en el proceso de la evolución, que ni es lineal ni tan parecida a la estructura dendriforme que Darwin imaginó.
Aunque los genes se han transmitido de generación en generación, esta herencia vertical no es el único factor involucrado en la evolución de las células.
Otro fenómeno- la transferencia lateral u horizontal de genes- ha afectado profundamente el curso evolutivo.
En vez de pasar de una célula progenitora a su descendiente, en la transferencia horizontal se transmiten genes individuales, o serie de ellos, de una especie a otra. 

Vía el mecanismo de transferencia lateral, los eucariotas evolucionados de una célula arquea obtuvieron genes bacterianos decisivos para el metabolismo:
los eucariotas recabaron de las bacterias genes y retuvieron los que demostraron su utilidad. 
Ese mecanismo explicaría por qué hay arqueas que terminaron por poseer genes habituales en bacterias.

Algunos teóricos de la filogenia molecular- entre ellos Mitchell L., Sogin y Russell F., Doolittle- atribuyen a la transferencia lateral un misterio que se resiste.
Muchos genes eucariotas difieren de los de bacterias y arqueas conocidas. 
Se ignora de dónde pudieron haber venido.
Nos referimos, en particular, a los genes responsables de la síntesis de los componentes del citoesqueleto y del sistema interno de membranas, un par de rasgos distintivos de las células eucariotas.
Sogin y Doolittle apuntan la existencia de un cuarto dominio de organismos, extinguido en la actualidad, que transfirió horizontalmente al núcleo de las células eucariotas los genes responsables de estos caracteres. 

Desde hace tiempo sabe la microbiología de la capacidad de las bacterias para el intercambio horizontal de genes.
Se lo confirma la cesión de genes de resistencia a los antibióticos entre bacterias infecciosas.
Pero muy pocos sospechaban que los genes esenciales para la supervivencia celular cambiaran frecuentemente de célula o que la transferencia lateral ejerciera un peso tan determinante en los albores de la historia de la vida microbiana.
Por lo que se ve, los expertos andaban errados.

Figura 5

VERSIÓN REVISADA DEL ÁRBOL DE LA VIDA

Mantiene una estructura ramificada en la parte superior del dominio Eukarya y acepta que los eucariotas adquirieron de las bacterias las mitocondrias y los cloroplastos.
Incluye también una extensa trama de enlaces transversales entre ramas, enlaces que se han distribuido al azar para simbolizar el alto grado de transferencia lateral de un gen o un grupo, un fenómeno persistente entre seres unicelulares.
El «árbol» reformado se caracteriza también por carecer de una célula única en su raíz; los tres dominios de la vida ( Bacteria , Archaea y Eukarya ) se originaron probablemente a partir de una población de células primitivas cuyos genes fueron divergiendo.

¿Sobrevivirá el árbol? 

¿Qué nos dicen los nuevos descubrimientos acerca de la estructura del árbol universal de la vida? 
Una primera enseñanza a extraer es que el progreso armonioso de arqueas a eucariotas, plasmado en el árbol actual, peca de un exceso de simplificación, si no es erróneo.
Al parecer los eucariotas no emergieron de una arquea, sino de alguna célula precursora resultante de una serie de transferencias horizontales de genes, que terminaron por modelarla en parte bacteriana, en parte arquea y en parte otras cosas.

Múltiples testimonios siguen respaldando todavía la hipótesis endosimbionte, según la cual las mitocondrias eucariotas derivan de las proteobacterias alfa, y los cloroplastos, de la ingestión de cianobacterias.
Con todo, sería arriesgado ceñir a ésas las transferencias génicas laterales que ocurrieron tras la aparición del primer eucariota.
Más tarde, para evitar la transferencia génica lateral los eucariotas multicelulares se blindaron con la adquisición de células germinales independientes (y protegidas).

La representación estándar de las relaciones de parentesco entre procariotas parece ser demasiado lineal para resultar cierta.
Un conjunto de genes y propiedades bioquímicas agrupa a las arqueas y los distinguen de las bacterias, siendo así que entre bacterias y arqueas (al igual que entre las diferentes especies englobadas dentro de cada uno de estos dos grupos) ha existido un amplio intercambio de genes.

Para definir parentescos evolutivos entre los procariotas, podríamos escoger los genes que parecen menos propensos a la transferencia.
En ese contexto, muchos filogenéticos consideran que los genes de los ARNr microsubunitarios y las proteínas implicadas en la transcripción y la traducción son renuentes a la transferencia horizontal, por cuya razón el árbol filogenético basado en ellos mantiene su validez.
Ahora bien, tal renuencia a la transferencia es un mero supuesto no demostrado; en cualquier caso, debemos aceptar que los árboles sólo compendian la historia evolutiva de una parte del genoma de un organismo. 
Lo que entendemos por árbol aceptado constituye una representación gráfica harto esquemática. 

¿Cuál sería el modelo más acorde con la realidad?
Aquel que en la copa presentara la estructura dendriforme ramificada de animales, plantas y hongos multicelulares. 
Las transferencias génicas implicadas en la formación de las mitocondrias y cloroplastos de los eucariotas a partir de las bacterias se representarían mediante la fusión de ramas principales.
Por debajo de estos puntos de transferencia (y dentro de los actuales dominios Bacteria y Archaea ) observaríamos numerosas fusiones adicionales de ramas.
En la profundidad del dominio procariota, en la base quizá del dominio eucariota, sería impropio imaginarse un tronco principal.

Aunque más compleja, esta imagen revisada seguiría pecando de una simplificación engañosa.
Casi una caricatura, pues las fusiones entre ramas no representarían la unión de genomas completos, sino la transferencia de genes o grupos de genes.

Para que pintáramos entero el cuadro, tendrían que plasmarse simultáneamente las pautas genealógicas superimpuestas de miles de familias génicas (una de ellas, la constituida por los genes del ARNr ).

Si no se hubiera producido nunca transferencia lateral, los árboles génicos presentarían la misma topología (igual orden de ramificación); los genes ancestrales de la raíz de cada árbol se hallarían en el genoma del último antepasado universal, la célula primitiva.
Pero si apelamos a una transferencia fluida, la historia tuvo que ser muy otra.
Diferirán los árboles de los genes, aunque algunos perfilen regiones de topología similar; además , no habrá en ningún caso una célula que pudiera reputarse el último antepasado común.

Como Woese adelantó, el antepasado ancestral no fue un organismo particular, un linaje exclusivo.
Se trató, por contra, de un conglomerado diverso, de escasa consistencia interna, de células primitivas que evolucionaron al unísono hasta alcanzar un estadio en que los lazos terminaron por cortarse y se formaron comunidades distintas, que a su vez terminaron por conformar tres líneas de descendencia ( Bacteria , Archaea y Eukarya ).
En otras palabras, las células primitivas, que contenían pocos genes, tomaron caminos muy dispares en su divergencia.
Mediante el intercambio de genes transaccionaron con las facultades de unos y otros.
Andando el tiempo, ese tropel de células eclécticas y cambiantes se fundieron en los tres dominios básicos que conocemos hoy.
Dominios que distinguimos merced a la transferencia génica:
la mayor parte de la misma, si no toda, se produce en su seno.

Creen algunos biólogos que por este camino sólo podemos llegar a la confusión y al desánimo.
Como si nos confesáramos incapaces de tomar el testigo de Darwin y recrear la estructura del árbol de la vida. 
Pero la ciencia tiene sus reglas.
Un modelo o hipótesis atractiva- la del árbol único- sugirió una serie de experimentos, en este caso la obtención de secuencias génicas y su análisis en el marco de la filogenia molecular.
Los datos demuestran que este modelo es demasiado simple.
Ahora se necesitan nuevas hipótesis cuyas implicaciones finales ni tan siquiera atisbamos.


Productos procedentes de los microorganismos 

Introducción

La diversidad bioquímica de los microorganismos, ilustrada por la facultad que tienen de crecer a partir de una gran diversidad de sustratos y producir una amplia variedad de sustancias, ha conllevado su explotación industrial por la industria fermentativa.
Los microbiólogos tienden a interpretar el término «fermentación» como un proceso destinado a la producción de un producto mediante el cultivo masivo de microorganismos, mientras que los bioquímicos utilizan dicha palabra con un sentido más estricto.
El verdadero significado bioquímico de la palabra es «un proceso que genera energía en el que los compuestos orgánicos actúan como dadores de electrones y aceptores terminales de electrones» (véase Sección 1.,1) Sin embargo, en esta Sección el término «fermentación» se usará en su más amplio sentido microbiológico.
Los avances en biología molecular y manipulación genética de los últimos años han proporcionado una gran oportunidad a la ya bien establecida industria fermentativa para iniciar nuevos procesos y mejorar los ya existentes.
Estos avances han servido para decir a todas voces que nacía una nueva era pero debe tenerse presente que la explotación de esos importantes avances depende de técnicas de cultivo a gran escala que la industria ha desarrollado durante muchos años.
Los objetivos de esta sección son: primero, introducir al lector en los diversos productos derivados de la fermentación y, en segundo lugar, concentrarse en los principios biológicos en que descansan dichos procesos.

Producción de biomasa microbiana 

El producto comercial de origen microbiano más evidente quizás sea la biomasa microbiana (las mismas células de microorganismos).
Las células de levadura, utilizadas en la industria de panadería, se han producido comercialmente desde principio de siglo.

Igualmente, se produjeron como alimento humano en Alemania durante la primera guerra mundial.
Estos procesos iniciales de producción de biomasa eran aeróbicos y, como se discute más adelante, el proceso se controlaba eficazmente. 
Aunque ha continuado la producción de levadura de panadería por fermentación, no ocurrió así con la biomasa que se producía durante la primera guerra mundial; esta producción tanto como alimento para el hombre como para pienso para animales quedó relativamente abandonada de tal forma que tales procesos no fueron de nuevo motivo de profundas investigaciones hasta la década de los años 60.
En ese tiempo se empezó a explotar el uso de una amplia variedad de microorganismos capaces de crecer sobre sustratos de diversos átomos de carbonos (entre ellos hidrocarburos) para la producción industrial de la biomasa denominada «proteína unicelular» o SCP.
Muy pocos de los procesos investigados en ese tiempo fueron adelante porque tropezaron con dificultades económicas y políticas.
La British Petroleurn fue probablemente la empresa líder en este campo; desarrolló una tecnología de tal magnitud que llevó a la empresa a diseñar una planta en Cerdeña para la producción de biomasa de levadura a partir de n-alcanos.
Sin embargo, la BP fue incapaz de convencer al gobierno italiano de la seguridad toxicológica de su producto y la fábrica nunca se puso en marcha.
Muchos otros procesos cayeron presa de las guerras de Oriente Medio y de la subsiguiente escalada de los precios de los productos petrolíferos.
Sin embargo, La Imperial Chemical Industries (ICI) persistió en sus planes de producción de biomasa bacteriana ( Methylophilus methylotrophus ) a partir de metanol y desarrolló el proceso Pruteen para la producción de proteína de alto grado destinada a la fabricación de pienso para animales.
El proceso utiliza un cultivo en continuo (véase Capítulo 7) a una gran escala (1.500 m3 ).
Este proceso es un buen ejemplo de la aplicación de la ingeniería para el desarrollo de procesos microbianos. 
La economía de la producción de SCP para la alimentación animal es todavía marginal pero la tecnología desarrollada por la ICI durante el proyecto Putreen ha conllevado que la empresa sea un líder mundial en biotecnología.
La ICI está explotando su experiencia en cultivos continuos a gran escala al colaborar con la Rank Hovis MacDougall en un proceso para la producción de biomasa fúngica a partir de carbohidratos destinada a la alimentación humana.
La economía de este proceso parece más atractiva.

Producción de productos microbianos 

La explotación de las facultades biosintéticas de los microorganismos ha dado como resultado el establecimiento de un gran número de procesos comerciales.
El crecimiento de un microorganismo conduce a la generación de numerosos metabolitos pero el que se sintetiza mayoritariamente depende de la naturaleza del microorganismo, de las condiciones de cultivo utilizadas y de la velocidad de crecimiento del cultivo en producción. 
Si un microorganismo se inocula en un medio nutritivo que satisface su crecimiento, el cultivo pasará por una serie de fases; el sistema se denomina cultivo discontinuo (véase Capítulo 7).

Inicialmente, no se produce crecimiento alguno, etapa a la que se denomina fase de latencia y puede considerarse como un período de adaptación.
A continuación sigue un intervalo durante el cual la velocidad de crecimiento de las células aumenta gradualmente hasta alcanzar una velocidad máxima constante, etapa que se conoce como fase logarítmica o exponencial .
A medida que se agotan los nutrientes, o se acumulan metabolitos tóxicos, la velocidad de crecimiento de las células se desvía del valor máximo y, eventualmente, cesa el crecimiento; se dice que el cultivo ha pasado a la fase estacionaria .
Después de un tiempo el número de células viables empieza a declinar y el cultivo entra en la fase de muerte .

El término descriptivo de cultivo discontinuo puede acarrear confusiones cuando se considera la actividad metabólica del mismo durante las distintas fases, dado que el metabolismo de las células durante la fase estacionaria puede ser muy diferente del de la fase logarítmica; no es, de ningún modo, estacionario.
Bullock et al. (1965) propusieron una terminología descriptiva que reflejara el comportamiento de las células microbianas desde un punto de vista metabólico en vez de en términos cinéticos.
Se utilizó el término trofofase para describir la fase logarítmica de un cultivo en las que los únicos compuestos resultantes del metabolismo son esenciales bien para el crecimiento (aminoácidos, nucleótidos, proteínas, ácidos nucleicos, lípidos, carbohidratos, etc.), o bien son subproductos del catabolismo energético (como etanol, acetona y butanol). 
Los metabolitos producidos durante la trofofase se denominan metabolitos primarios.
El término idiofase se utilizó para describir la fase del cultivo en la que se producían otros compuestos diferentes a los metabolitos primarios; se denominaron metabolitos secundarios. 
Los metabolitos secundarios se han definido como compuestos que se sintetizan cuando las células crecen lentamente o no crecen; no juegan un papel definido en el crecimiento celular y su distribución es, taxonómicamente, limitada. 
La idiofase, por tanto, corresponde aproximadamente a la fase estacionaria del cultivo discontinuo.

Las interrelaciones entre los metabolismos primarios y secundarios se ilustran en la Figura 6.1; puede observarse que los metabolitos secundarios tienden a sintetizarse a partir de los intermediarios clave y de los productos finales del metabolismo primario.
Aunque las primeras rutas metabólicas mostradas en la Figura 6.1 son comunes a la gran mayoría de los microorganismos, cada metabolito secundario puede ser sintetizado por sólo un muy pequeño número de grupos microbianos.
Por otra parte, no todos los grupos microbianos tienen un metabolismo secundario; es una característica normal de las bacterias y mohos filamentosos y de las bacterias esporuladas pero no es una propiedad, por ejemplo, de las enterobacterias. 
Por tanto, aunque la distribución taxonómica del metabolismo secundario es mucho más limitada que la del primario, la variedad de productos producidos es enorme. 
A primera vista, no parece lógico que los microorganismos sinteticen compuestos que no tengan una función metabólica y que sean verdaderamente subproductos del catabolismo.
Sin embargo, muchos metabolitos secundarios exhiben propiedades antimicrobianas y, por tanto, les pueden servir para entrar en competencia en los hábitats naturales (Demain, 1980). 
Se ha demostrado que otros, aunque se descubrieron en cultivos en idiofase, se producen durante la trofofase y se ha dicho que pueden desempeñar un papel regulador.
Por tanto, algunos «metabolitos secundarios» no cumplen todos los criterios que se agrupan en la definición ofrecida anteriormente, que debiera considerarse mas bien como una descripción general del fenómeno que como una verdadera definición.
Realmente, Campbell (1984) opina que el único aspecto definitivo de esta definición original es la distribución taxonómicamente limitada de los metabolitos secundarios.
Aunque las interrelaciones entre los metabolismos primario y secundario y el papel fisiológico del último son temas que están sometidos a un considerable debate, la relevancia de estos productos para la industria es el significado comercial de los mismos.
En la Tabla 6.1 se muestran algunos ejemplos del significado comercial de dichos productos.

Figura 6-1

Interrelaciones entre los metabolismos primario y secundario

El control del inicio del metabolismo secundario se ha investigado extensamente y, obviamente, es un área de un interés considerable para la industria fermentativa. 
Debido a estas investigaciones existe una información muy considerable acerca de los cambios que se producen en el medio al inicio del metabolismo secundario.
No obstante, se conoce poco sobre el control del proceso a nivel de DNA.
Se ha demostrado que los precursores metabólicos primarios de los metabolitos secundarios inducen su formación; por ejemplo, el triptófano en la biosíntesis de alcaloides (Robbers y Floss, 1970) y la metionina en la síntesis de cefalosporina (Komatsu, Mizumo y Kodaira, 1975).
Por otra parte, se ha demostrado también que los componentes del medio reprime el metabolismo secundario; las primeras observaciones al respecto fueron las de Soltero y Johnson (1953) quienes mostraron el efecto represor de la glucosa en la formación de bencilpenicilina. 
Se ha observado en varias ocasiones que las fuentes de carbono, nitrógeno y fosfato, que proporcionan un rápido crecimiento, inhiben la formación de metabolitos secundarios. 
Por tanto, es esencial evitar la presencia de nutrientes represores en los medios fermentativos o que estén presentes a niveles subrepresores.
Estos aspectos se consideran más detalladamente en el Capitulo 8.

Como se ha mencionado en la introducción, el advenimiento de la tecnología del DNA recombinante ha ampliado el número de productos potenciales de origen microbiano.
Se puede dotar a las células microbianas de la facultad de sintetizar productos normalmente asociados a las células superiores. 
Tales productos pueden constituir la base de nuevos procesos fermentativos; por ejemplo, la síntesis de interferón, insulina y renina.
Las técnicas in vitro de la tecnología de DNA recombinante se considera en los Capítulos 10, 11 y 12.

Las rutas biosínteticas y catabólicas que se dan durante la biosíntesis primaria y secundaria son todas reacciones catalizadas por enzimas.
Como se ilustra en la Tabla 6.2 muchos enzimas microbianos tienen aplicaciones comerciales y son, por tanto, productos, de gran valor comercial, resultantes de la fermentación.
La mayoría de las cepas silvestres de microorganismos regulan la síntesis de los productos metabólicos de tal forma que sólo los sintetizan en las cantidades requeridas para cubrir las necesidades del crecimiento celular.
Sin embargo, en los procesos comerciales es esencial producir estos compuestos en concentraciones muy elevadas y, por tanto, es necesario modificar las células productoras y controlar los factores ambientales hasta tal punto que sus sistemas de control normales sean superados.
El control de los factores ambientales y la modificación genética de los microorganismos se tratan en los Capítulos 8 y 9.

Tabla 6-1

Algunos ejemplos de metabolitos microbianos de significado comercial

- Metabolitos primarios
- Significado comercial
- Etanol
- «Ingrediente activo» en bebidas alcohólicas 
- Ácido cítrico
- Diversos usos en la industria alimentaria
- Acetona y butanol
- Disolventes
- Ácido glutámico
- Potenciador del sabor
- Lisina
- Aditivo alimentario
- Polisacáridos
- Aplicaciones en la industria alimentaria; potenciador para la recuperación de aceite
- Fe +++
- Lixiviador mineral
- Metabolitos secundarios
- Penicilina
- Antibiótico
- Tetraciclinas
- Antibióticos
- Cefalosporina
- Antibiótico
- Estreptomicina
- Antibiótico
- Griseofulvina
- Antibiótico antifúngico
- Pepstatina
- Tratamiento de úlceras
- Ciclosporina A
- Inmunosupresor
- Crestina
- Tratamiento del cáncer
- Bestatina
- Tratamiento del cáncer

Procesos de transformación microbiana

Al igual que se utilizan microorganismos para la producción de biomasa y metabolitos, las células microbianas pueden emplearse para catalizar la conversión de un producto en otro de estructura similar pero de mayor valor comercial. 
Aunque la producción de vinagre es el proceso de transformación más antiguo y mejor conocido (conversión de etanol en acético), la mayoría de estos procesos conllevan la producción de compuestos de gran valor.
Las reacciones que pueden estar implicadas en dichos procesos son oxidación, deshidrogenación, hidroxilación, deshidratación, condensación, descarboxilación, aminación, desaminación e isomerización.
La ventaja de los procesos microbianos sobre el uso de reactivos químicos es la especificidad y la posibilidad de operar a temperaturas y presiones relativamente bajas.
La desventaja de estos procesos de transformación es que se necesita producir una gran cantidad de biomasa para catalizar, quizás, una sola reacción.
Por ello, algunos procesos se le ha proporcionado un perfil más dinámico mediante la inmovilización, en soportes inertes, de las células mismas o de los enzimas aislados que específicamente catalizan una reacción. 
De este modo, pueden emplearse como catalizadores reutilizables. 
Estos aspectos se tratan más detalladamente en el Capítulo 14.

Tabla 6-2

Algunos ejemplo de aplicaciones comerciales de enzimas

- Enzima
- Origen microbiano
- Industria
- Aplicación
- Amilasa
- Fúngico
- Panadería
- Aceleración de la fermentación; reducción de la viscosidad de la masa
- Amilasa
- Fúngico/bacteriano
- Cervecería
- Liberación de azúcares solubles de la malta 
- Proteasa
- Fúngico/bacteriano
- Cervecería Protección contra la «turbidez» 
- Amilasa
- Fúngico/bacteriano
- Alimentaria
- Fabricación de jarabes
- Pectinasa
- Fúngico
- Alimentaria
- Preparación de zumos de frutas y concentrados de café
- Proteasa
- Bacteriano
- Lavandería
- Enzima que se incorpora al detergente en polvo

Estructura de un proceso fermentativo 

La parte dentral de un proceso fermentativo es el crecimiento del microorganismo industrial en unas condiciones ambientales que estimula la síntesis del producto comercial que se pretende obtener.
Se realiza en un fermentador que consta esencialmente de un gran recipiente (con una capacidad entre 1.000 dm3 y 1,5 millones de dm3 ) en el que se mantiene el microorganismo a la temperatura, pH, concentración de oxigeno disuelto y concentración de sustrato deseados.
Sin embargo, el cultivo del microorganismo en el fermentador es sólo una de las numerosas fases del proceso (véase Fig., 6.,2). 
El medio en el que crece el microorganismo ha de formularse de acuerdo con las materias primas y, posteriormente, esterilizarse. 
El contenido del fermentador se esteriliza y se inocula con un cultivo viable, metabólicamente activo.
Después del crecimiento se separan las células de la porción liquida y se recoge la fracción que interese (bien las células o el sobrenadante libre de células).
Se debe supervisar la evolución del programa impuesto para el desarrollo del proceso.

Para que pueda realizarse la fermentación es necesario, en primer lugar, obtener un microorganismo adecuado para el proceso que se pretenda llevar a cabo, lo que se efectúa normalmente seleccionando cepas naturales; la productividad del mismo debe aumentarse hasta obtener niveles económicos, lo que se realiza mediante mejoras por mutación, recombinación o diseño del proceso.
Por tanto, el éxito de los procesos fermentativos depende de la destreza de microbiólogos, bioquímicos, genetistas, ingenieros químicos, químicos e ingenieros de control de procesos.

Aunque este texto se concentra sólo en los aspectos biológicos no hay que olvidar la naturaleza interdisciplinar del tema.

Figura 6-2

Fases de un proceso fermentativo 

Cultivo de microorganismos

Cultivo discontinuo (batch cultura) 

Un microorganismo sólo crecerá en un medio de cultivo si contiene todos los nutrientes necesarios en una forma disponible y si son adecuados el resto de los factores ambientales.
El método de cultivo más simple es el cultivo discontinuo en el que el microorganismo crece a partir de una limitada cantidad de medio hasta que se agota un nutriente esencial o se acumulan subproductos tóxicos hasta niveles que inhiben el crecimiento.
En un sistema discontinuo el cultivo pasará por una serie de fases, según se ilustra en la Figura 7,1.

El crecimiento no comienza inmediatamente después de la inoculación del medio de cultivo; el período previo al crecimiento activo se denomina fase de latencia que puede considerarse como un periodo de adaptación.
Es evidente que en los procesos comerciales es conveniente reducir la fase de latencia tanto como sea posible no sólo para evitar la pérdida de tiempo sino también porque se consumen nutrientes para mantener el cultivo viable en dicho periodo previo al crecimiento.
El tiempo de la fase de latencia puede reducirse usando un inóculo relativamente grande (3-10%) de un cultivo en fase exponencial que haya sido preparado en el mismo medio (o similar) que el utilizado para la fermentación.

Preparación del inóculo 

Se debe disponer de una cantidad de cultivo suficiente para inocular el medio del fermentador; además debe ser metabólicamente activo, estar libre de contaminantes y ser capaz de producir el producto que se pretenda obtener en el cultivo subsiguiente.
Sin embargo, si, por ejemplo, el fermentador produjera 100.000 dm3 , el volumen del inóculo tendría que estar comprendido entre 3.000 y 10.000 dm3 . 
Este volumen tiene que prepararse a partir de un cultivo inicial de unos pocos cm3 , lo que conlleva un gran número de fermentaciones sucesivas a una escala cada vez mayor.
Existen diversas opiniones conflictivas sobre la conveniencia de utilizar el volumen «óptimo» de inóculo o preparar un inóculo puro y libre de cepas degeneradas (es decir, células que han perdido la capacidad de producir el compuesto deseado) porque mientras mayor sea el número de pases entre el cultivo madre y la fermentación final mayor será el riesgo de contaminación y de degeneración de cepas.
Por otra parte, también se requiere un considerable desembolso para los fermentadores preparadores del inóculo.
Si la planta tiene un gran número de fermentadores para la producción, una serie de recipientes para los subcultivos puede ser adecuada para suministrar todo el inóculo que se necesita, con lo que la inversión seria razonable.
Sin embargo, si la planta de producción consiste sólo de un gran fermentador en continuo, entonces es muy gravoso construir un fermentador para la preparación de inóculo de un décimo de capacidad que el industrial, ya que se utilizaría con muy poca frecuencia.
Por lo tanto, es necesario llegar a un compromiso entre el volumen del inóculo, las probabilidades de contaminación del mismo y las de degeneración de cepas y la inversión necesaria.

Figura 7-1

Representación esquemática del crecimiento de un microorganismo en un cultivo discontinuo 

Muchos microorganismos utilizados en las fermentaciones industriales son filamentosos y formadores de esporas asexuales. 
Entre estos microorganismos cabe citar Penicillium chrysogenum (utilizado para la producción comercial de penicilina), Aspergillus niger (para la producción comercial de ácidos orgánicos) y algunos actinomicetos (usados en la producción de diversos antibióticos y otras sustancias).

La facultad de formar esporas asexuales es de gran importancia para la preparación del inóculo dado que una relativamente pequeña cantidad de biomasa puede utilizarse para producir una gran concentración de células.
Stanbury y Whitaker (1983) han discutido detalladamente el uso de un inóculo de esporas y sólo se van a citar aquí los aspectos más importantes.
El micelio esporula profusamente en un medio de agar sólido o en la superficie de granos de cereales; ambos métodos se utilizan para la preparación de inóculos de esporas con fines industriales (Vezina y Singh, 1975).
Butterworth(1984) describió el uso de un frasco de Roux de aproximadamente 200 cm2 de superficie de agar para la esporulación de Streptomyces clavuligerus (utilizado para la producción comercial de ácido clavulánico, un inhibidor de la B-lactamasa).
Las esporas formadas en ese matraz pueden utilizarse para la preparación de 75 dm3 de subcultivo y éste emplearse para la inoculación de 1500 dm3 de medio en el fermentador industrial.
Podojil et al. (1984) cita el uso de matraces con mijo para la formación de esporas de Streptomyces aureofaciens , destinado a la producción comercial de clortetraciclina.
Muchos mohos esporulan en cultivos sumergidos (es decir, en un fermentador con agitación) si se utiliza el medio de cultivo óptimo (Vezina, Singh y Sehgal, 1965); por ejemplo las esporas de Penicillium patulum se producen en cultivos sumergidos en los programas de preparación de ionóculo para la producción comercial de griseofulvina (Haber y Tietz,1984).

La naturaleza filamentosa de muchos microorganismos comercialmente importantes presenta considerables problemas al tecnólogo de fermentaciones.
Un microorganismo filamentoso puede dar lugar a diversas formas de crecimiento en cultivos líquidos sumergidos y sólo algunas de ellas pueden ser deseables para el proceso comercial.
Estas formas de crecimiento van desde largos filamentos homogéneamente distribuidos por el medio a aglomeraciones compactas de micelio suspendidas en el caldo.
Las formas filamentosas originan un caldo extremadamente viscoso que puede ser muy difícil de oxigenar adecuadamente mientras las formas aglomeradas dan lugar a un caldo menos viscoso pero también menos homogéneo En los aglomerados, el micelio localizado en el centro de esas formaciones puede tener un escaso aporte de nutrientes y oxígeno debido a problemas de difusión de los mismos.
Además, existen datos que indican que la morfología del microorganismo influye en la producción del cultivo; si esto se debe a los fenómenos ya mencionados o a alguna forma de control metabólico es una cuestión que aún no esta aclarada (Whitaker y Long, 1973; Calam y Smith, 1981).
En definitiva, algunas fermentaciones se realizan con el microorganismo en su forma filamentosa mientras otras con crecimiento en forma aglomerada.
Por ejemplo, se ha informado que el crecimiento filamentoso es óptimo para la producción de penicilina por P. Chrysogenan mientras que en aglomerado lo es para la producción de ácido cítrico por A. Niger(Smith y Calam, 1980; Al Obaidi y Berry, 1980).
El crecimiento filamentoso es absolutamente necesario en el proceso ICI-RHM de producción de micoproteína para el consumo humano a partir de Fusarium gramineorium .
Se requiere una morfología altamente filamentosa para que el producto presente una textura que recuerde la consistencia de las carnes blancas y rojas que se aprecia al masticar (Marsh y Pinkney, 1985).
En este proceso hay que obtener hitas de una longitud media de 400 mm. 

La relevancia de esta consideración acerca de la morfología del micelio para preparar el inóculo radica en la influencia que la cantidad de esporas utilizadas como inóculo para la fermentación puede tener en la morfología final.

Una elevada concentración de esporas tiende a rendir formas filamentosas mientras que si la concentración de esporas es baja tiende a proporcionar formas aglomeradas (Whitaker y Long, 1973).
La naturaleza del medio utilizado en el programa de preparación de inóculo influye también en la morfología.
Aunque la gran mayoría de la información disponible en la bibliografía está relacionada con la morfología de los mohos en cultivos sumergidos, el tema es igualmente importante en las fermentaciones de actinomicetos.

La Figura 7,2 muestra algunos ejemplos de programas para la preparación de inóculos; puede deducirse que la naturaleza del programa varia considerablemente dependiendo del proceso.
Debe recordarse que el éxito del sistema de preparación de un inóculo debe juzgarse de acuerdo con la productividad del cultivo desarrollado en la fermentación subsiguiente.
De la exposición anterior hay que deducir que es extremadamente indeseable utilizar células procedentes de la fermentación final para inocular la próxima producción; se debe preparar un inóculo nuevo a partir del cultivo madre original para cada lote que se fabrique.
No obstante, existen dos excepciones a esta regla general; son en la producción de cerveza y vinagre. 
En la producción de vinagre por fermentación en un sistema discontinuo, las células de bacterias acéticas procedentes de lote producido se utilizan como inóculo para la producción del lote siguiente, lo que se realiza retirando aproximadamente el 60 % del cultivo.
El microorganismo es extremadamente sensible a la limitación de oxigeno y es esencial obtener un crecimiento rápido.
En el caso del vinagre los riesgos de contaminación y degeneración de cepas son menores debido al hábitat tan selectivo del medio.
En las factorías tradicionales de producción de cerveza tipo «ale» ha sido una práctica común inocular el medio con levaduras obtenidas de una fermentación anterior; este procedimiento puede que continúe así durante muchos años.
Sin embargo, Hansen, ya en 1896, desarrolló un sistema para la producción de un inóculo puro para la fabricación de cerveza tipo «lagar».
En las más modernas fábricas de cerveza «ale» la inoculación del lote anterior al siguiente es actualmente relativamente limitada pero todavía se practica en muchas de las pequeñas fábricas tradicionales y, además, hay que añadir que ¡la calidad de sus productos es habitualmente excepcional!

Figura 7-2

Algunos ejemplos representativos de programas para la preparación comercial de inóculos 

(a) Producción de ácido clavulánico por Streptomyces clavuligerus (reproducido con permiso de Butterworth, 1984).
(b) Producción de clortetraciclina por Streptomyces aureofaciens (reproducido con permiso de Podojil, 1984). 
(c) Producción de sagamicina por Streptomyces sp .

Crecimiento logarítmico o exponencial 

Suponiendo que se utiliza un inóculo activo (como se ha indicado anteriormente) puede acortarse el tiempo de la fase de latencia y la velocidad de crecimiento de las células aumenta progresivamente.
Transcurrido un período de tiempo, las células crecen a una velocidad máxima constante; esta etapa se denomina fase logarítmica o exponencial y puede describirse por la ecuación: xxx donde xxx y al integrar la ecuación se transforma en: xxx donde xxx y tomando logaritmos naturales en la ecuación se obtiene: xxx .

Por tanto, la representación del logaritmo natural de la concentración de células frente al tiempo da una línea recta cuya pendiente es xxx , la velocidad específica de crecimiento.
Durante la fase exponencial, la velocidad especifica de crecimiento es constante y máxima en las condiciones que se opera; se denomina entonces MmaX , velocidad específica de crecimiento máxima. 

El crecimiento de un microorganismo en un cultivo puede describirse también mediante un simple cálculo algebraico usando el término tiempo de duplicación, td , que es el tiempo necesario para que se divida una célula. 
Si un medio de cultivo se inoculara con sólo una célula viable y esta célula creciera a la velocidad máxima sin pasar por la fase de latencia, el incremento secuencial del número de células que se observaría sería: 1, 2, 4, 8, 16, 32, 64, 128, etc. 

Este incremento secuencial puede representarse expresando el número de células en base dos.
Entonces resultaría: 21, 22, 23, 24, 25, 26, 2, 28, etc. 
Los exponentes (o potencias o índices) en dicha serie representa el número de Generaciones que han ocurrido, por lo que después de n generaciones el número de células del cultivo será 2n. Si el medio se inoculara con No células, el número de células, Nt, presente después de n generaciones sería: xxx .
Al tomar logaritmos en base 2 la ecuación (3) se transforma en: xxx .

Entonces xxx .

Aunque el término de tiempo de duplicación se utiliza relativamente poco en fisiología microbiana, puede ser útil pum convertir la velocidad especifica de crecimiento en el tiempo de duplicación para lograr una mejor apreciación del significado de estos valores.
La ecuación que deriva del tiempo de duplicación utilizando el término número de células puede obtenerse también usando el término concentración de células.
Entonces: xxx .
Ya se ha mostrado que xxx .
Si en esta ecuación xxx .
Por lo tanto xxx .

De donde, una velocidad específica de crecimiento de 1 h- 1 equivale a un tiempo de duplicación de 0,693 h.
La Tabla 7.1 ofrece algunos valores representativos del Mmax para diversos microorganismos.

Las ecuaciones (2) y (3) ignoran el hecho de que el crecimiento acarrea el agotamiento de nutrientes y la acumulación de subproductos tóxicos y, por tanto, predice que el crecimiento continúa indefinidamente.
Sin embargo, a medida que se agota el sustrato o se acumulan subproductos tóxicos la velocidad especifica de crecimiento se desvía del valor máximo; el crecimiento con el tiempo cesa y se entra en la fase estacionaria.

La disminución de la velocidad del crecimiento y el cese del crecimiento debido al agotamiento de sustrato puede describirse por la relación entre xxx y la concentración residual del sustrato limitante.
El sustrato limitante es aquel componente del medio que primero se agota.
Monod (1942) demostró la siguiente relación: xxx donde s es la concentración residual del sustrato limitante y Ks es la constante de utilización o saturación para el sustrato limitante y equivale a la concentración de sustrato cuando el valor de m es la mitad que el de Mmax .

La ecuación (8) se representa gráficamente en la Figura 7,3; la zona B a C equivale a la fase exponencial del cultivo, con sustrato en exceso y un crecimiento con un Mmax .
La zona A a B corresponde a la desaceleración de la velocidad de crecimiento, desde el final de la fase exponencial al inicio de la fase estacionaria.
El valor numérico del Ks refleja la afinidad del microorganismo por el sustrato (un elevado valor del Ks indica una baja afinidad y un bajo valor del Ks refleja una gran afinidad).
Por lo tanto, si el microorganismo tiene una gran afinidad por el sustrato limitante (bajo Ks) la velocidad especifica de crecimiento en un cultivo discontinuo no se verá afectada hasta que la concentración de sustrato disminuya hasta niveles muy bajos.
En consecuencia, un microorganismo de estas características mostrará una fase de desaceleración corta.
Del mismo modo, si un microorganismo tiene una baja afinidad por el sustrato limitante (elevado Ks) la velocidad específica de crecimiento disminuirá cuando no existe una concentración de sustrato relativamente elevada y, por tanto, en el crecimiento de dicho microorganismo se observará una fase de desaceleración larga.
Algunos valores representativos del Ks de diversos microorganismos se muestran en la Tabla 7,2.

Tabla 7-1

Valores representativos del Mmax de diversos microorganismos (calculados de acuerdo con las condiciones especificadas en las referencias)

Microorganismo.
Referencia.


Beneckea natriegens.
Eagon,1961.


Methylomonas methanolytica.


Penicillium Chrysogenum .
Trinci,1969.


Figura 7-3

Efecto de la concentración residual del sustrato limitante en la velocidad específica de crecimiento de un microorganismo 

La cantidad de biomasa en la fase estacionaria depende de la composición del medio y de la eficacia del microorganismo para convertir los sustratos en células. 
De forma ideal, el medio debe formularse de tal manera que el factor limitante del crecimiento sea un sólo sustrato y no la acumulación de sustancias tóxicas.
La concentración de células en la fase estacionaria viene dada por la ecuación: xxx donde x es la concentración de células, Y es el factor de rendimiento para el sustrato limitante (g de biomasa por g de sustrato consumido), SR es la concentración original del sustrato en el medio.

El término Y constituye una medida de la eficacia de las células para convertir el sustrato en biomasa. 
La biomasa, en cualquier momento del cultivo discontinuo, viene dada por la ecuación: xxx donde s es la concentración residual de sustrato en aquel momento.
Entonces, xxx .

Por tanto, suponiendo que células y sustrato están en las mismas unidades, Yes una constante sin dimensiones.
El valor de Y dependerá, obviamente, del microorganismo y del sustrato.
La importancia de este valor se discute más detalladamente en 8,3,1.

Tabla 7-2

Valores representativos del Ks para diversos microorganismos y sustratos

Microorganismo.
Sustrato.
Referencia .


Escherichia coli .
Glucosa .
Shehatay Marr (1971).


Escherichia coli .
Iones fosfatos.
Shehatay Marr (1971) .


Pseudomonas sp .
Metanol.
Harrison(1973).


Aspergillus niger .
Glucosa.
Pirt(1973).


Saccharomyces cerevisiae.
Glucosa.
Pirty Kurowski(1970) .


Control de las condiciones ambientales del microorganismo del proceso

Introducción

Las condiciones ambientales que se le imponen al microorganismo para su crecimiento deben controlarse durante la fermentación para conseguir la máxima productividad.
Los factores ambientales más importantes son:

(1) La composición química del medio debe ser tal que permita al microorganismo sintetizar el producto óptimamente dentro de los aspectos económicos del proceso.

(2) La temperatura del proceso debe ser la óptima para la síntesis del producto.

(3) El cultivo debe mantenerse en un estado puro durante la fermentación.


Estos factores ambientales dependen de:

(1) Diseño del fermentador y programación de las condiciones en que se opere.

(2) Formulación del medio de cultivo.

(3) Modo de operación del proceso fermentativo. 


Diseño y funcionamiento del fermentador

El estudio detallado del diseño de un fermentador cae fuera del objetivo de este libro que se concentra en los principios biológicos de la Biotecnología.
Sin embargo, como la tecnología de los procesos fermentativos es una amalgama de técnicas biológicas e ingeniería química, es necesario proporcionar un breve resumen de los tipos de fermentadores disponibles y de sus principales características. 
Stanburyy Whitaker (1984) prepararon una lista de 13 puntos que consideraron eran los criterios más importantes para el diseño de un fermentador:

(1) El tanque debe diseñarse para que funcione asépticamente durante numerosos días, así como para las operaciones de más larga duración. 

(2) Se debe proporcionar un sistema adecuado de aireación y agitación para cubrir las necesidades metabólicas de los microorganismos.

(3) El consumo de energía debe ser tan bajo como sea posible.

(4) Debe tener un sistema para el control del pH. 

(5) El fermentador debe tener un sistema para la toma de muestras.

(6) Debe existir un sistema para el control de la temperatura.

(7) Las pérdidas por evaporación no deben ser excesivas.

(8) El diseño del tanque debe ser tal que las operaciones laborales durante el funcionamiento, recolección, limpieza y mantenimiento sean mínimas.

(9) El tanque debe ser versátil, para la aplicación de diversas modalidades de procesos.

(10) Las superficies interna del tanque deben ser lisas, utilizando, donde sea posible, soldaduras en vez de juntas de pestaña.

(11) La geometría del fermentador debe ser similar a otros tanques más pequeños o mayores de la planta o a los de la planta piloto para poder reproducir procesos a diferentes escalas.

(12) Deben emplearse los materiales más baratos que proporcionen resultados satisfactorios.

(13) Debe existir un servicio adecuado de repuestos para el fermentador.


El mantenimiento de un ambiente aséptico y unas condiciones aeróbicas son, probablemente, los dos puntos de mayor relevancia que hay que considerar.
Los fermentadores más ampliamente utilizados a nivel industrial están provistos de mecanismos de agitación, dispersión y aireación como de sistema para el control de la temperatura, pH y formación de espuma.
En la Fígura 8,1 se muestra la representación esquemática de un fermentador.
Aunque la gran mayoría de los fermentadores comerciales disponen de agitación mecánica para lograr una buena mezcla y transferencia de oxigeno, en algunos se realizan esta función mediante la inyección de aire, careciendo de agitación mecánica.
La Figura 8,2 ilustra algunos ejemplos de fermentadores que no tienen agitación mecánica; el mejor conocido quizás sea el que se utiliza para el proceso ICI Pruteen. 

Transferencia de oxígeno

El oxígeno no es una gas muy soluble (una solución saturada de oxigeno contiene aproximadamente 10 mg. de este gas) y su suministro se logra pulverizando aire en el fermentador durante el proceso.
La transferencia de oxigeno en solución viene definida por la ecuación: xxx donde CL es la concentración de oxigeno disuelto ( xxx ) t es el tiempo (h) C* es la concentración saturada de oxígeno disuelto ( xxx ) KL es el coeficiente de transferencia de masa ( xxx ) a es área interfacial gas/líquido por unidad de volumen d es la velocidad de transferencia de oxigeno ( xxx ).

Figura 8-1

Representación esquemática de un fermentador aireado mediante agitación

Los términos KL Y a son muy difíciles de medir en un proceso fermentativo y, por ello, ambos se combinan en la expresión KLa que se denomina coeficiente de transferencia volumétrica; se utiliza como un índice de la capacidad de aireación del fermentador.
El valor de este coeficiente depende del diseño del tanque, del grado de agitación (medido como la cantidad de energía consumida para agitar el contenido del tanque), del flujo de aire, de la viscosidad del medio de cultivo y de la presencia de agentes antiespumantes en el medio ( Stanburyy Whitaker, 1984).
Por tanto, el diseño de un fermentador y de las condiciones de operación debe ser tal que cubra la demanda de oxigeno del microorganismo.
Sin embargo, debe apuntarse que la concentración de oxigeno disuelto en el medio tiene una marcada influencia en la fisiología de un microorganismo aerobio.
En consecuencia el Kla de un fermentador debe tener un valor que permita que la cantidad deseada de oxigeno pueda transferirse por unidad de tiempo (o sea, cubrir la demanda de oxigeno del microorganismo) en presencia de una cierta concentración de oxigeno disuelto en el medio (que mantiene a las células en la correcta forma fisiológica).

El efecto de la concentración de oxígeno disuelto en la velocidad específica de consumo de oxigeno (mmol de oxigeno por g de peso seco de biomasa/h) es del tipo de la relación de Michaelis-Menten (o de Monod) según se muestra en la Fígura 8,3.
Puede deducirse que a medida que aumenta la concentración de oxígeno disuelto, aumenta la velocidad específica de consumo de oxigeno hasta alcanzar un valor máximo.
La concentración más baja de oxigeno disuelto a la que la velocidad específica de consumo de oxigeno es máxima se denomina Cajt.
Por tanto, el mantenimiento del cultivo en condiciones aeróbicas totales conlleva que el fermentador pueda sostener la concentración de oxígeno disuelto por encima del Cajt (en la Tabla 8,1 se ofrecen algunos valores del Cajt).
Sin embargo, las condiciones de aireación óptima para la formación del producto microbiano que se pretenda obtener pueden ser diferentes a las que proporcionan una producción máxima de biomasa, por lo que puede ocurrir que sea necesario que el fermentador opere a una concentración de oxigeno disuelto por debajo o por encima del Cajt.

Figura 8-2

Algunos ejemplos de fermentadores sin agitación mecánica

(a) Fermentador de ciclón (reproducido con permiso de Dawson, 1974).
(b)Fermentador de circulación de aire con ramal externo (reproducido con permiso de Taylor y Senior, 1978).
(c) Fermentador ICI de circulación de aire con ramal interno (reproducido con permiso de Smith, 1980).
(d) Fermentador Vogelbusch de chorro de aire.

La producción de alcohol comercial a partir de levaduras constituye un claro ejemplo en el que la concentración de oxigeno disuelto se mantiene por debajo del valor de CCrjt. 
Aunque se considera que la producción de alcohol es un proceso anaeróbico, se requiere oxígeno para la síntesis de esteroles y ácidos grasos insaturados que componen la membrana de la levadura.
En este caso, el fermentador debe diseñarse para suministrar cantidades relativamente bajas de oxígeno.

Figura 8-3

Efecto del oxígeno disuelto en la velocidad específica de consumo de oxígeno de un cultivo microbiano

Tabla 8-1

Concentraciones críticas de oxigeno disuelto de algunos microorganismos

- Microorganismo
- Azotobacter sp .
- Escherichia coli 
- Saccharomyces sp .
- Penicillium chrysogenum 

Un ejemplo excelente de los efectos de la concentración de oxigeno disuelto en la producción de aminoácidos se describe en el trabajo de Hirose y Shibai (1980).
Estos autores hallaron un Ccjt para Brevibacterium flavum de xxx y representaron el nivel de suministro de oxigeno como el grado en que se satisfacían las necesidades de oxígeno (velocidad respiratoria del cultivo expresada como fracción de la velocidad respiratoria máxima).
Si el grado en que se satisfacían las necesidades era inferior a 1, indicaba que la concentración de oxígeno disuelto está por debajo del C, x, t.
La producción de componentes de las familias de los aminoácidos glutamato y aspartato se veía afectada adversamente por valores de dicho grado de «satisfacción» inferiores a 1, mientras que la producción óptima de fenilalanina, valina y leucina se daba a grados de «satisfacción» de 0,55, 0,60 y 0,85, respectivamente.
Las rutas biosintéticas de estos aminoácidos se muestran en la Figura 8,4; puede observarse que la fenilalanina, la valina y la leucina derivan de intermediarios de la glicolisis, fosfoenolpiruvato y piruvato, mientras que las familias del glutamato y aspartato proceden de intermediarios del ciclo del TCA.
Por ello, la limitación de oxigeno y la disminución concomitante de las reacciones del ciclo del TCA conllevará la presencia de mas intermediarios disponibles para la biosíntesis de fenilalanina, valina y leucina.
Por otra parte, si el suministro de oxígeno satisface totalmente las necesidades del mismo, el ciclo del TCA operará plenamente y, por tanto, habrá un suministro electivo de precursores de glutamato y asparatato.

Figura 8-4

Rutas biosintéticas de diversos aminoácidos en Brevibacterium flavum

La producción de cefalosporina por Cephalosporium (Feren y Squires, 1969) es óptima a concentraciones de oxigeno disuelto considerablemente superiores que al valor de Cjt de la cepa.
En este caso, el fermentador debe tener la capacidad de suministrar una concentración de oxigeno disuelto mayor que la necesaria para mantener el cultivo en condiciones aeróbicas.

Las propiedades reológicas del caldo de fermentación tienen un considerable efecto en el KLa que se logra en el fermentador. 
Un microorganismo unicelular origina normalmente un caldo con una viscosidad relativamente baja que apenas se ve afectada por el nivel de agitación (o sea, es un fluido newtoniano). 
Sin embargo, un microorganismo de estructura miceliar origina, con frecuencia, un caldo con una gran viscosidad que varia con el grado de agitación (o sea, no es un fluido newtoniano).
El grado de agitación en el fermentador es diferente en las distintas partes del medio (es mayor en las regiones cercanas al agitador y más bajo en las zonas más distantes del agitador).
Por lo tanto, el caldo en la zona central tiende a ser menos viscoso, con lo que el aire puede ingresar mejor en esta región, desarrollándose unas condiciones más anaeróbicas en las zonas más viscosas.
Es importante, por ello, que el régimen de agitación durante la fermentación proporcione una buena transferencia de oxigeno y una zona de gran turbulencia pero que esa turbulencia se transmita también a través del caldo.

Una solución de este problema consiste en utilizar dos tipos diferentes de agitación en el sistema uno diseñado para producir una zona de gran turbulencia y el otro mediante la instalación de un sistema de bombeo del caldo Legrys y Solomon (1977) diseñaron un fermentador con una turbina discal en el fondo y una hélice impulsora en la parte superior; la turbina origina la turbulencia mientras que la hélice proporciona un gran flujo, con lo que el volumen del tanque se removía cada 20 30 segundos.
De este modo, el micelio circulaba a través de la zona de aireación del tanque antes que se agotara el oxígeno.
Marshy Pinkey (1985) describieron el proceso piloto para el cultivo de Fusarium graminearum que se utilizó para la producción de microproteína xxx .

Como ya se ha discutido (véase Capítulo 71) la estructura filamentosa del microorganismo influye de una forma crítica en la calidad al masticar de los muchos productos que se preparan a partir de micoproteína.
Por ello, el fermentador debe tener la capacidad de airear un caldo muy viscoso no newtoniano.
El fermentador piloto de 1,3 m3 tiene una turbina impulsora « Rushton» de seis hojas que ocasiona la transferencia de oxigeno y un sable de tres hojas que proporciona una rápida circulación en el interior del tanque.

Mantenimiento de las condiciones asépticas 

El fermentador y su contenido ha de esterilizarse antes del comienzo del proceso y las condiciones asépticas deben mantenerse durante la fermentación.
La consecución de unas condiciones asépticas es, en esencia, un problema de ingeniería y, por tanto, cae fuera del objetivo de este texto.
No obstante, se ofrece una lista de los puntos principales para familiarizar al lector con este aspecto tan importante:

(1)En un fermentador provisto de agitación, el eje del mismo debe entrar en el tanque a través de una junta aséptica.

(2) El fermentador debe construirse de tal forma que pueda esterilizarse, al igual que su contenido, con vapor. 
Por lo tanto, debe existir un suministro de vapor en cada punto de entrada y salida en el tanque.
La esterilización se realiza inyectando vapor por todos los puntos de entrada y salida, salvo en el de salida de aire que sirve para que escape el vapor.

(3) El aire que ingresa en el fermentador debe haberse esterilizado por filtratación.

(4) Durante la fermentación debe existir en el interior del tanque una sobrepresión para que cualquier fuga que se produzca ocasione la salida de aire del fermentador y no el ingreso de aire del exterior que pudiera estar contaminado. 

(5) El tanque ha de construirse de tal forma que las operaciones de inoculación, toma de muestra y adiciones posteriores necesarias puedan hacerse asépticamente.


Las operaciones de esterilización y adiciones asépticas en una planta industrial son maniobras complejas que pueden requerir, quizás, la manipulación de cientos de válvulas.
Por ello, la posibilidad de un error humano es muy elevada.
No obstante, el advenimiento del control por ordenadores ha sido de gran valía para el control y seguimiento de la esterilización y operaciones asépticas.
De acuerdo con Tonge (1980), el funcionamiento del fermentador ICI Pruteen seria extremadamente difícil sin el control informático.

Formulación del medio de cultivo 

El medio de cultivo que se utilice en la fermentación no sólo debe cubrir las necesidades nutritivas del microorganismo sino también las exigencias del proceso industrial.
Por ello, deben tenerse en cuenta una serie de factores importantes para la formulación del medio.
Entre ellos, el coste, la eficacia de la utilización, la reología y su efecto en las corrientes descendentes.

Todos los microorganismos requieren agua, fuentes de carbono y nitrógeno, elementos minerales y, posiblemente, nutrientes específicos, como vitaminas y aminoácidos.
La mayoría de los microorganismos comercialmente importantes son quimioorganotróficos y, por tanto, la fuente de energía y carbono es una y la misma, El carbono y el nitrógeno se suministran habitualmente en forma de mezclas complejas de productos naturales baratos o son subproductos de otro proceso. 
Los elementos traza existen normalmente en la materia prima que sirve de sustrato o en el agua corriente que se utiliza en la preparación del medio.

Cualquier nutriente especifico que se requiera, como vitaminas o aminoácidos, debe añadirse en forma pura pero puede que exista también en el mercado como un extracto de origen animal o vegetal.
Los fosfatos deben incorporarse al medio como agentes tamponadores aunque normalmente el pH se controla externamente.
La Tabla 8,2 ofrece algunos ejemplos de fuentes de carbono y nitrógeno que se utilizan habitualmente en las fermentaciones.
En la Tabla 8,3 se muestran algunos ejemplos de formulaciones de medios industriales.

Rendimiento de biomasa

El rendimiento de biomasa que puede obtenerse por unidad de carbono que se incorpora al medio es un criterio importante para la elección de la fuente de carbono, especialmente para la producción de proteína unicelular.
El coeficiente de rendimiento celular (Y) para una fuente de carbono viene definido por la expresión: xxx . 

En la Tabla 8,4 se ofrecen algunos valores representativos de Y; puede observarse que algunos hidrocarburos proporcionan un mayor rendimiento que los carbohidratos.
La razón de esto es que los hidrocarburos muy reducidos contienen más carbono por g de sustancia que los carbohidratos.
Sin embargo, el mayor rendimiento implica una mayor demanda de oxigeno de las células al crecer sobre estos sustratos muy reducidos (Fíg. 8,4).
Por lo tanto, se ha de suministrar más oxígeno a la fermentación basada en hidrocarburos que en la realizada a partir de carbohidratos. 
El consumo de grandes cantidades de oxigeno implica, además, la generación de un calor considerable que debe eliminarse para mantener la temperatura de fermentación al valor óptimo.

En consecuencia, la elección de la fuente de carbono influye de una forma importante en el diseño del fermentador para que se suministre el oxígeno suficiente que permita una completa utilización de la fuente de carbono.

Asimismo, es necesario también diseñar un área de intercambio de calor adecuada para eliminar la energía calorífica que se genere.
Un ejemplo excelente de estas consideraciones es el fermentador ICI Pruteen. 

Inductores

El medio, aparte de proporcionar los sustratos necesarios para el crecimiento, tiene también una considerable influencia en los productos finales generados por el microorganismo.
Los enzimas catabólicos son, con frecuencia, inducibles; es decir, sólo se sintetizan en presencia de un inductor que normalmente es el sustrato del enzima o una sustancia de estructura similar.
Por ello, un medio para la producción comercial de un enzima catabólico debe incluir en su composición un inductor relevante.
La Tabla 8,5 muestra algunos ejemplos de enzimas comercialmente importantes y el sistema inductor correspondiente. 

Se ha demostrado que la producción de ciertos metabolitos secundarios esta sometida también a fenómenos inductores. 
Por ejemplo, la a-manosidasa es un enzima clave en la biosíntesis de estreptomicina y su síntesis se induce por la presencia de manano.
Por ello, extracto de levadura es el sustrato que habitualmente se utiliza como fuente de nitrógeno en la fermentación destinada a la obtención de aquel antibiótico.

Tabla 8-2

Algunos ejemplos de fuentes de carbono y nitrógeno utilizadas habitualmente en los medios fermentativos

Fuentes de carbono.
Fuentes de nitrógeno .


Almidón.
Amoníaco .


Glucosa.
Sales amoníacas .


Malta.
Nitratos.


Melazas de remolacha.
Macerado de maíz.


Melazas de caña.
Gránulos de cacahuetes .


Aceites vegetales.
Harina de soja.


Hidrocarburos.
Sangre deshidratada.
Medios formulados.
Harina de soja.
Solubles de destilería.
Extracto de levadura.


Tabla 8-3

Algunos ejemplos de medios para fermentaciones industriales 

Tabla 8-4

Factores de rendimiento proporcionados por algunos microorganismos durante su crecimiento a partir de diversos carbohidratos

Sustrato.
Microorganismo.


Glucosa.
Escherichia coli .


Metanol.
Pseudomonas c.


Octadecano .
Pseudomonas sp .


Tabla 8-5

Algunos ejemplos de inductores utilizados en la producción de enzimas comercialmente importantes

Enzima.
Inductor.


Proteasas.
Diversas proteínas.


a-amilasa.
Almidón.


Celulasa.
Celulosa.


Pectinasa .
Pectina.


Penicilina acilasa .
Ácido penilacético.


Represores

La producción de enzimas y metabolitos secundarios está frecuentemente inhibida por la presencia de ciertas sustancias en el medio de cultivo.
Se ha observado que las fuentes de carbono que son rápidamente utilizadas inhiben la síntesis de amilasas y una amplia variedad de metabolitos secundarios, por ejemplo, griseofulvina, penicilina, bacitracina y actinomicina (Demain, 1984).
Después de muchos años de utilizar medios empíricos, la producción de la mayoría de los antibióticos se efectúa utilizando una fuente de carbono distinta a D-glucosa, o si se utiliza ésta, se alimenta el cultivo con un flujo bajo (véase Sección 8,4).

Igualmente se ha observado que las fuentes de nitrógeno tienen también efectos represores en la producción de metabolitos secundarios.
Por ejemplo, el que la harina de soja y la prolina sean las mejores fuentes nitrogenadas para la producción de estreptomicina se debe, posiblemente, a su lenta utilización, con lo que se evita el efecto represor en la síntesis del metabolito (Demain, 1984). 

Otro de los represores normales de los metabolitos secundarios son los fosfatos inorgánicos, como se ha demostrado en el caso de la producción de estreptomicina, bacitracina, oxitetraciclina y novobiocina.

En consecuencia, la elección del factor limitante (el componente que se agota primero y, por tanto, limita el crecimiento) en un medio de cultivo dependerá de los efectos represores de los componentes del medio en la productividad.
Por ello, dependiendo del sistema de control implicado, los medios comerciales pueden estar limitados por la sustancia carbonada o nitrogenada, por fosfatos u, ocasionalmente, por otros nutrientes. 

Precursores

La producción de un determinado metabolito puede incrementarse notablemente si se añade al sistema un precursor del metabolito.
El ejemplo clásico de esta situación es la adición de ácido fenilacético en la producción de penicilina que actúa como precursor de la cadena lateral de la bencilpenicilina.
Otro ejemplo de esta naturaleza es el papel precursor del ion cloruro en las fermentaciones de clortetraciclina y griseofulvina.

Modo de operación de los procesos fermentativos

Como se ha discutido en el Capitulo 7, los microorganismos pueden crecer en sistemas discontinuos, continuos 0 discontinuos alimentados.
Aunque el cultivo continuo es el que ofrece el mayor grado de control en el crecimiento y la fisiología de los microorganismos, su uso en fermentaciones industriales está limitado por las razones que se han expuesto anteriormente (Sección 7.4).
Sin embargo, la adopción del cultivo discontinuo alimentado ha proporcionado al tecnólogo de fermentaciones una herramienta muy útil para el control de las condiciones ambientales de las fermentaciones.
El tipo más común de sistema discontinuo alimentado que se utiliza en cultivos masivos es la adición de un componente del medio al sistema en fermentación, controlándose la velocidad de adición mediante un parámetro fácilmente medible.
El sustrato que con más frecuencia se añade es la fuente de carbono aunque puede utilizarse cualquier componente que tenga un efecto critico en el control de la fermentación.
Los parámetros que más frecuentemente se utilizan para el seguimiento de la alimentación son la concentración de oxígeno disuelto y el pH, aunque también se pueden utilizar otros como la viscosidad. 
La principal ventaja de añadir un componente del medio en vez de incorporar su totalidad al principio es que el nutriente puede mantenerse a muy bajas concentraciones durante la fermentación.

Un nivel bajo de un nutriente (pero que se está reponiendo constantemente) tiene las siguientes ventajas:

(1) Mantiene la condiciones del cultivo dentro de los limites de la capacidad de aireación del fermentador. 

(2) Elimina los efectos represores de los componentes del medio (fuentes de carbono, de nitrógeno o fosfatos) al ser utilizados rápidamente.

(3) Evita los efectos tóxicos de un componente esencial del medio.

(4) Proporciona un nivel limitante del nutriente requerido por un mutante auxótrofo.


El ejemplo más antiguo del uso de un cultivo discontinuo alimentado es la producción de levadura de panadería.
Ya en 1915 se observó que un exceso de malta en el medio ocasionaba una producción de biomasa muy rápida y, por tanto, una demanda de oxigeno que no podía ser satisfecha por el fermentador (Raed y Peppler, 1973), con lo que se favorecía el establecimiento de condiciones anaeróbicas que originaba la formación de etanol por la biomasa.
La solución del problema consistió en hacer crecer las levaduras inicialmente en un medio poco concentrado y añadirle más medio concentrado a un ritmo menor que el que el microorganismo podía utilizar el sustrato.
Hoy se sabe que una elevada concentración de glucosa ejerce un efecto represor en la actividad respiratoria, por lo que en las modernas fabricas de producción de levadura la alimentación con melazas está bajo un estricto control que se efectúa midiendo el etanol existente (cantidades trazas) en el gas que sale del fermentador.
Tan pronto como se detecta etanol se reduce la velocidad de alimentación. 
Aunque estos sistemas originan que el crecimiento sea más lento que el que se consigue en otras condiciones, el rendimiento esta próximo al máximo que puede obtenerse teóricamente (Fiechter, 1982).

La producción de penicilina constituye un buen ejemplo del uso de cultivos discontinuos alimentados para la producción de un metabolito secundario (Hersbach, Van der Beek y Van Dijck, 1984).
La producción de penicilina es un proceso de «dos fases»; la inicial es una etapa de crecimiento seguida por una fase de producción o idiofase.
Durante la fase de producción, el fermentador se alimenta con glucosa a un ritmo que permite una velocidad de crecimiento relativamente alta (y, por tanto, una rápida acumulación de biomasa) pero que limita la demanda de oxigeno al nivel que puede ser cubierto por el fermentador.

La velocidad de la alimentación se controla midiendo la concentración de oxígeno disuelto o el pH del caldo.
Si la concentración de oxigeno disuelto cae por debajo del Cjt (véase Sección 8,2) se disminuye el flujo.
Si se utiliza el pH como medio de seguimiento de la fermentación, un descenso del pH indica que se están formando ácidos orgánicos y, por tanto, se están creando condiciones de anaerobiosis.
En tal caso es necesario disminuir el ritmo de adición de glucosa.

Durante la fase de producción, la biomasa debe mantenerse a una velocidad de crecimiento relativamente baja, por lo que la glucosa se añade a un pequeño flujo de tal forma que se mantenga su concentración en el medio a un nivel inferior al que se inhibe la formación de penicilina (véase Sección 8,3,3).

Durante esta fase, la concentración de oxigeno disuelto se utiliza para controlar la velocidad de dilución por retroalimentación (feedback).
El ácido fenilacético es un precursor de la penicilina G (benzilpenicilina) (véase Sección 8,3,4) pero es también tóxico para el moho a una concentración tres veces superior a la que se utiliza.
Por ello, el ácido fenilacético es otra de las sustancias que va incluida en el medio de alimentación del fermentador durante la fase de producción.

En el Capitulo 9 se ha discutido el uso de mutantes auxótrofos en la producción de productos de origen microbiano Estos mutantes pueden crecer sólo cuando se suministran ciertos nutrientes, que no necesitan las cepas silvestres.
Además, para conseguir la producción óptima del producto deseado, el crecimiento de un mutante auxótrofo debe limitarse por la disponibilidad del nutriente que requiera.
Por lo tanto, la adición del compuesto requerido para la fermentación a un flujo inferior que al que puede consumirse facilita al tecnólogo controlar el proceso dentro de unos márgenes estrechos. 

Resumen

Las condiciones ambientales influyen marcadamente en el crecimiento microbiano.

El entorno del microorganismo depende del modo de operación, del diseño del fermentador y de la composición del medio de cultivo.
El fermentador hay que diseñarlo con el objetivo de mantener el cultivo en estado puro y que facilite que el proceso fermentativo se desarrolla en unas condiciones físicas y químicas óptimas.
El suministro de oxigeno al microorganismo debe ser tal que sea óptima la formación del producto que se pretenda obtener, lo que puede implicar que la concentración de oxigeno disuelto pueda ser diferente a la necesaria para el crecimiento óptimo. 
El medio utilizado en la fermentación debe tener todos los nutrientes que el microorganismo requiera para su crecimiento así como las sustancias que se necesiten para proporcionar una síntesis del producto óptima.

Las condiciones ambientales dependen del tipo de fermentación; el sistema que se utiliza de una forma más común es el cultivo discontinuo alimentado Los sistemas discontinuos alimentados permiten al tecnólogo mantener condiciones aeróbicas, controlar los efectos represores de los componentes del medio y proporcionar componentes esenciales (que a veces son sustancias tóxicas) en concentraciones bajas.
Por lo tanto, el mantenimiento de unas condiciones ambientales constantes y controladas requiere una combinación de conocimientos biológicos y de ingeniería química.


Microsatélites de ADN

Las secuencias repetitivas de ADN desempeñan un papel destacado en la adaptación de las bacterias a los ambientes donde medran.
¿Cumplen una función similar en organismos superiores?

El código genético humano está constituido por unos 3000 millones de bases de ADN.
Sólo de un 10 a un 15 por ciento de esas bases forman parte de los genes, los planos maestros que la célula utiliza para construir sus proteínas.
A otras secuencias de pares de bases les competen - en el hombre y otros muchos organismos- funciones muy importantes; por ejemplo, promover la activación o inactivación de los genes y mantener unidos los cromosomas. 
Queda una buena parte del ADN sin manifiesta misión obvia.
Algunos la llaman «chatarra».

En ese «ADN chatarra» hay secuencias de características singulares, agrupadas bajo la denominación común de ADN satélite.
Lo forman, en efecto, secuencias repetitivas de las cuatro bases del ADN-adenina ( A ), citosina ( C ), guanina ( G ) y timina ( T )-, en diversa combinación y repetidas en una suerte de tartamudeo.
Los microsatélites, que contienen las repeticiones más cortas, encierran un interés muy superior al que cabría deducir de su tamaño, realizan funciones múltiples y sorprendentes. 

Se hace cada vez más patente que la naturaleza repetitiva del ADN microsatélite le capacita para crecer o disminuir en longitud y que estos cambios pueden acarrear consecuencias buenas o malas para los organismos portadores.

En determinadas bacterias patógenas las secuencias repetitivas promueven la aparición de propiedades nuevas que capacitan a esos microorganismos para sobrevivir ante cambios del entorno potencialmente letales.
Es probable que algunos microsatélites ejerzan efectos importantes en el hombre, pues hay en el genoma humano- el conjunto del ADN de cada una de nuestras células- unos 100.000.
Hasta la fecha, la única misión asignada a los microsatélites del hombre es negativa; causan diversas enfermedades neurológicas. 
Sin embargo, podrían ser restos supervivientes de procesos evolutivos que desembocaron en la conformación de nuestra especie.

Mientras unos indagan los motivos de la cuantiosa presencia de ADN repetitivo en el hombre, otros se apoyan en los microsatélites para el diagnóstico de enfermedades neurológicas y para la detección de personas en riesgo de padecerlas. 
Se está comprobando que los microsatélites cambian de longitud en fases precoces de ciertos cánceres, lo que les convierte en valiosos marcadores para el diagnóstico precoz de tales patologías.
Y puesto que la longitud de los microsatélites varía de una persona a otra, en ellos comienzan a apoyarse la identificación de criminales y la determinación de la paternidad.
A este procedimiento se le denomina «identificación dactilar por ADN»(«fingerprinting»). 

En los años sesenta se conoció el primer ADN satélite.
Se descubrió que, cuando se centrifugaba ADN en determinadas condiciones, el ácido nucleico se disponía en dos o más capas:
una banda o estrato principal, que contenía los genes, y bandas secundarias, o bandas satélites.
Las bandas satélites constaban de secuencias muy largas de ADN repetitivo. 
En 1985 Alec J., Jeffreys encontró otras regiones más cortas de ADN repetitivo; tales minisatélites - así los llamó - eran zonas repetidas de 15 o más bases. (El grupo de Jeffreys determinó que el número de repeticiones en un minisatélite difería de un individuo a otro, y en ello basó su diseño de la técnica de la identificación por ADN.) A finales de los ochenta, James L., Weber y Paula L., May, por un lado, y Michael Litt y Jeffrey A., Lutty, por otro, aislaron satélites constituidos por zonas repetitivas de ADN menores incluso.
Los denominaron microsatélites.
Se han mostrado muy eficaces para la identificación de las personas por ADN.

Se admite que el ADN microsatélite está constituido por secuencias de hasta unos seis pares de bases, iterados y unidos en secuencia continua.
Debe su interés especial en los procesos evolutivos a una altísima tasa de mutación: 
la probabilidad de que un microsatélite adquiera o pierda, de una generación a la siguiente, una de las regiones repetidas multiplica 10.000 veces la probabilidad de que tal ocurra en el gen responsable de la anemia falciforme, en el que la mutación de una base da lugar a la enfermedad. 
Y si es raro que esa mutación singular de la anemia falciforme revierta el gen a su estado inocuo, no puede predicarse lo mismo de los microsatélites, que vuelven sin problema a sus longitudes previas, incluso a las pocas generaciones.

En 1986 el grupo de Thomas F., Meyer sacaba a la luz la función de los microsatélites en las bacterias patógenas.
El equipo de Meyer investigaba Neisseria gonorrhoeae , la bacteria causante de la gonorrea, una enfermedad de transmisión sexual.
N. gonorrhoeae , unicelular, posee una familia de hasta 12 proteínas de la membrana externa determinadas por genes Opa . (Les viene el nombre de la «opacidad» que presentan las colonias bacterianas que sintetizan proteínas Opa.) Merced a las proteínas Opa, la bacteria se adhiere e invade a las células epiteliales - las que tapizan el tracto respiratorio- y los fagocitos, células del sistema inmunitario.
Cada gen Opa contiene un microsatélite compuesto por múltiples copias de un motivo de cinco bases, CTCTT .

El enorme potencial de variación que arrastran las repeticiones de los microsatélites obedece a su proclividad a equivocarse en la replicación del ADN; cometen un error frecuente: el desajuste por deslizamiento entre secuencias complementarias. 
Antes de que una célula - bacteriana o de otro tipo- se divida, ha de duplicarse su ADN.
En este proceso complicado, cada molécula de ADN es una doble hélice en escalera de caracol, cuyos peldaños corresponden a los pares de bases.
El código genético se escribe con las bases de un lado de la escalera; las del otro lado son complementarias ( A siempre se empareja con T , y C con G ).

Durante la replicación del ADN,la escalera se parte por la mitad de cada peldaño; se separan los pares de bases al alejarse ambas cadenas helicoidales.
Las ADN polimerasas copian cada una de las hebras.
Mientras se va construyendo la nueva hebra, ésta se empareja con la que le sirve de molde.
Pueden darse errores de replicación por corrimiento cuando la hebra vieja, el molde o la hebra que se está formando se deslizan y establecen un emparejamiento inadecuado con la zona repetitiva de la otra hebra.
Este deslizamiento es la causa de que la ADN polimerasa añada o elimine una o más copias de las zonas repetitivas de la nueva hebra de ADN.

La frecuencia de este mecanismo de deslizamiento alcanza valores altos en N. gonorrhoeae .
En cada tanda de división bacteriana, de cada 100 o 1000 células hijas habrá una que porte una mutación que modifica el número de repeticiones de CTCTT .
Esta modificación puede ejercer efectos drásticos sobre los genes Opa , porque la información genética se interpreta en «palabras» de tres bases, los codones.
Las proteínas son ristras de aminoácidos y cada codón determina qué aminoácido ha de incorporarse en la cadena proteica.
Como la secuencia repetitiva no está constituida por tres bases, un aumento o una disminución del número de estas secuencias desplaza el significado de los codones subsiguientes.

En el caso de los genes Opa , la delación de una secuencia CTCTT conduce a la producción de una proteína recortada.
No podrá ya adherirse a la célula hospedadora; en consecuencia, la bacteria portadora de la proteína acortada queda incapacitada para penetrar en las células.
Pero el deslizamiento subsecuente abre la posibilidad de recuperar la zona repetitiva, permitiendo, por tanto, al gen Opa producir de nuevo una proteína funcional.

Conocido por variación de fase, este cambio reversible se da en muchas bacterias causantes de enfermedades.
Con los cambios alternativos que llevan a la conexión y desconexión de los genes Opa de una generación a otra, N. gonorrhoeae incrementa sus posibilidades de supervivencia.
Hay momentos en que al microorganismo le conviene adherirse a la célula hospedadora y penetrar en ella.
Tal acontece cuando la bacteria se propaga y llega a un nuevo huésped.
En otras ocasiones le resulta a ésta más ventajoso no interaccionar con la célula huésped; en particular si se trata de fagocitos, que destruyen las bacterias tras atraparlas en su interior.

Las implicaciones del emparejamiento incorrecto por deslizamiento y su relación con la capacidad para modificar las moléculas de superficie se han estudiado in extenso a propósito de la bacteria Hemophilus influeuzae .
Las células del tipo b de esta bacteria pueden provocar una meningitis infecciosa que afecta al cerebro y puede resultar letal.
Hasta la introducción de la vacuna a finales de los ochenta, uno de cada 750 infantes menores de cinco años contraía la meningitis por H. influenzae .

La membrana externa del H. influenzae está festoneada con moléculas de lípidos y azúcares unidas entre sí para formar un lipopolisacárido ( LPS ).
Una parte del LPS , el colín-fosfato, facilita la adhesión de H. influenzae a las células de la mucosa de la nariz y de la garganta, donde la bacteria se instala sin producir síntomas.
Al menos tres de los genes requeridos para la síntesis del LPS contienen microsatélites construidos a partir de la secuencia de cuatro bases CAAT .
Igual que sucede con los genes Opa de Neisseria gonorrhoeae , los cambios aquí en el número de repeticiones de CAAT pueden hacer que el H. influenzae sintetice LPS dotado de colín-fosfato o sin él.

Jeffrey N., Weiser ha demostrado que las cepas de H. influenzae que tienen colín-fosfato ( ChoP ) en sus moléculas de LPS , las cepas ChoP+ , colonizan nariz y garganta de un modo más eficaz que las cepas que carecen de ella, las cepas ChoP- .
Sin el colín-fosfato, la bacteria resiste mejor la agresión letal de factores diversos vehiculados por la sangre del huésped y otros líquidos tisulares. 
Las células bacterianas pueden alternar entre los dos estados, según se encuentren sin obstáculos para medrar en el tracto respiratorio o se estén difundiendo por la sangre hacia otros lugares, donde pueden sufrir el ataque del sistema inmunitario.

La mayoría de las bacterias de H. influenzae aisladas de humanos son variantes ChoP+ , sensibles al ataque inmunitario.
Las variantes ChoP- surgen de los emparejamientos incorrectos producidos en el deslizamiento; no persisten habitualmente en el tracto respiratorio, pues se adhieren a las células huésped sin la fuerza de las cepas ChoP+ .
Ahora bien, si el huésped contrae una infección vírica y se produce una inflamación de la mucosa nasal, se refuerza la exposición de las bacterias a las proteínas defensivas del sistema inmunitario del huésped. 
En ese caso, las variantes ChoP- tendrían ventaja para repeler el ataque.
Una vez que la infección vírica remite, las mutantes ChoP+ generadas por ulteriores emparejamientos defectuosos del ADN microsatélite tornarán a predominar.

Este tipo de genes que se activan o inactivan sin problemas se cobijan bajo la denominación común de genes de contingencia; capacitan, por lo menos a unas cuantas bacterias de una determinada población, para adaptarse a los avatares o riesgos de un nuevo escenario.
Entre los rasgos codificados por los genes de contingencia no podemos omitir los que gobiernan el reconocimiento inmunitario, la motilidad general, el movimiento hacia las señales químicas (quimiotaxis), la unión a las células huésped y su ulterior invasión, la adquisición de nutrientes y la sensibilidad a los antibióticos. 
Los genes de contingencia, que representan una fracción muy pequeña del ADN de una bacteria, le confieren una gran flexibilidad en su funcionamiento.
Si sólo 10 de los 2000 genes de una bacteria típica son genes de contingencia, por ejemplo, la bacteria podría dar lugar a 210, es decir 1024, combinaciones diferentes de genes «activos» o «inactivos».
Esta diversidad asegura que al menos una bacteria de la población pueda sobrevivir al ataque inmunitario del huésped o a la acción de otras defensas, para replicarse luego y fundar una nueva colonia. 

La inducción de la enfermedad - que se torna en su contra al destruir el huésped que le permite vivir- podría ser parte del precio que las bacterias pagan por disfrutar de esa capacidad de generar tantas variantes.
Una de las variantes puede aventurarse allende su nicho ecológico habitual en el tracto respiratorio o el intestino y desencadenar una infección potencialmente letal en otra región del organismo.
Habida cuenta de que tal sucede sólo en contadas ocasiones, las ventajas que los genes de contingencia aportan a la pervivencia de una especie bacteriana superan los inconvenientes de destruir algunos huéspedes.

Los microsatélites de estas bacterias son auténticas adaptaciones evolutivas.
Es poco probable que secuencias repetitivas tan singulares surgieran al azar; aparecerían, y se han conservado, porque capacitan a las poblaciones bacterianas para una pronta adaptación a los cambios del entorno. 

Pese a su interés, diríase que los genes de contingencia se circunscriben a las bacterias.
En los eucariotas, nosotros mismos, cuyas células contienen un núcleo, los microsatélites cumplen otras misiones. 
Ninguno de los microsatélites eucariotas identificados hasta la fecha ha logrado, que se sepa, liar el ADN para que determine proteínas no funcionales.
Se sitúan fuera de los genes, salvo un 10 por ciento que cae en su interior. 
De este 10 por ciento, casi todos pertenecen a los tripletes repetidos, que tienden a extenderse o a contraerse en tríos de bases.
Igual que al añadir el artículo «las» o al suprimir el artículo «los» de una frase no se pierde por lo común el sentido, de modo similar estas repeticiones pueden iterarse o perderse sin perturbar el mensaje del gen.
Por tener la misma longitud que un codón, podrían dar lugar a la inserción o delación de unos cuantos aminoácidos repetidos sin que se alterase la secuencia de todos los demás de la ristra.

¿Qué funciones desempeñan, pues, los microsatélites en los organismos superiores?
Se sospecha que algunos al menos tienen su cometido.
Los eucariotas poseen más microsatélites que las bacterias y muchos de ellos suelen estar alojados cerca o dentro mismo de genes implicados en vías reguladoras de procesos fundamentales. 
Sin embargo, hasta el momento sólo disponemos de pistas dispersas acerca de su posible tarea.

Los contados efectos cuya pista se ha podido seguir y atribuir a microsatélites eucariotas son, por lo general, nocivos. 
Así la enfermedad de Huntington, una afección neurodegenerativa grave caracterizada por la aparición tardía de demencia y pérdida progresiva del control motor.
Esta patología se dispara con la intervención de un gen defectuoso que codifica una proteína de gran tamaño, la huntingtina, de función desconocida. 
El gen normal contiene un microsatélite largo, constituido por repeticiones de tripletes, que añade una ristra de glutaminas cerca del extremo inicial de la proteína. 

El número de glutaminas del extremo inicial de la huntingtina varía de 10 a 30.
Pero las personas con Huntington portan un microsatélite que codifica una ristra de glutaminas excesivamente larga, de hasta 36 unidades o más.
Basta la herencia de una copia del gen defectivo - la paterna o la materna- para dictaminar la enfermedad.
Se ignora en virtud de qué mecanismo esa ristra de glutaminas repetidas insta la patogénesis.

Hay una docena larga de enfermedades asociadas a tripletes repetitivos; la mayoría son neuropatías raras. 
Casi la mitad de los microsatélites desencadenantes se alojan en el interior de un gen; en su mayoría, cifran glutaminas.
El resto se encuentra cerca de genes y pueden condicionar la función de éstos.

Neuropatía inhabitual es la atrofia muscular bulboespinal, provocada por la extensión de un microsatélite en el interior de un gen del cromosoma X.
El gen en cuestión determina un receptor de un andrógeno, hormona sexual masculina.
Los individuos con 40 0 más repeticiones de estos tripletes en una parte de uno de los genes de receptores de andrógenos acabar n desarrollando la enfermedad.
Un grupo dirigido por E.,L., Yong ha ido más lejos, demostrando que, cuando estas repeticiones son ligeramente más largas que lo normal, se presentan complicaciones clínicas.
Los varones con entre 28 y 40 repeticiones en la parte del gen del receptor codificadora de glutaminas son con frecuencia infértiles. 

Pero una cifra baja de repeticiones en el receptor de andrógenos acarrea también consecuencias adversas.
Se ha revelado que los varones con 23 o menos repeticiones son más proclives a padecer cáncer de próstata, aunque se trata de casos bastante raros.

¿Por qué tenemos todas estas bombas de relojería alojadas en nuestro genoma?
Es llamativo que las enfermedades relacionadas con tripletes repetitivos afecten al dominio nervioso y ninguno de dichos tripletes se dé en el chimpancé y otros primates.
Si este tipo de enfermedades resulta privativo de la especie humana, podría representar el tributo genético que hemos tenido que pagar por la evolución vertiginosa de nuestro cerebro.
Es posible que microsatélites largos. en medió de genes determinados o en su vecindad, condicionen la función del cerebro; por cuya razón persistieron quizá durante la evolución, aun cuando se extiendan alguna vez en exceso y sean causa de enfermedad. 

Hilvanando un razonamiento teórico, Wills, uno de los autores, propuso en 1989 que algunos genes habrían adquirido capacidad para evolucionar.
Según esa hipótesis, en un ambiente que fluctúe de un modo predecible- hacia mayor calor o hacia una situación de frío mayor- la posesión de un mecanismo genético de ágil adaptación tendría ventajas.
Los genes de contingencia de las bacterias constituyen ejemplos excelentes de genes con capacidad evolutiva.
Merced a una frecuente mutación hacia delante y hacia atrás, las bacterias se adaptan presto a cambios ambientales predecibles y revierten a las condiciones previas cuando reaparece la situación de entonces.

Tal vez los microsatélites eucariotas ejercen una forma más sutil de regulación que la que proporcionan a las bacterias sus genes de contingencia.
En el hombre los microsatélites alojados en medio de genes afectan a la velocidad de producción de ciertas proteínas, desde la que acompaña al pigmento biliar bilirrubina a determinados neurotransmisores, sustancias químicas que transmiten mensajes entre neuronas.
Para David G., King, estos microsatélites constituirían una suerte de «botones de ajuste fino», que habrían evolucionado actuando como reóstatos de la función del gen, reforzando la síntesis de una proteína en unos casos frenándola en otros.

El grupo de Walter Schaffner ha demostrado que la adición de microsatélites que añaden hileras de glutaminas o prolinas (otro aminoácido) en el comienzo de un determinado gen puede incrementar su capacidad para producir proteína. 
Tal vez, por ser menos perturbadora que la conmutación de los genes de contingencia, esta forma de regulación génica emergió durante la evolución de organismos pluricelulares complejos.

Han comenzado los primeros sondeos sobre las funciones desempeñadas por los microsatélites en nuestra especie.
Estas secuencias repetitivas, con su capacidad de cambiar rápidamente entre un número limitado de estados, nos ofrece una pista nueva para conocer nuestro poder de adaptación al cambio ambiental, a imagen de lo realizado por los genes de contingencia en bacterias.

Figura 1

EL MICROSATÉLITE ( región resaltada )ilustrado consta de una secuencia de unidades de ADN o bases - CAAT (citosina- adenina -adenina -timina)- que se repite cinco veces

Cada secuencia de CAAT se empareja con su complementaria GTTA (guanina-timina-timina-adenina) en la hebra opuesta de la escalera de ADN.
Las secuencias repetitivas, o motivos, pueden contener en los microsatélites hasta seis bases, y cada secuencia puede aparecer en múltiples copias.

Errores en la formación de pares por culpa del deslizamiento

En este proceso, el número de repeticiones de microsatélites aumenta o disminuye cuando una célula copia su ADN antes de dividirse.
Durante la replicación del ADN (a) las enzimas que constituyen el complejo de la ADN polimerasa abren la doble hélice parental de ADN y copian ambas hebras. 
Una de las copias se fabrica de retales:
el complejo de la polimerasa sintetiza un fragmento corto (1) comenzando por el cebador de ARN después salta hacia delante para generar un segundo fragmento también corto (2).
Cuando la polimerasa termina la síntesis de este segundo fragmento, se elimina el cebador de ARN, y los dos fragmentos establecen contacto con ADN.
Se produce un aumento del número de secuencias repetitivas de microsatélites (b) cuando la nueva hebra se desliza hacia atrás sobre una de las repeticiones en la hebra molde; ello determina que la polimerasa agregue una repetición extra en la nueva hebra para llenar el hueco.
Se acorta (c) cuando la hebra vieja se desliza, dando lugar a que las enzimas de reparación eliminen una repetición.

- REPLICACIÓN DEL ADN
- AUMENTO DE REPETICIONES
- DISMINUCIÓN DE REPETICIONES

En busca de papá Chimpancé 

Se ha recurrido a microsatélites de ADN para identificar criminales a través del análisis de las huellas nucleotídicas.
Se emplean también esas secuencias para conocer la vida sexual de los animales, en el marco de programas conservacionistas de especies en peligro de extinción. 
La huella de ADN, peculiar de cada individuo, revela que la longitud de las secuencias nucleotídicas de los microsatélites difiere en cada persona.
Se obtienen esas huellas mediante enzimas especiales, que permiten fabricar millones de copias exactas de varios microsatélites de un individuo.
Luego, en un gel se separan las copias en razón del tamaño.
Se configura así un patrón de bandas, similar al código de barras. 

Pascal Gagneux, David S., Woodruff y Christophe Boesch han empleado microsatélites de ADN como trazadores para sondear los hábitos de apareamiento de un grupo de chimpancés del bosque Taï de Costa de Marfil.
Han recogido pelos de los nidos que cada animal se construye en la copa de los árboles, donde se refugian para dormir. 
De las raíces de estos pelos se ha extraído ADN y determinado sus huellas características.
Al comparar las huellas de los microsatélites de ADN de hembras y machos adultos con los de 13 crías, Gagneux, Woodruff y Boesch han observado que siete crías no hablan sido engendradas por machos pertenecientes al grupo.
Aunque los investigadores nunca vieron esos encuentros furtivos entre las hembras y machos de bosques vecinos.

Estas aventuras nocturnas podrían explicar la diversidad genética mostrada incluso por grupos reducidos de chimpancés. 
La diversidad refuerza la resistencia frente a enfermedades y favorece, sin duda, la pervivencia.

Es probable que la conservación de la variedad sea esencial para la supervivencia de las poblaciones de chimpancés en libertad.
Por desgracia, a medida que estas poblaciones se fragmentan y las distancias que las separan son mayores, la capacidad de las hembras para encontrarse con machos de otros grupos e introducir nuevos genes en sus grupos queda drásticamente recortada.

Figura 2

LAS HEMBRAS DE CHIMPANCE, como ésta que aparece aquí con su cría, hacen frecuentes escapadas furtivas para aparearse con machos pertenecientes a otros grupos, según se deduce de estudios con huellas de ADN de microsatélites 

Detección del cáncer 

El ADN microsatélite puede convertirse muy pronto en un poderoso condicionante de nuestra vida.
Con su ayuda podrá afinarse en la detección precoz del cáncer.
Las pruebas capaces de descubrir mutaciones en genes cuya alteración predispone al cáncer, pensemos en el p53 y el ras , se emplean ya para detectar hasta una célula maligna en medio de 10.000 normales.
Pero las mutaciones en estos genes ni se dan en todos los tipos de cáncer, ni en todos los cánceres de un mismo tipo.

Los microsatélites ofrecen una nueva posibilidad para la detección precoz del cáncer.
En efecto, la tasa de expansión o de contracción de microsatélites en las células se dispara en ciertas formas tumorales.
Estos cambios repentinos ponen en jaque muchos microsatélites diferentes, fenómeno que puede detectarse con bastante facilidad.
Con dicho enfoque se descubre ahora una célula cancerosa entre 500 normales. 

Fue Manuel Perucho quien descubrió en 1993 los cambios operados en los microsatélites de células cancerosas en el marco de una investigación de un tipo de cáncer de colon, hereditario, que no se acompaña de la formación de pólipos.
Perucho observó que muchos de los microsatélites de las células cancerosas eran o más largos o más cortos que los de las células normales del mismo paciente.
No tardó en demostrarse que uno de los defectos causantes de estas alteraciones se encontraba en un gen determinante de una enzima cuya función era corregir la longitud de los microsatélites que se extienden o menguan durante la replicación del ADN.
La pérdida de este gen funcional habría de conducir a un aumento de la probabilidad de que los errores quedaran sin corregir. 

La prueba se completó cuando Richard C., Boland y otros insertaron un cromosoma humano portador de un gen normal reparador de ADN en células cancerosas cultivadas.
Observaron que el gen insertado corregía la tendencia a matar que presentaban los microsatélites de células cancerosas.
Más, por muy sorprendentes que parezcan estos hallazgos, la inestabilidad de los microsatélites puede constituir un síntoma, no la causa del cáncer. 
Aunque los ratones en que se ha eliminado el gen que codifica una de las principales proteínas reparadoras de emparejamientos incorrectos de ADN viven poco y adquieren diversos tipos de cáncer, ninguna de las células malignas muestra niveles elevados de mutaciones en los microsatélites. 
Según parece, este tipo de alteraciones forma parte de los cambios genéticos que se suceden en cascada a lo largo del genoma celular, una vez incoado el proceso de carcinogénesis; ello significa que podría tratarse de productos secundarios del proceso carcinogenético y no de agentes que intervinieran en su desarrollo.

Ahora bien, esas asociaciones se dan con suficiente frecuencia. 
Y los clínicos empiezan a servirse de la inestabilidad de los microsatélites como un nuevo y poderosos instrumento para la detección precoz del cáncer de colon y de vejiga.
Las pruebas clínicas se han visto coronadas por el éxito, lo que explica la voluntad de llevar su aplicación a otros tipos de cáncer.
De momento, ninguna de esas pruebas ha salido todavía de los muros de los laboratorios.
A medida que los clínicos adquieran experiencia con estos patrones, podrá diagnosticarse el cáncer antes y contar con datos más fiables del tipo de cáncer en razón del patrón de microsatélite.

Figura 3

PRUEBA para la detección del cáncer, que descubre los cambios de longitud de microsatélites; por ejemplo, los formados por repeticiones de la secuencia CA 

Las células pulmonares normales ( izquierda ) condenen regiones repetitivas de dos longitudes diferentes- una heredada de la madre y otra del padre del individuo- que pueden separarse en un gel, de acuerdo con el tamaño. 
Los tumores tempranos, así el adenoma de pulmón ( derecha ), constan de células normales y células cancerosas, que pueden detectarse porque sus microsatélites se han acortado o alargado y dan más de dos bandas en un gel.


Terapia génica para el sistema nervioso

Llegará el día en que la inserción de genes en las células cerebrales brinde a los médicos una forma de paliar, o incluso invertir, la lesión producida por una enfermedad neurodegenerativa A todos nos inquieta la perspectiva de contraer una enfermedad crónica.
Si la amenaza se refiere a un deterioro neurológico, la angustia que nos atenaza es mayor.
La enfermedad de Parkinson, la esclerosis lateral amiotrófica (enfermedad de Lou Gehrig) y otras de parecido tenor van usurpando poco a poco el control del organismo.
La lesión de la médula espinal puede crear, de golpe, igual degradación. 
Por no hablar de la enfermedad de Alzheimer, que ataca a la esencia misma de la personalidad de quien la padece con el deterioro progresivo de sus facultades mentales.

Muy poco han avanzado la ciencia médica básica y la aplicada en su lucha contra esas enfermedades, debido en buena parte a la vulnerabilidad del cerebro y la médula espinal.
A diferencia de muchos tipos celulares, las neuronas del sistema nervioso central de los adultos no se dividen.
En ese fenómeno de la biología estriba la tragedia central de la enfermedad o la lesión neurológica: en circunstancias normales, las neuronas que se pierden desaparecen para siempre; no cabe esperar que el tejido nervioso lesionado del cerebro y la médula espinal se repare por sí mismo.

Pero la ciencia podría tener en su mano la posibilidad de cambiar ese negro horizonte.
La investigación más ambiciosa, en el dominio de la neurología, aspira a sustituir las células perdidas del tejido lesionado con neuronas trasplantadas o mediante la liberación de factores de crecimiento (sustancias químicas que estimulan a las neuronas supervivientes para que amplíen su campo de acción o pueden despertar la capacidad de regeneración latente de las células).
Aunque esos métodos terapéuticos aportarían inmensos beneficios, es probable que transcurran muchos años antes de que se empleen de forma habitual.
Una meta más modesta, quizá no tan lejana, sería la de evitar primero la pérdida de neuronas.

En los últimos años se ha avanzado bastante en el conocimiento de la muerte de las neuronas subsiguiente a una lesión súbita del tipo del accidente cerebrovascular, las convulsiones o las lesiones cerebrales; hemos aprendido mucho también de su pérdida durante la enfermedad de Parkinson o la de Alzheimer, de carácter progresivo.
Tras empeños diversos por sacarle partido a tales descubrimientos, todo indica que la administración de ciertos fármacos podría proteger las neuronas amenazadas; parece incluso que el rebajar la temperatura del cerebro evita la muerte de las células frágiles durante una crisis neurológica. 
Y lo que reviste mayor interés: recientes avances en nuestra intelección de los mecanismos a través de los cuales las neuronas sucumben ante la agresión de tales enfermedades han hecho emerger la excitante posibilidad de proteger esas células mediante la modificación ingenieril de sus genes.

Reprogramar para sobrevivir

Los genes instruyen a las células para sintetizar proteínas específicas; ejemplo de éstas son las enzimas, que catalizan reacciones químicas.
Las neuronas producen enzimas que sintetizan neurotransmisores; así se llaman ciertas sustancias que transportan las señales químicas a través de angostas hendiduras -los espacios sinápticos- que quedan entre una neurona y otra.
Aplicada a las neuronas languidecientes, la terapia génica podría suministrarles, en principio, un gen que especificara una proteína protectora contra cualquier peligro amenazante. 

Para crear esos remedios, hay que decidir primero qué proteínas serían las más adecuadas.
En algunos casos, se trataría de aumentar la síntesis de una proteína cerebral cuya versión natural no cumpla bien con su cometido o bien se produzca en cuantía insuficiente.
En otros, y hablamos en teoría, podríamos proponernos añadir una proteína nueva, encontrada en un tipo de tejido diferente o incluso en un organismo distinto. 

La llamada aproximación antisentido constituye otra estrategia a considerar en terapia génica [véase "Las nuevas medicinas genéticas", de Jack S. Cohen y Michael E. Hogan; INVESTIGACIÓN Y CIENCIA, febrero de 1995].
Dicha táctica se ordena a limitar la fabricación de las proteínas causantes de la lesión.
Ciertas formas de esclerosis lateral amiotrófica y otras enfermedades neurológicas son consecuencia de la actividad intensamente destructiva de una proteína normal o de la acción de una proteína anómala que funciona de una manera perjudicial para la salud.
Podría recurrirse también a la terapia antisentido cuando las neuronas sintetizan proteínas que (por razones aún incomprensibles) exacerban una crisis neurológica.
En ese dominio laboran los investigadores que buscan la manera de bloquear la producción de las proteínas de la muerte; ese nombre reciben las que inducen a las neuronas comprometidas al suicidio celular.

Una vez desentrañado el fundamento básico de la enfermedad neurológica, no se requiere mucha imaginación para proponer una lista de genes que podrían salvar a las neuronas de la destrucción.
La dificultad estriba, por contra, en cómo lograr que los genes seleccionados lleguen a su destino.
En principio, podríase insertar un gen en el tejido cerebral mediante inyección directa de segmentos apropiadamente codificados de ADN puro.
A la hora de la verdad, este método de fuerza bruta rara vez tiene éxito, porque la eficacia de las neuronas para absorber el ADN "desnudo" no es, ni de lejos, satisfactoria. 
Una técnica mejor consiste en encerrar el gen en un liposoma, una suerte de burbuja grasa.
Por su propia naturaleza química, estas bolsas diminutas transportan con facilidad el ADN hasta el interior de las neuronas deseadas, pues se fusionan con la membrana celular y vierten su contenido en el interior celular.
La neurona, por razones no del todo entendidas todavía, incorporará algo de este material en su núcleo, donde reside su propio ADN; empleará entonces el gen como una especie de plantilla para fabricar la proteína terapéutica.

Otro método de insertar genes aún mejor es el que se vale de los virus.
En el curso de una infección común, los virus insieren su material génico en las células de la víctima; allí, el código genético añadido dirige la síntesis de las moléculas necesarias para fabricar nuevas partículas víricas.
Aunque los virus naturales encierran un ingente potencial destructor, podemos atenuar algunos y convertirlos en caballos de Troya microscópicos: es decir, hacerles transportar un gen terapéutico para que lo depositen en el interior de la célula sin dañarla.
Ciñéndonos a la terapia génica del sistema nervioso central, la investigación concentra su esfuerzo en unos pocos tipos víricos, entre ellos los adenovirus y los herpesvirus.

Combatir la enfermedad de Parkinson 

Los experimentos llevados a cabo con esos vectores víricos nos indican la viabilidad real de la terapia génica aplicada al sistema nervioso.
Destaca, a este respecto, la investigación de la enfermedad de Parkinson.
Este trastorno devastador refleja la degeneración de la sustancia negra con el paso del tiempo.
Compete a esa región del cerebro regular el control motor; por culpa de su destrucción, les cuesta mucho a los pacientes iniciar un movimiento o ejecutar movimientos coordinados complejos.
La pérdida causa también el clásico temblor parkinsoniano
[véase "La enfermedad de Parkinson", por Moussa B. H. Youdim y Peter Riederer; INVESTIGACIÓN Y CIENCIA, marzo de 1997].

La enfermedad de Parkinson sobreviene después de la muerte de las neuronas de la sustancia negra que segregan el neurotransmisor dopamina.
Por razones complejas, esas neuronas generan también radicales oxígeno, grupos químicos insidiosos que desencadenan reacciones lesivas dentro de la célula.
Quiere ello decir que se da cierto grado de destrucción de la sustancia negra en el proceso normal de envejecimiento. (Fenómeno que contribuye al ligero temblor típico de la senescencia).
A veces, la enfermedad de Parkinson parece cebarse en las personas con predisposición a presentar un exceso de radicales de oxígeno en su tejido cerebral o en quienes han estado expuestos a toxinas ambientales que inducen la formación de esos radicales. 
En otras ocasiones, las personas afectadas poseen niveles normales de esas sustancias químicas, pero sus defensas antioxidantes están mermadas.

Cualquiera que sea la causa determinante, de lo que no hay duda es de que los síntomas de la enfermedad de Parkinson se deben fundamentalmente a la falta de dopamina, resultado de la muerte de demasiadas neuronas de la sustancia negra.
Por tanto, una forma sencilla de corregir tal deficiencia, al menos de manera transitoria, sería aumentar la cantidad de dopamina allí donde escasea su suministro.
La dopamina no es una proteína; sí lo son, en cambio, las enzimas que sintetizan este neurotransmisor.
En consecuencia, la intensificación de la síntesis de una enzima crucial para ese proceso (la hidroxilasa de tirosina) debería potenciar la producción de esta sustancia química cerebral tan necesaria, siempre que sobrevivan las células secretoras de dopamina de la sustancia negra.

Aunque la administración de un precursor químico de la dopamina (una sustancia denominada L-dopa) aumenta también los niveles del neurotransmisor, el fármaco reacciona por todo el cerebro y acarrea efectos secundarios importantes. 
El interés de la terapia génica en este contexto reside en que los cambios correctores se desarrollarían dentro mismo de la sustancia negra.

Se ha puesto particular empeño en explotar esta posibilidad.
En un par de estudios recientes realizados en colaboración, cinco equipos de investigación publicaron resultados satisfactorios.
Se valieron de la cirugía para inducir en ratas síntomas de la enfermedad de Parkinson; los corrigieron empleando, por vector génico, herpesvirus.
En efecto, la aplicación de la terapia génica aumentó la producción de la enzima correctora, elevó el nivel de dopamina en las proximidades de las células privadas de este neurotransmisor y eliminó en parte los trastornos cinéticos de los roedores.

El equipo de Dale E. Bredesen, del Instituto Burnham en La Jolla, ha explorado un método más refinado todavía. 
Con anterioridad, habían demostrado que el trasplante de neuronas fetales pertenecientes a la sustancia negra de ratas corregía déficits parkinsonianos quirúrgicamente inducidos en las ratas adultas.
La estrategia resultó porque las neuronas jóvenes, en pleno vigor, se reprodujeron y sintetizaron dopamina para las células necesitadas de las inmediaciones.
Surgió un problema, sin embargo. 
Por alguna razón, las neuronas injertadas tendían a activar un programa suicida interno (un proceso de apoptosis) y morían al poco.
Ante esa situación, el equipo de Bredesen modificó por ingeniería genética las neuronas fetales antes de trasplantarlas; esperaban engañar a esas células para que produjeran grandes cantidades de xxx , proteína que bloquea el suicidio celular. 

El resultado no admitía dudas.
Cuatro semanas más tarde, las ratas que habían recibido injertos sin manipular de antemano mostraban una mejoría sólo marginal, en tanto que los animales que recibieron el gen añadido en sus injertos habían mejorado de forma sustancial.
El tratamiento de la enfermedad de Parkinson precisa un período de eficacia más prolongado.
Los injertos deberían sobrevivir años.
Se han realizado ya trasplantes de células fetales en pacientes en avanzado estado de la enfermedad, con resultado desigual.
Quizás una o dos modificaciones más inteligentes de las células fetales humanas antes de su trasplante podrían mejorar el rendimiento.

La batalla contra el accidente cerebrovascular 

El éxito obtenido con animales aviva la esperanza de que acabarán por surgir nuevos tratamientos para la enfermedad de Parkinson y otras derivadas de la degeneración progresiva del cerebro.
Se confía también en la terapia génica para detener la lesión tisular inducida durante crisis neurológicas agudas del tipo de la sobreestimulación producida en un episodio de convulsión o la pérdida de oxígeno y de nutrientes acaecida en un accidente cerebrovascular.

Cuando se produce tal percance, las células más vulnerables del cerebro son las neuronas, muchas, que responden a la presencia de glutamato.
Este potente neurotransmisor suele inducir la captación de calcio por parte de las neuronas receptoras, calcio que causa cambios de larga duración en la excitabilidad de las sinapsis estimuladas por el glutamato. 
En ese proceso podría residir la base celular de la memoria.

Ahora bien, durante las convulsiones o el ictus, las neuronas se ven incapaces de limpiar el glutamato de las sinapsis o de amortiguar la marea de calcio que fluye hacia el interior de muchas células cerebrales.
Glutamato y calcio no inducen ya cambios sutiles en las sinapsis, sino que producen lesiones importantes: se derrumba la arquitectura celular de las neuronas afectadas y los radicales de oxígeno recién generados provocan todavía males mayores.
Esta destrucción mata directamente las células o señala la iniciación de los programas suicidas internos que causarán la muerte rápida de las neuronas languidecientes.

Nuestro grupo ha examinado la posibilidad de que la terapia génica cortara esa cadena calamitosa de acontecimientos. 
De entrada, extrajimos algunas células cerebrales de una rata y las cultivamos en una placa de petri.
Luego, expusimos dichas neuronas a un virus herpes cuyo genoma habíamos manipulado para que portara el gen codificante de una proteína que acarrea moléculas de glucosa ricas en energía a través de la membrana celular.
En un paciente que sufriera una crisis neurológica, un tratamiento de este tipo incrementaría la absorción de glucosa justo cuando las neuronas asediadas necesitaran con mayor apremio la energía extra (que se precisa, entre otras tareas, para bombear el exceso de calcio fuera de las células involucradas).

Los experimentos iniciales revelaron que nuestro tratamiento potenciaba la ingesta de glucosa y contribuía a mantener el metabolismo adecuado de las neuronas sometidas al equivalente, en probeta, de una convulsión o un accidente cerebrovascular. 
Comprobamos luego ya en ratas que podíamos reducir la lesión causada por el accidente cerebrovascular inyectando el vector vírico en la región vulnerable del cerebro antes de que se produjera el percance.
Ni que decir tiene que no podemos aventurar cuándo una persona va a experimentar una convulsión o un accidente cerebrovascular.
Pero, como descubrieron Matthew S. Lawrence y Rajesh Dash en nuestro laboratorio, después de una convulsión había una ventana de unas pocas horas en la que el tratamiento génico de las ratas seguía protegiendo las neuronas de una lesión adicional (lo que da pie a esperar que los seres humanos también podrían beneficiarse de una clase similar de terapéutica).

Otra estrategia de posible terapia génica del accidente cerebrovascular y el traumatismo se centra en los programas de suicidio.
Los equipos de Howard J. Federoff, de la Universidad de Rochester, y de Lawrence construyeron, cada uno por su lado, vectores herpesvirus que incluían el gen supresor del suicidio xxx .
La aplicación de este vector tiende a proteger de la lesión a las células cerebrales de las ratas cuando se producen condiciones de crisis, sin que importe que el tratamiento comience después de la lesión. 

Se han conseguido también avances en el camino de la prevención de las enfermedades de almacenamiento de lípidos: trastornos genéticos que causan defectos en ciertas enzimas y llevan a una acumulación letal de moléculas grasas en el cerebro.
Para explorar esa vía se han preparado ratones que porten una mutación en el gen que determina la enzima beta-glucuronidasa. (Las personas con síndrome de Sly tienen la misma mutación génica).
El grupo encabezado por John H. Wolfe, de la Universidad de Pennsylvania, trasplantó en un ratón aquejado de esa enfermedad neuronas fetales tratadas mediante ingeniería genética para producir beta-glucuronidasa.
Descubrieron que los implantes eliminaban por completo del cerebro del animal los lípidos dañinos.

Pese a las etapas alentadoras recorridas en la aplicación de la terapia génica al sistema nervioso, persisten muchos y diversos obstáculos.
Por ejemplo, en la preparación de los vectores víricos.
Mutile demasiado el virus y resultará difícil mantener la potencia suficiente para que infecte las células.
Altérelo demasiado poco y el virus dañará las neuronas huéspedes. 
Dado que los vectores víricos disponibles adolecen de uno u otro fallo, habrá que perfeccionarse mucho antes de empezar a ensayar sin peligro las terapias génicas en enfermos neurológicos.

Piénsese, además, que el cerebro (un órgano vital y delicado) se halla encerrado en un cráneo bastante inexpugnable, lo que convierte en proeza harto difícil la inyección directa del fármaco en el tejido afectado. 
Quienes trabajan en ese ámbito recurren por lo común a la neurocirugía: trepanan el cráneo e inyectan el vector directamente en la parte del cerebro que corre peligro.
Pero los pacientes humanos precisarían algo menos agresivo para el tratamiento de rutina.
Aunque fuera posible introducir un vector por vía intravenosa (en el supuesto de que se consiguiera diseñar uno que sólo entrara en el sistema nervioso), sería improbable que el virus atravesara la barrera hematoencefálica, una red especializada de capilares que tan sólo permite el paso al tejido cerebral de moléculas pequeñas. 
Así pues, sin otras medidas especiales, el vector vírico acabaría despilfarrándose en otros lugares distintos del cerebro.

Todavía más.
Aun cuando pudieran superarse tales obstáculos, nos aguardarían otros al final del camino.
El vector pierde toda eficacia en cuanto alcanza el grupo de neuronas destinatario. 
(Los virus capaces de replicarse pueden propagarse con facilidad en el tejido cerebral, pero esos agentes no sirven para terapia génica, pues provocarían una respuesta inmunitaria lesiva).
Un vector vírico seguro atraviesa un área limitada, donde infecta sólo a un pequeño porcentaje de neuronas.
Por consiguiente, esos virus carecen de eficacia suficiente para abarcar el tejido enfermo.
Además, la actividad de la mayoría de los vectores ensayados persiste, a lo sumo, breves semanas: período demasiado corto para combatir las enfermedades degenerativas, lentas e inexorables.
Se impone, pues, buscar formas de mejorar la propagación, la eficacia y la duración de esas infecciones teledirigidas.

¿Qué nos depara el futuro? 

Pese a los muchos retos, la investigación en tratamientos génicos de las enfermedades del sistema nervioso (como otros muchos empeños que han cosechado algún éxito inicial) ha generado una atmósfera de optimismo.
Quizá, con una dedicación adecuada, la terapia génica del cerebro acabe por convertirse en un método rutinario.

Por dónde vendrían las posibilidades del futuro, comienza a vislumbrarse gracias al trabajo que lleva a cabo el equipo de Anders Björklund, de la Universidad de Lund.
Pioneros en el desarrollo de métodos de trasplante de neuronas fetales, estos investigadores han preparado por ingeniería genética injertos para producir grandes cantidades de un factor de crecimiento nervioso.
Implantaron algunas de tales células así manipuladas en ratas maduras, eligiendo como blanco una región crucial para el aprendizaje y la memoria (un área del cerebro que degenera lentamente durante el envejecimiento normal, lo cual no es sorprendente).
Lo notable fue que esta maniobra invirtió el declive cognitivo de las ratas viejas.

Este éxito nos indica que la terapia génica no sólo podría servir para domeñar la enfermedad, sino también para mejorar la memoria, la sensación y la coordinación en las personas mayores.
Queda todavía mucho por avanzar antes de soñar en socorrer de ese modo a los pacientes geriátricos.
Pero cabe esperar que los terapeutas génicos logren crear poderosas medicinas para el rejuvenecimiento de los cerebros envejecidos. 

Esos tratamientos podrían conseguir también reforzar la mente de personas más jóvenes.
No son muchas las investigaciones que se atreven a jugar con esa caja de Pandora para potenciar la función del cerebro normal.
Pero esta perspectiva (y sus posibles abusos) será difícil de esquivar si insistimos en la búsqueda de tratamientos genéticos contra enfermedades neurológicas. 
Lo que obviamente provocará que salgan a flote espinosas cuestiones éticas.


LOS NUEVOS GIROS DEL ADN

Se está gestando una genética distinta sobre nuevas bases, que habrá de modificar a su vez nuestras ideas sobre la evolución y la herencia de las enfermedades. 

Margaret G. Kidwell boga contra corriente. 
Dos especies de la mosca del vinagre cohabitan en los botes de su laboratorio.
Como no pueden aparearse entre ellas, cabe esperar que su destino sea permanecer genéticamente aisladas.
Pero si Kidwell tiene razón, algún segmento de ADN acabará pasando de una especie a otra. 
La genética mendeliana clásica reputa imposible semejante tipo de herencia.
Bien es verdad que Mendel nunca imaginó que los genes se movieran por el interior de los ácaros que parasitan las dos especies de moscas. 
Kidwell posee pruebas de la realidad de esa transferencia en la naturaleza.

Las leyes de Mendel dejan abiertos unos resquicios del mayor interés.
Las nuevas herramientas desarrolladas por la biología molecular para manipular cromosomas nos permiten conocer mejor la maquinaria genética.
Gracias a ello, los modelos tradicionales del comportamiento del ADN, harto más sencillos, sufren unos giros sorprendentes. 
En contra de lo esperado, los genes saltan de un cromosoma a otro, o se alargan y contraen como un acordeón.
Los cromosomas portan señales químicas que denuncian su origen paterno o materno.
Hay proteínas cifradas por genes que, a tenor de la doctrina oficial, no existen.
Se habla incluso de que los organismos podrían alterar sus genes en respuesta a cambios ambientales.

"Debemos prestar mucha más atención a los múltiples detalles que rodean a las formas en que los organismos hacen uso de las propiedades básicas del ADN, ARN y proteínas", afirma Joshua Lederberg;
este premio Nobel demostró en los años cuarenta que las bacterias mantienen un intercambio sexual de material genético, idea que entonces rozaba la herejía, y hoy es un principio aceptado.
También el nuestro es tiempo de heterodoxias, en el que se niega con rotundidad el supuesto carácter estable de la molécula de ADN;
suerte de metabolito, "es parte de la célula y responde a lo que sucede a su alrededor", sentencia Jeffrey W. Pollard, de la Facultad de Medicina Albert Einstein.

La genética nació a mediados del siglo XIX, en el jardín del monasterio moravo donde Mendel experimentaba con guisantes.
Las reglas matemáticas de la herencia que él y otros investigadores después desarrollaron se convirtieron en la piedra angular de la biología moderna. 
Ninguno de ellos sabía qué eran los genes, lo que no impidió que describieran las líneas generales de la transmisión de caracteres.
Hasta los años cincuenta, no se identificó el ADN (ácido desoxirribonucleico) con el material genético, incoando así la era de la biología molecular.

La sustitución de "gen" por "ADN" no alteró los fundamentos de la genética.
En la mayoría de los artículos, seguía abordándose el ADN como una entidad ideal que, salvo mutaciones ocasionales, persistía inalterado en medio del turbulento ambiente interno de la célula.
Se halló que la información genética estaba escrita en la doble hélice de ADN, en una secuencia formada por cuatro bases nucleotídicas. 
Una cadena sencilla de ADN actuaba de molde para sintetizar la complementaria de ARN mensajero (ácido ribonucleico), portadora de la información a los ribosomas.
Estos orgánulos leían el ARN mensajero como si fuera una cinta de música, interpretando sus bases de tres en tres, y engarzando los aminoácidos correctos, hasta fabricar una proteína.
Pero conforme se ha ido profundizando en la actividad del genoma (serie completa de genes de un organismo) y sus moléculas asociadas, se ha descubierto que sus propiedades y comportamiento ofrecen mayor diversidad y riqueza.

Figura 1

ESTE ACARO (izquierda) puede haber transferido fragmentos de ADN de una especie de la mosca de la fruta a otra distinta, portándolo en su aparato bucal (derecha).

Los biólogos prestan cada vez más atención a estos posibles casos de transferencia genética horizontal y a otros fenómenos que no encajan dentro de la genética tradicional.
La mancha roja del abdomen del ácaro -pigmento procedente de una mosca de la fruta. 

Genes saltarines

Uno de los primeros en percatarse del dinamismo del genoma fue Barbara McClintock, fallecida el año pasado.
En 1947, mientras realizaba cruzamientos con el maíz, en el laboratorio de Cold Spring Harbor, McClintock observó patrones extraños en la herencia de la pigmentación, inexplicables por las reglas al uso.
Después de darle muchas vueltas a sus resultados, llegó al convencimiento de que había genes que no tenían sitio fijo en el cromosoma, sino que saltaban de un punto a otro en cada generación. 
El calor y otros factores de estrés ambiental parecían incrementar la tasa de transposición de tales genes.

Como ocurrió con Mendel, la idea de McClintock de los elementos genéticos transponibles, transposones o "genes saltarines", languideció durante décadas. 

No se veía ningún indicio, en el flujo unidireccional de información que cursaba del ADN al ARN y las proteínas, que sustentara la posibilidad de movimiento génico.
Pero a principios de los setenta la teoría de McClintock volvió por sus fueros.
Los experimentos demostraban el traslado de segmentos de ADN, de un sitio a otro dentro del mismo cromosoma, o entre cromosomas distintos, provocando cambios en la expresión de los genes.
El salto de un transposón a las proximidades, o al interior de un gen, por ejemplo, podía inactivarlo.

El origen de los transposones sigue siendo oscuro.
Muchos los consideran una reliquia de virus integrados en el cromosoma de sus hospedadores.
Algunos virus producen retrotranscriptasa, enzima que les permite convertir cadenas de ARN en ADN, revirtiendo así el flujo normal de información dentro de la célula.
Los genes saltarines suelen utilizar un mecanismo similar para moverse.
Algunos transposones portan su propia retrotranscriptasa, en tanto que otros aprovechan las enzimas producidas por elementos más capaces o por virus.

Cualquiera que sea su mecanismo de salto, los transposones ejercen un influjo poderoso sobre el organismo que los aloja. 
A ellos se han atribuido alteraciones genéticas en animales y plantas.
Pueden incluso afectar al hombre. 
En 1991, el equipo de Francis S. Collins, de la Universidad de Michigan, identificó en un paciente la mutación causante de la neurofibromatosis, un tumor:
tras la inserción de Alu, un elemento genético común, quedó bloqueada la actividad de un gen regulador del crecimiento celular. 
Dos meses después, el grupo encabezado por Haig H. Kazazian, de la Universidad Johns Hopkins, anunciaba que había cazado otro gen que acababa de saltar;
mientras estudiaban hemofílicos, descubrieron la causa de la enfermedad en un niño:
un transposón había inactivado el gen responsable de un factor esencial para la coagulación de la sangre.
El transposón venía a ser idéntico a un gen que los padres del paciente portaban, sin embargo, en un sitio distinto.

Los elementos transponibles se suelen transmitir "verticalmente", de una generación a la siguiente, como los genes.
¿Podrían saltar "horizontalmente" de un organismo a otro e incluso entre especies diversas?
Demostrado el intercambio de genes en las bacterias, se sospecha la existencia de transferencia entre bacterias y plantas e insectos.
Nadie, sin embargo, ha comprobado que los transposones pasen de unos organismos superiores a otros.
John F. McDonald , de la Universidad de Georgia, apunta que, desde hace muchos años, se han observado peculiares semejanzas genéticas entre especies sin relación de parentesco, si bien compartían un mismo hábitat.
El fenómeno sugería poderosamente un trasvase de información genética entre ellas, pero no había forma de probarlo, y la hipótesis se descartaba.

Los ácaros sí pueden 

Lo demostraron en 1991 Marilyn A. Houck y Kidwell, tras hallar pruebas de que cierto elemento transposón, el P, saltó de una especie a otra de la mosca de la fruta.

A lo largo de los años setenta, Kidwell y otros observaron una extraña incompatibilidad genética entre cepas de D melanogaster procedentes del laboratorio y cepas cazadas en la naturaleza.
Los cruzamientos de unas con otras eran a menudo estériles, o producían descendientes anormales, incompatibilidad que se atribuyó a la presencia de elementos P en las moscas silvestres.
Sólo las que habían permanecido aisladas en el laboratorio durante años estaban libres de elementos P.
La "epidemia" de elementos P debió haber comenzado unos 30 o 50 años antes.

Las secuencias génicas analizadas por Stephen B. Daniels, de la Universidad de Connecticut, revelan que los elementos P de D melanogaster son virtualmente idénticos a los de D willistoni, una especie distinta.
Sería, pues, en los años cuarenta cuando algunos elementos P pasaron de las moscas willistoni a las melanogaster.

El mecanismo de dicha transferencia hubiera permanecido rodeado de misterio de no haber sido por un hecho fortuito:
las cepas de Kidwell habían sido parasitadas por cierta variedad de ácaros.
Houck, instalada en el piso inferior, recibió muestras del par sito para ver si encontraba alguna forma de exterminarlo.
Pertenecía a la especie Proctolaelaps regalis.
Al microscopio electrónico, su aparato bucal recordaba los finos tubos de cristal utilizados por los biólogos moleculares para realizar los experimentos de transferencia génica.

Tal semejanza sugirió a Houck que la naturaleza podía haberse servido de los ácaros para mediar en una suerte de ingeniería genética:
tras alimentarse en una D willistoni, el ácaro con un elemento P aún en la boca, o en el tubo digestivo, podría haberse deleitado con un huevo de D melanogaster.
Los elementos P que entraron así en el huevo pudieron integrarse en su ADN.
Si el huevo sobrevivió, el resultado pudo ser una mosca con un elemento P procedente de otra especie. 
A Houck no le cabe la menor duda de que se trata de un fenómeno de transferencia horizontal.
Kidwell corrobora, además, que los elementos P se propagan de manera desaforada, una vez integrados.

En la integración reside, por tanto, el nudo de la cuestión.
Houck y Kidwell, que han demostrado que los ácaros llevan en su tubo digestivo elementos P reconocibles, se afanan en nuevos experimentos para cerrar el proceso de transmisión.
Kidwell cría poblaciones de D melanogaster y D willistoni juntas, en presencia de ácaros, para reproducir la transferencia interespecífica; 
Houck está centrada en un aspecto más específico del problema.

No se descarta la intervención de los virus en los saltos entre especies.

Durante muchos años, se concedió la posibilidad teórica de que los virus vehicularan los transposones, es decir, que transportaran éstos en su propia dotación genómica hasta otras células hospedantes.
Consta al menos un ejemplo vírico de transposón que infecta a varias especies de insectos.

Si se demostrara que los elementos transponibles se mudan de una especie a otra, deberíamos concederles la categoría de agentes de cambio evolutivo.
La doctrina darwinista tradicional enseña que la evolución procede gradualmente mediante la acumulación de mutaciones puntuales, encargándose la selección de eliminar lo inadaptado.
Ahora bien, los defensores de los elementos transponibles prefieren admitir que éstos producen macromutaciones -cambios fenotípicos importantes y repentinos- en un tiempo menos dilatado.
Suprimiendo la expresión de otros genes, los transposones actuarían a modo de reguladores genéticos, que, en el plano fenotípico, se traduciría en la aparición de nuevos patrones de desarrollo.
En esa línea, Diane M. Robins, de la Universidad de Michigan, ha descubierto que la secuencia reguladora de un gen de ratón guarda un estrechísimo parecido con cierto segmento de transposón muy frecuente en el genoma del animal.
La inserción y retención de ese componente del transposón en ese sitio determina que el gen sea sensible a ciertas hormonas. 

Linda C. Samuelson ha aportado otros indicios de la incidencia, más benéfica, de los transposones en la evolución humana.
En muchos mamíferos, el páncreas segrega amilasa, enzima necesaria para digerir los almidones de la dieta.
Pero el hombre también segrega amilasa en su saliva, lo que parece útil para ampliar el rango de alimentos que puede ingerir.
De acuerdo con Samuelson, un transposón sería el responsable de esta doble expresión de la amilasa mediante alteración de la regulación del gen de la enzima.

Se habla incluso del papel importante que desempeñarían los transposones en la originación de nuevas especies. 
Se les asocia, en particular, con el patrón de evolución "a saltos" del registro fósil;
los organismos que vivieran en ambientes desconocidos sufrirían tensiones que intensificarían la frecuencia de saltos en los genes.
Como resultado de ello, los organismos experimentarían una mayor tasa de mutación y evolucionarían más deprisa.

Genes enormes

Cualquiera que sea el alcance de sus efectos, los genes saltarines representan un tipo de mutación no prevista por los fundadores de la genética.
En los últimos años, se ha descubierto otro tipo de mutación que en muchos aspectos se aleja todavía más de la ortodoxia. 
Se trata de genes anómalos que adquieren de repente una talla enorme, con trágicas consecuencias.
Su estudio ha llevado a comprender mejor extraños patrones de herencia asociados con varias enfermedades.
Por ejemplo, el síndrome del X frágil, causa frecuentísima de retraso mental.
Debe su nombre a la deformación cromosómica que presentan los afectados, quienes portan un cromosoma X cuyo extremo, en el brazo largo, se mantiene unido al resto a través de una hebra sutil de ADN.
En 1991, los equipos de Jean-Louis Mandel, del INSERM, Grant R. Sutherland, del Hospital Infantil de Adelaida, y Stephen T. Warren, de la facultad de medicina de la Universidad de Emory, descubrieron que el origen de la enfermedad era una mutación hasta entonces desconocida.

En los individuos normales, el gen FMR-I contiene unas 60 repeticiones en tándem de cierta secuencia de bases formada por tres nucleótidos.
Los portadores sanos del síndrome X frágil presentan hasta 200 copias de esa secuencia- en los individuos enfermos, la región de repetición, de tamaño desproporcionado, se repite cientos o miles de veces.
Los niños con X frágil son descendientes de portadores sanos;
por tanto, los genes mutantes crecen de una generación a la siguiente. 

Iteraciones génicas similares causan, se ha observado, la distrofia miotónica, forma común de distrofia muscular en adultos, y la atrofia muscular espinal y bulbar. 
Se ignora cuál sea el mecanismo de este explosivo cambio, aunque se sospecha de cierta forma aberrante de polimerasa, enzima que añade nucleótidos a la cadena de ADN que se está sintetizando.

La verdad es que hay muchas regiones repetidas en el genoma. 
¿Por qué no se observa un crecimiento masivo parecido?
David E. Housman, del Instituto de Tecnología de Massachusetts, ha encontrado secuencias del genoma del ratón que aumentan y disminuyen de tamaño, aunque de forma menos espectacular que en el caso del FMR-I. 
Se ha esgrimido la idea de la inestabilidad de determinadas configuraciones del FMR-I, que predispondrían un crecimiento disparatado.
En una o más generaciones, el alelo (una forma del gen) inestable crece hasta el tamaño que se observa en los portadores sanos del síndrome del X frágil.
Cuando el gen alcanza una longitud crítica, se prima un desarrollo más espectacular, lo que sucede en los individuos afectados.
Por razones que se nos escapan, el gen se dispara en individuos que han heredado el cromosoma X frágil de su madre.
Habría primero un cambio de alelo estable a inestable.

El fenómeno de crecimiento es el que nos retrotrae a la genética humana cuantitativa.
La mayoría de las mutaciones alcanzan una frecuencia de equilibrio estable en las poblaciones a las pocas generaciones, ya que la selección las favorece o las elimina de forma proporcional a los efectos de la mutación.
Pero la mutación de que estamos hablando no se deja sentir ni en la primera ni en la segunda generación, sino más adelante.

Las mutaciones que aumentan de tamaño podrían explicar el patrón peculiar de herencia que caracteriza al síndrome X frágil: la paradoja de Sherman.
Según establece la genética clásica, cuando se trata de enfermedades ligadas al cromosoma X, deberían hallarse afectados todos los varones portadores del cromosoma X frágil;
pero acontece que más del 20 por ciento son normales, ya que portan formas "premutadas" del FMR-I, más cortas.
Sus hijos también son normales, porque sus genes crecen muy poco.
Sin embargo, los nietos y nietas de los portadores originales suelen estar afectados, ya que las regiones repetidas de sus genes se han multiplicado sin tasa.

El patrón de crecimiento génico que se observa es suficiente para explicar la paradoja de Sherman.
Aunque algunos sostienen que algo debe estar ocurriendo, pues no resulta fácil justificar que los principales cambios de las secuencias nucleotídicas se produzcan sólo en la línea germinal de la madre;
se habla, a este respecto, de la posible implicación de otro fenómeno que viola el dogma genético tradicional: la impronta cromosómica dependiente del sexo.

Impronta génica

Dicta un principio fundamental de la genética mendeliana que el efecto ejercido por un gen no depende de su procedencia, paterna o materna.

Pero se han hallado ejemplos claros en los que machos y hembras ejercen una impronta, o marca, sobre los genes que transmiten. 
Nos han demostrado los experimentos con ratones que los embriones portadores de un juego completo de cromosomas procedente de uno de los padres no llegan a desarrollarse, aunque sean genéticamente idénticos a un ratón normal.
Sin la impronta materna en sus cromosomas, el embrión es anormal; sin la influencia paterna, la placenta no llega a desarrollarse. 

En el hombre, los desequilibrios en los complementos de cromosomas con impronta materna y paterna suelen acabar en trastornos. 
Robert D. Nicholls ofrece pruebas interesantes de los efectos de la impronta genómica en dos enfermedades, al menos.
Los niños con síndrome de PraderWilli, que se caracterizan por presentar retraso mental y obesidad, parecen haber heredado las dos copias del cromosoma 15 de sus madres.
A la inversa, niños a los que les faltan porciones de su cromosoma 15 materno (y cuyo cromosoma paterno está por tanto desproporcionalmente representado) presentan el retraso mental y los movimientos anormales típicos del síndrome de Angelman.
Carmen Sapienza y otros asocian también casos de impronta anormal con cánceres infantiles.

No se conoce aún con certeza el mecanismo preciso de la impronta génica, pero el proceso puede implicar la metilación de ciertas citosinas, una de las bases del ADN.
La metilación del ADN guarda relación, por lo que se ve, con la inactivación génica.
Habida cuenta de que el patrón de metilación permanece durante la replicación del ADN, la impronta de un óvulo fecundado se transmite a todas las células del cuerpo. 

Charles D. Lairdsostiene que la clave de la paradoja de Sherman es la impronta del cromosoma X frágil, y no el crecimiento del gen.
Ha desarrollado un modelo para explicar el comportamiento de la enfermedad, que mejora las conclusiones que se obtienen utilizando sólo datos moleculares.
Se basa en la observación de las hembras; 
éstas poseen dos cromosomas X en cada célula e inactivan normalmente uno de ellos.
Esta forma de impronta suele eliminarse del todo antes de que los cromosomas entren en meiosis, que es el proceso de división celular que origina los oocitos.
En opinión de Laird, una mutación cromosómica en las portadoras del X frágil impide a veces borrar la impronta del sitio X frágil.

Síguese de ello que la mitad más o menos de los hijos de una mujer sana y portadora del defecto genético recibirán un cromosoma X parcialmente inactivado, aunque de manera permanente, y serán retrasados mentales.
Algunas de sus hijas también recibirán un X frágil inactivo, y también quedarán afectadas.
Desde el punto de vista de Laird, las mutaciones de crecimiento sólo serían efectos secundarios de la impronta inactivadora en el sitio frágil.

Mutaciones dirigidas

La genética molecular anda también revuelta por una polémica de largo alcance, que tiene que ver con los efectos del ambiente sobre el genoma.
Se sabe que las radiaciones, los productos químicos carcinogénicos y otros agentes provocan mutaciones aleatorias.
Se investiga, ahora, si el estrés del entorno puede dirigir el tipo de mutaciones que se producen.
No todos están de acuerdo en despertar los viejos fantasmas de la evolución lamarckista, las mutaciones intencionadas y la herencia de los caracteres adquiridos.

La idea de que el medio configurara la herencia cayó en las postrimerías del siglo pasado con la distinción y separación entre células de la línea germinal, formadora de ovocitos y espermatocitos, y células somáticas (cuerpo).
Las células del sistema inmune y otros tejidos pueden cambiar sus genes en respuesta al ambiente, pero no así las células germinales.
Pero en biología las generalizaciones son arriesgadas.
Y así suele olvidarse que la mayoría de los organismos no secuestran sus células germinales:
los tejidos sexuales de las flores, por ejemplo, se producen a partir de células somáticas normales;
más palmaria, si cabe, resulta la inaplicabilidad de esa doctrina a los organismos unicelulares. 
Estas excepciones abren la posibilidad de que, al menos en algunas especies, los cambios genéticos que favorezcan la supervivencia puedan ser inmediatamente transmitidos a la siguiente generación.
Desde una perspectiva teleológica, los organismos mutarían para adaptarse mejor al ambiente. 

El debate en tomo a las mutaciones dirigidas arranca de 1988, con la publicación de un artículo de John Cairns, adscrito entonces a la Escuela de Salud Pública de Harvard.
Movido por su interés hacia la mutagénesis, decidió reexaminar el principio genético según el cual las mutaciones favorables para la supervivencia no son más probables que las desfavorables.
Cultivó bacterias en un medio donde escaseaba el único azúcar que ellas podían metabolizar, si bien abundaba otro azúcar, la lactosa.
El resultado del experimento le llevó a pensar que las mutaciones que reactivaban el gen bacteriano defectuoso necesario para utilizar la lactosa ocurrían con una frecuencia mayor de lo que cabría esperar por puro azar.
Las fuerzas selectivas no se limitaban a cribar los organismos menos adaptados, sino que conducían de forma activa las mutaciones en una dirección beneficiosa. 

Unos meses después, Barry G. Hall, hoy en la Universidad de Rochester, presentó pruebas más convincentes de mutaciones inducidas por la selección.
Realizó experimentos similares, en los que las bacterias hambrientas necesitaban dos mutaciones independientes ninguna de las cuales parecía conferir por sí sola beneficio para utilizar una nueva fuente de alimento.
Hall calculó que la probabilidad de que ocurrieran las dos mutaciones de forma espontánea era bajísima.
Halló, sin embargo, que un número altísimo de bacterias había conseguido adaptarse al ambiente.
Muchos otros confirman haber observado mutaciones inducidas por la selección en bacterias.
El pasado verano, Hall anunció que lo había comprobado también en levaduras.

Pero se desconoce la naturaleza del mecanismo;
a lo más, se rechaza la propuesta de Cairns de que la retrotranscriptasa podría transcribir la información correspondiente a las mutaciones beneficiosas desde ARN anormal hacia ADN bacteriano. 
Más plausible parecía el mecanismo defendido en el modelo de la mutagénesis transcripcional;
de acuerdo con el mismo, el ADN de los genes activos sufre mutaciones rapidísimas por hallarse durante la transcripción en forma unicatenaria, más vulnerable.
Pero tampoco la experimentación lo respalda.

Se han propuesto más ideas.
El propio Hall piensa que aún es v lida su hipótesis de que, en una población sometida a estrés, una pequeña porción de la misma puede entrar en un estado hipermutable. 
Sólo las células hipermutables que adquieran una mutación beneficiosa sobrevivirán; las otras no serán viables.

Con independencia de cuán razonables puedan parecer esos hipotéticos mecanismos, muchos dudan de la existencia de las mutaciones dirigidas.
Milita en esa tendencia Richard E. Lenski, de la Universidad estatal de Michigan.
Los experimentos esgrimidos, aduce, carecen de los controles suficientes, además de resultar complicado calcular el número de mutaciones responsables de una población observada. 
Los fenómenos bioquímicos pueden sesgar la tasa de mutación y crear la ilusión de las mutaciones dirigidas.
Ello no le impide alabar el trabajo que se está realizando.
"Parece observarse, en algunos casos, un incremento en la tasa de mutación cuando las células están sometidas a situaciones de estrés", reconoce; 
"no se trata de ninguna mutación dirigida, como se asegura, sino de una dependencia fisiológica de ciertas tasas de mutación".

Revisión de estilo del ARN

Los nuevos tipos de mutaciones no son los únicos responsables de las capas de complejidad que se van añadiendo a la genética molecular.
Hemos de traer a colación ciertos descubrimientos relativos a la transcripción y traducción de los mensajes genéticos en proteínas. 
Las moléculas de ARN transcritas a partir del ADN deben sufrir a veces importantes modificaciones antes de convertirse en mensajeras para la síntesis de proteínas. 

Las instrucciones para fabricar una proteína, presentes en el transcrito primario de ARN, están interrumpidas por largas secuencias que no significan nada.
Durante el proceso de maduración del ARN, estas secuencias, o "intrones", son eliminadas de la molécula, y se empalman los "exones", o secuencias portadoras de sentido, en un proceso que termina con la formación de una molécula final más corta, pero coherente.
En 1982, Thomas R. Cech, de la Universidad de Colorado, y Sidney Altman, de la Universidad de Yale, realizaron el descubrimiento, que mereció el premio Nobel, de que algunas secuencias intrónicas de ciertos ARN tenían propiedades enzimáticas que permitían al propio ARN autocortarse y empalmarse.

Quizá la manera más extraordinaria de maduración sea la revisión de estilo o corrección del ARN, proceso en el que la información crucial que no está especificada en el ADN se añade en la molécula del ARN.
Con la expresión "corrección de estilo del ARN" se designan varios fenómenos diferentes. 
En todas sus formas, la corrección del ARN implica la adición específica de bases a las moléculas de ARN, o la modificación de ciertas bases ya presentes. 
Se hallan implicados distintos mecanismos utilizado para ello.

El caso mejor conocido de corrección de ARN, y posiblemente el más sorprendente, es el estudiado en los parásitos tripanosomas, causantes de la enfermedad del sueño.
El ADN mitocondrial de los tripanosomas posee una estructura singular.
Consta de varias docenas de grandes lazos, o maxicírculos, y miles de otros menores, o minicírculos. 
Todos ellos se organizan en una estructura semejante a una cota de mallas.

Las peculiaridades de este ADN no terminan en su estructura. 
Según parece, el ADN de los minicírculos no tiene información genética útil.
Los maxicírculos portan la mayoría de los genes que se encuentran normalmente en las mitocondrias de otros organismos. 
Faltan, sin embargo, genes cruciales, como los que cifran la información para los ARN transferentes.

La sorpresa llegó con la investigación pormenorizada de la estructura del ARN mensajero.
En muchos casos, la molécula resultaba más larga que el ADN del que habíase transcrito.
La longitud de una cadena de ARN doblaba su correspondiente ADN.
Se habían añadido estratégicamente algunas uridinas al ARN, dándole sentido a lo que en el ADN no eran sino "criptogenes" sin sentido, un término acuñado por Larry Simpson, de la Universidad de California en Los Angeles.

En 1990, Simpson, Beat Blum y Norbert Bakalara propusieron un mecanismo en el que los maxicírculos y minicírculos producen pequeños "ARN guías", que buscan omisiones en los ARN mensajeros y las corrigen.
El ARN guía y la cadena de mensajero a la que se ancla encajan uno en otro como las dos mitades complementarias de una cremallera.
Allí donde falta una uridina en el ARN mensajero se produce una distorsión en el alineamiento.
Ese desemparejamiento provoca la introducción de una uridina en la secuencia del ARN mensajero.
El proceso de corrección del ARN crea la secuencia de ARN mensajero madura.

Desconocemos las causas por las que ha evolucionado un sistema con esas características.
Podríamos suponer que ello proporciona a los par sitos cierto control sobre la expresión de genes durante diferentes momentos de sus ciclos de vida.
Se ignora de qué modo el ARN guía dispone los cortes y empalmes durante el proceso de corrección. 
Un modelo, publicado por Cech, e independientemente propuesto también por Blum y Simpson sostiene que los ARN guías pueden llevar a cabo ellos mismos la operación, donando uridinas de sus propias cadenas.

Tras los cortes y empalmes, correcciones y demás, el proceso de maduración termina y el ARN mensajero está listo para su traducción en proteína por los ribosomas. 
Incluso esta etapa encierra mayor complejidad de la imaginada. 
Para llevar a cabo la traducción, el ribosoma debe descifrar la información presente en el ARN.
Cada codón, o secuencia de tres bases en el ARN, se corresponde con una instrucción:
añadir determinado aminoácido o detener la síntesis de la proteína. 

Un código no tan universal

Diríase que un código genético fundamental para la vida debe ser universal.
Pero no lo es.
El código utilizado para descifrar el ARN procedente del núcleo difiere ligeramente del que se emplea para el ARN de mitocondrias y cloroplastos.
En el ARN mitocondrial, por ejemplo, el codón uridina-guanina-adenina es una instrucción para añadir el aminoácido triptófano a la cadena polipeptídica que se está formando;
en el ARN nuclear, una señal de terminación de la síntesis de proteínas.

Muchos experimentos han demostrado que los códigos a veces son flexibles.
Para sintetizar algunas proteínas, el ribosoma debe alterar la traducción de codones en la cadena del ARN mensajero.
Se ha demostrado que los ribosomas que sintetizan determinada proteína de la sangre de mamíferos interpretan un codón terminador aunque no todos -como una instrucción para añadir el aminoácido selenocisteína.

No siempre se leen los codones en secuencia lineal.
Los ribosomas pueden "cambiar de fase", saltando hacia delante o hacia detrás una o más bases hasta encontrar un determinado codón.
En un caso identificado por Wai Mun Huang, del Hospital Clínico de la Universidad de Utah, los ribosomas ignoran una secuencia de 50 bases del ARN mensajero.
John F. Atkins, del Colegio Universitario de Cork, ha propuesto la expresión "recodificación del ARN" para esta reinterpretación del ARN mensajero por los ribosomas.

La recodificación del ARN y la maduración, la impronta genómica de los cromosomas y todas las nuevas mutaciones son sólo algunos de los fenómenos que animan hoy día la genética molecular.
Esta nueva imagen que se atisba de un genoma dinámico no menoscaba en modo alguno el modelo tradicional, un notable logro intelectual por otra parte.
Las reglas establecidas por los genéticos son aplicables a la herencia de los caracteres en organismos tan diversos como bacterias, rosas, jirafas y el propio hombre. 
En su sencillez reside la fuerza de la genética tradicional.
Sus generalizaciones describen la mayoría de los fenómenos genéticos en la mayoría de los organismos, la mayoría de las veces.

Queda el gran desafío de explorar las implicaciones de las excepciones y encontrar nuevas reglas más eficaces, si es que existen.


El código de la vida, descifrado

En el desentrañamiento del genoma de diversas especies hallaremos respuesta a algunas de las cuestiones más apasionantes sobre la vida.

Cuando, en el futuro, los historiadores vuelvan su mirada hacia el cambio de milenio que estamos atravesando, reconocerán que el principal avance científico de esa etapa fue la caracterización minuciosa de las instrucciones genéticas que conforman nuestro ser.
De los éxitos del Proyecto Genoma Humano -que se propone cartografiar y descifrar letra a letra el código encriptado de la vida, el ADN- sacarán partido todas las ramas de la biología.
El descubrimiento de la secuencia completa del ADN de un número creciente de especies, hombre incluido, traerá las respuestas que se plantean sobre la evolución de los organismos, la síntesis in vitro de la vida, la terapia de múltiples patologías y otras cuestiones de parejo tenor.

En el marco del Proyecto Genoma Humano se están recabando datos biológicos en cuantía hasta ahora desconocida.
El mero listado de bases, o unidades de ADN que constituyen el genoma humano, llenaría más de 200 gruesos directorios telefónicos; y eso sin determinar qué es lo que opera cada una de dichas secuencias de ADN. 
Dentro de unos meses deberíamos tener entre las manos un borrador de trabajo del 90 por ciento de la secuencia completa del ADN humano.
La secuencia entera se coronará en el año 2003.
Con ello dispondremos ya del esqueleto. 
Necesitaremos muchas capas de anotaciones para revestirlo de su significado cabal.
De la comprensión de las proteínas codificadas por los genes habrá de llegarnos el pleno sentido de tamaña investigación.

Las proteínas no sólo constituyen el armazón estructural del organismo;
entre ellas encontramos también las enzimas que catalizan las reacciones bioquímicas de la vida.
Se componen de unidades, que son los aminoácidos, enlazados en largas cadenas.
Cada cadena se pliega de manera precisa para determinar así la función de la proteína.
El orden de los aminoácidos viene dictado por la secuencia de las bases del ADN, que determina una proteína, a través del ARN intermediario.
De los genes que forman activamente ARN se dice que se "expresan". 

El Proyecto Genoma Humano busca identificar todas las proteínas que se sintetizan en el hombre.
Se propone, además, avanzar en varios frentes de comprensión: el mecanismo en cuya virtud se expresan los genes que cifran las proteínas, el empaquetamiento de las secuencias de tales genes en función de genes equiparables de otras especies, la variabilidad génica en el seno de nuestra especie y la traducción de las secuencias de ADN en caracteres observables.
Las capas de información superpuestas sobre la secuencia de ADN revelarán el conocimiento encerrado en ésta.
A lomos de esos datos cabalgará la investigación biológica de los próximos cien años.
En una suerte de círculo "virtuoso", cuanto más aprendamos, tanto más podremos extrapolar, emitir nuevas hipótesis y profundizar.

Pensamos que, para el año 2050, la genómica contará con los recursos precisos para contestar las siguientes cuestiones:

¿Se podrá predecir la estructura tridimensional de las proteínas a partir de su secuencia de aminoácidos?

Los 6000 millones de bases del genoma humano determinan unas 100.000 proteínas.
Aunque podemos conocer la secuencia de aminoácidos de una proteína, a partir de la secuencia del ADN de un gen, no está todavía a nuestro alcance pergeñar la forma de la proteína valiéndonos de reglas teóricas.
Y determinar experimentalmente las estructuras resulta bastante laborioso. 
Ahora bien, la estructura de una proteína se conserva, esto es, persiste relativamente constante a través de la evolución, mucho más que la secuencia de sus aminoácidos.
Secuencias que difieren en su serie de aminoácidos componentes pueden dar proteínas con morfología similar;
por eso mismo, podemos inferir las estructuras de proteínas diversas al estudiar pormenorizadamente un subgrupo de proteínas.

Hace poco, un grupo internacional de biólogos ha acometido la llamada "Iniciativa de Estructura de Proteínas" con el fin de coordinar sus trabajos estructuralistas.
Estos expertos "resuelven" las formas de las proteínas por un doble camino: creando cristales muy puros de una proteína en cuestión y bombardeándolos luego con rayos X, o bien sometiendo la proteína de marras a un análisis de resonancia magnética nuclear (RMN).
Ambas técnicas, tediosas, resultan muy caras.
Busca ese consorcio internacional extraer la máxima información de cada nueva estructura aprovechando el conocimiento recabado de estructuras emparentadas, para agrupar así las proteínas en familias que verosímilmente compartan los mismos rasgos arquitectónicos. 
En una etapa ulterior, habrán de examinar representantes de cada familia con técnicas físicas.

Conforme va creciendo el catálogo de estructuras resueltas y se van perfilando esquemas más refinados que agrupen las estructuras en un compendio de formas básicas, los bioquímicos depuran progresivamente sus programas de ordenador para obtener el modelo de las estructuras de proteínas recién descubiertas o incluso inventadas.
Los biólogos estructurales aventuran que habrá unos 1000 motivos básicos en el plegado de las proteínas.
A tenor de los modelos actuales, bastará resolver de 3000 a 5000 estructuras seleccionadas, amén de las ya conocidas, para deducir en adelante de forma rutinaria las estructuras de nuevas proteínas. 
Puesto que los biólogos moleculares resuelven más de 1000 estructuras de proteínas cada año y, habida cuenta de la aceleración del progreso, cabe presumir que se completará el inventario no mucho después de la secuenciación absoluta del genoma humano.

¿Se producirán in vitro formas de vida?

Acabamos de ver que los biólogos estructurales se afanan en agrupar las proteínas en distintas categorías para así resolver las estructuras de una manera eficiente. 
Pues bien, esa proclividad de las proteínas a someterse a clasificación refleja un claro significado biológico.
Testimonia el decurso evolutivo en la Tierra y nos lleva de la mano hacia las cuestiones centrales del fenómeno de la vida.
¿Hay un grupo de proteínas compartido por todos los organismos?
¿Cuáles son los procesos bioquímicos necesarios para que exista vida?

Con unos cuantos genomas ya secuenciados -del mundo bacteriano sobre todo-, se han abordado los primeros inventarios de genes conservados en esos organismos, puesta la mente en la pregunta crucial de qué es lo que constituye la vida, al menos en el ámbito unicelular.

Si dentro de unos años se lograra listar un elenco de productos génicos -ARN y proteínas- necesarios para la vida, tal vez pudiera construirse un ADN concatenando bases hasta obtener un genoma de nuevo cuño, que cifre productos inéditos.
Y si ese genoma inventado sirve para asentar una célula nueva en torno a él, y si esa célula tiene virtualidad replicativa, el ejercicio demostraría que se habían descifrado los mecanismos fundamentales de la vida.
Un experimento de ese tipo plantearía cuestiones éticas, teológicas y de seguridad que no pueden dejarse de lado.

¿Lograremos construir un modelo informático de célula que contenga todos los componentes, identifique todas las interacciones bioquímicas y prevea con fundamento las consecuencias de cualquier estímulo ejercido sobre esa célula?

En los últimos 50 años transcurridos, un gen o una proteína absorbía la carrera investigadora de un biólogo.
En el medio siglo próximo, los investigadores pasarán a estudiar las funciones integradas de muchos genes, la red de interacciones entre vías génicas y la influencia de estímulos externos en el sistema.

Cierto es que, desde hace bastante tiempo, los biólogos se han esforzado en describir los mecanismos de interacción mutua entre componentes celulares;
por ejemplo, la unión de los factores de transcripción a zonas precisas del ADN para controlar la expresión de un gen o el engarce entre la insulina y su receptor en la superficie de una célula muscular, con el desencadenamiento consiguiente de una cascada de reacciones dentro de la célula que multiplica el número de transportadores de glucosa en su membrana.
Pero el Proyecto Genoma extenderá análisis semejantes a millares de genes y componentes celulares.
En los cincuenta años próximos, con todos los genes identificados y todas las interacciones y reacciones celulares cartografiadas, los farmacólogos al desarrollar una nueva medicina o los toxicólogos al predecir si una sustancia es venenosa podrán dirigir su atención a los modelos informáticos de las células para encontrar la respuesta que demanda su búsqueda.

¿Se conocerán en todo su pormenor los mecanismos en cuya virtud los genes determinan el desarrollo de los mamíferos?

La construcción de un modelo celular será un hito.
Mas para comprender plenamente las formas vivas que nos son familiares tendremos que considerar niveles adicionales de complejidad.
Habrá que examinar el comportamiento, en tiempo y lugar, de los genes y sus productos;
es decir, aclarar su conducta en una u otra región del organismo y en un cuerpo que además cambia a lo largo de su vida. 
Los biólogos del desarrollo comienzan a seguir la pista de conjuntos de productos génicos que varían durante el desarrollo de los tejidos, con el fin de acotar los productos que definen las etapas del desarrollo.
Empiezan a pergeñar matrices de expresión, así se llaman, que inspeccionan miles de productos génicos a la vez, cartografiando cuáles se activan, cuáles se desactivan y cuáles presentan una expresión de intensidad fluctuante.
Merced a ese tipo de técnicas salen a la luz muchos genes idóneos para dirigir el desarrollo y establecer el diseño del organismo animal.

Lo mismo que antaño, la mosca Drosophila , el nemátodo Caenorhabditis elegans y el ratón seguirán siendo los prototipos animales en investigación de biología del desarrollo.
Con la secuencia del C. elegans ya terminada, la de la Drosophila a punto de culminarse, la secuencia humana que estará completa para el año 2003 y la del ratón dentro de cuatro o cinco años, las comparaciones entre secuencias se multiplicarán y ofrecerán claves sobre dónde buscar las fuerzas motrices de la configuración del organismo. 
Con el abaratamiento de los costes, se irán sucediendo las secuenciaciones de genomas representativos de las diversas ramas del árbol evolutivo.

Hasta ahora el énfasis se ha puesto en la búsqueda de señales de interés universal en el diseño del plan corporal, la disposición de miembros y órganos. 
Con el tiempo se estudiarán las variaciones -de secuencia génica y tal vez de regulación de genes- que dan cuenta de la diversidad de formas que reina entre especies diferentes.
Mediante la comparación de especies descubriremos las modificaciones sufridas en los circuitos genéticos que les indujeron a ejecutar distintos programas, de suerte tal que, con redes de genes casi equivalentes, se moldeen las patas peludas del ratón y los brazos del hombre.

¿Nos permitirá el conocimiento del genoma humano transformar la práctica médica en punto a prevención, diagnóstico y terapia?

La biología molecular ha venido auspiciando la transformación de la medicina desde un empirismo aleatorio hasta un método inquisitivo racional, basado en la comprensión fundamental de los mecanismos de la vida.
Su progreso repercute ya en la praxis médica.
La genómica intensificará esa tendencia.
De aquí a cincuenta años es de esperar que conozcamos las bases moleculares de las enfermedades, podamos prevenirlas en muchos casos y contemos con los medios para prescribir tratamientos precisos e individualizados.

En el próximo decenio, las pruebas genéticas revelarán la tendencia o exposición a una enfermedad. 
Entre otras misiones, compete al Proyecto Genoma Humano identificar variaciones genéticas comunes.
Una vez confeccionada la lista, la epidemiología establecerá la correlación entre variaciones y riesgo de contraer una enfermedad.
Cuando conozcamos el genoma en su integridad, se nos revelará el papel de los genes que, si bien por sí mismos contribuyen sólo débilmente a las enfermedades, al interaccionar con otros genes y ciertos factores ambientales (dieta, infecciones y exposición prenatal), afectan a la salud.
Del año 2010 al 2020 la terapia génica deberá haberse implantado ya como un tratamiento habitual, al menos para un cuadro restringido de condiciones.

De aquí a 20 años habrán aparecido fármacos nuevos, fruto del avance en el conocimiento molecular de enfermedades comunes, como la diabetes o la hipertensión.
Los fármacos, de notable potencia, se dirigirán hacia determinadas moléculas y carecerán de efectos secundarios.
Las medicinas anticancerosas, por señalar un ejemplo, se ajustarán a la respuesta que dé el paciente, prevista de acuerdo con el examen de sus "huellas digitales" de ADN.
Se afinará en el diagnóstico de muchas afecciones;
así, el paciente informado de una concentración excesiva de colesterol, sabrá también qué genes son los responsables, qué efectos acarrea esa demasía y qué dieta y medios farmacológicos le son recomendables.

Hacia el año 2050 muchas enfermedades potenciales se curarán en el ámbito molecular antes de que se manifiesten.
Mas, por culpa de las desigualdades mundiales en el acceso a esos avances, se seguirán levantando tensiones. 
Ante un enfermo, la terapia génicas y la farmacológica se centrarán en el gen responsable, lo que habrá de permitir un tratamiento preciso, personalizado.
La vida media llegará a los 90 o 95 años;
y su ocaso se retrasará con el conocimiento profundo de los genes del envejecimiento.

¿Reconstruiremos la historia de las poblaciones humanas?

Pese a la diversidad manifestada en el seno de nuestra especie, a lo largo de los últimos diez años se ha demostrado que la especie humana es más homogénea que otras muchas.
Presenta, sin ir más lejos, una variabilidad menor que los chimpancés.
En los humanos suelen darse en todos los grupos de población las mismas variaciones genéticas.
A las diferencias entre grupos sólo podemos atribuirles una pequeña fracción de la variación total: entre un 10 y un 15 por ciento. 
De esa comprobación algunos biólogos de poblaciones extraen la conclusión según la cual la especie humana constaba, no hace mucho, de un grupo restringido, quizá no superior a los 10.000 individuos;
la dispersión de las poblaciones por la Tierra sería un fenómeno reciente.
La mayoría de las variaciones genéticas acontecerían antes de ese momento.

Armados con las técnicas analíticas del ADN, los genéticos de estos últimos 20 años han abordado, con claridad inédita, cuestiones del máximo interés antropológico.
Las migraciones, los cuellos de botella y las expansiones de población alteran las frecuencias de genes y dejan así un registro detallado y completo de los avatares de la historia del hombre. 
De acuerdo con los datos genéticos, el hombre moderno apareció hará sólo unos 100.000 o 200.000 años, en Africa;
desde allí se dispersó poco a poco por el resto del mundo.
Los antropólogos se han servido del ADN para someter a prueba tradiciones culturales acerca del origen de los gitanos o los judíos, para seguir la pista de la migración del hombre hacia las islas del sur del Pacífico y América, y para esbozar una idea de conjunto sobre la difusión de las poblaciones en Europa, entre otros ejemplos.
Conforme resulte más fácil la acumulación de secuencias de ADN, se descubrirán con mayor nitidez las relaciones entre grupos de poblaciones, los episodios de mezcla y los períodos de separación y migración.
Se verá que los conceptos de raza y etnia son meros constructos sociales y culturales, carentes del menor respaldo biológico.

En el ecuador del siglo XXI conoceremos mejor las poblaciones humanas.
Pero, ¿cuánto más podremos saber?
A lo largo de su historia, los hombres se han cruzado con el abandono suficiente para que ningún árbol familiar encierre la explicación cabal de su pasado.
La historia de las poblaciones no emergerá de un árbol, sino de una suerte de enrejado con cruces frecuentes de líneas y mezcla tras intervalos de separación.
Veremos también, dentro de 50 años, cuánta ambigüedad queda todavía en la reconstrucción de nuestro ayer. 

¿Podremos reconstruir las etapas principales de la evolución de la vida sobre la Tierra?

Desde los años sesenta la taxonomía ha contado, entre sus útiles indispensables, las secuencias moleculares. 
En buena medida, las secuencias de ADN cifran 3500 millones de años de evolución:
distribuyen los seres vivos en tres dominios -Archaea (unicelulares de origen remotísimo), Bacteria y Eukarya (organismos cuyas células poseen un núcleo)- y revelan los patrones de ramificación de múltiples reinos y divisiones.
Por culpa de cierto aspecto de la herencia se aleja la esperanza de asignar todos los seres vivos a ramas de un mismo árbol.
En numerosas ocasiones, según qué genes miremos trenzaremos una historia familiar u otra para los mismos organismos. 
Ello se debe a que el ADN no se hereda siempre de una manera rectilínea, de padres a hijos, con una mayor o menor cadencia temporal de mutaciones.
A veces los genes saltan a través de grandes hiatos evolutivos.
Lo vemos en mitocondrias y cloroplastos, orgánulos de animales y plantas encargados del suministro de energía.
Mitocondrias y cloroplastos albergan su propio material genético y descienden de bacterias engullidas enteras por células eucarióticas.

Semejante estrategia de "transferencia lateral de genes" parece haber sido un fenómeno bastante común en la historia de la vida.
Por tanto, cuando comparemos genes de distintas especies no obtendremos siempre un dendrograma único y universal.
Igual que acontecía con los linajes humanos, para explicar la historia de la vida se ajusta mejor la imagen de un enrejado, en el que líneas separadas divergen y se unen de nuevo, que la representación de un árbol en el que las ramas nunca vuelven a trabarse.

Así que pasen 50 años se habrán rellenado muchos huecos de la historia de la vida, aunque quizá sigamos ignorando cómo surgieron exactamente los primeros organismos que se autorreplicaban.
Sí sabremos cuándo y cómo surgieron diversas líneas, se adoptaron o se adaptaron genes para adquirir grupos nuevos de reacciones bioquímicas o diferentes planes corporales. 
La perspectiva genética habrá permeado de tal modo la biología, que la unidad básica habrá dejado de ser el organismo o la especie para cederle ese estatuto al gen.
Se cartografiarán las rutas recorridas juntas por varios genes, su tiempo de coincidencia y en qué genomas.
Y se planteará, una vez más, la cuestión que inquieta a la gente desde Charles Darwin: 
¿Qué es lo que nos hace humanos?
¿Qué es lo exclusivo de nuestra especie?

Surgirán otras preguntas.
Como en cualquier campo fecundo del saber, los resultados abrirán nuevos interrogantes.
Paradójicamente, conforme adquiera mayor importancia, la genómica podría perder sus propios perfiles, al irradiar otras áreas y terminar absorbida en la infraestructura de todas las disciplinas biomédicas. 

¿Cómo responderán los individuos, familias y sociedad a esta explosión de conocimientos sobre nuestro legado genético?

A diferencia de las cuestiones precedentes, de carácter científico, técnico y médico, esta pregunta de corte social no parece admitir una respuesta tajante.
La información y la técnica genética aportarán grandes posibilidades de mejora de la salud y remedio del dolor. 
Pero no hay técnica eficaz sin riesgos;
cuanto más drástica la técnica, mayores serán también los riesgos.
Con perversa voluntad algunos se escudan hoy en supuestos argumentos genéticos para justificar su racismo abierto o solapado.
De la información extraída de análisis del ADN cada vez más extendidos se han servido empresas de seguros y jefes de personal para negar el acceso a mutuas médicas o a un puesto de trabajo.

¿Se aquietarán los movimientos contrarios al progreso técnico ante los descubrimientos de la genética? 
Quizá sea negativa la respuesta que tengamos que dar.
La tensión entre los avances científicos y el deseo profundo de un estilo de vida "natural" se intensificará a buen seguro, conforme la genómica vaya penetrando en nuestra vida diaria.
Habrá que esforzarse por lograr un equilibrio saludable y asumir la responsabilidad de que el progreso que trae la genómica no se ponga al servicio de fines torvos.


Integrinas y salud

Descubierta muy recientemente, estas moléculas adhesivas presentes en la superficie celular se han revelado imprescindibles para el buen funcionamiento del cuerpo y para la vida propia. 

Las células del cuerpo se mantienen pegadas unas a otras y a un material cohesivo, o matriz extracelular, que las circunda.
Esa cohesión es esencial para la supervivencia, ya que mantiene unidos los tejidos.
Resulta importante también para el desarrollo embrionario y toda una serie de procesos que acontecen en el organismo adulto, como la coagulación sanguínea, la curación de las heridas y la erradicación de las infecciones.
Paradójicamente, esa misma adhesividad celular puede facilitar la aparición de la artritis reumatoide, los ataques cardíacos, los accidentes cerebrovasculares y el cáncer, entre otras condiciones patológicas. 

Aunque se conocía desde tiempo atrás la importancia de las interacciones adhesivas que se producen en el cuerpo, sólo recientemente se ha empezado a desentrañar sus diversos efectos fisiológicos.
Los primeros resultados esclarecedores se obtuvieron hace unos 20 años, al aislarse algunas de las moléculas de la matriz que se pegan a las células.
Las investigaciones de los últimos 15 años han puesto de manifiesto que unas moléculas presentes en la superficie celular, las integrinas, desempeñan un papel fundamental en muchos fenómenos relacionados con la adhesión.
Como no podía ser menos, los laboratorios farmacéuticos están ya aplicando esos conocimientos al desarrollo de nuevos tratamientos contra ciertas enfermedades.

Me cupo la fortuna de ser uno de los investigadores que identificaron las primeras integrinas y descubrieron su actividad. 
La historia de las integrinas, sin embargo, no es patrimonio de un solo centro.
Antes al contrario, el conocimiento de su mecanismo de funcionamiento ha sido posible gracias a la cooperación entre equipos que investigaban procesos muy distintos.
A unos nos interesaba el desarrollo embrionario; a otros, el funcionamiento del cuerpo adulto o el desarrollo de determinadas enfermedades.
Pero el flujo de información entre todos los comprometidos ha sido incesante, lo que ha permitido progresar a un ritmo acelerado.

Las interacciones que se establecen entre los componentes de la matriz y las células del epitelio mamario constituyen un ejemplo espectacular de la importancia que la adhesión encierra para el correcto funcionamiento celular.
Las células epiteliales, en general, forman la piel y el revestimiento de la mayoría de las cavidades del organismo.
Acostumbran disponerse en una monocapa que se apoya sobre la lámina basal, una matriz peculiar.
Las células epiteliales que recubren las glándulas mamarias producen leche en respuesta a un estímulo hormonal.
Si se extraen las células epiteliales mamarias de un ratón y se cultivan en el laboratorio, pierden rápidamente su forma cuboide regular, así como la capacidad para fabricar proteínas lácteas.
Sin embargo, si se cultivan en presencia de laminina (la principal proteína adhesiva de la lámina basal), recuperan su morfología propia, organizan una lámina basal y configuran estructuras pseudoglandulares capaces nuevamente de producir componentes de la leche.

A principios de los años ochenta, los empeñados en descubrir de qué modo la matriz extracelular controlaba la actividad de las células habían conseguido algunos progresos a partir del estudio de la propia matriz.
Averiguaron que la matriz venía a ser un gel formado por cadenas de azúcares y proteínas fibrosas interconectadas, si bien la cantidad de matriz y los pormenores de su estructura variaban de un tejido a otro.
Proteínas de la matriz son, por ejemplo, la laminina, fibronectina (otra molécula adhesiva) y el colágeno; este último, aunque a veces es adhesivo, constituye el componente estructural primario de la mayoría de las matrices.
Y gracias a la microscopía se sabía que las moléculas adhesivas de la matriz estaban ligadas, presumiblemente a través de una o más moléculas intermediarias, al sistema de fibras intracelulares (citoesqueleto) que proporciona a las células su forma tridimensional.

También se sabía que las uniones célula-matriz podían afectar a las células de diversas maneras según el tipo celular, el estado de la célula en ese momento y la composición específica de la matriz. 
Unas veces, las células responden cambiando su forma; otras, emigran, proliferan, se diferencian (se especializan) o modifican ligeramente sus actividades.
Los cambios pueden deberse a una alteración en la actividad de algunos genes. 
Los genes especifican la secuencia de aminoácidos de las proteínas, moléculas encargadas de ejecutar la mayoría de las funciones celulares.
Cuando los genes se activan, esto es, se expresan, producen la proteína especificada.
Según qué genes se hallen activos o inactivos, el contenido proteínico de las células cambia y, por tanto, el tipo de operaciones que dichas células pueden acometer.

Para acotar de qué manera la matriz extracelular provocaba tales cambios en las células, había que identificar los receptores celulares, que son los sitios que reconocen las proteínas de la matriz.
Un método usual de aislar el receptor para una molécula determinada consiste en purificar los componentes del extracto celular que se adhieren a dichas moléculas.
En este caso, sin embargo, los receptores se resistían a la purificación. 

Culpable de ello, lo sabemos ahora, era, por un lado, cierto fenómeno:
las moléculas de adhesión de la matriz pueden unirse a muchos tipos de sustancias, algunos de los cuales son también componentes de la matriz.
El problema pudo resolverse al descubrirse el sitio por donde la fibronectina se engarzaba en las células.
Curiosamente, esa región es una secuencia de tres aminoácidos: arginina, glicina y aspartato. (Se le suele denominar RGD, por ser esas letras las correspondientes al código utilizado para nombrar a los aminoácidos.) Por otro lado, ocurre a menudo que los receptores, individualmente considerados, no se traben con fuerza a las proteínas de la matriz.
La adhesión fuerte se logra mediante una suerte de "efecto velcro", en virtud del cual muchas uniones relativamente débiles forman en conjunto una vinculación muy intensa.
Por ello, las uniones con los receptores individuales no duraban lo suficiente para "cazarlos" a la primera.

A pesar de las dificultades, a mediados de los años ochenta se habían aislado ya varios receptores.
Se conocía la secuencia de algunos aminoácidos de esas moléculas.
En 1987, gracias a esa conjunción de resultados, se comprobó que los receptores pertenecían a una gran familia de moléculas estructuralmente emparentadas, algunas de las cuales estaban presentes en prácticamente todos los tipos celulares del reino animal.
En reconocimiento a la importancia de esa familia para la integridad estructural de células y tejidos, se impuso a las moléculas el nombre de "integrinas".
Andando el tiempo, y por otras razones que explicaré, se vio cuán pertinente había sido tal denominación.

Mientras tanto, las investigaciones sobre la estructura y función de las integrinas empezaban a dar sus frutos. 
El descubrimiento de la ubicuidad de las integrinas en la mayoría de los tejidos celulares nos hizo caer en la cuenta de que algunas preguntas de difícil respuesta en ciertos tipos celulares podrían abordarse en otros de más fácil manipulación.
Merced a tamaña flexibilidad y al intercambio de resultados de muchas disciplinas distintas, se avanzó rápidamente y se abrieron nuevas perspectivas.

Comprobamos muy pronto que las integrinas constaban de dos cadenas o subunidades.
De la subunidad denominada "alfa" conocemos unas 15 variantes y de la "beta", unas ocho. 
Las cadenas, que se designan con un número o una letra, se combinan para crear al menos 20 integrinas diferentes. 
También comprobamos que ciertas integrinas se adhieren a un solo tipo de moléculas efectoras, en tanto que otras reconocen varias dianas distintas.
La mayoría de las integrinas interaccionan con la matriz extracelular.
Algunas participan en las adhesiones intercelulares y la mayoría de las moléculas responsables de ese tipo de adhesión célula-célula pertenecen a grupos que se conocen con los nombres de cadherinas, selectinas o inmunoglobulinas. 
Hallamos, asimismo, que las integrinas atraviesan la membrana celular.
La parte de las cadenas alfa y beta que sobresale de las células forma el receptor de la fibronectina y de otras moléculas extracelulares o ligandos. Para que las conexiones extracelulares permanezcan prietas, la porción de integrina que se insiere en el citoplasma (o fluido interno de la célula) debe anclarse en el citoesqueleto.
Vale la pena exponer cómo se establecen tales engarces. 
Cuando se produce la unión molécula-receptor fuera de la célula y el receptor se agrega a otras integrinas, que a su vez se han unido con otras moléculas, se crean unos complejos muy organizados: las "formas de adhesión focales".
Estas adhesiones, que pueden variar en tamaño y estructura, incorporan varios tipos de moléculas, incluidas las colas citoplásmicas de las agrupaciones de integrinas y algunos componentes del citoesqueleto.
La unión y la agregación con el receptor provocan también la reorganización del citoesqueleto; las células, que eran redondeadas, adoptan una estructura definida.

La unión, mediada por integrinas, de las células a una matriz facilita el movimiento de las células, sobre todo durante el desarrollo embrionario de los organismos y en el caso de los leucocitos del sistema inmunitario.
La emigración comienza con el despliegue de una suerte de pseudópodos en la región frontal de la célula. 
A continuación, las moléculas de integrina cercanas al extremo delantero de ese pseudópodo interaccionan con la matriz sobre la que se sustentan;
esa interacción genera una tracción que va a permitir el movimiento celular. 
Contemporáneamente, se sueltan otros ligámenes de la región posterior de la célula, lo que provoca que esta parte trasera se mueva hacia delante, como si se dejara libre el extremo de un resorte.
Desconocemos qué fuerzas son las responsables de que se liberen las conexiones de la matriz con la región trasera de la célula. 
E ignoramos qué fuerzas mecánicas y bioquímicas inducen el avance celular, desde la región frontal o desde el impulso trasero, en un ciclo que se repite una y otra vez. 

Además de hacer las veces de "pegamento" celular y de facilitar la emigración, las integrinas ejercen un tercer efecto, muy potente, sobre las células.
Alrededor del año 1990, se supo que la unión extracelular entre las integrinas y las moléculas provocaba, en el interior de las células, la activación de las rutas de transducción de señales.
Esas vías, constituidas por moléculas que liberan mensajes a través del citoplasma, desencadenan, entre otras respuestas, la expresión génica, división celular y la inducción de procesos que impiden la autolisis de las células.

Poco después, se descubrió que las integrinas modulaban los mensajes que los factores de crecimiento transmiten a las células.
Ya se sabía que los factores de crecimiento, que viajan de una célula a otra, activaban rutas específicas de transmisión de señales que controlan qué genes deben expresarse y si una célula debe reproducirse o incluso sobrevivir.
Lo que el estudio de las integrinas demostraba era que las células normales (no malignas), para vivir y reproducirse, debían anclarse en una matriz específica cuando las estimulaban los factores de crecimiento.
La falta de factores de crecimiento o la pérdida de los contactos adhesivos hacen que una célula en división deje de proliferar y termine por morir.
Casi tres años después de recibir su bautismo, las integrinas hacían honor a su nombre por un motivo distinto.
Estas moléculas participan en la integración de muchos de los tipos de señales que afectan a las células y determinan, en tal cometido, su destino.

Parece que buena parte de estos sistemas de señalización interna dependen de la activación de moléculas que también están presentes, junto con las integrinas, en las adhesiones focales.
Esta función se ha estudiado más a fondo en fibroblastos (células del tejido conectivo), donde los complejos son bastante grandes, pues constan de una veintena larga de moléculas diferentes.

Se sabe, desde hace tiempo, que algunas de las moléculas presentes en esas adhesiones focales (como las enzimas de la familia de quinasas Src) intervienen en rutas de señales que se activan por factores de crecimiento.
Su presencia en los complejos sugiere que las moléculas de la matriz y los factores de crecimiento pueden a veces modular sus mensajes mutuamente, enviando señales allá donde las rutas convergen o se cruzan.
Otras moléculas de adhesión focal que se activan por integrinas (quinasas de adhesión focal, paxilinas y tensinas) apenas si experimentan los efectos de los factores de crecimiento.
No obstante, ofrecen sitios de unión para componentes conocidos de transducción de señales, una propiedad que sugiere que ellas también coadyuvan a transmitir mensajes desde las integrinas a los genes y a otras partes de la célula.

Lo que no está tan claro es cómo las integrinas estimulan a las moléculas señalizadoras presentes en las adhesiones focales.
Ciertos tipos de moléculas de superficie celular, en particular muchos receptores de factores de crecimiento, son quinasas de tirosinas.
Estas moléculas añaden grupos fosfato a residuos de tirosina que hay en otras proteínas, regulando así la actividad de las moléculas diana.
Pero las integrinas carecen de actividad quinasa.
Tampoco son fosfatasas, enzimas que controlan a otras moléculas, quitándoles grupos fosfatos.
Ni poseen las integrinas sitios normales de acoplamiento mediante los cuales las moléculas transductoras de señales se traban entre sí; lo que significa, a buen seguro, que las integrinas no alteran directamente esas moléculas.
¿Cómo provocan, entonces, tal profusión de señales internas?
Podría ser que coadyuvaran a que las moléculas señalizadoras entraran en contacto unas con otras; podría ser, pero nadie tiene aún una respuesta definitiva.

Las integrinas no responden sólo a señales recibidas desde el exterior de las células.
También reaccionan a mensajes recibidos desde el interior.
Estas señales de dentro a fuera pueden hacer que las integrinas se vuelvan más o menos selectivas a la hora de aceptar las moléculas con las que deben unirse y, asimismo, pueden cambiar la fuerza con que las integrinas se traban.
La integrina xxx , por ejemplo, puede estar inactiva, ser un receptor para el colágeno o un receptor para el colágeno y la laminina, según la célula que la produzca y las señales que reciba desde el interior celular.

Las señales de dentro a fuera han sido objeto de exhaustiva investigación en las plaquetas, glóbulos rojos sin núcleo que participan en la formación de los coágulos sanguíneos o trombos.
Los trombos se forman en las zonas dañadas de los vasos sanguíneos e impiden, momentáneamente, que se pierda sangre.
Las plaquetas que circulan por la sangre lo hacen de forma independiente, y no se pegan unas a otras.
No obstante, cambian su comportamiento cuando los vasos sanguíneos, que están recubiertos por una monocapa de células endoteliales, resultan dañados. 

En primer lugar, las plaquetas se pegan (sin ayuda de las integrinas) a zonas de la matriz extracelular que quedan a la vista tras la desorganización de las células endoteliales. 
Esta adhesión, o la posterior unión con una proteína denominada trombina, envía una señal al citoplasma, que finalmente determina la activación de la integrina xxx , ubicada en la superficie de las plaquetas.
En este caso, la señal provoca que la integrina se torne más adhesiva.
Una vez activada, la integrina xxx se aferra a las moléculas circulantes de fibrinógeno o factor de von Willebrand, tendiendo puentes moleculares con otras plaquetas y con la matriz.
El agregado de plaquetas y proteínas resultante culmina en un denso entramado de células y fibras.

Además de examinar los efectos de las integrinas sobre células individuales, los expertos se han interesado por el papel de esas moléculas en el conjunto del cuerpo. 
Paradójicamente, sabemos menos del papel que desempeñan las integrinas en la maduración de un embrión que de su participación en algunos otros procesos, a pesar de que fue el interés por conocer las bases moleculares del desarrollo lo que fomentó las investigaciones que han llevado al descubrimiento de las integrinas y otras moléculas de adhesión.

Tenemos pruebas, no obstante, de que el desarrollo normal requiere el funcionamiento correcto de las integrinas.
A medida que las células de un embrión proliferan y se diferencian para formar los tejidos y órganos del cuerpo, van añadiendo y quitando integrinas de su superficie, indicio claro de que las integrinas que se añaden participan en la transición de un estadio a otro nuevo.
Otra prueba de la necesidad de su presencia la tenemos en el descubrimiento de que las células embrionarias poseen integrinas específicas que les ayudan a moverse y encontrar sus destinos finales.

La ingeniería genética ha permitido producir moscas de la fruta y ratones que carecen de una u otra integrina. 
Esos animales son deformes o mueren durante el desarrollo, demostrando así que la falta de tales integrinas es la causa de su fallo.
En los embriones con malformaciones, la desorganización de los tejidos se debe, aparentemente, a que las células son incapaces de moverse hasta los sitios adecuados o no producen los contactos adhesivos que se necesitan para la cohesión.
En efecto, en ciertos mutantes de la mosca de la fruta, los músculos recién formados se deshacen a las primeras contracciones, pues las células musculares se separan del tejido conectivo.

Las integrinas cumplen una misión importante en la fisiología normal de los organismos maduros.
Sabemos, además, de su intervención en determinadas patologías. Un proceso crítico que requiere integrinas es la inflamación, serie compleja de respuestas que se ponen en marcha tras una lesión o una infección. 
Cuando se lesiona un tejido o lo coloniza un microorganismo patógeno, ciertos leucocitos, como los neutrófilos y los monocitos, abandonan el torrente sanguíneo y se dirigen a la zona afectada.
Allí eliminan restos y sustancias extrañas y atacan a los patógenos. 
Además, los leucocitos secretan sustancias que retrasan el avance de la infección.
Si es necesario, las células reclutan también linfocitos para destruir a los invasores.

Para que los leucocitos puedan viajar al tejido interesado, deben primero abandonar el torrente sanguíneo principal. 
De ello se encargan las células endoteliales que detectan un problema en sus cercanías.
Reacciones adhesivas que suelen implicar a las selectinas, no a las integrinas, hacen que los leucocitos reduzcan su velocidad y se desplacen por la capa endotelial de los vasos sanguíneos.
A continuación, las señales procedentes del interior instan a que ciertas integrinas de los leucocitos (principalmente las que contienen subunidades de tipo xxx o xxx ) adquieran afinidad por las moléculas de la familia de las inmunoglobulinas, en especial las ICAM (moléculas de adhesión intercelular), presentes en las células endoteliales.
En virtud de esas interacciones, los leucocitos se detienen, se abren paso entre las células endoteliales y atraviesan la pared de los vasos sanguíneos en el tejido dañado o infectado.

La importancia de las integrinas en las inflamaciones se pone de manifiesto en la deficiencia de adhesión leucocitaria. 
Los individuos que padecen esta enfermedad carecen de la subunidad xxx de la integrina o sintetizan una versión defectuosa.
Puesto que sus leucocitos no pueden llegar a los sitios dañados, esas personas suelen sufrir infecciones que amenazan su vida.

La respuesta inflamatoria protege de infecciones graves al cuerpo.
Pero puede contribuir a una enfermedad si persiste durante demasiado tiempo o se produce de forma inadecuada.
Para mejorar el tratamiento de los casos en los que se sufren inflamaciones crónicas, se están desarrollando compuestos (dirigidos contra las integrinas xxx y xxx ) que impiden las interacciones, mediadas por integrinas, entre leucocitos y células endoteliales.
Esos fármacos se hallan en fase de prueba en pacientes asmáticos, con inflamaciones del aparato digestivo y artritis reumatoide.

Las inflamaciones pueden ser también muy destructivas durante la reperfusión; así se llama el proceso de restauración de flujo sanguíneo a los tejidos que han quedado desprovistos de sangre durante, por ejemplo, un episodio de congelación, un ataque al corazón o un accidente cerebrovascular. (Muchos ataques al corazón y accidentes cerebrovasculares se deben a la oclusión de algún vaso sanguíneo importante que abastece el corazón o el cerebro.) La pérdida transitoria de sangre puede dañar o destruir el tejido.
Cuando se restablece el flujo sanguíneo, los neutrófilos de la sangre detectan el daño, emigran a las zonas afectadas y liberan oxidantes.

Los oxidantes pueden destruir a los patógenos, pero pueden también causarles más daños a las ya frágiles células.
Los investigadores intentan frenar la agresión debida a la reperfusión con fármacos cuyas dianas son las integrinas xxx de los neutrófilos o las ICAM de las células endoteliales. 
Separando las integrinas de las ICAM, los fármacos impedirían que los neutrófilos salieran del torrente sanguíneo para invadir el tejido sometido a reperfusión. 

Igual que sucede con las inflamaciones, un exceso o un defecto en la formación de trombos resultan peligrosos. 
Las personas que carecen de integrina xxx (de suma importancia para la agregación plaquetaria) padecen trombastenia de Glanzmann.
Las plaquetas no se agregan correctamente y las víctimas corren riesgo de desangrarse. 
En el otro extremo del espectro, un desarrollo excesivo de los trombos, algo que suele ocurrir cuando hay una acumulación de depósitos grasos (placas ateroscleróticas) en los vasos sanguíneos, puede ocasionar un ataque al corazón o un accidente cerebrovascular si el trombo acaba obstruyendo el vaso.

Existen procedimientos angioplásticos para desatascar las arterias ateroscleróticas.
Pero tales medidas pueden interesar las células endoteliales y hacer que ocasionalmente se formen trombos en las arterias al día siguiente del tratamiento.
Además, en el proceso de restenosis, los vasos sanguíneos de muchos pacientes pueden resultar obstruidos nuevamente en un período de muchos meses, debido en este caso a la emigración y proliferación de las células del músculo liso.
Según parece, este proceso se produce en respuesta a determinadas sustancias presentes en los trombos que se forman cerca de los sitios dañados. 
Para impedir la aparición de trombos en los días posteriores al tratamiento, se emplean fármacos que impiden, transitoriamente, que la integrina xxx interaccione con el fibrinógeno.
Por demorar la formación de trombos, ese tipo de fármacos podría colaborar, además, en el control de la restenosis.

En principio, la restenosis se evitaría actuando contra una integrina diferente, la xxx , que abunda en las células de la musculatura lisa cuando se lesionan los vasos sanguíneos.
Puesto que se trata de una integrina que promueve, así parece, la supervivencia y el movimiento de esas células, el bloqueo de su actividad podría limitar la participación de las células a la restenosis.

Entre los trastornos en los que se detecta una actividad de las integrinas indeseable recordaremos la osteoporosis, un número cada vez mayor de enfermedades infecciosas y el cáncer.
La osteoporosis se caracteriza por la pérdida de tejido óseo y el consiguiente incremento del riesgo de fracturas, principalmente en las mujeres de edad madura.
Este tipo de problemas puede obedecer a la hiperactividad de células que se unen al tejido óseo y lo degradan. 
El engarce en cuestión viene mediado por la integrina xxx ; y ése es el motivo de que se esté ensayando el empleo de señuelos, que esconden la integrina, para impedir que las células destructivas se adhieran al tejido óseo.

En el caso de infecciones, parece ser que ciertos microorganismos entran en las células sirviéndose de las integrinas. 
Procederían así los agentes de la gripe, meningitis, diarreas y parálisis.
Ante tales hallazgos, se habla de la posibilidad de diseñar nuevos fármacos contra las integrinas implicadas en esos procesos.

Aunque no cabe la menor duda de que las integrinas participan también en el cáncer, queda mucho por averiguar sobre el mecanismo responsable.
Aparece cáncer cuando las células se saltan los controles normales sobre la división y movimiento celular;
las células se multiplican entonces sin freno y adquieren la capacidad de invadir tejidos y metastizar, esto es, propagarse a sitios distantes y desarrollarse en regiones que no les son propias.
Se ha observado que, en ciertos tumores, la producción de integrinas específicas se ha detenido, no son éstas las de esperar o su distribución normal se encuentra alterada. 
Las consecuencias de esos cambios no son siempre manifiestas, aunque se supone que algunas integrinas facilitan la emigración de células que de suyo no se moverían.

Las integrinas promueven la formación de vasos sanguíneos (angiogénesis) en los tumores.
Esos vasos nutren a los tumores y proporcionan a las células metastásicas vías de acceso a la circulación.
Para crear nuevos vasos sanguíneos, las células endoteliales en proliferación deben formar uniones adhesivas unas con otras y con la matriz circundante.
Existen pruebas de la presencia de grandes cantidades de integrina xxx en las células endoteliales que están formando nuevos vasos, presencia que evita la muerte de las células proliferantes. 
A este respecto, los compuestos que arruinan la adhesión de las células endoteliales mediada por integrina xxx inducen la autoaniquilación de las células endoteliales que proliferan de una manera indebida.
Se trata de agentes que inhiben, además, la formación de nuevos vasos y producen la regresión de tumores en animales, por la presumible razón de que cortan el suministro de sangre a los tumores.

Espoleados por tales resultados, los expertos se aprestan a investigar la eficacia anticancerígena de los inhibidores de xxx .
Esos agentes u otros del mismo tenor pueden acudir en auxilio de las personas que padecen retinopatía proliferativa, una de las complicaciones derivadas de la diabetes. 
En este trastorno se produce una proliferación de vasos sanguíneos anormales, que daña la retina y causa ceguera.
Los fármacos en cuestión no deben afectar a los vasos sanguíneos sanos, ya que la integrina xxx no abunda en las células endoteliales de los vasos que no están desarrollándose.

La mayoría de las posibles terapias contra enfermedades vinculadas con la adhesión se proponen impedir el engarce de las integrinas con las moléculas extracelulares.
Pero a su vez, los agentes que favorecen esos anclajes podrían facilitar el movimiento de células dérmicas sanas hacia zonas de la piel dañadas gravemente, acelerando de esa manera la curación de las heridas.
Productos similares podrían incorporarse en matrices artificiales, para guiar el desarrollo de nuevos tejidos que sustituyan a los dañados por enfermedades, tratamientos agresivos o lesiones de cualquier tipo.

A la mayoría de los que nos venimos dedicando al estudio de las integrinas nos interesan, sobre todo, los aspectos básicos de la investigación: averiguar los mecanismos en virtud de los cuales las integrinas y otras moléculas de adhesión gobiernan el desarrollo y la fisiología. 
Independientemente de nuestra óptica de partida, la experiencia científica está resultando apasionante. 
Lo que en un principio fue estudio de un aspecto de un problema ceñido ha terminado por llevarnos al inmenso campo de los efectos producidos por las integrinas y otras moléculas de adhesión.
Los progresos obtenidos ponen una vez más de manifiesto la importancia de la cooperación entre las diferentes áreas de la ciencia.


¿Una determinación biológica? 

Aun cuando los rasgos genéticos y neuroanatómicos guardaran cierta correspondencia con la orientación sexual de los individuos, no está en absoluto probada la relación de causalidad entre aquellos y ésta.

Los defensores de los derechos humanos, las organizaciones religiosas y los tres poderes, de los Estados Unidos y otras naciones, debaten si la orientación sexual de las personas tiene base biológica.
Esta polémica ha acaparado titulares en los periódicos, pero los expertos en comportamiento la consideran agua pasada.
La pregunta fundamental sobre biología e inclinación sexual no es si la primera está involucrada en la segunda, sino la forma en que lo está .
En última instancia todos los fenómenos psicológicos son biológicos.

Pero aunque el debate público apareciera formulado en términos más precisos, seguiría desenfocado. 
Casi ninguno de los pasos de la argumentación que ligan la biología con la orientación sexual y la política social resisten el análisis.
En el ámbito político, no deja de ser un criterio inhumano el que una sociedad exija la condición de inmutabilidad o de característica innata para determinado rasgo, a la hora de decidir quiénes merecen o no su tolerancia.
Aun cuando la homosexualidad fuera mera cuestión opcional, los empeños por erradicarla mediante el uso de sanciones sociales y penales recortan las libertades fundamentales y la diversidad del género humano.

Además, la idea de que la homosexualidad o es innata e inmutable o por el contrario, de opción libre, resulta, a su vez, inexacta.
Fijémonos en el chingolo cejiblanco, pájaro que aprende el canto de su especie a lo largo de un corto intervalo en su desarrollo.
La manada de los gorriones expuestos a diversos trinos, entre ellos el de su especie, aprenden este último, pero algunos no lo hacen.
En cuanto el pájaro ha aprendido un canto determinado, ni lo olvida ni adquiere otro nuevo.
Aunque la inclinación sexual no constituya una cuestión de mimetismo, a la manera de nuestra ave es obvio que la conducta aprendida sí puede perdurar inmutable.

Por último, todas las pruebas presentadas hasta la fecha en pro de una base biológica, congénita, de la homosexualidad adolecen de puntos débiles.
Las de corte genético se resienten de la inevitable confusión entre lo natural y lo adquirido que arruina los trabajos relativos a la herencia de los rasgos psicológicos.
Las pruebas extraídas de estudios sobre el cerebro descansan en dudosas hipótesis que sostienen la existencia de diferencias entre el cerebro de varón y el de mujer.
Los mecanismos biológicos que se han propuesto para explicar la homosexualidad masculina no pueden, por lo común, generalizarse para dar cuenta de la homosexualidad femenina (muy poco atendida en las investigaciones).
Y la propia naturaleza de gradación continua de las variables biológicas está reñida con la escasez de adultos bisexuales sugerida en la mayoría de los estudios.

Para saber cómo influyen los factores biológicos en la orientación sexual, hay que empezar por definir qué se entiende por ésta.
Muchos investigadores, Simón LeVay en particular, consideran dicha inclinación un rasgo sexualmente dimórfico:
los varones están normalmente «programados» para sentir atracción por las mujeres, y las mujeres lo están en general para verse atraídas por los hombres.
A tenor de este planteamiento, los varones homosexuales tendrían programación femenina y las lesbianas, masculina.
Algunos estudiosos sugieren que esta programación corre a cargo de factores biológicos, que la realizarían durante la gestación; otros creen que ocurre después de nacer, en respuesta a estímulos sociales y experiencias subjetivas.
Puesto que la función del cerebro se halla vinculada a su estructura y fisiología, sería lógico suponer que los cerebros de los homosexuales exhibieran características propias del sexo contrario.

La validez de este cuadro «intersexual» es cuestionable.
Por un lado, la orientación sexual no es dimórfica, sino polimórfica.
Las motivaciones conscientes e inconscientes asociadas con la atracción sexual divergen incluso entre personas del mismo sexo e inclinación. 
Son innumerables las experiencias (e interpretaciones subjetivas de esas experiencias) que pueden intervenir para que personas diferentes sientan el mismo grado relativo de atracción sexual hacia hombres o hacia mujeres.
Personas distintas pueden tener razones diferentes para sentirse sexualmente atraídas por varones; a priori, no hay motivo alguno para que todos los que experimentan atracción sexual hacia el varón compartan alguna estructura cerebral concreta.

La idea de que los varones homosexuales están feminizados y las lesbianas masculinizadas refleja más nuestra cultura que la biología de la respuesta erótica.
Ciertos mitos griegos reclamaban orígenes intersexuales para el deseo heterosexual, no para el homosexual:
las personas atraídas por otras de su mismo sexo se consideraban las más viriles entre los hombres y las más femeninas entre las mujeres.
Por contra, se atribuía una naturaleza mixta, masculina y femenina, a los que deseaban al sexo opuesto.
La cultura clásica encomiaba las manifestaciones homosexuales de héroes arquetípicamente masculinos, como Zeus, Hércules y Julio César. 
Hasta hace diez años (cuando los misioneros repudiaron tal práctica), entre los miembros del pueblo sambia de Nueva Guinea regía la costumbre según la cual los adolescentes realizaban prácticas sexuales con los hombres, felación incluida; nadie consideraba afeminado ese comportamiento. 

Antes bien, creían necesaria la ingestión de semen para alcanzar fuerza y virilidad.

El cuadro intersexual ha de habérselas, sin embargo, con un problema mas tangible:
no se ha conseguido una demostración concluyente de que los rasgos de los cuales los homosexuales tienen claras versiones correspondientes al sexo opuesto difieran entre hombres y mujeres.
De las múltiples diferencias que, en razón del sexo, se atribuían en el siglo pasado al cerebro, sólo una puede corroborarse:
el tamaño, que varía con el tamaño corporal.
Así, los varones tienden a tener cerebros ligeramente mayores que las mujeres. 
Esta situación contrasta de forma notable con lo observado en otros animales, que si reflejan rasgos dispares en el cerebro según el sexo.

Si el cerebro presenta ya un sistema de conexiones o cualquier otro tipo de programación concerniente a la inclinación sexual,
¿qué fuerzas son las responsables? 
Hay tres posibilidades.
De acuerdo con un primer modelo, se daría una relación biológica causal y directa; genes, hormonas u otros factores actuarían directamente sobre el cerebro en desarrollo, a buen seguro antes del nacimiento, para determinar la orientación sexual. 
Para un modelo alternativo, el del aprendizaje social, la biología proporcionaría una pizarra en blanco de circuitos neuronales sobre la cual la experiencia inscribiría la inclinación sexual.
Por último, según el modelo indirecto, los factores biológicos no condicionarían la orientación del cerebro; se limitarían a predisponer hacia ciertos rasgos de personalidad, que influyen en las relaciones y experiencias, las cuales, en último término, conformarían la sexualidad.

Durante los últimos decenios, la especulación en torno a la biología y la orientación solía centrarse en el papel de las hormonas.
Llegó a afirmarse que los niveles de andrógenos y estrógenos determinaban la orientación sexual del adulto, hipótesis que se desvaneció por falta de pruebas.
Y pasó a suponerse que las hormonas determinaban la orientación sexual del cerebro durante el período prenatal.

Según esta postrera hipótesis, unos altos niveles prenatales de andrógenos durante una época decisiva producirían heterosexualidad en los hombres y homosexualidad en las mujeres.
A la inversa, bajos niveles fetales de andrógenos provocarían homosexualidad en los varones y heterosexualidad en las mujeres.
Esta hipótesis descansa, sobre todo, en estudios realizados en roedores.
Se ha observado que la exposición precoz a hormonas determina la opción entre los patrones masculinos y los femeninos de la conducta de apareamiento exhibida por los animales adultos.
Las hembras de roedores que fueron expuestas a andrógenos al principio de su desarrollo muestran una conducta de apareamiento masculinizante con mayor frecuencia que las hembras adultas normales.
Los machos privados, por castración, de andrógenos durante el mismo período crítico adoptan una postura femenina de apareamiento denominada lordosis (arqueo del lomo) cuando son cubiertos.

Muchos especialistas consideran homosexuales a estas últimas ratas macho (igual que a la rata hembra que monta a otras); la lordosis, sin embargo, es poco más que un reflejo:
la rata macho adoptará la misma postura al rozarle el lomo.

Consideran heterosexual al macho que monta a otro macho y a la hembra que exhibe lordosis en la cubrición por otra hembra.
La aplicación de este razonamiento a los seres humanos implicaría que, de dos individuos del mismo sexo que mantienen relaciones sexuales, sólo uno es homosexual; cuál de ellos lo sea dependerá del rol o postura que adopte.

Además de establecer las pautas de apareamiento, la exposición precoz a andrógenos determina si el cerebro de un animal va a poder regular la función ovárica normal.
El cerebro de una rata macho no puede responder a los estrógenos desencadenando una retroalimentación positiva, cadena de acontecimientos que culmina en el aumento brusco de la hormona luteinizante en el torrente sanguíneo, la cual a su vez inicia la ovulación.
A partir de ese fenómeno, ciertos investigadores dedujeron que los varones homosexuales (cuyos cerebros, alegan, están insuficientemente masculinizados) podrían tener una reacción de retroalimentación positiva más fuerte que los heterosexuales.

Aunque dos laboratorios corroboraron tal inferencia, la refutaron otros trabajos cuidadosamente diseñados y ejecutados, en particular el realizado por Luis J., G., Gooren, de la Universidad Libre de Amsterdam.
Agréguese, además, que los mecanismos de retroalimentación son irrelevantes para la orientación sexual humana:
se ha demostrado que el mecanismo de retroalimentación positiva no constituye ningún rasgo sexualmente dimórfico en los primates, humanos incluidos.
Por tanto, si se trata de un fenómeno que no discrimina entre varones y mujeres, carece de sentido sugerir que está «feminizado» en los varones homosexuales.

De la hipótesis de feminización de las respuestas a la hormona luteinizante en los varones homosexuales se sigue este corolario:
las respuestas deberían estar «masculinizadas» en las mujeres lesbianas.
De ocurrir tal, las mujeres homosexuales ni menstruarían, ni tendrían hijos. 
La abrumadora proporción de lesbianas con ciclos menstruales normales y el creciente número de madres declaradamente homosexuales ponen al descubierto la inconsistencia de esa idea. 

Si la hipótesis hormonal prenatal fuera correcta, cabría esperar que fuera homosexual el número enorme de varones con afecciones asociadas a una deficiencia androgénica prenatal, y que lo fueran también las mujeres expuestas a un exceso de andrógenos antes del nacimiento.
Ni una cosa ni otra son verdad.

Puesto que los andrógenos son necesarios para el desarrollo de los genitales externos normales en los varones, es posible que el sexo de los individuos afectados no se manifieste con claridad al nacer: varones con genitales de aspecto femenino y mujeres con genitales de apariencia masculina.
En estos casos se recurre a la cirugía plástica para la construcción de genitales acordes al sexo real; la decisión de educarlos como niños o como niñas no se basa a veces en el sexo genético, sino en las posibilidades de reconstrucción genital.

Cuando la investigación sobre la inclinación sexual se centra en esos individuos, tiende a respaldar el modelo de aprendizaje social.
Con independencia de su sexo genético o de la naturaleza de la exposición hormonal prenatal, los sujetos se convierten en heterosexuales con respecto al sexo en que les han educado sus padres, siempre que su asignación se haya realizado sin ambigüedad antes de los tres años de edad.

No obstante, algunos trabajos recogen un aumento de las fantasías o conductas homosexuales de mujeres que estuvieron expuestas a andrógenos durante el período fetal. 
De acuerdo con la hipótesis de los efectos biológicos directos, esas observaciones constituirían una prueba de que la exposición prenatal a andrógenos trenza las conexiones cerebrales que determinan la atracción sexual hacia las mujeres.

Ruth H., Bleier, neurobióloga y estudiosa feminista, ha propuesto una interpretación alternativa. 
No se trata de que esas tendencias homosexuales reflejen el efecto de las hormonas masculinizantes sobre la diferenciación sexual del cerebro; por contra, las adaptaciones de las mujeres prenatalmente masculinizadas reflejan a buen seguro el impacto de haber nacido con genitales masculinizados o reflejan el conocimiento de haber estado expuestas a niveles aberrantes de hormonas sexuales durante el desarrollo.
«El género al que se pertenece sería una construcción frágil y arbitraria», concluye Bleier, «si dependiera de la cirugía plástica».

Stephen Jay Gouid, de la Universidad de Harvard, ha escrito sobre el descrédito en que cayó, ante los anatomistas del siglo pasado, la búsqueda en el cerebro de diferencias relacionadas con el sexo y otras categorías sociales; andaban ellos mismos confundidos al pensar que las medidas cerebrales justificaban los prejuicios sociales de su época.
La búsqueda cerebral de diferencias relacionadas con el sexo tornó a tomar auge en las postrimerías de los años setenta, cuando el equipo de Roger A., Gorski, de la Universidad de California en Los Angeles, descubrió que cierto grupo de células situadas en la región preóptica del hipotálamo de rata era mucho mayor en los machos que en las hembras.
A este grupo celular lo denominaron núcleo con dimorfismo sexual ( NDS ) del área preóptica ( APO ).
Desde hacía tiempo se venía implicando el área preóptica en la regulación de la conducta sexual.

Como ocurría con las diferencias, asociadas al sexo, observadas en la conducta de apareamiento y en los mecanismos reguladores de la hormona luteinizante, se encontró que la disparidad de tamaño del NDS-APO obedecía a una distinta exposición a andrógenos en las primeras etapas del desarrollo.
Poco después, Bleier y el autor, trabajando en la Universidad de Wisconsin en Madison, examinaron el hipotálamo de varias especies de roedores; vimos que el NDS-APO es sólo parte de un dimorfismo sexual que abarca a otros varios núcleos hipotalámicos.

Tres laboratorios han buscado núcleos con dimorfismo sexual en el hipotálamo humano.
Laura S., Allen identificó, en el laboratorio de Gorski, cuatro posibles candidatos, homólogos potenciales del NDS- APO de rata; los denominó núcleos intersticiales del hipotálamo anterior ( NIHA1-NIHA4 ).
Otros equipos que han medido también estos núcleos, han obtenido, sin embargo, resultados contradictorios:
el grupo de Dick Swaab, del Instituto holandés de investigación del cerebro en Amsterdam, encontró que el NIHA1 era mayor en los hombres que en las mujeres, en tanto que Allen no halló diferencia alguna en ese núcleo, pero observó que el NIHA2 y el NIHA3 eran mayores en los varones.

Más recientemente, LeVay no descubrió diferencias, acordes con el sexo, ni en el NIHA1 ni en el NIHA2 , pero corroboró los hallazgos de Allen acerca de un tamaño mayor del NIHA3 en los varones.
LeVay publicó también que, en homosexuales masculinos, el NIHA3 tiende a ser menor, semejante al de las mujeres. 
(Con Clifford Saper, de Harvard, estoy midiendo núcleos intersticiales, pero no hemos conseguido todavía resultados definitivos).

Se ha visto en el estudio de LeVay una sólida prueba de la intervención directa de los factores biológicos sobre las conexiones cerebrales que determinan la orientación sexual.
No me parece a mi tan segura la conclusión. 

Por varias razones.
En primer lugar, su trabajo ni se ha repetido ni cabe esperar que pueda fácilmente reproducirse, habida cuenta de la naturaleza de estos estudios neuroanatómicos que no suelen disponer de un registro de pautas.
Y, de hecho, líneas de trabajo similares a las seguidas por LeVay en su identificación de los núcleos terminaron en callejones sin salida.

Manfred Gahr, ahora en el Instituto Max Planck de Fisiología Animal en Seewiesen, utilizó una técnica de tinción celular similar a la empleada por LeVay para observar lo que parecían variaciones estacionales en el tamaño de un núcleo involucrado en el canto de los canarios.
Dos métodos de tinción más específicos revelaron, sin embargo, que el tamaño del núcleo no cambiaba.
Gahr sugirió que el método menos específico podría haberse visto condicionado por variaciones hormonales estacionales que alteraron las propiedades de las células del núcleo.

Además, en el estudio de LeVay, todos los cerebros de varones homosexuales pertenecían a sidosos.
La inclusión de unos cuantos sidosos heterosexuales no basta para comprobar, de manera fehaciente, si la caída de los niveles de testosterona que experimentan casi todos los sidosos en los momentos finales de su vida resulta de la propia enfermedad o de los efectos colaterales de determinados tratamientos. 
LeVay sólo ha examinado el cerebro de un homosexual masculino que no murió de sida.
Cabe, pues, que los efectos sobre el tamaño del NIHA3 que él atribuía a la orientación sexual se deban a trastornos hormonales asociados con el sida. 
El trabajo de Deborah Commins y Pauline I., Yahr, de la Universidad de California en Irvine, respalda esta hipótesis: 
hallaron que el tamaño de una estructura equiparable al NIHA3 en jerbillos mongoles variaba con la cantidad de testosterona presente en el torrente sanguíneo.

Por último, el estudio de LeVay se basa en un análisis impreciso de la investigación animal pertinente.
Ha sugerido que el NIHA3 , como el NDS-APO en la rata, está situado en una región hipotalámica que se sabe interviene en la pulsión sexual masculina.
Ahora bien, las investigaciones realizadas en diversas especies coinciden en indicar que la región hipotalámica involucrada en la conducta sexual masculina no es la misma que ocupan tales núcleos.
Gorsky y Gary W., Arendash, este último hoy en la Universidad del Sur de Florida, hallaron que la destrucción bilateral de los NDS-APO del cerebro de rata macho no afectaba al comportamiento sexual.

Poco antes de mi incorporación en el grupo de Robert W., Goy, del Centro Regional de Investigación Primatológica de Wisconsin, Jefferson C., Slimp acometió varios experimentos cuyos resultados sugerían que, en el mono rhesus , la región implicada en el comportamiento sexual de los machos se hallaba por encima del área equivalente a la ocupada por el NIHA3 en el cerebro humano.
Los machos con lesiones en esa región cubrían a las hembras con menor frecuencia que antes de ser operados, pero no varió su frecuencia de masturbación. 
Aunque algunos han interpretado estos resultados en el sentido de que las lesiones disminuían selectivamente el impulso heterosexual, tal conclusión carece de fundamento; después de la operación, los machos accionaban la palanca que les permitía el acceso a las hembras más a menudo que antes.
Por desgracia, estos machos no tuvieron oportunidad de relacionarse con otros machos, por lo que el estudio no nos aporta nada sobre la motivación o la conducta homosexual en contraposición a la heterosexual, tal conclusión carece de fundamento; después de la operación, los machos accionaban la palanca que les permitía el acceso a las hembras más a menudo que antes.
Por desgracia, estos machos no tuvieron oportunidad de relacionarse con otros machos, por lo que el estudio no nos aporta nada sobre la motivación o la conducta homosexual en contraposición a la heterosexual.

Los núcleos hipotalámicos intersticiales no son las únicas partes del cerebro sometidas a investigación para ahondar en la inclinación sexual.
La neuroanatomía ha sacado a la luz ciertas diferencias, de potencial interés, en regiones que no habían sido involucradas directamente en la conducta sexual.
Swaab y Michel A., Hofman encontraron que otro núcleo hipotalámico, el supraquiasmático, era mayor en los varones homosexuales que en los heterosexuales.
Pero el tamaño de esta estructura no varía en razón del sexo, y aun en el caso de que los resultados de esta investigación se confirmaran, no apoyarían el supuesto de que el cerebro de los homosexuales tiene rasgos intersexuados.

Allen, de UCLA , ha puesto de manifiesto que la comisura anterior, una estructura que participa en la transferencia de información de un hemisferio cerebral a otro, es mayor en las mujeres que en los varones.
Llegó a la conclusión de que, en los homosexuales masculinos, la comisura anterior está feminizada; es decir, es mayor que en los heterosexuales.
Por su parte, Steven Demeter, Robert W., Doty y James L., Ringo, de la Universidad de Rochester, han encontrado exactamente lo contrario: comisuras anteriores mayores en hombres que en mujeres. 
Además, aun suponiendo que los resultados de Allen fueran los correctos, el mero tamaño de la comisura anterior no permitiría sacar conclusión alguna sobre la inclinación sexual de una persona.
Además, si bien ella descubrió una diferencia estadísticamente significativa en el tamaño medio de la comisura anterior entre varones homosexuales y heterosexuales, los tamaños de las comisuras anteriores de 27 de los 30 hombres homosexuales de su estudio pertenecían al mismo intervalo de tamaño que el de los 30 hombres heterosexuales con los que los comparó. 

Algunos investigadores han acudido a la genética en su búsqueda de un vínculo biológico para la orientación sexual.
Los resultados de varios estudios recientes sugieren una mayor probabilidad de comportamiento homosexual en los hermanos de varones homosexuales que en los varones sin hermanos homosexuales.

De estos trabajos, sólo en el de J., Michael Bailey, de la Universidad Noroeste, y Richard C., Pillard, de la Universidad de Boston, se incluyeron hermanos biológicos no gemelos y hermanos adoptados (sin parentesco biológico), además de gemelos idénticos y dicigóticos.

La investigación de tales autores produjo resultados paradójicos.
Unas estadísticas apoyan la hipótesis genética, mientras que otras la refutan. 
Los gemelos monocigóticos presentaban la mayor probabilidad de ser homosexuales los dos; al 52 por ciento, frente al 22 por ciento de los gemelos dicigóticos, les unía la homosexualidad.
Este resultado respaldaría la interpretación genética, ya que los gemelos idénticos comparten toda la dotación hereditaria, mientras que los gemelos dicigóticos comparten sólo la mitad.
Los hermanos no gemelos de homosexuales comparten la misma proporción de genes que los gemelos dicigóticos.
Sin embargo, sólo el 9 por ciento de ellos tendían también a la homosexualidad.
La hipótesis genética predice que la proporción sería la misma.

Bailey y Pillard observaron, además, que la incidencia de homosexualidad en los hermanos adoptados de homosexuales (11 por ciento) era mucho mayor que la calculada recientemente para la población general (del 1 al 5 por ciento).
De hecho, era igual al porcentaje registrado entre hermanos biológicos no gemelos.
Este estudio pone en tela de juicio la hipótesis genética simple y presta un sólido respaldo a la hipótesis que atribuye al entorno un peso significativo en la inclinación sexual de los individuos.

En dos de otras tres investigaciones recientes se detectó, asimismo, un porcentaje de homosexualidad entre hermanos idénticos mayor que entre gemelos dicigóticos de homosexuales.
En cada caso, sin embargo, los gemelos se habían criado juntos.
Sin conocer qué experiencias del desarrollo contribuyen a la orientación sexual y si dichas experiencias son más parecidas entre gemelos idénticos que entre gemelos dicigóticos, se hace difícil separar los efectos de una dotación genética común de los de una educación común.
La resolución de esta cuestión pasa por estudiar gemelos criados por separado.

Con todo, el principal hallazgo de estos estudios de heredabilidad quizá resida en el hecho siguiente:
pese a compartir los mismos genes y haber vivido un ambiente perinatal lo más parecido posible, aproximadamente la mitad de los gemelos idénticos mostraban una orientación sexual distinta.
El dato viene a subrayar cuán poco sabemos acerca de los orígenes de la inclinación sexual.

Debemos al equipo de Dean H., Hamer, del Instituto Nacional de la Salud, la prueba más directa de la base genética de la orientación sexual.
El grupo concentró su atención en la región Xq28 del cromosoma X ; ese segmento contiene cientos de genes.
Las mujeres poseen dos cromosomas X y, por tanto, dos regiones Xq28 , pero sólo transfieren la copia de una de ellas a sus hijos (que portan un solo cromosoma X ).
La probabilidad teórica de que dos hijos reciban una copia de la misma región Xq28 es del 50 por ciento.
Hamer encontró que, de sus 40 pares de hermanos homosexuales masculinos, 33, y no 20 como se esperaba, habían recibido la misma región Xq28 de sus madres.

Suele caerse en una interpretación errónea del hallazgo de Hamer y afirmar que demuestra que los 66 varones que integraban los 33 pares compartían la misma secuencia Xq28 .
La verdad es que el trabajo demuestra algo muy distinto; a saber:
que cada miembro de los 33 pares concordantes comparte su región Xq28 sólo con su hermano, no con cualquiera de los otros 32 pares.
No se identificó ninguna secuencia exclusiva y específica (un supuesto «gen homosexual») en los 66 varones.

Por desgracia, el equipo de Hamer no examinó la región Xq28 de los hermanos heterosexuales de los individuos homosexuales para ver cuántos compartían la misma secuencia.
Hamer sugiere que la inclusión de hermanos heterosexuales habría confundido su análisis porque el gen asociado con la homosexualidad podría tener una «penetración incompleta»; esto es, que los varones heterosexuales fueran portadores del gen sin expresarlo.
En otras palabras, la inclusión de hermanos heterosexuales hubiera revelado tal vez que la responsabilidad de la orientación sexual recae en algo más que en los genes.

Por último, Neil J., Risch, de la Universidad de Yale, uno de los que han desarrollado las técnicas estadísticas empleadas por Hamer, ha cuestionado la significación estadística de los resultados de éste.
Según Risch, hasta no disponer de más detalles sobre el agrupamiento («clustering») familiar de la homosexualidad, las implicaciones de estudios como el de Hamer siguen siendo poco claras.

Los trabajos que conceden categoría de rasgo hereditario a la homosexualidad (admitido que puedan repetirse) no indican nada sobre el modo en que operaría tal heredabilidad. 
Los genes, de suyo, especifican proteínas, no fenómenos conductuales ni psicológicos.
Aunque apenas sabemos nada sobre cómo materializa el cerebro los complejos fenómenos psicológicos, entra dentro de lo razonable que haya secuencias de ADN capaces de establecer conexiones cerebrales específicas de una orientación homosexual.
Pero no deja de resultar significativo que la transmisión de caracteres hereditarios no necesite de tales mecanismos.

En cambio, determinados genes podrían influir en rasgos de la personalidad que, a su vez, condicionaran las relaciones y las experiencias subjetivas que contribuyen al aprendizaje social de la orientación sexual.
Podemos imaginarnos muchas formas mediante las cuales una diferencia de temperamento originaría orientaciones diferentes en ambientes distintos. 

A modo de analogía veamos qué acontece con Achillea .
En esta planta, las variaciones genéticas producen fenotipos dispares en función de la altitud.
La altitud a la que crece un esqueje no tiene un efecto lineal sobre su crecimiento, ni este efecto se limita a un solo atributo.
También se ven afectados la talla, el número de hojas y tallos, amén del patrón de ramificación.
Si una planta despliega una respuesta tan variopinta ante los cambios del medio,
¿qué no podrá hacer un organismo mucho más complejo, capaz de modificar a voluntad su entorno?

Hemos de limitarnos a un esbozo muy sumario de la posible interacción entre genes y ambiente para la conformación de la orientación sexual.
Muchos investigadores sostienen que la aversión de los niños a los juegos violentos advierte ya, hasta cierto punto, de un desarrollo homosexual. 
(Para los partidarios del modelo directo esta aversión constituye una palmaria expresión infantil de un cerebro determinado hacia la homosexualidad.)
Por su parte, los psicoanalistas han observado que, entre los varones homosexuales que asisten a sesiones de terapia, muchos recuerdan haber mantenido escaso trato con su padre.

Sugieren que una relación paterno - filial deteriorada conduce a la homosexualidad.

Se podrían combinar ambas observaciones e inferir que una aversión genéticamente determinada hacia los juegos violentos en los niños arruina el entendimiento con el padre, que exige de ellos una adhesión a los rígidos estereotipos del papel de cada sexo.
El padre que no se atiene a estos cánones mantendría una buena relación con sus hijos.
Por consiguiente, el hipotético gen afectaría a la orientación sexual en algunos casos, pero no en otros.
Incluso un ejemplo tan reduccionista (basado en rasgos que reflejan estereotipos culturales más que biológicos) muestra cómo ni temperamento ni ambiente familiar serían decisivos.
Los estudios centrados en uno u otro darían resultados cuestionables. 

Estas inferencias especulativas remachan la necesidad de avanzar mucho más para poder desentrañar los factores, biológicos o fruto de la experiencia vivida, que intervienen en la orientación sexual.
Aun cuando resultara cierto que el tamaño de ciertas estructuras cerebrales guarda relación con la inclinación sexual de cada sujeto, nuestro conocimiento del cerebro es todavía insuficiente para explicar cómo tales diferencias cuantitativas generarían diferencias cualitativas en un fenómeno psicológico tan complejo como el de la orientación sexual.
De igual manera, la confirmación de los hallazgos genéticos que pretenden demostrar que la homosexualidad es hereditaria no aclara qué es lo que se hereda ni cómo influye en la orientación sexual.
En el futuro previsible, por tanto, la interpretación de estos resultados seguir girando en torno a suposiciones de validez cuestionable.

Mientras continúan los esfuerzos por repetir tales hallazgos provisionales, hemos de resistir la tentación de tomarlos por algo más que meras aproximaciones.
Y quizá lo más importante, debemos preguntarnos por qué nos sentimos todos tan visceralmente involucrados en esta investigación.
¿Influirá en la postura adoptada cómo nos percibimos a nosotros o a los demás, cómo vivimos nuestras propias vidas y dejamos que los demás vivan las suyas?
Tal vez las respuestas a las preguntas más acuciantes de este debate no residan tanto en la biología del cerebro cuanto en las culturas que los cerebros han creado.

Figura 1

NÚCLEOS CON DIMORFISMO SEXUAL del área preóptica ( NDS-APO ) del cerebro de rata: se trata de regiones cuyo tamaño varía de los machos a las hembras

Los intentos de localizar un grupo celular análogo en el cerebro humano arrojan diferentes resultados.
La existencia de algunos de estos núcleos no se ha confirmado en otros roedores.
Las regiones que son mayores en los machos están sombreadas en marrón y las mayores en las hembras lo están en azul.

Figura 2

NÚCLEOS HIPOTALÁMICOS a los que se ha atribuido dimorfismo sexual en los seres humanos

Ello no obstante, cualquier afirmación sobre su incidencia en la orientación sexual de los individuos es apresurada, toda vez que no se ha demostrado de forma concluyente que existan diferencias en estas regiones entre hombres y mujeres. 

Figura 3

ESQUEJES de Achillea 

Tienen la misma dotación genética, pero se desarrollan de forma distinta según el entorno. 
Además, saber cómo se diferencian las cinco variedades genéticas en un medio no ayuda a predecir sus características en otro.
El que las plantas muestren una respuesta tan compleja al medio pone de manifiesto biológico de esperar vínculos directos y predecibles entre los genes humanos y un rasgo tan vago como la orientación sexual. 

Figura 4

VARÓN-VARÓN , un ser de las razas primigenias según las imaginó Platón en el Banquete , de acuerdo con un dibujo extraído de un comentario decimonónico a esa obra clásica

Exposición a hormonas y conducta de apareamiento en ratas

La exposición de las ratas a hormonas durante la gestación afecta a su conducta de apareamiento.
Los machos expuestos a insuficiente cantidad de andrógenos adoptan posturas típicamente femeninas, mientras que las hembras que reciben un exceso de dichas sustancias se empeñan en comportamientos masculinos.
La extrapolación de estos datos a la orientación sexual de los humanos resulta, cuando menos, difícil.


PRESENTE Y FUTURO DE LOS ANTICUERPOS MONOCLONALES

Ideados para curar el cáncer y otras enfermedades, no lograron alcanzar el fin previsto.
Las dificultades encontradas podrían subsanarse ahora con prototipos de nuevo cuño.

El optimismo ilimitado que despertaron los anticuerpos monoclonales en los años ochenta resultó contagioso. 
A propósito del cáncer, creíase que, a la manera de misiles teledirigidos, los anticuerpos monoclonales portadores de tóxicos o isótopos radiactivos podrían orientarse contra las células tumorales y depositar su carga letal, eliminando las células cancerosas y dejando intactas las normales.
Si de enfermedades infecciosas se trataba, los anticuerpos monoclonales sitiarían a los virus y bacterias introducidos en el organismo, confinándolos en recintos donde las células del sistema inmunitario acabarían con ellos.

Pero la realidad ha mostrado que las cosas son más difíciles.
Los anticuerpos monoclonales son proteínas del sistema inmunitario muy puras que atacan dianas moleculares específicas.
Para su infortunio, los individuos que recibieron la primera hornada de anticuerpos monoclonales con fines terapéuticos tendían a desarrollar sus propios anticuerpos contra los anticuerpos inyectados.
Eso hizo que, por causas que no acabamos de conocer, progresaran aún más en su patología.
El hígado secuestraba tales anticuerpos monoclonales y los eliminaba antes de que pudieran alcanzar sus objetivos.
Fracasaron los ensayos clínicos.
Las inversiones se hundieron, con pérdidas millonarias.
Cundió el escepticismo entre científicos y laboratorios farmacéuticos. 

Pero no todos abandonaron.
Gracias a su esfuerzo se descubrieron vías para corregir las deficiencias de los primeros fármacos.
Al mercado ha llegado ya una decena de anticuerpos monoclonales.
Tres más esperan una pronta aprobación por la norteamericana Oficina de Alimentación y Farmacia (FDA).
Dos estarán equipados para llevar una dosis de radiactividad.
Hay un centenar largo en fase de ensayo en humanos, superadas con éxito las pruebas en animales.
La cifra, en opinión del analista Franklin M. Berger, no dejará de crecer. 

La FDA ha puesto sordina a la nueva euforia, sin embargo. 
En julio el ente administrativo comunicaba a la empresa Genentech que deberá presentar una documentación más exhaustiva de los estudios clínicos con humanos que demuestren la seguridad a largo plazo de sus anticuerpos monoclonales contra el asma (Xolair).
Estos eliminan los anticuerpos que desempeñan una función clave en el asma y las alergias.
Algunos observadores ven en ese requerimiento una muestra del rigor que la FDA piensa aplicar en el examen de los efectos secundarios de los anticuerpos monoclonales, en particular cuando se trate de los indicados para enfermos crónicos.

Las ventajas de los anticuerpos monoclonales son evidentes. Donald L. Drakeman, presidente de Medarex, reconoce que resulta más fácil la producción de anticuerpos que la de medicamentos tradicionales, elaborados a partir de pequeñas moléculas inorgánicas.
Por ser macromoléculas los anticuerpos, quizá no puedan indicarse en todas las enfermedades.
Ahora bien, el desarrollo de un anticuerpo monoclonal hasta la fase de ensayo clínico sólo requiere uno o dos años, en comparación con los cinco que se necesitan en el caso de una molécula pequeña.
La rapidez significa ahorro.
Drakeman estima en un par de millones de dólares la obtención de un anticuerpo monoclonal a punto para las pruebas clínicas, mientras que se necesitan 20 millones de dólares para un medicamento tradicional. A pesar de las reticencias de la FDA ante el fármaco de Genentech, los anticuerpos monoclonales son más eficaces para eliminar moléculas mediadoras implicadas en las enfermedades que cualquier otro medicamento basado en pequeñas moléculas.
Los anticuerpos casi nunca son tóxicos.

Por ironía de las cosas, los anticuerpos monoclonales podrían ser víctimas de su propio éxito. 
Sostienen los analistas del mercado que las industrias carecen de instalaciones suficientes para fabricarlos todos. 
Para adelantarse a ese problema, los laboratorios han comenzado a dirigir sus investigaciones hacia la producción de anticuerpos en la leche de animales de granja (corderos, vacas, etc.) o en plantas.

Métodos para producir monoclonales 

El fracaso de los anticuerpos monoclonales en el pasado se debió, en parte, al proceso de su elaboración inicial.
La técnica de manufactura clásica fue ideada por Georges J. F. Köhler y César Milstein, inmunólogos del Laboratorio de Biología Molecular del Consejo de Investigaciones Médicas en Cambridge, por cuya innovación se les otorgó el premio Nobel de fisiología y medicina en 1984.
A grandes rasgos, el procedimiento consistía en la inyección de un antígeno (sustancia que el sistema inmunitario reconoce como extraña o peligrosa) en un ratón.
Se inducía así la proliferación, en el animal, de linfocitos B (células productoras de anticuerpos) que sintetizaban anticuerpos específicos contra el antígeno inyectado.
Para obtener los anticuerpos específicos, los inmunólogos debían ceñirse a los linfocitos B que los fabricaban.
Pero resulta ser una tarea harto difícil el determinar de qué linfocitos B se trata, para separarlos de los que no producen tales anticuerpos.

En ese procedimiento complejo, los linfocitos B del ratón han de fusionarse con células de cultivo inmortalizadas (que se replican ilimitadamente) para crear hibridomas.
Estas nuevas células resultantes de la fusión producen anticuerpos murinos, que el sistema inmunitario humano puede percibir como intrusos.
Por eso, los pacientes que han recibido inyecciones de anticuerpos monoclonales murinos han experimentado una respuesta AHAM, denominada así por los anticuerpos humanos antimurinos que se generan.
Las respuestas AHAM provocan una hinchazón de las articulaciones, eritemas e insuficiencia renal, que pueden poner en peligro la vida.
Tales respuestas destruyen también los anticuerpos murinos.

Con el fin de evitar la respuesta AHAM y la inactivación prematura de los anticuerpos de ratón por el sistema inmunitario, se han desarrollado distintas técnicas.
Se busca con ellas hacer más humanos los anticuerpos.
Estas macromoléculas, con forma de Y, se unen a los antígenos a través de los brazos, o regiones Fab, de dicha estructura. 
El vástago de la Y, la región Fc, interacciona con las células del sistema inmunitario.
La región Fc reviste particular interés en la eliminación de las bacterias: en cuanto los anticuerpos recubren una bacteria mediante la unión con las regiones Fab, las regiones Fc atraen a las células encargadas de engullir los microorganismos para su aniquilación.

Para que los anticuerpos murinos se parezcan más a los humanos podemos reemplazar todo el anticuerpo monoclonal murino, salvo la parte de unión al antígeno, con los componentes de anticuerpos humanos.
Cuatro de los anticuerpos comercializados en Estados Unidos están hechos de esta forma, parte de ratón y parte humana.
Recordaremos ReoPro, elaborado por la empresa Centocor, que previene la formación de coágulos mediante su unión a un receptor específico de las plaquetas.

Podemos seguir otro método, que consiste en la humanización de los anticuerpos.
En esa técnica se fundan cinco fármacos que se expenden ya en farmacias.
Uno de ellos es Herceptin, de Genentech, un anticuerpo monoclonal contra el cáncer de mama.
La humanización se sirve de la ingeniería genética para sustituir, de forma selectiva y en todo lo posible, el anticuerpo murino con proteína humana, incluida la mayor parte de las regiones de unión al antígeno.
Campath, fabricado por Millennium Pharmaceuticals, se receta a los pacientes con leucemia linfoide crónica cuando han fracasado otras medidas terapéuticas. 
Se une a un receptor que está presente en distintos tipos celulares normales y cancerosos del sistema inmunitario; tras el tratamiento, los pacientes producen un mayor número de células normales.
Los otros anticuerpos monoclonales del mercado son anticuerpos totalmente murinos.

Tras más de 25 años de esfuerzo, se ha logrado por fin fusionar linfocitos B humanos con células inmortalizadas; 
los hibridomas resultantes generan anticuerpos plenamente humanos.
En febrero, Abraham Karpas y su grupo anunciaron la proeza, aunque todavía es demasiado pronto para saber si los anticuerpos monoclonales producidos utilizando células humanas serán más seguros, más eficaces o más baratos de fabricar que los obtenidos a través de otras técnicas.

Medarex y Fremont han encontrado el método para inducir en el ratón la producción de anticuerpos plenamente humanos.
Mediante ingeniería genética consiguen que el ratón aloje los genes de los anticuerpos humanos.
Cuando se inyectan antígenos en los ratones, los animales fabrican anticuerpos plenamente humanos.
Los laboratorios Abgenix han llevado a la fase de ensayo clínico un anticuerpo plenamente humano contra la interleucina-8 ( IL-8 ); en cuanto citocina, esta sustancia química natural activa las células del sistema inmunitario.
Cuando el organismo produce IL-8 en demasía, pueden aparecer enfermedades inflamatorias autoinmunes, como la artritis reumatoide o la psoriasis.
En fase de ensayo clínico Medarex tiene anticuerpos monoclonales plenamente humanos de potencial aplicación en pacientes con cáncer y enfermedades autoinmunes.
Trabaja, asimismo, en anticuerpos de diseño, preparados para liberar directamente una toxina en una célula enferma o para reclutar células inmunitarias que ataquen los tumores.

Otros laboratorios farmacéuticos se afanan en la obtención de anticuerpos monoclonales sin pasar por los ratones.
La compañía inglesa Cambridge Antibody Technology y la alemana MorphoSys AG aplican la técnica de selección de fagos para lograrlo.
Además de facilitar la producción en masa, el método ayuda a encontrar anticuerpos monoclonales específicos contra un antígeno determinado.

La selección de fagos está basada en un virus fibroso que infecta las bacterias.
Tras aislar el ADN a partir de los linfocitos B humanos (cada célula produce un solo tipo de anticuerpo contra un antígeno determinado), se inserta dicho ácido nucleico en Escherichia coli , para infectar la bacteria.
Puesto que estos fagos filamentosos producen nuevas copias de ellos mismos, fabricarán automáticamente las proteínas codificadas por los genes de los anticuerpos de varios linfocitos B que se expresarán en la superficie de los nuevos fagos sintetizados.
Llegado ese momento, puede utilizarse el antígeno contra el que se desea obtener anticuerpos (pensemos en un receptor de una célula cancerosa), y así extraer los fagos portadores del gen que cifra el anticuerpo con mayor especificidad para ese antígeno.
Para producir grandes cantidades del anticuerpo, se induce la infección de muchas bacterias o se inserta el gen del anticuerpo en las bacterias en cultivo.

Dar en la diana Tomados en su conjunto, los nuevos tipos de anticuerpos monoclonales -quiméricos, humanizados y humanos- ofrecen óptimas perspectivas ante una serie de enfermedades.
Dos de estos medicamentos, una vez aprobados por la FDA, convertirán en realidad los soñados anticuerpos monoclonales conjugados.
Se da tal apelativo a los portadores de sustancias radiactivas o productos tóxicos hasta el propio tumor.
Constituirán una nueva terapia del cáncer.
Zevalin (desarrollado por IDEC Pharmaceuticals y Schering AG) y Bexxar (fabricado por Corixa y Glaxo-SmithKline) van dirigidos contra CD20, un antígeno que se encuentra en la superficie de los linfocitos B, células éstas que crecen de una forma descontrolada en el linfoma no Hodgkin. 
Zevalin transporta un isótopo de itrio (Y90) y Bexxar uno de yodo ( xxx ).

En fase de ensayo clínico se hallan otros anticuerpos monoclonales contra moléculas de las células del sistema inmunitario que desempeñan una importante función en muchas enfermedades.
Genentech tiene muy avanzada la prueba de Xanelim, un anticuerpo monoclonal contra CD11a.
Esta proteína reside en la superficie de los linfocitos T, a los que ayuda a infiltrarse en la piel provocando la inflamación de la psoriasis, que afecta a unos siete millones de personas en los Estados Unidos.
De acuerdo con un estudio reciente de unos 600 pacientes de psoriasis, el 57 por ciento de los enfermos a los que se había administrado la dosis más alta del medicamento experimentaron al menos un 50 por ciento de disminución de la gravedad de la enfermedad.
Hay varios laboratorios farmacéuticos que preparan anticuerpos monoclonales contra el CD18, proteína de los linfocitos T que interviene en la inflamación y en la lesión hística subsecuente a cardiopatías.

El receptor del factor de crecimiento de la epidermis (EGF) constituye una diana muy sugestiva para los que investigan anticuerpos monoclonales.
Un tercio de los pacientes con tumores sólidos produce receptores EGF en demasía;
Gleevec, elaborado por Novartis y basado en una molécula pequeña, se interpone en la capacidad de las células tumorales para recibir las señales de proliferación emitidas por estos receptores.
Quizá la forma más eficaz de erradicar el tumor sería administrar los anticuerpos monoclonales anti-receptor EGF en combinación con quimioterapia tradicional.
Se ha demostrado que el cetuximab, un anticuerpo anti-receptor EGF producido por ImClone Systems, recupera la eficacia perdida por la quimioterapia en la cuarta parte de los pacientes con cáncer colorectal avanzado.

Otros laboratorios centran su interés en anticuerpos monoclonales contra moléculas de la superficie de las células que tapizan los vasos sanguíneos.
Algunas de estas moléculas, como las xxx , intervienen en la angiogénesis, o formación de nuevos vasos sanguíneos, que es, además, un proceso clave en el crecimiento tumoral.

Remicade, un medicamento basado en un anticuerpo monoclonal, ha demostrado su éxito en el combate contra el factor de necrosis tumoral (TNF).
De esta molécula se vale el organismo para estimular el sistema inmunitario, aunque se halla también implicado en las enfermedades inflamatorias. 
Por la venta de ese fármaco, indicado para la enfermedad de Crohn (una patología inflamatoria de la pared intestinal) y la artritis reumatoide, la empresa Centocor ganó el año pasado 370 millones de dólares. 
Según Carol Werther, reputada financiera, los tratamientos que eliminen el TNF tienen un mercado anual de dos mil millones de dólares.
Los laboratorios Immunex comercializan Embrel, que por su eficacia contra el TNF se indica para pacientes con artritis reumatoide moderada o grave.
No se trata, en sentido estricto, de un anticuerpo monoclonal, pues sólo se utiliza la parte estructural del anticuerpo; esa región se une a otra molécula, el receptor celular normal del TNF.

Cuestiones emergentes

Ante ese rico muestrario de posibilidades, los laboratorios farmacéuticos podrían aumentar sus líneas de producción, anticipándose a un mercado prometedor. 
Pero en todo el mundo hay sólo 10 plantas industriales debidamente equipadas.

En la fabricación de anticuerpos monoclonales a partir de hibridomas se emplean unas cubas enormes llamadas biorreactores. 
Si oímos a V. Bryan Lawlis, de Diosynth ATP, un gigantesco biorreactor de 60.000 litros sólo podría producir cuatro preparados.
Si se calcula que habrá 100 anticuerpos monoclonales en el mercado hacia el año 2010, las industrias necesitarán al menos 25 nuevas plantas para satisfacer la demanda.
Si se considera que eso supone una inversión superior a los 5000 millones de dólares y requiere un tiempo de tres y cinco años para su construcción y aprobación por la FDA, la perspectiva de realización se volatiliza.
Así las cosas, algunas empresas vuelven la mirada hacia la explotación de animales y plantas transgénicos, manipulados para que porten los genes de determinados anticuerpos. 
Los mamíferos transgénicos que secretan anticuerpos monoclonales en su leche pueden generar un gramo de anticuerpo por unos 100 dólares, un tercio del coste de los métodos de producción tradicionales. Centocor y Johnson & Jonson se disponen a obtener Remicade de cabras transgénicas e Infigen busca extraer anticuerpos monoclonales de la leche de las vacas.

Aunque los animales transgénicos constituyen una opción atractiva, los laboratorios deberán seguir un procedimiento tedioso para aislar de otras proteínas de la leche los anticuerpos monoclonales.
Si bien se ahorra el gasto de los 10.000 litros del biorreactor, quedan por solucionar los problemas de la purificación. Sin olvidar que la ingeniería genética y la mejora animal requieren años.

También se ve en las plantas la llave para paliar la escasez de anticuerpos monoclonales.
Los vegetales ofrecen la ventaja de su economía, amén de poder aumentar fácilmente su producción ante la presión de la demanda: pueden rendir toneladas de monoclonales.
Sin embargo, los problemas de purificación persisten y sigue siendo poco claro de qué modo la FDA legislará los fármacos producidos por las plantas transgénicas. 

Los laboratorios Epicyte, partidarios de esa línea, se han asociado con Dow para producir plantas de maíz que rindan anticuerpos monoclonales de aplicación en cremas o ungüentos para mucosas superficiales de labios y genitales y en medicamentos de administración oral para las infecciones gastrointestinales o respiratorias.
Un proyecto avanzado espera la aprobación oficial de los anticuerpos monoclonales sintetizados en el maíz para prevenir la transmisión, de adultos a neonatos, del virus del herpes.
Se trabaja en anticuerpos monoclonales que, unidos a los espermatocitos, sirvan de anticonceptivo, así como en otros que protejan contra el papilovirus causante de las verrugas genitales y el cáncer cervical.


Las nuevas medicinas genéticas 

A partir de cadenas sintéticas de ADN se fabrican nuevas drogas .
Oligonucleótidos antisentido y tríplices se enfrentan a virus y tumores sin dañar los tejidos sanos .

A cualquiera que esté interesado en sanar una enfermedad nada le puede resultar más gratificante que descubrir un «proyectil mágico», una droga capaz de acabar con la patología sin producir efectos no deseados. 
Durante la mayor parte de este siglo, los investigadores que buscaban esos proyectiles mágicos pensaban en agentes que se uniesen a los sitios activos de las proteínas responsables de las enfermedades.
Al rellenar esos sitios activos, que difieren de unas proteínas a otras, los fármacos deberían inhibir específicamente la actividad de las proteínas en cuestión, sin obstaculizar la función de las demás.

Se esta trabajando ya en ese campo.
Recientemente, sin embargo, algunos investigadores han centrado su atención en unas dianas distintas.
Con estrategias basadas en antisentidos y tríplices, se aprestan a diseñar drogas que se unan a sitios escogidos de los ácidos nucleicos (ADN y ARN) que dirigen la síntesis de las proteínas relacionadas con la enfermedad.

La lógica del proyecto es sencilla.
Para que se fabrique una proteína, debe expresarse el gen que cifra su composición.
Esto es, debe transcribirse, o copiarse, de una molécula de ADN bicatenaria en moléculas individuales de ARN mensajero unicatenarias.
Luego, esas moléculas de ARN se traducen en la correspondiente proteína. 
Los investigadores esperan que las nuevas drogas se unan a segmentos elegidos del ADN (si se utiliza la estrategia de los tríplices) o del ARN mensajero (si es la de los antisentido) e impidan la transcripción o la traducción de los genes seleccionados, sean genes microbianos o formas aberrantes de variedades humanas.
Este tipo de compuestos, por tanto, no se limitan a desarmar las proteínas deletéreas, sino que impiden que lleguen siquiera a sintetizarse.

De los agentes capaces de unirse a los ácidos nucleicos, los que suscitan una atención mayor son los oligonucleótidos de ADN, conocidos también por oligómeros:
breves trechos de nucleótidos, que son las unidades que constituyen el ADN.
En parte, la elección de oligómeros se debe a que los ácidos nucleicos se unen entre sí de acuerdo con unas reglas perfectamente acotadas.
En consecuencia, pueden diseñarse compuestos que reconozcan un sitio único en un gen elegido o en su transcrito de ARN mensajero.
Potencialmente, por tanto, tienen la especificidad que se busca en un proyectil mágico.

Para que las terapias basadas en los antisentido o en los tríplices lleguen a tener un uso rutinario hay que salvar aún algunos obstáculos.

Con todo, tenemos buenas razones para el entusiasmo.
De hecho, la eficacia de algunas drogas oligonucleotídicas ha justificado ya su utilización en ensayos clínicos. 
Se están experimentando en pacientes con leucemia, sida y otras enfermedades necesitadas de tratamientos más eficaces.

Como las drogas oligonucleotídicas son, en esencia, retazos de material genético, suelen a menudo tener la consideración de terapias genéticas.

Debemos aclarar, sin embargo, que ese tipo de tratamientos difiere sustancialmente de las terapias genéticas tradicionales. 
En la mayoría de los tratamientos genéticos, se suministran genes enteros y sanos para sustituir las versiones que faltan o que son defectuosas e incapaces de dirigir la síntesis de una proteína necesaria.

Los oligonucleótidos no pueden producir proteínas. 
Su virtud radica en la capacidad para bloquear la expresión de genes existentes.

El nombre de tríplice, aplicado a una estrategia bloqueadora de la transcripción, proviene de la configuración siguiente:
los oligonucleótidos que se emplean para detener la transcripción se «abrazan» a la doble hélice formando una hélice triple.
En la estrategia «antisentido», cuyo objetivo es impedir la traducción, la lógica del nombre es menos evidente.
Pero se puede comprender fácilmente si se conoce un poco sobre la estructura del ARN y el modo en que una molécula de ARN se combina con otra o con el ADN. 

Las moléculas de ARN son tiras de nucleótidos. 
Los nucleótidos contienen un azúcar ribosa, un grupo fosfato (PO4) y una de las cuatro bases nitrogenadas: adenina (A), guanina (G), citosina (C) y uracilo (U).
(Los nucleótidos de ADN son similares con dos salvedades, a saber, el azúcar es desoxirribosa y tiene la base timina, T, en lugar de uracilo).
Cuando se forma un segmento de nucleótidos de ARN o ADN, los azúcares y fosfatos constituyen la columna vertebral de la cadena resultante; las bases sobresalen de esa columna vertebral.
Como James D., Watson y Francis Crick demostraron para el ADN en 1953, dos ácidos nucleicos pueden combinarse en una estructura de doble cadena gracias a los enlaces de hidrógeno que se establecen entre sus bases.
Adenina, en una cadena, se combina con timina o uracilo en la otra cadena; guanina se combina con citosina.
Por tanto, si una cadena de ARN contiene la secuencia AUCCGUG , se puede emparejar con otro tramo de nucleótidos que tenga la secuencia complementaria, UAGGCAC en el caso de ARN, o TAGGCAC en el caso de ADN.

La secuencia de bases a lo largo de una molécula de ARN mensajero cifra la secuencia de aminoácidos que debe ensamblarse para formar una proteína (en otras palabras, porta la información que necesita una célula). 
Se dice de esa secuencia de ARNm que tiene sentido.
Para producir una molécula capaz de unirse a la cadena dotada de sentido, hay que construir su complementaria, o cadena «antisentido».
Por ello, se denomina estrategia antisentido a la práctica de inhibir con oligonucleótidos la traducción.

Los oligonucleótidos antisentido son la primera de las nuevas medicinas genéticas que han llegado a la etapa de ensayo clínico.
De su potencialidad terapéutica se sospechó ya a principios de los ochenta, cuando se descubrió que ciertos microbios, además de fabricar ARN mensajero, también producían de forma natural ARN antisentido.
La explicación más lógica para ese comportamiento parecía ser que el organismo utilizaba las moléculas antisentido como forma (desconocida hasta entonces) de regular la expresión génica.
Si el ARN antisentido se empareja con su molécula complementaria de ARN mensajero, cabe pensar que bloqueará la maquinaria celular encargada de traducir en proteína la cadena con sentido.

Igual que los microbios, las células vegetales y animales emplean a veces la estrategia antisentido para controlar la expresión génica.
Pero, volviendo atrás en el tiempo, los investigadores trataban de comprobar si su hipótesis era correcta.
Con la ayuda de las técnicas de ADN recombinante, crearon genes que producían versiones antisentido de ciertos ARN mensajeros seleccionados.
Cuando se introducían esos genes en las células se comprobaba que, en efecto, ocasionalmente se conseguía disminuir la producción de la proteína cifrada por la cadena de ARN que era blanco del antisentido.

Los resultados sugerían la posibilidad de diseñar moléculas antisentido que funcionasen a la manera de drogas e impidiesen selectivamente la traducción.

Sin embargo, para utilizarlas como tales drogas, había que enviarlas al interior de las células del cuerpo, y hacerlo de una forma fácil y eficaz.
En la mayoría de los casos, lo mejor sería administrar pequeños fragmentos de antisentido sintéticos, en vez de genes enteros.
Algunos investigadores, no obstante, siguen trabajando con genes antisentido.

A mediados de los ochenta, ya se disponía de herramientas adecuadas para sintetizar oligonucleótidos de ADN.
Pero quedaba por ver si tales oligómeros podrían servir de fármacos.
Algunos años antes, en 1978, Paul C., Zamecnik, de la Fundación Worcester, y Mary L., Stephenson, de Harvard, habían conseguido construir con sumo esfuerzo un oligómero de 13 nucleótidos, diseñado para reconocer una secuencia de bases presente en muchas moléculas de ARN mensajeros fabricados por el virus del sarcoma de Rous.

Ese oligómero reducía la replicación del virus en las células señal de que había bloqueado la producción de proteínas necesarias para elaborar nuevas partículas víricas.
Dos problemas potenciales, sin embargo, preocupaban a los investigadores. 

En primer lugar, los oligonucleótidos sin modificar portan una carga negativa en cada grupo fosfato.
Las moléculas dotadas de carga suelen tener dificultades para atravesar las membranas celulares, que no están cargadas.
Por tanto, la mayoría de los investigadores daban por supuesto que los oligómeros con muchas cargas serían incapaces de entrar eficazmente en las células.
En segundo lugar, las células poseen numerosas enzimas que degradan el ADN foráneo.
Para la mayoría de los casos, cabía pensar en una pronta degradación de los oligómeros que consiguiesen entrar en las células.

Con el fin de resolver estos inconvenientes, Paul O., P., Ts'o y Paul S., Miller, de la Universidad Johns Hopkins, modificaron los oligonucleótidos habituales.
Para ello, reemplazaron un átomo de oxígeno de cada grupo fosfato por un grupo metilo (CH3).
Convertían así los fosfatos cargados negativamente en unidades sin carga: los metilfosfonatos. 
De esa manera los oligonucleótidos entraban mejor en las células y resistían el ataque enzimático. 
Mas, al eliminar las cargas, los oligómeros se volvían hidrofóbicos (repelían el agua) y se hacían insolubles en soluciones acuosas.
La falta de solubilidad puede dificultar la producción en masa.

Kazuo Shinozuka y Gerald Zon, de la norteamericana Oficina de Alimentación y Farmacología, en colaboración con uno de los autores (Cohen), idearon una alternativa a los oligonucleótidos metilfosfonatados.
Intercambiaron un átomo de oxígeno de los grupos fosfatos normales por un átomo de azufre cargado negativamente.
Esos oligómeros fosfotiolados, conocidos por «oligos S.,», son solubles en agua, fáciles de producir automáticamente y resistentes a la degradación enzimática.
Quizá por esas razones, en la mayoría de las investigaciones con drogas antisentido se emplean ahora oligómeros S.

Sabemos que las células toman oligos S., y oligonucleótidos sin modificar con mayor facilidad de lo que en un principio se pensaba.
Según parece, los agentes dotados de carga entran en las células, no sólo por difusión directa a través de la membrana celular, sino también por endocitosis.

En este segundo proceso, la membrana celular engloba a los compuestos formando una pequeña vesícula, que luego se deposita en el citoplasma.
Lo que no está muy claro es si el contenido de tales endosomas se libera fácilmente. 

Además de eliminar algunas de las barreras iniciales que tenía el uso terapéutico de los oligonucleótidos antisentido, las investigaciones realizadas en los últimos 15 años han puesto de manifiesto otros factores importantes que deben tomarse en consideración a la hora de diseñar drogas antisentido.
Por ejemplo, los oligómeros deben constar de un mínimo de 15 nucleótidos para que se unan eficazmente a sus dianas y no lo hagan en sitios inespecíficos.
Se ha averiguado también que los oligonucleótidos antisentido pueden bloquear la traducción al menos de dos maneras.
De acuerdo con lo previsto, obstaculizan la «lectura» normal del ARN con sentido.
Y, además, al unirse al ARN mensajero estimulan a una enzima (ribonucleasa H) que corta, y por tanto destruye, al ARN mensajero que participa en la unión. 

Estos descubrimientos permitían albergar esperanzas en las posibilidades terapéuticas de la estrategia antisentido. 
Quedaba, sin embargo, por saber si el ADN antisentido podía realmente retrasar la producción de proteínas implicadas en enfermedades.
Experimentos realizados con células cultivadas y datos provisionales de ensayos en humanos así lo sugieren. 
(Los ensayos clínicos emplean oligos S.).
Trabajos anteriores, realizados con animales en diversos laboratorios, indicaban también la escasa o nula toxicidad de los agentes antisentido.
Se trata de una buena noticia, pues en las células cultivadas los oligos S., acostumbran unirse a proteínas o a otras dianas no deseadas.
A veces, esas interacciones inespecíficas contribuyen a reforzar el bloqueo directo de la traducción. 

Pero existe siempre el temor de las secuelas inesperadas. 

Los primeros oligómeros antisentido que salgan al mercado serán probablemente los diseñados para combatir infecciones víricas.
La compañía farmacéutica ISIS, de Carlsbad, está realizando un ensayo clínico a gran escala con un oligonucleótido dirigido contra transcritos de un gen vírico importante para la replicación del papilomavirus humano.
Este virus produce verrugas genitales.
Se ha comprobado que las verrugas desaparecen o se achican en muchos pacientes que han recibido inyecciones directamente en las zonas cutáneas afectadas.

Está también en marcha, aunque en una fase más embrionaria, un ensayo clínico que afecta a pacientes infectados con el virus de inmunodeficiencia humana (VIH) , la causa del sida.
Científicos de la francesa Agencia Nacional del Sida, de la empresa Hybridon en Worcester y otros evalúan la eficacia de un oligómero dirigido contra el ARN del gen gag del VIH, que es necesario para la replicación del virus.

Se han iniciado también ensayos clínicos con oligómeros dirigidos contra el cáncer.
Uno de esos ensayos se está realizando con pacientes afectados de leucemia mielógena aguda, un cáncer de la sangre de rápida progresión.
En el experimento, un grupo del hospital clínico de la Universidad de Nebraska colabora con científicos de Lynx Therapeutics, de Hayward.
Se proponen comprobar, a través de un procedimiento designado como purga «ex vivo» de médula ósea, si un oligonucleótido antisentido es capaz de destruir células cancerosas dentro del cuerpo.

En este procedimiento, los cirujanos extraen médula ósea de un paciente y la tratan con un agente que destruye selectivamente las células cancerosas.
A continuación, administran quimioterapia al sujeto y le vuelven a introducir las células «limpiadas».
En este estudio, el oligómero se dirige contra el ARN del gen p53 .
El gen, considerado un supresor de tumores, parece que se sobreexpresa en los pacientes con leucemia mielógena aguda.

Lynx colabora también con Alan M. Gewirtz, de la Universidad de Pennsylvania, en el desarrollo de una droga antisentido contra la leucemia mielógena crónica. 
Su ensayo, que de nuevo valora la capacidad de un fármaco antisentido para eliminar células cancerosas durante una purga de médula ósea ex vivo , está dirigido contra el ARN mensajero del gen c-myb .
En su estado «sano», c-myb promueve la proliferación normal de las células sanguíneas.
Una regulación anormal del gen parece la causa de ciertas leucemias humanas y otras enfermedades.
Gewirtz ha abordado otros ensayos para valorar la eficacia de la introducción directa del agente antisentido en los pacientes leucémicos. 

En fase de proyecto, o incluso de desarrollo, hay asimismo ensayos clínicos contra otros virus y genes relacionados con el cáncer.
Aun cuando los resultados sean positivos, los agentes antisentido se administrarán con otras drogas anticancerígenas o antivíricas.
Dos o más compuestos que operan con mecanismos diferentes deben ser más eficaces que uno solo.

Como cabía esperar de toda nueva técnica, serán muchas las dificultades que habrá que superar antes de que la estrategia antisentido se consolide.

Por una parte, los oligonucleótidos son muy caros (aunque cada vez menos).
La estabilidad de los compuestos debe mejorarse.
Y habrá que elaborar métodos más idóneos para introducirlos en las células en las cantidades adecuadas.
Las investigaciones con los tríplices, que analizaremos a continuación, plantean problemas similares.
También es necesario aumentar la potencia.
Para ello, algunos investigadores están adosando a las moléculas ciertos agentes con capacidad para degradar ARN.
Otros, como Claude Hélène y sus colegas, del Museo de Historia Natural de París, han fabricado oligómeros que portan grupos químicos capaces de insertarse entre las bases del dúplice DN-ARN; con ello se consigue estabilizar la unión y prolongar la actividad antisentido.
El grupo de Cohen y otros utilizan modelos moleculares para diseñar oligonucleótidos con nuevas estructuras espaciales, al objeto de mejorar su potencia y especificidad.

Mientras tanto, existe un planteamiento antisentido muy diferente que progresa de forma espectacular.
No se emplean oligonucleótidos de ADN, sino ribozimas.
Estas moléculas de ARN reconocen secuencias de bases específicas en el ARN mensajero y cortan al ARN por esos sitios, destruyendo los transcritos.
En los próximos meses, Flossie Wong-Staal, de la Universidad de California, espera iniciar un ensayo clínico con una ribozima dirigida contra el transcrito de ARN de un gen del VIH.

Aunque la atención prestada a la otra nueva estrategia genética, basada en los tríplices, sea más reciente, los indicios sobre sus posibilidades se remontan a los años cincuenta.
Cuatro años después de que Watson y Crick descubrieran la estructura en doble hélice del ADN, Gary Felsenfeld y Alexander Rich, a la sazón becarios posdoctorales del Instituto Nacional de la Salud, estudiaban las estructuras pluricatenarias formadas por los ácidos nucleicos.
Sintetizaron polímeros de ARN de nucleótidos con uracilo y los mezclaron con cadenas complementarias, esto es, polímeros sintéticos con nucleótidos de adenina.
De acuerdo con lo esperado, consiguieron dobles hélices de AU .
Mas, para su sorpresa, también obtuvieron tríplices UAU , en los que una cadena adicional de nucleótidos de uracilo se había unido a la doble hélice de AU .

Más tarde, Michael J. Chamberlin y sus colaboradores, de la Universidad de California en Berkeley, demostraron que los polímeros de ADN correspondientes, formados exclusivamente de nucleótidos de timina o de adenina, podían ensamblarse en tríplices TAT .
Richard A., Morgan y Robert D., Wells, a la sazón en la Universidad de Wisconsin, obtuvieron otro tipo de tríplices de ADN.
Demostraron que una cadena de nucleótidos de citosina se unía a una doble hélice GC .
En este caso, la triple hélice sólo se producía cuando las moléculas se hallaban en una solución ligeramente acídica, con abundancia de protones.
La acidez era necesaria porque la triple hélice sólo se formaba si la cadena de citosinas extra adquiría un protón adicional.
La estructura resultante responde al nombre de tríplice C+GC .

Pasaron unos diez años en que los tríplices permanecieron en el reino de las curiosidades.
A nadie se le ocurrió imaginarse que pudieran servir para regular genes nativos en las células.
Además, los genes de los organismos vivos no contienen «homopolímeros» simples, en los que un solo nucleótido se repite de forma monótona.
Para que fueran útiles, los oligonucleótidos instauradores de tríplices deberían reconocer genes cuyas cadenas estuvieran constituidas por más de un tipo de pareja de bases.

A finales de los años sesenta Morgan y Wells consiguieron tríplices que contenían unidades TAT y C+GC .
Para ello, combinaron un oligómero que portaba timinas y citosinas con una doble hélice que presentaba pares de bases AT y GC .
Era el primer indicio serio de que se podrían lograr tríplices en los genes naturales, no sólo en las dobles hélices sintéticas.
Pero no se contaba todavía con las herramientas necesarias para llevar a cabo tales experimentos.

No se habían inventado todavía las técnicas automáticas para clonar genes y sintetizar oligonucleótidos de ADN.

En las postrimerías de los años ochenta, esos problemas pertenecían al pasado.
Por entonces, los investigadores conocían bastante bien la regulación de la transcripción de los genes.
Se descubrió que los genes contienen básicamente dos elementos:
uno es la región reguladora, que controla la tasa de transcripción, y el otro es la región que cifra la secuencia de aminoácidos de la correspondiente proteína. 
Los genes se transcriben cuando los factores de transcripción (que son proteínas) se unen a sitios específicos de la región de control.
Producida la unión, una enzima, la polimerasa de ARN, copia la información de la región que cifra la información, y produce el ARN mensajero.
Los segmentos que cifran la información suelen ser más inaccesibles a los fármacos que algunas partes de las regiones reguladoras.

Los investigadores debían, pues, concentrar sus esfuerzos en el diseño de oligonucleótidos capaces de unirse a la región de control del gen diana, para impedir que los factores de transcripción iniciasen la transcripción. 
Antes de ello, los científicos necesitaban asegurarse de que los oligonucleótidos sintéticos estaban realmente capacitados para reconocer determinadas secuencias de bases de los genes naturales.

Dichas pruebas se obtuvieron en 1987.
Peter B., Dervan, del Instituto de Tecnología de California, y Hélène, independientemente, confirmaron lo que Morgan y Wells habían demostrado con anterioridad en cadenas sintéticas de ADN bicatenario.
En ciertas condiciones, los genes naturales que contenían sitios ricos en pares de bases AT y GC podían combinarse con un oligonucleótido diseñado para formar tripletes TAT y C+GC .
Los tríplices se producían siempre que una de las cadenas del ADN dúplice diana portase la mayoría de las bases púricas (adenina y guanina) y la otra las pirimidínicas (timina y citosina).
Hoy sabemos que los oligonucleótidos suelen unirse a una de las cadenas de la doble hélice, y sólo a las bases púricas de esa cadena.

Los descubrimientos de Dervan y Hélène eran muy sugestivos.
Sin embargo, aún persistía un serio obstáculo contra la terapia fundada en los tríplices. 
Según cabía esperar de los resultados de Morgan y Wells, los tríplices sólo se formaban en condiciones acídicas, que no son las fisiológicas. 

Dervan logró soslayar en parte el problema añadiendo un átomo de yodo o un grupo metilo a los oligonucleótidos habituales.
En 1988, uno de los autores (Hagan) y sus colaboradores de la Universidad de Princeton emplearon con fortuna un método distinto, que además incrementaba la afinidad de los oligonucleótidos por sus dianas y generaba una nueva variedad de tríplices. 

Observaron que, si introducían nucleótidos de guanina en los lugares donde había citosina en los oligonucleótidos diseñados para formar tríplices, se eliminaba la necesidad de acidificar el pH .

Estos nuevos oligonucleótidos generaban tripletes GGC , en vez de la clase C+GC .
Las guaninas de la cadena sintética enlazaban con las guaninas del ADN bicatenario.

El equipo de Hogan y otros demostraron posteriormente que los oligonucleótidos diseñados para formar tripletes TAT y GGC en las regiones reguladoras de los genes solían también unirse a las proteínas que normalmente reconocen esos sitios. 
Aparecía así la posibilidad de construir oligonucleótidos que impidiesen la unión de las proteínas reguladoras a sus genes correspondientes.

Para diseñar fármacos formadores de tríplices, había que conocer con exactitud de qué modo interaccionaba un determinado tipo de oligonucleótido con su diana.
Se sabe ahora que los oligonucleótidos que forman tríplices no bloquean los puentes de hidrógeno que sustentan la doble hélice original.
Cada base del oligonucleótido establece dos nuevos puentes de hidrógeno con una purina de la región diana del dúplice.
Ocurre, además, que algunos oligonucleótidos se orientan en paralelo con sus dianas, y otros lo hacen de forma antiparalela. 

Las cadenas de ADN son asimétricas.
Uno de sus extremos se designa 5' y el otro 3' .
En una doble hélice clásica, las cadenas son antiparalelas: 
el extremo 5 ' de una de las cadenas queda junto al extremo 3' de la otra.
Los oligonucleótidos que contienen citosinas y timinas tienden a situarse en paralelo a la cadena rica en purinas del dúplice diana.
Los que contienen guanina y timina tienden a colocarse de forma antiparalela.
Si no se atiende a esas diferencias, nos encontraremos con que un oligonucleótido se muestra incapaz de reconocer a su ADN diana.

En 1988, y en el curso de unas investigaciones con el gen c-myc , relacionado con el cáncer, Edith H. Postel y Michel Cooney, del laboratorio de Hogan, demostraron que un oligonucleótido diseñado para formar un tríplice en la región controladora de un gen, inhibía la iniciación de la síntesis del ARN mensajero. 
Para ese experimento utilizaron un ensayo de transcripción libre de células:
no emplearon células intactas, sino que mezclaron en un tubo de ensayo el gen, los oligonucleótidos y varios componentes celulares.
Dervan, utilizando también un sistema libre de células, demostró posteriormente que un tríplice impedía la transcripción de ciertos genes víricos.

Resultados igualmente prometedores se han obtenido con otros genes.
Se da ya por sentado que, al menos en el tubo de ensayo, se pueden diseñar oligonucleótidos que formen tríplices con las regiones controladoras de genes seleccionados, y que, por tanto, bloqueen la transcripción del ARN mensajero correspondiente.
Pruebas recientes abonan también el diseño de oligonucleótidos dirigidos contra la región del gen que cifra la información de la proteína.
En esos casos, el tríplice interrumpe el trabajo de la polimerasa de ARN, impidiendo la síntesis de ARN mensajero.
Significa ello que, cuando no es posible impedir la iniciación de la transcripción, se pueden diseñar tríplices para frustrarla en su fase de desarrollo.

Los estudios realizados con células vivas son en favor del recurso a los tríplices para luchar contra ciertas enfermedades.
Los oligonucleótidos dirigidos contra la región controladora del gen que cifra el receptor de la interleucina-2, lo ha demostrado Hélène, inhiben la expresión de dicho gen.
Por su parte, Postel y otros, del laboratorio de Hogan, han comprobado que un oligonucleótido dirigido contra la región reguladora del gen c-myc humano bloquea la transcripción del gen en una línea celular humana cancerosa.
Otros laboratorios han diseñado oligonucleótidos que reducen, en células vivas, la transcripción de diversos genes víricos relacionados con el cáncer.
Aunque no se conocen datos de ensayos realizados con animales ni existe ningún experimento con humanos, el panorama podría cambiar en los próximos años.

No quiere ello decir que nos espera un camino de rosas. 
A las dificultades habituales que encontramos en la solución de cualquier problema científico, hay que añadir aquí las inherentes a los propios fármacos.
La dosis justa es una de ellas:
los oligonucleótidos construidos para inhibir la transcripción dentro de las células sólo resultan eficaces si se emplean en elevadas concentraciones; de ahí que una línea de investigación esté consagrada a rebajar dichas dosis.

Topamos también con otra limitación mucho más restrictiva, a saber, la que nos dice que las dianas deben presentar sus purinas en la misma cadena.
Poco puede hacerse contra eso, de momento.
Ocurre, sin embargo, que más de la mitad de las dianas potenciales en el ADN dúplice contiene purinas y pirimidinas mezcladas en cada cadena.
Como primera solución de ese problema, Dervan y sus colegas han sustituido los grupos fosfato de los oligómeros por grupos químicos que hacen que el oligonucleótido salte de una cadena a otra.
Se investigan también cambios estructurales aún más refinados.
Tarde o temprano, se conseguirá ampliar el abanico de dianas de ADN accesibles a los oligonucleótidos formadores de tríplices.

Las estrategias basadas en las cadenas antisentido y tríplices están aún lejos de la perfección.
Pero, considerados los éxitos obtenidos en los últimos años, todo indica que las mejoras son sólo cuestión de tiempo.

Albergamos la esperanza de que, un día no muy lejano, estos fármacos constituyan un tratamiento rutinario de las enfermedades que carecen de otra terapia eficaz, así las de origen genético y vírico.

Figura 1

SE ENCUENTRAN EN FASE DE ENSAYO dos estrategias innovadoras encaminadas a impedir la síntesis de proteínas relacionadas con enfermedades

En la síntesis de una proteína (izquierda) se requiere que el gen que cifra su composición se transcriba (a) en moléculas de ARN mensajero.
Luego, el ARN debe traducirse (h) en copias de la proteína.
La estrategia basada en los triplicas (centro) se propone detener la producción de proteínas no deseadas.
Para ello inhibe la transcripción del gen correspondiente. 
En la estrategia antisentido (derecha), el objetivo es impedir selectivamente su traducción.

Figura 2

LOS NUCLEOTIDOS SON LA BASE de las nuevas medicinas genéticas

Estas drogas suelen estar formadas por 15 o 20 nucleótidos, similares a los del ADN normal:
contienen un azúcar (azul),un grupo fosfato (marrón) y una base nitrogenada (rojo) de las que se enumeran arriba a la derecha. 
En los fármacos antisentido, un átomo de oxígeno de cada grupo fosfato suele estar reemplazado por un átomo de azufre (S) o un grupo metilo (CH3) (recuadro, abajo a la derecha).
Estas modificaciones hacen que la droga no sea degradada por las enzimas celulares.

Figura 3

LOS OLIGONUCLEOTIDOS (rojo) se unen a sus dianas de ácidos nucleicos (verde) en la forma indicada en los modelos espaciales (izquierda)

Arriba, un oligonucleótido antisentido se abraza a una cadena de ARN mensajero, creando una hélice doble.
El otro oligonucleótido se enrolla sobre el surco mayor de una doble hélice de ADN, produciendo una hélice triple (abajo).
A la derecha de los respectivos modelos se indican los oligonucleótidos (identificados por la primera letra de sus bases)que participan en esas uniones. 

Figura 4

TRANSCRIPCIÓN DE UN GEN, que se produce (a) cuando el grupo de proteínas que forman el complejo transcripcional se une a la región controladora del gen

Ese complejo es la señal para que la enzima polimerasa de ARN (púrpura) copie la información que contiene el gen en una molécula de ARN mensajero (verde oscuro).

La mayoría de los agentes diseñados para formar tríplices (rojo) se dirigen contra la región controladora.
Ello impide que la polimerasa de ARN pueda unirse al gen (b).
Los fármacos diseñados contra la región informativa del gen también pueden detener la transcripción en marcha (c).

Figura 5

TRADUCCIÓN, llevada a cabo (a) por los ribosomas, estructuras que recorren el ARN y van simultáneamente sintetizando las proteínas

La unión de un fármaco antisentido (naranja) al ARN puede inhibir la traducción de dos maneras, por lo menos.
Puede impedir que los ribosomas comiencen o terminen su tarea (b).
También pueden hacer que una enzima, la ribonucleasa H, corte al ARN por el sitio donde se une el fármaco (c).
El ARN roto no puede traducirse y se degrada rápidamente.

Figura 6

CÉLULAS CANCEROSAS (puntos oscuros) infiltradas en la cavidad craneal (zona blanquecina) de un ratón leucémico (izquierda)

En un ratón leucémico tratado con un oligonucleótido antisentido dirigido contra el gen c-myb , relacionado con el cáncer, las células no invaden dichas áreas (derecha). 
Investigadores de la Universidad de Pennsylvania realizan ahora ensayos con la misma droga en pacientes afectados de leucemia. 


Absorción celular de la glucosa 

La glucosa, un nutriente esencial, sólo puede entrar en las células con la ayuda de cierta proteína transportadora.
¿Cuáles son la estructura y la función de ese transportador?
¿De qué modo lo regula la insulina?

La glucosa es la divisa oficial del metabolismo.
Absorbida en el intestino y producida por el hígado, este azúcar viaja, vehiculada por la sangre, a todos los tejidos del organismo, que la emplean como recurso energético y precursor primordial de otros compuestos que contienen carbono. 
Un acceso a las células que, sin embargo, resulta ser bastante alambicado.
Ha de transportarla al interior una proteína específica que se halla anclada en la membrana de la célula.
Durante los últimos diez años, varios laboratorios, el nuestro incluido, han venido desentrañando la estructura de esa proteína y su curiosa manera de proceder.
Se han descubierto ya cinco formas moleculares del transportador, cada una de las cuales está adaptada a las necesidades metabólicas del tejido donde reside.

Los acontecimientos mediados por estas moléculas vitales comienzan tras la ingestión de carbohidratos. 
El intestino degrada esos hidratos de carbono en glucosa, que pasa al torrente sanguíneo.
Esa subida de glucosa en sangre estimula las células beta del páncreas para que liberen insulina, hormona que limpia de glucosa la sangre por doble vía:
impide la liberación hepática de glucosa y activa la captación del azúcar por el músculo y el tejido adiposo.
Las células musculares convierten la glucosa en glucógeno, un carbohidrato polimerizado, de rápida reconversión en glucosa. 
Los adipocitos convierten la glucosa en gotas de grasa para un almacenamiento duradero.
Cuando bajan los niveles de glucosa en sangre, las células beta suspenden la secreción de insulina, y el metabolismo torna al estado basal.

El exceso de insulina provoca la caída de glucosa en sangre y se produce hipoglucemia, situación que puede tener un desenlace fatal al causar la inanición del cerebro y de los órganos que viven principalmente de glucosa. 
En la condición antagónica de escasez de insulina -o de resistencia a su efecto por parte de células musculares y adipocitos-, la glucosa en sangre se eleva y se produce hiperglucemia.
La alta concentración de glucosa crea un desequilibrio osmótico, que obliga a la sangre a extraer agua de los tejidos, y los riñones la excretan en la orina junto con un exceso de sales.
La deshidratación y la pérdida de sales por culpa de una hiperglucemia grave pueden conducir al coma y a la muerte. 
Una hiperglucemia menos grave contribuye, probablemente, a ciertas complicaciones a largo plazo que sufre el paciente de diabetes mellitus: cardiopatías, parálisis cerebral, ceguera, pérdida de la capacidad funcional del riñón y gangrena en las extremidades.

La ausencia de insulina causa la diabetes mellitus insulino-dependiente (DMID), o diabetes tipo I, que suele desarrollarse en niños o adolescentes cuando una reacción autoinmune destruye las células beta del páncreas.
La diabetes no-insulino-dependiente (DMNID), o tipo II, aparece por lo común en la madurez y es con mucho la forma más frecuente; por dar una cifra, afecta a un 5% de la población de Estados Unidos de más de 40 años.
En los primeros estadios de la DMNID algunos pacientes no carecen de insulina, observación que permite atribuir el origen de la enfermedad a un insuficiente efecto hormonal en miocitos, adipocitos y hepatocitos. 

La absorción de la glucosa reviste mayor complejidad de lo que podría sospecharse.
La célula, para sobrevivir, debe evitar que su interior se mezcle con el medio acuoso externo.
De ese aislamiento se ocupa la membrana citoplasmática, doble capa de moléculas lipídicas que repelen el agua y las sustancias que, como la glucosa, se disuelven fácilmente en ella. 
(Se dice que los lípidos son hidrofóbicos, mientras que la glucosa es hidrofilica.)
En virtud de esa configuración, las células no pueden absorber glucosa por difusión simple.
Necesitan, por contra, recurrir a una proteína especial: a un transportador. 

El primer transportador de glucosa fue aislado, en 1977, a partir de eritrocitos humanos (glóbulos rojos) por Michihiro Kasahara y Peter C. Hinkle, de la Universidad de Comell.
Ocho años más tarde en un proyecto de colaboración, dirigido por uno de nosotros (Mueckler) y por Harvey F. Lodish, del Instituto Whitehead para la investigación biomédica, se dilucidó la secuencia de aminoácidos del transportador. 
Siguieron un proceso retrógrado, "hacia atrás", que era en este caso la estrategia más fácil:
aislaron el ADN que codifica la proteína y determinaron su secuencia de nucleótidos.
Entonces aplicaron el código genético para traducir la secuencia nucleotídica en secuencia aminoacídica.

La proteína descrita por este método consiste en una cadena de 492 aminoácidos, organizada en 25 segmentos. 
Trece de ellos, extensamente hidrofílicos, muestran inclinación por los medios acuosos del interior y del exterior celular;
estos segmentos se alternan con otros doce fundamentalmente hidrofóbicos, que prefieren el medio lipídico de la membrana.
Semejante configuración, sumada a la información química directa sobre las partes de la proteína que se orientan al interior o al exterior del eritrocito, da pie a suponer que la proteína atraviesa 12 veces la membrana de un lado a otro.

¿Cómo logra tal estructura transportar la glucosa hasta el interior celular?
Forzoso es que cree un poro a través de la membrana.
La arquitectura de tal poro viene sugerida por el plegamiento de la cadena polipeptídica y por el patrón de aminoácidos de los segmentos transmembrana.
La espectroscopía nos descubre que cada segmento se arrolla en hélice; hasta el 80% de la cadena polipeptídica adquiere esa disposición helicoidal. 
En razón de la forma cilíndrica que desarrolla la hélice, los grupos químicamente reactivos de los aminoácidos desplegarán una distribución periódica en la superficie.
Y de ello resulta que, en cinco segmentos transmembrana -los números 3, 5, 7, 8 y 11-,los grupos serán hidrofílicos a un lado del cilindro e hidrofóbicos en el otro.
Unidos con sus caras hidrofóbicas orientadas hacia afuera -hacia los restantes segmentos transmembrana y el entorno lipídico de la membrana-, los cinco segmentos podrían formar un poro cuya superficie interna estaría presta para enlazarse con la glucosa.

Adelantemos, sin embargo, que este modelo de la estructura tridimensional del transportador de glucosa se mueve en el terreno de la hipótesis.
Para determinar la estructura real, los cristalógrafos de rayos X tendrían que convertir los transportadores en cristales bien ordenados, empeño en el que han fracasado debido a la resistencia que opone la naturaleza lipídica de estas proteínas.

En consecuencia, nuestro modelo de transportador se funda, en parte, en los ejemplos suministrados por un manojo de proteínas de membrana cristalizables y susceptibles de ser representadas. 
Una de ellas, el centro de reacción fotosintético de bacterias, está constituida por tres cadenas de proteínas, de las que dos cruzan la membrana cinco veces y la otra una. 
Uno por uno, los once segmentos transmembrana poseen estructura helicoidal semejante a la que nosotros proponemos para el transportador de glucosa.

Los procesos moleculares implicados en la entrada de glucosa en la célula encierran mayor complejidad de la que da a entender nuestra descripción estructural.
Se supone que el transportador modifica la glucosa al uncirse a ella mediante puentes de hidrógeno débiles y provisionales. 
Tales puentes tienden un átomo de hidrógeno entre un nitrógeno o un oxígeno de un compuesto y el par de electrones sin compartir de un átomo de oxígeno o de nitrógeno de otro compuesto.
Los segmentos transmembrana 3, 5, 7, 8 y 11 contienen muchos aminoácidos que tienen grupos hidroxilo (OH) y carbamida (CONH2) que pueden participar en la formación de puentes de hidrógeno con los numerosos grupos hidroxilo de la glucosa.
La proteína transportadora se presenta, además, en dos formas: en una se engarza a la glucosa por el lado extracelular de la membrana, mientras que en la otra se une por el lado intracelular.

Abundan los datos experimentales que distinguen cuatro etapas en el ingreso de la glucosa en la célula.
Comienza, en la primera, por ocupar el sitio de unión que se encuentra orientado al exterior.
En la segunda etapa, el complejo formado por transportador y glucosa cambia de conformación, en virtud de la cual la glucosa está ahora en el sitio de unión encarado hacia el interior celular.
En la etapa siguiente, el transportador suelta la glucosa en el citoplasma.
Por último, el transportador, libre de su carga, toma a la conformación en que el sitio de unión con la glucosa mira al exterior celular.

Esta cuarta etapa devuelve al transportador a su forma inicial, presto para vehicular una nueva molécula de glucosa. 

Aunque desconocemos la estructura de una y otra conformación, es de o presumir que en cada caso el canal esté abierto por un extremo y cerrado el por el otro y que la glucosa se engarce en la cavidad del extremo abierto.

La glucosa podría avanzar entonces cuando el extremo abierto se cierre tras de ella y se abra el que, ante ella, se hallaba cerrado.

Para nosotros, pues, la proteína transportadora constituye un oscilador conformacional que traslada la cavidad de unión de la glucosa a los dos polos opuestos de la membrana.
La investigación cinética, acometida por James R. Appleman y uno de nosotros (Lienhard) en la Facultad de Medicina de Darmouth, atestigua la extraordinaria rapidez con que transcurre tal oscilación.
En ausencia de la glucosa, y a 20 grados Celsius, cada molécula de transportador de eritrocito se transforma de un estado a otro unas cien veces por segundo.
Cuando la glucosa se une al transportador, la velocidad se acelera hasta 900 veces por segundo.
La glucosa acelera la velocidad de transformación rebajando la barrera energética entre las dos conformaciones. 

Pero,
¿por qué opera de forma tan tortuosa el transportador?
A uno se le ocurren estrategias más directas:
la proteína transportadora podría mantener permanentemente abierto un poro o podría haber oscilado entre esa conformación y otra en la cual el poro se hallara cerrado.

La explicación se esconde probablemente en la necesidad que posee la célula de mantenerse en desequilibrio químico con el medio extracelular.
Por ejemplo, la concentración de iones sodio en el exterior celular decuplica normalmente su abundancia en el interior celular.
Considerando que la molécula de glucosa adquiere un tamaño similar al de ese ion, un poro que estuviera siempre abierto dejaría paso libre para la filtración del sodio por el mismo. 
Obviamente, el transportador podría acarrear de vez en cuando algún ion sodio hacia el interior de la célula; pero esta filtración de iones sería insignificante:
las células poseen canales transmembrana especiales por donde, cuando se abren, pasan 10 millones de iones sodio por segundo, 100.000 veces más rápido de lo que opera el transportador de glucosa.

Cada tejido necesita su cuota específica de glucosa. 
¿Significa ello que precisa también de un transportador propio?
Desde comienzos de los ochenta, se vienen acumulando indicios que dan respuesta afirmativa a ese interrogante.
Los transportadores respondieron de forma distinta según el inhibidor químico, es decir, se mostraron más o menos eficaces en el acarreo del azúcar. 
Así, aunque todos vehicularon glucosa con velocidad creciente a medida que subía la concentración (desde un arranque de cero), el ritmo alcanzó su máximo; dicho de otro modo, las velocidades se saturaron a distintas concentraciones de glucosa.

Pues bien, la concentración de glucosa a la cual la velocidad de transporte es la semivelocidad de saturación constituye el criterio del que se valen los expertos para clasificar los sistemas de transporte.
Dicha concentración de semisaturación es alta para el transportador del hepatocito; baja, en cambio, para el transportador de las células del cerebro.

La prueba demostrativa de la diversidad de los transportadores llegó después de 1985, año en que se descubrió la secuencia del ADN que determina el transportador de glucosa del eritrocito humano; secuencia que sirvió de sonda para aislar el ADN codificador de los transportadores de otros tejidos. 
Han seguido ese enfoque muchos investigadores, entre otros, Graeme I. Bell, de la Universidad de Chicago, Morris J. Bimbaum, de la Facultad de Medicina de Harvard, Lodish y dos de nosotros (Mueckler y James).
Hasta el momento se han descubierto un total de cinco transportadores de glucosa.

Los transportadores forman una familia cuyos miembros guardan un estrecho parecido en su estructura y función.
Cada uno consta de una cadena polipeptídica de unos 500 aminoácidos.
Las cinco secuencias comparten la mitad aproximada de los restos aminoacídicos, o son muy similares.
Además, el modelo esbozado de plegamiento para cada transportador está integrado por 12 segmentos que abarcan de un lado a otro de la membrana.

Cada transportador de glucosa (GluT) se numera en el orden de su descubrimiento.

- El primero, GluT1 , abunda en las células endoteliales que tapizan los vasos sanguíneos y forman la barrera hematoencefálica.
Se halla especializado, así parece, en suministrar el flujo uniforme de glucosa que necesita el cerebro.
En cuantía menor se ha descubierto GluT1 en otros tejidos, señal de que acarrea la glucosa que necesitan cuando sus células se encuentran en un estado relativamente activo.

- GluT2 aparece en las células beta del páncreas, que secretan insulina, y en los órganos que liberan glucosa al torrente sanguíneo, como el intestino, el hígado y el riñón.
Las altas concentraciones de glucosa requeridas para alcanzar la semisaturación de GluT2 revelan que éste transporta glucosa en proporción a la concentración del azúcar en la sangre.
Por consiguiente, GluT2 transmite puntualmente, al hígado o a las células beta del páncreas, las variaciones en los niveles de glucosa sanguínea producidas durante las comidas o durante el ejercicio.

- GluT3 opera en las neuronas del cerebro.
Su afinidad por la glucosa es mayor que la de GluT1 , asegurándose de ese modo la aportación constante de azúcar a dichas células.
Y, así, un transportador coopera con otro, para que no le falte a las neuronas el suministro de nutriente tan vital.

- GluT4 es el transportador principal de las células del músculo y del tejido adiposo; absorben éstas glucosa en abundancia y la convierten en otros compuestos ricos en energía.
Caracteriza a este transportador su extraordinaria capacidad para desplazarse desde el interior celular hasta la superficie externa.

- GluT5 reside sobre todo en el intestino delgado.
Queda mucho por descubrir en tomo a su funcionamiento.


Los cinco GluT trasladan la glucosa, a través de la membrana, siguiendo el gradiente de concentración, esto es, desde la concentración mayor de glucosa hacia la menor.
Es una familia que difiere del co-transportador, encargado de extraer glucosa en contra de gradiente.
El co-transportador acopla el acarreo de una molécula de glucosa al transporte de un ion sodio.
La energía necesaria para extraer la glucosa se obtiene gracias al movimiento del sodio a favor de su gradiente de concentración.
Este mecanismo de co-transporte hace posible que las células que recubren el lumen del intestino y del riñón absorban incluso pequeñas cantidades de glucosa de la comida y orina, respectivamente.
GluT2 interviene entonces y vierte ese azúcar en el torrente sanguíneo.

Los transportadores divergen también en su respuesta a la insulina.
Se trata de peculiaridades presumibles en tejidos que ofrecen notables diferencias en sus necesidades metabólicas.
La respuesta más pronunciada la muestra GluT4 , identificado en 1988 por James, entonces en la facultad de medicina de la Universidad de Boston, y su colaborador Paul F. Pilch.

La insulina ejerce un efecto drástico en el transporte de glucosa.
Nosotros solemos trabajar con cultivos de adipocitos de ratón, adipocitos 3T3-Ll.
La adición de insulina a dichos cultivos, mantenidos a 37 grados C, multiplica por 15 la absorción de glucosa.
El pico de absorción se alcanza en 10 minutos.

El modo de comportarse la insulina constituyó un misterio hasta 1980, cuando Lawrence Wardzala y Samuel W. Cushman, del Instituto Nacional de la Salud, y Kazuo Suzuki y Tetsuro Kono, de la facultad de medicina de la Universidad de Vanderbilt, descubrieron, simultánea e independientemente, el fenómeno del reclutamiento de los transportadores.
Observaron que las células poseían una reserva de transportadores de glucosa (se trataba, lo sabemos ahora, de GluT4 ) y trasladaban algunos hasta la membrana en respuesta a la insulina.
Luego, cuando mengua la glucosa en sangre y con ello la secreción de insulina, se invierte el proceso de reclutamiento.

Podemos seguir ese reclutamiento auxiliados por técnicas microscópicas.
En cierta de ellas, se marcó el GluT4 de adipocitos 3T3-Ll con anticuerpos que emitían fluorescencia en la longitud de onda del verde al exponerlos a luz azul.

Las células que no fueron estimuladas con insulina mostraron muchos puntos en el interior de la célula que emitían luz verde, y casi ninguno en la superficie celular. 
Pero después del tratamiento con insulina, la superficie de la célula brilló de color verde: 
tuvo que producirse un traslado masivo de moléculas de GluT4 desde el interior hacia la superficie celular.

Estos resultados se confirmaron y cuantificaron en un estudio que empleó un marcador diferente.
En finos cortes de tejido, se engarzaron anticuerpos específicamente en GluT4 .
Se trataron luego los cortes con diminutas partículas de oro unidas a proteínas que se pegan a los anticuerpos, marcando específicamente el GluT4 .
Las partículas pueden percibirse sin dificultad en el microscopio electrónico. 
Slot, uno de los autores del artículo, se sirvió de esa técnica para localizar y contar moléculas de GluT4 en células sensibles a la insulina.
Antes de la estimulación por la hormona, sólo había en la superficie celular un 1% de las moléculas de GluT4 marcadas; después de la estimulación, la proporción se elevó al 40%.

De qué se vale la célula paratrasladar GluT4 hasta la membrana y recuperarlo de allí, en respuesta a la insulina?
No puede darse todavía una contestación cabal, aunque trabajan en ello muchos expertos.
Ya que el transportador de glucosa es una proteína que se encuentra anclada en la membrana, por la cara interna y por la externa, podemos inferir que migrará integrada en una vesícula de membrana.
Probablemente, la insulina insta a las vesículas intracelulares que contienen GluT4 para que se trasladen hasta la superficie interna de la membrana celular y se fundan con ella.

El proceso de regreso manifiesta mayor complejidad.
En una secuencia de sucesos, cuyos pormenores ignoramos, las microvesículas que contienen GluT4 se separan, por estrangulación, de la cara interna de la membrana y se funden con los endosomas, unos sacos membranosos mayores.
Dentro ya de los endosomas, las moléculas de GluT4 se segregan en prolongaciones tubulares, prominencias que acabarán por constituir nuevas vesículas. 
Esta transferencia de GluT4 hasta el interior celular se realiza en cualquier momento; ahora bien, en presencia de insulina, las vesículas se refunden de inmediato con la superficie celular.
La retirada de la insulina rompe el ciclo y provoca la acumulación de las vesículas. 

Por ser GluT4 el principal isotipo que se traslada, y pasar del interior celular a la superficie (y viceversa, según el nivel de insulina), nos entró la sospecha de que poseyera un mecanismo de guía, ausente en cualquier otro isotipo. 
La clave característica de este mecanismo podría hallarse en el emplazamiento de GluT4 en vesículas intracelulares si no hay insulina;
los GluT de otros tejidos acostumbran residir en la superficie celular, esté o no presente la insulina.

El mecanismo en cuestión debe depender de un único segmento de aminoácidos de GluT4 , capaz, verosímilmente, de dirigir al transportador hasta las vesículas mediante su interacción con otras proteínas.
Este segmento desempeñaría, pues, la función de dirección y sello de una carta.
La validez de tal hipótesis se pretende dilucidar a través de técnicas de biología molecular que permitan producir formas mutantes de GluT4 susceptibles de expresarse en las células. 
Si se descubriera que la forma mutante reside en la superficie de una célula en su estado basal, resultaría que el lugar donde ocurrió la mutación vendría a coincidir probablemente con esa marca de identificación. 

Cuando la insulina se une a la célula, desencadena una serie de procesos moleculares que acaban por redistribuir los transportadores de glucosa en la membrana celular.
Conocemos sólo el comienzo y el final de esa cascada de acontecimientos.
Se inicia con la unión de la insulina de la sangre a proteínas específicas que se encuentran ancladas en la membrana celular.
La proteína emerge de ambos costados de la membrana.
Cuando la insulina se une al saliente de la parte externa, el receptor se reestructura de nuevo, para permitir que la proyección interior fosforile el aminoácido tirosina de regiones específicas dentro de proteínas diana específicas.
No se sabe de ninguna proteína diana que esté implicada en el reclutamiento de transportadores de glucosa, habiéndose descartado el candidato obvio: el propio transportador de glucosa. 

Para desentrañar la cadena de acontecimientos iniciados por el receptor de insulina, han aparecido dos enfoques.
De acuerdo con el primero, importa descubrir la proteína diana por aislamiento y caracterización de proteínas dotadas de tirosinas que se fosforilan en respuesta a la hormona. 
El enfoque complementario atiende a las proteínas que se encuentran en las vesículas de almacenamiento de GluT4 .
La insulina podía cambiar una de estas proteínas para que acelere el curso de la vesícula hacia la superficie celular o para que induzca su fusión con tal superficie. 
La vesícula podría vehicular también otras proteínas para promover funciones que ignoramos. 
Esas posibilidades nos vienen sugeridas por el propio mecanismo vesicular.
Por tortuoso camino que nos parezca para regular la velocidad de transporte de glucosa a la célula, resultaría eficaz en el traslado de proteínas de un lado a otro de la superficie celular.

Estos hallazgos no conciernen a la causa de la diabetes mellitus dependiente de insulina; sí importan a la segunda forma: DMNID.
La predisposición a adquirir la DMNID es genética habrá, pues, un gen al menos que predisponga a sus portadores a contraer la enfermedad.
Cabe, así, la posibilidad de que un gen defectivo deteriore la capacidad de movimiento de GluT4 en respuesta a la insulina.

Una de las primeras manifestaciones de la DMNID es la resistencia a la hormona: incapacidad del músculo, tejido adiposo o hígado de responder eficientemente ante elevados niveles de insulina en sangre.
El páncreas replica a esa oposición con una sobreproducción de insulina, motivo por el cual algunos pacientes muestran, en los primeros estadios de la DMNID, lo mismo una hiperglucemia transitoria después de una comida rica en carbohidratos que unos niveles firmemente elevados de insulina en sangre.
Con el avance de la enfermedad, el páncreas suele perder la capacidad de segregar suficiente insulina como para compensar la resistencia insulínica.
Cuando esto ocurre, la hiperglucemia persiste entre las comidas, y el paciente puede requerir la administración de insulina o de otras drogas para bajar los niveles de glucosa en sangre.

Esa recreación de la realidad se apoya en los resultados recientes de Lodish y Bemard Thorens, del Instituto Whitehead, y Roger H. Unger, de la facultad de medicina de la Universidad del Suroeste de Texas.
Estudiando dos modelos animales de DMNID diferentes, hallaron que las células beta del páncreas tenían una cantidad reducida de GluT2 , su isotipo.
Esta reducción guardaba correlación con la menguada secreción de insulina en respuesta a elevados niveles de glucosa.
Si recordamos que el transporte de más glucosa a las células beta normalmente dispara la liberación de insulina, la reducción de GluT2 podría constituir la causa de una inadecuada secreción de insulina.

La clave de la resistencia a la insulina podría esconderse en el músculo esquelético, que da cuenta del 80% de la glucosa consumida por el cuerpo en el período posterior a la ingestión de una comida rica en carbohidratos. 
(En otras situaciones, cuando el metabolismo se encuentra en estado basal, el cerebro consume el 60% del azúcar.) 
Las células musculares convierten el exceso de glucosa en glucógeno a una velocidad que se encuentra limitada por el transporte de glucosa.
Debido a que los pacientes con DMNID acumulan glucógeno muscular a una velocidad que ronda la mitad de la normal, probablemente tengan un defecto en al menos una de las proteínas que regulan el transporte de glucosa.

Hay otro aspecto que ayuda a delimitar el campo de investigación. 
La resistencia insulínica no puede obedecer a una carencia absoluta de GluT4 en las células musculares. 
Las mediciones recientes de GluT4 en biopsias de músculo de individuos normales y diabéticos manifiestan que los pacientes de DMNID poseían cantidades de esta proteína iguales o ligeramente por debajo de lo normal.
Pero GluT4 podría causar resistencia insulínica por otra ruta, encaminándosele quizás hacia compartimentos intracelulares equivocados -de donde no pudiese ser reclutado hacia la superficie celular.
O pudiera ocurrir que hubiera alguna lesión en otra parte de la ruta de señales a través de la cual la insulina promueve el reclutamiento de GluT4 .
Lesión en que podría verse involucrado el receptor insulínico de la superficie celular o alguna de las proteínas desconocidas que traducen la señal desde el receptor hasta las vesículas de GluT4 .

La cuestión principal reside, pues, en averiguar si está dañado el mecanismo de reclutamiento de GluT4 en las células musculares de los pacientes diabéticos. 
En caso afirmativo, la dilucidación de los sucesos que comienzan en el receptor de insulina y terminan con el reclutamiento nos habrá de mostrar de qué manera el mensaje de la insulina se extravía o se equivoca de dirección. 


Evolución y orígenes de la enfermedad

Los principios de la evolución por selección natural empiezan a aplicarse en medicina .

La contemplación detenida del cuerpo humano infunde respeto en pareja medida a la perplejidad que provoca.
Fijémonos en el ojo, por ejemplo.
El tejido transparente y vivo de la córnea describe la curva apropiada, el iris se adapta a la intensidad de la luz y el cristalino se ajusta a la distancia, todo de suerte tal que la cantidad óptima de luz quede enfocada exactamente sobre la superficie de la retina. 
La admiración que produce tamaña perfección cede pronto a la consternación.
Contra toda lógica, los vasos sanguíneos y los nervios atraviesan la retina y forman una mancha ciega en su punto de salida.

El cuerpo es un cúmulo de contradicciones sorprendentes. 
Por cada exquisita válvula cardíaca tenemos una muela del juicio.
Las mismas cadenas de ADN que gobiernan el desarrollo de los diez billones de células de un ser humano adulto permiten también su deterioro progresivo y, con el tiempo, la muerte.

Nuestro sistema inmunitario identifica y destruye un millón de elementos extraños; aun así, son muchas las bacterias que nos pueden matar.
Estas contradicciones producen la desagradable impresión de que el cuerpo ha sido diseñado por un equipo de magníficos ingenieros con la ayuda ocasional de un chapucero.

Pero lo que a primera vista parece una serie de despropósitos encierra sentido.

El encontrarlo nos exige estudiar el origen de la vulnerabilidad del cuerpo urgidos por las avisadas palabras de Theodosius Dobzhansky: 
«Nada tiene sentido en biología si no es a la luz de la evolución».
Se puede afirmar que la biología evolutiva es el fundamento de toda biología y ésta, la base de toda medicina.
Por ello sorprende que apenas ahora empecemos a reconocer el estatuto de la biología evolutiva como ciencia médica primaria.
El estudio de los problemas médicos en el contexto de la evolución se ha dado en denominar medicina darwinista.
La investigación médica se propone explicar las causas de la enfermedad del individuo y busca los tratamientos para curar, o al menos aliviar, las afecciones deletéreas.
Semejante planteamiento se ha venido ciñendo a las cuestiones inmediatas, al estudio directo de los mecanismos anatómicos y fisiológicos. 
La medicina darwinista, por contra, se pregunta por qué el diseño del cuerpo humano le hace vulnerable al cáncer, la aterosclerosis, la depresión o la parálisis; dilata, pues, el horizonte y el contexto de la investigación. 

Las explicaciones de tipo evolutivo relativas a la menesterosidad del cuerpo humano se estructuran en ciertas categorías, muy pocas.
En primer lugar, algunos estados desagradables - el dolor, la fiebre, la tos, el vómito o la ansiedad- no constituyen tanto enfermedades o defectos de diseño cuanto defensas desarrolladas.
En segundo lugar, los conflictos con otros organismos, y aquí vale lo mismo para Escherichia coli que para los cocodrilos, son inherentes a la vida.
En tercer lugar, determinadas circunstancias (la disponibilidad de grasas en la dieta) son tan recientes, que la selección natural no ha tenido tiempo todavía de obrar sobre ellas.
En cuarto lugar, ciertas peculiaridades pueden resultar del compromiso entre los beneficios que reportan y los inconvenientes que ocasionan.
En un ejemplo clásico, el gen implicado en la anemia falciforme (drepanocitosis) protege de la malaria.
Por último, el proceso de selección natural condiciona el desarrollo de diseños subóptimos, como en el caso de los ojos de los mamíferos.

La tos pudiera ser el mecanismo de defensa más útil. 
Los sujetos que no eliminan los cuerpos extraños de sus pulmones presentan mayor probabilidad de morir de neumonía. 
Benéfica es también la capacidad de sentir dolor.
Los contados individuos que carecen de tal sensación tampoco experimentan molestia alguna cuando permanecen en la misma posición durante largo rato.
Este estatismo antinatural dificulta el riego de las articulaciones y fomenta su deterioro.

Tales individuos suelen morir precozmente en la edad adulta por lesión de los tejidos y por infecciones.
Tos y dolor se reputan a menudo expresión de enfermedad o agresión; la verdad es que están más cercanos a la solución que al problema.
Los sistemas de defensa desarrollados por la selección natural se mantienen en reserva hasta que se les necesita.

Quizá se haga más duro admitir la fiebre, el vómito, la diarrea, la ansiedad, la fatiga, el estornudo y la inflamación entre los mecanismos de defensa.
La utilidad de la fiebre pasa inadvertida incluso para algunos médicos.

Más que un incremento de la actividad metabólica, la fiebre es un aumento cuidadosamente regulado del punto de ajuste del termostato corporal.
La subida de la temperatura corporal facilita la destrucción de los patógenos. 
Matthew J., Kluger, del Instituto Lovelace de Albuquerque, en Nuevo México, ha observado que los propios lagartos, animales de sangre fría, buscan lugares más templados cuando padecen una infección, para que su temperatura corporal se eleve varios grados por encima de la habitual.
Si se les impide trasladarse a la parte más cálida de la jaula, se multiplica el riesgo de morir de infección.
En otro estudio similar realizado por Evelyn Satinoff, de la Universidad de Delaware, las ratas viejas, que ya no pueden alcanzar las altas fiebres de sus compañeras de laboratorio, buscan instintivamente ambientes más calientes cuando sufren una infección. 

La reducción de los niveles de hierro en sangre, otro mecanismo de defensa, se presta también a interpretación errónea.
Quienes padecen infecciones crónicas presentan a menudo concentraciones inferiores a la normalidad. 
A veces se culpa de esa caída a la enfermedad, cuando se trata, por contra, de una respuesta protectora; durante la infección, el hígado secuestra el hierro para evitar que las bacterias accedan a elemento tan vital.

Tradicionalmente se ha considerado la náusea un enojoso efecto secundario de la gestación.
Las náuseas coinciden con el periodo de diferenciación rápida del feto, época en que se manifiesta más vulnerable a la exposición a toxinas.
La gestante que las experimenta tiende a restringir la ingesta de sustancias de sabor fuerte, potencialmente dañinas.
Ante esa observación, Margie Profet planteó la hipótesis de que las náuseas del embarazo fueran una adaptación por la que la madre protegía al feto de la exposición a toxinas.
Profet se aprestó a comprobar la hipótesis estudiando los resultados del embarazo.
Halló, con suficiente seguridad, que el fruto de las mujeres con más náuseas presentaba menos malformaciones. 
(El estudio respalda la teoría, pero resulta todavía incompleto.
Si la hipótesis de Profet es correcta, la investigación ulterior deber descubrir que las hembras de muchas especies modifican sus preferencias dietéticas durante la gestación.
Predice también un incremento de las malformaciones en los embarazos de mujeres con menos náuseas o sin náuseas en absoluto y que, como consecuencia de ello, hubieran seguido una dieta más variada durante la gestación.)
Parece obvio que el estado de ansiedad, muy común, se originase como mecanismo de defensa en situaciones de peligro para favorecer la huida y la evitación.
En 1992, Lee A., Dugatkin, de la Universidad de Louisville, ponderó los beneficios del miedo en los gupis.
Los clasificó en tímidos, cautos y temerarios, en función de su reacción ante la presencia de un pez potencialmente depredador, la perca americana de boca pequeña.
Los tímidos se escondían, los cautos sencillamente nadaban en otra dirección y los temerarios se quedaban quietos y fijaban la vista en el posible verdugo.
Dugatkin colocó a cada gupi solo en un tanque con la perca.
Sesenta horas después, habían sobrevivido el cuarenta por ciento de los tímidos, por sólo un quince por ciento de los cautos.
El completo exterminio de los temerarios, por su parte, facilita la transmisión de los genes del depredador más que los suyos propios.

La selección de genes que favorecen la ansiedad predice la existencia de personas que experimenten un exceso de ansiedad.
Las hay.
También debería haber individuos hipofóbicos, con un nivel de ansiedad insuficiente, ya sea por tendencias genéticas o por fármacos ansiolíticos.
La naturaleza exacta y la frecuencia de este síndrome es asunto por establecer.
Pocas personas acuden al psiquiatra quejándose de una aprensión insuficiente.
Al flemático patológico tal vez haya que buscarlo en los servicios de urgencia, en las prisiones o en la cola del paro.

La utilidad de afecciones tan comunes y desagradables como la diarrea, la fiebre o la ansiedad, no se adivina de entrada. 
Si la selección natural determina los mecanismos que regulan las respuestas de defensa,
¿por qué las personas que recurren a fármacos para bloquearlas salen tan bien paradas, sin que inflijan a su cuerpo un daño obvio?
La verdad es que a menudo nos hacemos un flaco servicio cuando nos interponemos en esos mecanismos.

Herbert L., DuPont, de la Universidad de Texas en Houston, y Richard B., Hornick, del Hospital Regional de Orlando, han estudiado la diarrea causada por Shigella .
Descubrieron que las personas que tomaban antidiarreicos tenían una enfermedad de curso más largo e incapacitante que los que tomaron placebo. 
En otro ejemplo, Eugene D., Weinberg, de la Universidad de Indiana, ha documentado un aumento de las enfermedades infecciosas, en especial amebiasis, en determinadas zonas de Africa, donde se pretendía con la mejor intención corregir la deficiencia de hierro.
Los suplementos orales de hierro no parecen afectar a personas con infecciones habituales- y sanas en lo demás -, en tanto que pueden ser muy nocivos para las malnutridas que padecen infecciones.
No son capaces éstas de sintetizar una cantidad suficiente de proteínas ligaduras de hierro, que queda así libre para el metabolismo de los agentes infecciosos.

Hace poco se responsabilizó a un fármaco antinauseoso de producir efectos teratógenos.
Parece que no se consideró la posibilidad de que el fármaco fuera en sí mismo inocuo para el feto y que la asociación a defectos congénitos se debiera a su interferencia con el mecanismo defensivo de la náusea materna.

Otro obstáculo para percibir los beneficios de las defensas deriva del carácter rutinario que se atribuye a las reacciones innecesarias de ansiedad, dolor, fiebre, diarrea o náusea.
La explicación requiere el análisis de las respuestas defensivas en el marco de una teoría de detección de señales.
Una toxina de la sangre puede tener su origen en una sustancia presente en el estómago.
El organismo puede eliminarla mediante el vómito, pero ha de pagar por ello un precio.
El coste de una falsa alarma- el vómito que se produce cuando no hay toxina alguna que expulsar- son apenas unas calorías. 
Pero el tributo que satisface el silencio de una alarma auténtica- esto es, que no provoque el vómito en presencia de una toxina- puede suponer la muerte.

La selección natural tiende a estructurar los mecanismos de regulación con activadores sutilísimos, según el principio del «detector de humos».
Una alarma de incendios fiable que despierte a una familia en caso de incendio, producir necesariamente una falsa alarma cuando se queme una tostada.
El tributo a pagar por los numerosos «detectores de humos» repartidos por el cuerpo humano es el sufrimiento a menudo innecesario.
Este principio explica por qué al bloqueo de las defensas no sigue la mayoría de las veces un trágico desenlace.
En buena medida, las reacciones defensivas se desencadenan ante estímulos insignificantes y su interferencia suele ser inocua.
Casi todas las alarmas que evitamos al quitar la pila del detector de humos son falsas, por lo que esta estrategia puede parecer razonable hasta que algo arda.

La selección natural no puede dotarnos de una protección perfecta contra toda suerte de patógenos.
Tienden éstos a evolucionar de forma mucho más rápida de lo que es capaz el hombre.
La elevada tasa de reproducción de Escherichia coli , por ejemplo, le confiere más oportunidades de mutación y de selección en un solo día que a la humanidad en un milenio.
Nuestras propias defensas, sean naturales o artificiales, actúan como una potente fuerza de selección, por cuya razón los patógenos se ven obligados a desarrollar sus defensas para no extinguirse.
Paul W., Ewald, del Colegio Amherst, ha propuesto una clasificación de los fenómenos asociados a la infección según se beneficie de ellos el huésped, el patógeno, ambos o ninguno de los dos.
Consideremos la mucosidad que acompaña a cualquier resfriado.
La secreción nasal puede expulsar los patógenos, acelerar su transmisión a otros huéspedes o ambas cosas a la vez.
Para averiguarlo habría que investigar y determinar si el bloqueo de las secreciones nasales acorta o prolonga la enfermedad.
No abundan ese tipo de trabajos.

Con el desarrollo de los antibióticos y las vacunas la humanidad ha ganado importantes batallas en su guerra contra los agentes infecciosos.
Las victorias brillaron tanto, que en 1969 William H., Stewart se atrevió a afirmar que había llegado «el momento de cerrar el libro de las enfermedades infecciosas».
Se subestimó al enemigo y el poder de la selección natural.
La realidad es que los patógenos parecen adaptarse a cualquier producto que los investigadores desarrollen.

La resistencia a los antibióticos ejemplifica una demostración clásica de los mecanismos por los que opera la selección natural.
Las bacterias cuyos genes les permiten sobrevivir incluso en presencia del antibiótico, se reproducen en menos tiempo que las otras.
De esta forma, los genes que confieren resistencia se diseminan con rapidez. 
Joshua Lederberg, de la Universidad Rockefeller, ha demostrado que estos genes pueden saltar de bacteria en bacteria incluidos en pequeños fragmentos infecciosos de ADN.
Hoy en día, algunas cepas de tuberculosis de la ciudad de Nueva York resisten a los tres tratamientos antibióticos principales.
Los pacientes infectados por estas cepas no tienen mejor pronóstico que los tuberculosos de hace un siglo.
Stephen S., Morse, de la Universidad de Columbia, apunta que la cepa multirresistente que se ha diseminado por toda la costa este pudo emerger en un asilo próximo al Hospital Presbiteriano de Columbia.
Este fenómeno sería previsible en un medio donde una feroz presión de selección elimina con rapidez las cepas menos virulentas. 
Los bacilos supervivientes se han seleccionado en razón de su resistencia.

Todavía hay quien cree en la trasnochada teoría según la cual los patógenos, tras una larga asociación con el huésped, caminan hacia la benignidad.
Este prejuicio débese a una exposición superficial del problema.
Un organismo que mate pronto a su hospedante quizá llegue a tiempo de infectar a otro.
La selección natural primaria en este caso una virulencia baja.
La sífilis, por ejemplo, era una enfermedad de extrema virulencia cuando entró en Europa, pero a medida que transcurrieron los siglos se volvió cada vez más benigna.
La virulencia de un patógeno es, de todas formas, una característica circunstancial en la vida del mismo.
Aumentar o disminuir en función de lo que en cada momento les resulte más ventajoso a sus genes.

La virulencia baja suele ser ventajosa para el agente que se transmite directamente de un individuo a otro. pues permite al hospedante mantener su relación con nuevas víctimas potenciales.
Otras enfermedades, así la malaria, se transmiten de forma igualmente eficaz, cuando no mejor, a partir de enfermos muy postrados.
Este tipo de patógenos depende de vectores intermediarios (mosquitos); para ellos una virulencia elevada puede suponer una ventaja selectiva.
El principio halla una aplicación directa en el control de las infecciones en los hospitales, donde las manos del personal sanitario pueden actuar de vectores que seleccionen las cepas virulentas.

En el cólera la red pública de distribución del agua y alcantarillado desempeña el papel de los mosquitos. 
Si el agua destinada al consumo o a la higiene se contamina con heces de los enfermos, la selección tiende a incrementar la virulencia de la bacteria.
Cuanto mayor es la diarrea. mayor ser la diseminación del organismo. aunque el sujeto hospedante muera.
Ewald ha demostrado que, al mejorar la higiene, la selección actúa en contra del biotipo clásico de Vibrio cholerae y a favor del más benigno El Tor.
En estas condiciones, la muerte del huésped es un callejón sin salida y un enfermo menos afectado y más móvil, capaz de contagiar a otros durante un período mucho más largo, es un vector eficaz para propagar el patógeno de baja virulencia.
En otro ejemplo, una mayor higiene favorece que Shigella sonnei desplace a la más agresiva S. flexneri .

Estas consideraciones pueden ser de utilidad a la hora de diseñar las políticas sanitarias.
A propósito del virus de la inmunodeficiencia humana (VIH) la teoría de la evolución prevé que con agujas limpias y medios profilácticos se consigue más que mediante tratamientos terapéuticos.
Si nuestra misma conducta puede frenar las tasas de transmisión del VIH, las cepas que no acaben en poco tiempo con el huésped adquirirán ventaja selectiva a largo plazo; por contra, los virus más virulentos, los que acaben presto con la vida del huésped, verán limitada su capacidad de diseminación.
Las medidas públicas pueden cambiar la naturaleza del VIH.

La pugna no se limita a los patógenos.
Los grandes depredadores representaron antaño una auténtica amenaza para el hombre.
Salvo en contados puntos, los grandes carnívoros han dejado de ser un riesgo.
Mayor peligro revisten serpientes y arañas venenosas. 
Aunque resulta paradójico que las fobias provocadas por estas criaturas produzcan más daño que cualquier tropiezo real con ellas.
Mucho más peligrosos que los predadores o los organismos venenosos son otros miembros de nuestra misma especie.
Nos atacamos unos a otros, pero no para conseguir carne, sino por la pareja, el territorio o los recursos.

Los conflictos violentos entre los individuos se dan sobre todo en jóvenes, que se agrupan en organizaciones que respalden sus intereses.
Los ejércitos, cuyas filas se nutren también de hombres jóvenes, tienen objetivos parecidos y con un coste desmesurado.

Incluso las relaciones humanas más íntimas pueden convertirse en fuente de conflictos con secuelas médicas. 
Al principio, madre e hijo parecen compartir los mismos intereses, pero pronto divergen.
Robert L., Trieves señala en un artículo clásico publicado en 1974 que, cuando el niño tiene unos pocos años de vida, los intereses genéticos de la madre pueden quedar mejor satisfechos con una nueva gestación, mientras que su retoño se beneficia de que se prolonguen los cuidados. 
Antes incluso del parto, ya existe un conflicto.
Desde el punto de vista de la madre, el tamaño óptimo del feto es algo menor de lo que sería ideal para padre e hijo.
De acuerdo con David Haig, de la Universidad de Harvard, el conflicto da pie a una carrera armamentística entre el feto y la madre por la regulación de los niveles de tensión arterial y de glucosa en sangre.
A veces, el resultado es la aparición de hipertensión y diabetes del embarazo.

Un paseo por cualquier hospital recoge testimonios más que suficientes de las enfermedades que la humanidad se ha buscado. 
Los ataques al corazón, por ejemplo, se deben en su mayoría a aterosclerosis, un problema de nuestro siglo y raro antaño.
La investigación epidemiológica proporciona la información necesaria para controlar la enfermedad.
Bastaría con limitar la ingesta de grasas, comer mucha verdura y realizar ejercicio diario.
Sin embargo, proliferan las hamburgueserías, las comidas dietéticas acumulan polvo en los estantes y los aparatos de gimnasia sólo sirven de improvisados roperos.
En Estados Unidos, la proporción de obesos, cifrada en un tercio, sigue aumentando.
Si sabemos lo que nos conviene, 
¿por qué tomar decisiones que perjudican? 

Las elecciones relativas a la dieta y el ejercicio dimanan de cerebros diseñados para hacer frente a un entorno distinto del que nos rodea hoy en día.
La estructura humana se ajustaba mejor a la sabana africana, donde la grasa, la sal y el azúcar eran escasos y preciosos.
Los individuos con tendencia a consumir grandes cantidades de grasa cuando se les presentaba la rara ocasión tenían una ventaja selectiva.
Para ellos era más probable sobrevivir a las hambrunas que diezmaban a sus estilizados compañeros. 
Nosotros, que descendemos de ellos, conservamos esa urgencia por hartarnos, cuando ya no escasea la comida.
La humanidad ha creado una sociedad que nada en leche y miel, para ver cómo ese éxito se ha trocado en origen de enfermedades modernas y de muertes precoces.

Tenemos también acceso cada vez más franco a diversos tipos de drogas, en particular alcohol y tabaco, culpables de una elevada proporción de enfermedades, de la carestía de la sanidad y de muertes anticipadas.

Si bien los individuos han recurrido siempre a substancias psicoactivas, el problema se ha agravado con la disponibilidad de drogas concentradas y vías de administración directas, especialmente mediante inyección.
La mayoría de estas sustancias - nicotina, cocaína y opio, por ejemplo- son productos de la selección natural para proteger de los insectos a las plantas.
Los humanos compartimos con los insectos un bagaje genético común y la mayoría de estas substancias también afectan a nuestro sistema nervioso.

No cabe, pues, restringir la vulnerabilidad a individuos problemáticos o a sociedades desordenadas.
Todos nosotros nos hallamos expuestos en razón de la larga historia de interacción de nuestra bioquímica con las drogas. 

El incremento rápido y reciente de la incidencia de cáncer de mama probablemente obedezca también a cambios operados en el entorno y en los hábitos de vida; sólo unos pocos casos serían atribuibles a aberraciones genéticas.
De acuerdo con los trabajos del grupo de Boyd Eaton, de la Universidad de Emory, las tasas de cáncer de mama en las sociedades «no modernas» actuales representan sólo una pequeña fracción de la registrada en Estados Unidos.
Plantean la hipótesis de que el lapso que transcurre entre la monarquía y la primera gestación es un factor de riesgo crucial, al igual que el número total de ciclos menstruales.
En sociedades de cazadores-recolectores, la monarquía ocurre a los quince años o más tarde, seguida en pocos años por una gestación, dos o tres años de puerperio y de nuevo, en poco tiempo, otra gestación.
La mujer sólo menstrúa desde el final del puerperio hasta la siguiente gestación; durante este período las hormonas alcanzan niveles elevados que pueden lesionar las células mamarias.

En las sociedades modernas la menarquía ocurre a los doce o trece años, probablemente, al menos en parte, por una ingesta de grasa suficiente como para permitir a una mujer jovencísima alimentar al feto, mientras que la primera gestación tendrá lugar en décadas sucesivas o tal vez nunca.
Una cazadora-recolectora tendrá un total de ciento cincuenta ciclos, mientras que en las sociedades modernas una mujer normal sumará 400 o más ciclos. 
Pocos se atreverían a propugnar que las mujeres quedaran embarazadas durante la adolescencia para prevenir el cáncer de mama, pero la administración de dosis importantes de hormonas para simular la gestación podría reducir el riesgo.
En la Universidad de California en San Diego se llevan a cabo ensayos clínicos para comprobar esta hipótesis.

Toda adaptación implica un compromiso.
Si los huesos del brazo triplicaran su diámetro actual, resistirían casi cualquier fractura, pero el Homo sapiens quedaría reducido a un armario ambulante ávido de calcio.
A veces sería útil que el sentido del oído fuera más fino, pero nos distraeríamos con el sonido de las moléculas de aire chocando contra el tímpano.

Otro tanto plantea la genética.
Si una mutación ofrece una ventaja neta en la reproducción, su frecuencia en la población tender a aumentar, aunque conlleve un aumento de la vulnerabilidad a alguna enfermedad.
Las personas con dos copias del gen de la anemia falciforme padecen dolores terribles y mueren jóvenes.
Los individuos con dos copias «normales» tiene una alto riesgo de muerte si contraen la malaria.
Pero los individuos con una copia de cada gen están protegidos contra la malaria y contra la anemia.
Allá donde se impone la malaria, estos individuos se hallarán mejor adaptados, en el sentido darwinista, que los individuos de los otros dos grupos.
El gen de las células falciformes se selecciona en lugares exentos de malaria, aunque va asociado a otra enfermedad.
¿Cuál es el alelo «sano» en este ambiente? 
Es una pregunta sin respuesta.
No existe un genoma humano que se pueda considerar «normal», sólo hay genes.

Muchos de los genes cuya presencia se asocia a alguna enfermedad tienen que haber reportado algún bien, al menos en ciertos entornos, o no serían tan comunes.
La fibrosis quística mata a uno de cada 2500 caucasianos, por lo que los genes responsables deberían tener una alta probabilidad de desaparecer del acervo génico.
Durante años, los investigadores han atribuido una presumible ventaja a la persistencia del gen de la fibrosis quística, igual que en el caso de la anemia de células falciformes.
Gerald B., Pier, de la Facultad de Medicina de Harvard, respalda ese razonamiento.
Los individuos con una copia del gen muestran una menor probabilidad de contraer fiebre tifoidea, que tiempo atrás provocaba una mortalidad del 15 por ciento. 

El envejecimiento puede ser el ejemplo definitivo de compromiso genético.
En 1957, uno de nosotros (Williams) propuso que los genes causantes del envejecimiento, y la muerte andando el tiempo, podrían seleccionarse si ejercieran otros efectos que ofrecieran algún tipo de ventaja en la juventud, época en que la fuerza de la selección es mayor.
Pensemos en un hipotético gen que regulara el metabolismo del calcio de forma que favoreciera una reparación rápida de los huesos y a la vez determinara el depósito progresivo de calcio en las paredes arteriales.
Dicho gen podría seleccionarse, aunque por ello murieran algunas personas ancianas. 
La influencia de genes pleiotrópicos (los que tiene muchos efectos), estudiada en moscas de la fruta y en escarabajos de la fruta, se desconoce todavía en humanos.
En este contexto, la gota reviste un interés especial.
Aparece cuando se forman cristales de ácido úrico, un potente antioxidante, que precipitan en las articulaciones. 
Los antioxidantes frenan el envejecimiento y se ha observado que los niveles plasmáticos de ácido úrico en distintos primates guardan correlación con el promedio de vida adulta.

Tal vez los altos niveles de ácido úrico benefician a la mayoría de los humanos enlenteciendo el envejecimiento de los tejidos.
La factura la pagan algunos en forma de gota.

Otros rasgos contribuyen al envejecimiento prematuro.
Unas defensas inmunitarias potentes previenen la infección, pero también producen un daño continuado de baja intensidad a los tejidos.
Cabe, por supuesto, la posibilidad de que la mayoría de los genes que producen el envejecimiento no reporte beneficio alguno en ninguna etapa de la vida.
Sencillamente nunca habría decrecido la capacidad de reproducción en el ambiente natural en la medida necesaria para forzar su selección negativa.
De aquí a unos decenios se identificarán genes responsables del adelantamiento de la senescencia.
La ciencia podrá obstruir su acción, si no revertir su curso.
Pero antes de intervenir habrá que averiguar si esos genes confieren alguna ventaja en épocas tempranas de la vida. 

La evolución actúa en el sentido de la flecha del tiempo.
El diseño de un organismo está limitado por las estructuras preexistentes.
El ojo de los vertebrados está organizado al revés del órgano del calamar, donde vasos sanguíneos y nervios están situados por fuera, penetran lo necesario y fijan la retina para evitar su desprendimiento.
El ojo humano es cuestión de mala suerte:
hace cientos de millones de años, la capa de células de nuestros antepasados que devino sensible a la luz estaba en una situación opuesta a la correspondiente capa de los antepasados del calamar.
Cada diseño evolucionó de forma independiente por un camino que no admite retorno.

La dependencia que ata al camino recorrido explica por qué la mera deglución puede resultar peligrosa para la vida.
Nuestros aparatos respiratorio y digestivo se cruzan porque en un pez pulmonado que nos precedió la abertura respiratoria superficial estaba colocada en lo alto del morro, como es natural, y conducía a un espacio que compartía con el aparato digestivo.
La selección natural no puede partir de cero y los humanos vivimos con la amenaza constante de que la comida se quede atravesada en la entrada de los pulmones.

El proceso de la selección natural puede también desembocar en un callejón sin salida, fatal a veces, al menos en potencia.
Es el caso del apéndice, un vestigio de una cavidad que en nuestros antepasados se empleaba para la digestión y ha tiempo que no desempeña tal función.
Dado que su infección puede tener un desenlace letal, cabría suponer que la selección forzara su desaparición.
Pero la realidad es más compleja.
La apendicitis aparece cuando una inflamación causa un aumento del tamaño del apéndice y la arteria que lo irriga queda comprimida.
El flujo sanguíneo protege de la infección bacteriana y su compromiso la favorece.
La infección provoca, a su vez, una mayor inflamación.
Cuando el aporte de sangre queda cortado, las bacterias tienen vía libre para proliferar y el apéndice puede llegar a perforarse.
Un apéndice delgado es especialmente vulnerable a esta cadena de acontecimientos, así que la apendicitis induce la selección de apéndices grandes.
Lejos de considerar el cuerpo algo perfecto, el análisis evolutivo revela que portamos legados muy desafortunados que, en algunos casos, la misma selección natural se encarga de perpetuar.

Pese a la potencia del paradigma darwinista, dista mucho de haber adquirido la biología evolutiva estatuto de disciplina básica de la medicina.
La enfermedad supone un alejamiento de la idoneidad y alguien podría aducir que la selección explica la salud, no la enfermedad.
El enfoque darwinista sólo tendrá sentido si se cambia el objeto de la explicación.
Para ello hay que centrarse en las características que nos hacen vulnerables a la enfermedad, no en ésta.
Tampoco es verdad que la selección natural potencie la salud; lo que maximiza es el éxito reproductor de los genes.
Los genes que favorezcan ese éxito reproductor abundarán más, aunque comprometan en última instancia la salud del individuo.

El desarrollo histórico y, en no menor medida, las interpretaciones descarriadas han dificultado la aceptación de la medicina evolutiva.
En el enfoque evolutivo del análisis funcional podrían sospecharse reminiscencias de teleologías ingenuas o del vitalismo, desterrados ya del pensamiento médico.

Por último, allá donde evolución y medicina aparecen juntas asoma el espectro de la eugenesia.
El enfoque darwinista facilitar un mejor conocimiento del cuerpo humano que reportar beneficios a los individuos.
Ello no significa que podamos o siquiera debamos plantearnos mejorar la especie.
Antes bien, la aproximación evolutiva sugiere que aparentes defectos genéticos pueden esconder un significado adaptativo inadvertido, que no existe un genoma «normal» y que las nociones de «normalidad» son simplistas.

La aplicación sistemática de la biología evolutiva a la medicina es una empresa novedosa.
Al igual que la bioquímica de principios de siglo, la medicina darwinista necesitará un período de desarrollo en la incubadora hasta que pueda mostrar su potencia y rendimiento. 

El punto de vista evolutivo descubre las profundas conexiones entre los estados de enfermedad y normal.
Es capaz de integrar líneas de investigación alejadas entre sí, dar pie a nuevas ideas y plantear áreas inéditas de investigación.
Su utilidad y poder han de desembocar en el reconocimiento de la biología evolutiva como una ciencia médica básica.

Limitaciones

Ejemplo:
el diseño del ojo humano comporta la existencia de una mancha ciega y permite que la retina se desprenda.
El ojo del calamar no presenta estos problemas. 

Defensas

Ejemplo:
la tos o la fiebre no son alteraciones, sino expresión de las defensas del cuerpo.

Compromisos

Ejemplo:
el diseño exagerado de cualquier sistema, como un par de brazos irrompibles, desorganizaría el funcionamiento del cuerpo.

Conflictos

Ejemplo:
los humanos se hallan en lucha constante con otros organismos que están bien ajustados por la evolución. 

Nuevo ambiente

Ejemplo:
la exposición del cuerpo humano a un medio donde abunda lo que antes era escaso, como la comida rica en grasas, es muy reciente.

Figura 1

EL PRINCIPIO DEL DETECTOR DE HUMOS que rige la activación de nuestras defensas es responsable de inconveniencias innecesarias e inevitables 

El precio que hay que pagar por una falsa alarma - una fuerte reacción, como el vómito, que se produce en ausencia de amenaza real para la vida- es una situación desagradable, aunque pasajera.
Pero un peligro real- una toxina en la dieta- inadvertido puede suponer la muerte.
La falta de respuesta defensiva durante la gestación puede tener consecuencias muy serias para el feto.

Evolución de la virulencia

Los cambios de la virulencia de un agente infeccioso están relacionados con el momento evolutivo y el mecanismo de transmisión. 
Paul E., Ewald, del Colegio Amherst, ha determinado que la transmisión que requiere contacto directo suele llevar al patógeno a reducir su virulencia.
De esta forma el huésped puede interaccionar con otros.
En cambio, los intermediarios encargados de diseminar a los microorganismos, incluso a partir de huéspedes totalmente incapacitados, pueden redoblar la virulencia.
La modificación de los hábitos, como la profilaxis sexual, también puede determinar cambios en el patógeno.

- (mosquitos, manos de los profesionales de la salud, suministros de agua potable con deficiente higiene)
- Sexo sin protección: promiscuidad


- (estornudo, tos, contacto cutáneo)
- Sexo con protección; monogamia


Ambientes nuevos, peligros nuevos 

- Accidentes
- Inanición
- Predadores
- Enfermedades infecciosas


- Infarto y otras complicaciones de la aterosclerosis 
- Cáncer
- Otras enfermedades crónicas asociadas a los hábitos de vida y la longevidad
- Diabetes no insulinodependiente
- Obesidad
- Nuevas enfermedades infecciosas


Figura 2

LA PRESION de la evolución actúa en contra de los apéndices pequeños (arriba), porque la inflamación puede comprometer el flujo sanguíneo protector, empeorando el pronóstico de la infección

La evolución tiende a seleccionar apéndices grandes.

Algunos principios de la medicina darwinista 

El enfoque darwinista propicia una nueva perspectiva.
Los principios siguientes dan fundamento a la consideración de la salud y la enfermedad en un contexto evolutivo:

Tabla 1

Las DEFENSAS y las ALTERACIONES son dos manifestaciones de enfermedad fundamentalmente diferentes.
La SUPRESION de las defensas tiene ventajas e inconvenientes.
La selección natural ha determinado la regulación de los sistemas de defensa según el PRINCIPIO del DETECTOR de HUMOS , lo que implica que la activación de las defensas y las molestias que se le asocian sean a menudo innecesarias para el sujeto.
En la epidemiología moderna desempeña un importante papel el desajuste entre el DISEÑO FISIOLÓGICO y los ASPECTOS NUEVOS de nuestro ambiente.
Nuestros DESEOS , aunque encaminados en nuestro ambiente original a llevarnos a acciones de mayor éxito reproductor, ahora nos abocan a la enfermedad y a la muerte prematura.
El cuerpo es un cúmulo de COMPROMISOS .
No existe el «cuerpo NORMAL ».
No existe el «genoma humano NORMAL » .
Algunos GENES que se asocian a enfermedad pueden proporcionar también beneficios.
Otros sólo causan enfermedad cuando interaccionan con factores ambientales nuevos.


El INTERES GENÉTICO guía las acciones individuales incluso a expensas de la salud y la longevidad del individuo creado por esos genes.
La VIRULENCIA es una propiedad de los patógenos; puede aumentar o disminuir.
Los SÍNTOMAS de una infección pueden beneficiar al patógeno, al huésped, a ambos o a ninguno.
La enfermedad es INEVITABLE debido a la forma en que la evolución caracteriza a los organismos.
Toda enfermedad precisa una EXPLICACIÓN INMEDIATA de por qué afecta a algunos individuos y a otros no y una EXPLICACIÓN EVOLUTIVA de por qué los miembros de una especie son vulnerables.
Las enfermedades no son productos de la selección, pero la mayoría de las VULNERABILIDADES que conducen a la enfermedad obedecen a causas evolutivas.
El envejecimiento es más un COMPROMISO que una enfermedad .
Las recomendaciones clínicas específicas deben basarse en ESTUDIOS CLÍNICOS; las intervenciones clínicas que sólo tienen fundamento teórico no se pueden considerar científicas y pueden resultar lesivas.



EL PODER DE LOS MEMES

Las conductos e ideas copiados por imitación de una persona a otra -los memes- podrían haber obligado a los genes a hacer de nosotros lo que hoy somos 

Figura 1

COPIADOS DE UN CEREBRO A OTRO, los memes se difunden a través de la sociedad.Evolucionan a ciegas a medida que configuran nuestra cultura, piensa la autora. 

INTRODUCCIÓN

Las personas dedican mucho tiempo a copiar y transmitir unas entidades denominadas memes.
Un meme es una idea, un comportamiento, un estilo o una manera de ser que se propaga de un individuo a otro en el seno de una cultura.

Cuando las personas se saludan dándose la mano, cantan "Cumpleaños feliz" o depositan su voto en las urnas, están difundiendo memes.

En eso no hay discusión.
Pero la polémica se ha desatado a raíz de la tesis de la psicóloga Susan Blackmore.
Detallada a lo largo de este artículo, propone que la misteriosa capacidad de los humanos para imitar, y transmitir, pues, los memes, es lo que nos distingue de las demás especies.
Los memes, aduce, han sido y siguen siendo un factor poderoso en la conformación de nuestra evolución cultural y biológica.
Intervienen en el debate suscitado Lee Alan Dugatkin, ecólogo, Robert Boyd, antropólogo, Peter J. Richerson , genético de poblaciones, y Henry Plotkin, psicólogo.

El hombre es un animal extraño.
La teoría evolucionista explica de un modo brillante los rasgos que compartimos con otros, desde el código genético que dirige la construcción del cuerpo hasta los mecanismos de funcionamiento de músculos y neuronas.
Pero nuestra especie se singulariza en infinitos aspectos.
Poseemos un cerebro de tamaño excepcional, tenemos un lenguaje gramatical, componemos sinfonías, conducimos coches, comemos fideos con tenedor y nos interrogamos sobre el origen del universo. 

El problema estriba en que tales aptitudes parecen desproporcionadas para nuestras necesidades, las que hemos de satisfacer para sobrevivir. 
Como señalaba Steven Pinker, del Instituto de Tecnología de Massachusetts, en How the mind works , "por lo que a la relación de causa a efecto concierne en el dominio de la biología, la música no sirve para nada".
Podríamos decir lo mismo del arte, del ajedrez o de la matemática pura.

La teoría evolucionista clásica, la darwinista, que se ocupa de los caracteres hereditarios de los organismos, no puede aportar una justificación directa de tales dones. 
Dicha teoría, expresada en términos actuales, sostiene que los genes controlan los caracteres de los organismos. 
En el transcurso de las generaciones, los genes que otorgan a sus portadores una ventajosa supervivencia y favorecen una descendencia numerosa (heredera de tales genes), tienden a proliferar a expensas de otros.
Los genes compiten, pues, entre sí; los que se impongan, pasarán de una generación a la siguiente.

Muy contados biólogos rechazarían la teoría darwinista.
Pero no explica la razón por la cual los humanos han invertido tantos recursos en aptitudes superfluas para la tarea biológica principal, es decir, para la propagación de los genes.
¿Qué otra teoría explicaría tales comportamientos?

En los memes, defiendo aquí, se halla la respuesta. 
Los memes son relatos, canciones, hábitos, técnicas, inventos y conductas que los individuos adquieren por imitación. 
La teoría de la evolución explica la naturaleza humana siempre y cuando integre la transmisión de los memes, además de la de los genes.

Por sugestivo que sea considerar simples "ideas" los memes, resulta más apropiado entenderlos como una vía de información. 
(Los genes son también información: instrucciones escritas en el ADN para sintetizar proteínas.)
Así, el meme de las ocho primeras notas de la melodía de La Verbena de la Paloma puede grabarse no sólo en las neuronas de una persona (que las reconocerá en cuanto las oiga), sino también en la banda magnética de una videocasete o en los signos escritos en una partitura. 

El nacimiento de los memes

Aunque el concepto de meme se gestó hace unos 25 años, no se advirtió su poderosa virtualidad en la evolución humana hasta ayer mismo Richard Dawkins, de la Universidad de Oxford, acuñó el término en 1976 en su libro El gen egoísta , donde describía el principio básico de la teoría darwinista en función de tres procesos generales:
la replicación de una información, las variaciones surgidas y la selección de unas variantes frente a otras, eso es la evolución.
Tras sucesivas iteraciones del ciclo, la población de copias supervivientes irá adquiriendo de forma gradual nuevas propiedades que la habilitan para triunfar en el proceso de competición por la reproducción. 

Aunque se trata de un ciclo carente de intencionalidad, produce orden estructural a partir del caos.

Dawkins llamó "replicador" a la información copiada y señaló que el más conocido era el gen.
Pero se proponía resaltar que la evolución podía basarse en cualquier replicador y así, a modo de ejemplo, inventó la idea de meme.
La copia de memes entre personas es imperfecta, como lo es en la replicación de genes de padres en hijos.
Podríamos adornar un relato, olvidar una palabra de una canción, adaptar una técnica anticuada o crear una nueva teoría a partir de viejas ideas.
De todas estas variaciones, unas se copiarán muchas veces, en tanto que otras desaparecerán. 
Los memes son, pues, auténticos replicadores, dotados de las tres propiedades necesarias para engendrar un nuevo proceso de evolución darwinista: replicación, variación y selección.

Dawkins cuenta que abrigaba intenciones más modestas para su vocablo:
evitar que sus lectores pensaran que "el gen es el principio y final de la evolución, la unidad fundamental de la selección", pero su idea resultó pura dinamita.
Si los memes son replicadores, entonces competirán, igual que los genes, para ser copiados por mor de sí mismos .
Afirmación que contradice la hipótesis, defendida por la mayoría de los psicólogos evolucionistas, según la cual cumple a la cultura humana ayudar a la supervivencia de los genes. 
El fundador de la sociobiología E. O. Wilson pronunció la famosa frase de que los genes metían en cintura a la cultura.
La cultura podría evolucionar transitoriamente en determinada dirección refractaria a la difusión de los genes, mas, a la larga, regresaría a su sitio impulsada por la selección natural basada en los genes, lo mismo que el dueño da correa a su perro.
Bajo este punto de vista, los memes serían esclavos de los genes;
éstos construyen los cerebros que copian los memes, los cuales sólo prosperarían a través de la ayuda brindada a la proliferación de los genes.
Pero si Dawkins anda en lo cierto, es decir, si los memes son replicadores, entonces servirán a sus egoístas intereses, replicándose siempre que puedan.
Esculpirán nuestra mente y cultura, cualquiera que sea su efecto sobre los genes.

El ejemplo de los memes "víricos" ilustra ese fenómeno. 
Las cadenas epistolares, enviadas por correo tradicional y electrónico, contienen retazos de información escrita, que incluye una orden de "cópiame" acompañada de amenazas (si rompes la cadena, la mala suerte caerá sobre ti) o promesas (recibirás dinero y podrás ayudar a los amigos).
No importa que amenazas y promesas sean vacuas y el esfuerzo de copia sea empeño baldío. 
Estos memes tienen una estructura interna que asegura su propia difusión.

Para Dawkins, ocurre así con las grandes religiones del mundo.
Entre miríadas de cultos que han aparecido a lo largo de la historia, sólo unos pocos han logrado subsistir gracias, en su opinión, a las instrucciones del tipo "cópiame", acompañadas de promesas y amenazas. 
Se amenaza a los réprobos con la muerte o la condenación eterna y se promete a los fieles la bendición perdurable. 
El coste es una parte de los ingresos, una vida dedicada a propagar la palabra o recursos dedicados a la construcción de magníficas mezquitas y catedrales que promoverán los memes.
A veces los memes podrían impedir la propagación de los genes, con la imposición, por ejemplo, del celibato.

Por supuesto, no todos los cultos (ni las cartas en cadena) se propagan con el mismo éxito.

Algunas promesas y amenazas son más eficaces, o virulentas, que otras.
Todas compiten por captar la atención de los limitados recursos de la atención humana, frente a la experiencia y el escepticismo (que suponen una especie de sistema inmunitario en la metáfora del virus).

Las religiones no son, por supuesto, enteramente víricas; confortan e infunden el sentido de pertenencia a un grupo.
En cualquier caso, no todos los memes son víricos.
La mayoría conforma el material clave de nuestras vidas, incluidos el lenguaje, el sistema político, las instituciones financieras, la educación, la ciencia y la técnica. 
Todo esto son memes (o grupos de memes), que pasan por imitación de un individuo a otro y pugnan por subsistir en el limitado espacio de la memoria y la cultura.

El pensamiento memético alumbra una nueva visión del mundo.
En su óptica, cada ser humano es una máquina fabricante de memes, un vehículo de propagación, una oportunidad de replicación y unos recursos por los que competir.
No somos ni los esclavos de nuestros genes, ni agentes libres y racionales que crean cultura, arte, ciencia y técnica para nuestro propio disfrute.
Somos parte de un vasto proceso evolutivo en el que los memes son los replicadores evolutivos y nosotros, las máquinas de memes.

Esta nueva visión es notable e inquietante.
Llama la atención que una simple teoría abarque cultura, creatividad de la mente y evolución biológica. 
Inquieta que pretenda reducir retazos de nuestra humanidad, actividad y vida intelectual a un fenómeno carente de intención.
Pero,
¿está fundada la memética?
¿Pueden los memes ayudar a entendernos?
¿Pueden someterse a contrastación empírica?
¿Desempeñan alguna función científica?
Si nada de ello hubiera la memética sería un camelo.

Estimo que la idea del meme, en cuanto replicador, es el concepto que venía faltando en las teorías sobre la evolución humana.
La memética explica la singularidad de nuestra especie y la aparición de culturas y sociedades avanzadas.
Somos únicos porque, sólo nosotros, en el pasado, adquirimos la capacidad de una imitación generalizada.
Esa aptitud creó nuevos replicadores, los memes, que se propagaron utilizándonos como máquinas copiadoras, de la misma manera que los genes usan los mecanismos replicadores del interior celular.
En adelante, evolucionaremos bajo la acción de dos replicadores, los genes y los memes. 

En eso nos distinguimos de los millones de especies que pueblan el planeta.


MEMES Y GRUPOS MEMÉTICOS 
- Anécdotas, leyendas urbanas, mitos
- Vestidos, peinados, adornos corporales
- Cocina, fumar cigarrillos
- Aplaudir, brindar
- Lenguaje, acentos, lugares comunes
- Canciones, música, danzas
- Creer en ovnis, fantasmas, Santa Claus
- Frases racistas, chistes sexistas
- Religiones
- Inventos, teorías, ciencia
- Sistemas judiciales, democracia
- La historia de la magdalena de Proust


SIN MEMES
- Experiencias subjetivas, emociones complejas, percepciones sensoriales
- Comer, respirar, relaciones sexuales
- Conductas innatas aunque contagiosas: bostezar, toser, reír
- Respuestas condicionadas: temor ante el sonido del torno del dentista
- Mapas cognitivos: conocimiento del trazado del propio barrio 
- Asociaciones con sonidos y olores

Figura 2

EL DARWINISMO UNIVERSAL explica la evolución con un sistema de replicadores que presentan variación, selección y herencia.

La variación surge de la recombinación y de la copia imperfecta.
Se produce la selección cuando no hay recursos para todos los variantes. 
La herencia asegura la transmisión de las cualidades deseables. 
Desde este algoritmo ciego se arriba hasta entidades sumamente elaboradas.

Figura 3

EL DESARROLLO DE LA CULTURA partió probablemente en el momento en que nuestros antepasados homínidos aprendieron a imitarse entre sí ( izquierda ).

Los individuos mejor preparados para imitar las nuevas técnicas de supervivencia -el dominio del fuego, por ejemplo- prosperaron y primaron a los genes que les dota ron de mayor capacidad mimética.
Más adelante, cuando la imitación resultó ventajosa en términos de selección ( centro ), los genes debieron desarrollar estrategias para asegurar la imitación de las conductas más valiosas.
Con tácticas del estilo "imitar a los mejores imitadores" se lograrían copias fieles de nuevas técnicas de supervivencia, así como de conductas carentes de interés especial, como la ornamentación y engalanamiento.
Los buenos imitadores subirían en la escala social, seducirían y tendrían mayor descendencia, induciendo ulteriormente a los genes a desarrollar cerebros mayores, capacitados para elaborar procesos de mimesis.
La imitación se convertiría en parte intrínseca de la naturaleza humana.
Los memes, en permanente evolución, crearían de un modo gradual sistemas culturales acabados ( derecha ), construirían monumentos y realizarían sacrificios humanos sin beneficio genético, aunque propicios para la transmisión de los memes asociados [1] .

Los animales también imitan

Estoy de acuerdo con el empeño puesto por Susan Blackmore en infectar la mente de la gente con el meme encerrado en la afirmación "la imitación es importante". 
Pero discrepo de su idea de que los memes -las entidades imitadas- condicionen sólo la evolución del comportamiento humano.
En el reino animal, desde los peces hasta los primates, se copian unos a otros a la hora de decidir qué comer o con quién aparearse.
Los memes influyen en los hábitos de muchos animales con la misma razón que en el comportamiento humano.
La observación atenta de los mirlos nos descubre que los memes no son patrimonio único del hombre, ni siquiera del chimpancé y otros primates.
Pero antes habrá que aclarar qué entender por imitación .

Los psicólogos debaten sobre el significado de esa palabra.
Abundan los escritos donde se divide el significado en un rosario de subcategorías.
Pero en su relación con los memes, adoptemos de entrada la definición dada por Blackmore en su libro The Meme Machine .
Ofrece allí dos perspectivas diferentes.
La definición más restringida establece que la imitación abarca tres comportamientos complejos: decidir qué imitar, pasar del estatuto de observador al de imitador y producir una acción corporal adecuada. 
Bajo estos criterios tan estrictos, la imitación no se daría en el mundo animal.
Resulta extraordinariamente difícil averiguar si los animales pueden transformar un punto de vista en otro; además,
¿cómo saber qué deciden imitar?

Blackmore aduce otra definición mucho más lata, que ejemplifica en la transmisión de historias. "No es que imitemos a quien nos lo ha contado con sus gestos y frases, sino que copiamos lo esencial de la historia que, a continuación, alguien copia de nosotros", afirma.

Probablemente existen cientos de ejemplos de imitación animal que encajan en esta definición, y no es una excepción la manera en que los pájaros aprenden a reaccionar ante sus depredadores.

En 1978, Eberhard Curio y su grupo, de la Universidad de Bochum, montaron un escenario donde se desarrollaba la acción siguiente:
un mirlo podía ver a otro que batía alas y agitaba la cola ante la cercanía de un depredador. 
El segundo pájaro respondía a un verdadero depredador, un pequeño búho, pero el primer mirlo no podía ver al búho, celado por una serie de compartimentos. 
Mediante una serie de estratégicas manipulaciones, se obligó al observador a que pensara que su compañero estaba reaccionando ante la presencia de un ruidoso frailecillo, que, por lo común, no constituye ninguna amenaza para los mirlos.
A continuación, los investigadores situaron al mirlo observador frente a un frailecillo y también reaccionó con un movimiento agitado de plumas y cola. 
Curio y sus colegas descubrieron que el falso mensaje-"el frailecillo es un depredador"- podía difundirse en cadena hasta un número de seis mirlos.

Pero el simple hecho de que algo se copie no lo convierte en un meme.
Blackmore sostiene que un mensaje debe cumplir tres requisitos más:
debe copiarse con fidelidad, han de realizarse muchas copias y éstas tienen que perdurar largo tiempo.
El mensaje "el frailecillo es un depredador" se transmitió con exactitud y las copias del mensaje pasaron de un individuo a otro.
Aunque es imposible conocer la longevidad de este meme fundado en experimentos de laboratorio, en principio no parece haber motivo que impida, una vez asentado, su propagación definitiva en poblaciones de campo.

En mi trabajo habitual como ecólogo del comportamiento, me he encontrado con docenas de ejemplos sobre el comportamiento animal que encajan en la definición de meme, y no me extrañaría que su número total fuera ingente.
Los memes podrían revelarse más antiguos y fundamentales para la evolución biológica que lo alegado por Blackmore.
Y quizá la diferencia entre memes animales y humanos sea más cuantitativa que cualitativa.
Los partidarios de la teoría memética podrían defender la idea de que los memes animales son reales y con ello reivindicar su fuerza de importancia universal en la evolución.
Pero si los memes no nos diferencian de los animales, como Blackmore da a entender, la teoría pierde fuerza:
resultaría incapaz de explicar el carácter exclusivamente humano de la cultura.

Figura a

LOS MIRLOS aprenden a reconocer a los depredadores mediante la observación de qué es lo que les hace temblar a sus compañeros.

Por ese motivo, recelan a veces de otros que no les suponen ninguna amenaza.

Influencia de los memes sobre el cerebro

Tal evolución particular nos dotó de un cerebro poderoso, de un lenguaje y de nuestro "excedente" de aptitudes. 
La memética resuelve el tamaño singular del cerebro humano, que le viene dado por los genes: tres veces mayor, con relación al peso corporal, que el cerebro de nuestros parientes próximos, los primates.
Costoso en su formación y mantenimiento, muchas madres y bebés mueren en el parto por complicaciones derivadas de la magnitud de la cabeza.
¿Por qué ha permitido la evolución que el cerebro se desarrollara tanto?
Las teorías tradicionales lo atribuyen a la ventaja genética que redunda en la mejora de las habilidades para la caza y el forrajeo o en la capacidad de sustentar grupos cooperativos mayores, que redoblan las posibilidades de supervivencia individual.
La memética da una explicación totalmente diferente. 

La transición crítica de los homínidos fue, según esa teoría, la aparición de la imitación, surgida quizás hace dos millones y medio de años, antes del descubrimiento de los instrumentos de piedra y del crecimiento del cerebro.
La verdadera imitación consiste en copiar un comportamiento 0 una habilidad nuevos de otro animal.
Es una operación difícil que requiere inteligencia notable, insólita en el reino animal.
Aunque muchas aves reproducen cantos y las ballenas y los delfines imitan sonidos y acciones, la mayoría de las especies son incapaces de ello.
A menudo, la "imitación" animal, ejemplificada en el aprendizaje de respuesta al ataque de un nuevo depredador, estriba lisa y llanamente en una conducta innata.

La propia imitación de los chimpancés se limita a una pequeña serie de conductas, como los métodos para cazar termitas.
Por contra, la imitación generalizada de cualquier actividad observada -que los humanos realizan con absoluta naturalidad-es arte macho más difícil y, por ende, más valioso, con el que el imitador se beneficia del aprendizaje y del ingenio de otro.
En experimentos llevados a cabo en 1995 en el Centro Regional de Investigación de Primates de Yerkes, cuando se les planteaban los mismos problemas a orangutanes y a niños, sólo los humanos recurrían a la imitación para resolverlos.

Nuestros antepasados remotos imitaban las técnicas nuevas de encender el fuego, cazar, transportar y cocinar alimentos. 
Conforme se fueron difundiendo estos primeros memes, la capacidad para adquirirlos cobró importancia creciente de cara a la supervivencia.
En breve, las personas con mejores dotes para la imitación prosperaron y los genes que les concedieron los mayores cerebros necesarios para ello se difundieron en el acervo génico.
A partir de ahí, todos mejoraron en sus aptitudes miméticas, intensificando la presión por un aumento del cerebro en una suerte de carrera de armamentos cerebral.

Una vez comenzaron todos a imitar, se abrió camino el segundo replicador cambiando para siempre el curso de la evolución humana.
Los memes empezaron a tomar el control.

Además de aprender a encender el fuego y otras aptitudes útiles, se copiaron habilidades menos prácticas, como el adorno corporal o las casi gratuitas danzas de la lluvia, tan inútiles cuán dispendiosas de energía. 

Los genes se encontraron con el problema de asegurar que sus portadores copiaran sólo los comportamientos útiles. 
Los memes recién surgidos pueden propagarse por imitación a través de la población en una sola generación, a una velocidad, pues, muy superior a la tasa de evolución genética.
Para cuando los genes podían desarrollar una bien tramada predilección por encender el fuego y una aversión hacia la ejecución de danzas de la lluvia, pudieron aparecer y desaparecer novedades completamente diferentes.

Los genes pueden desarrollar sólo amplias estrategias a largo plazo para conseguir que sus portadores sean más exigentes en los hechos a imitar.

Los genes parecen haber conducido a los individuos a copiar sólo a los mejores imitadores, personas que poseyeran la versión más adecuada de los memes útiles.
(La expresión "mejores imitadores" se sustituiría ahora por "marcadores de tendencias".)
Además de su bagaje útil para la supervivencia, los mejores imitadores adquirirían un mayor rango social, multiplicando sus oportunidades de supervivencia y ayudando a difundir los genes responsables de su talento imitador, es decir, los genes que dieron lugar a sus grandes cerebros especializados en una imitación atenta.

Los genes habrían fomentado las preferencias innatas de las personas para elegir el objeto a imitar, pero la respuesta genética, que necesita de varias generaciones para aparecer, siempre iría muy por detrás de los avances meméticos. 
Al proceso a través del cual los memes controlan la selección genética lo llamo "guía memética": 
los memes compiten entre sí y evolucionan con rapidez en una dirección, en tanto que los genes deben responder mejorando la imitación selectiva, verbigracia, aumentando el volumen encefálico y su potencia.
De este modo, los memes triunfantes empezarían a decidir qué genes les serían favorables.
Los memes llevarían las riendas.

En un postrero viraje, se apostaría por emparejar los imitadores mejor dotados, pues se trataría de los que poseyeran las bazas óptimas para la supervivencia. 
Mediante este efecto la selección sexual, guiada por los memes, contribuiría al aumento de talla del cerebro. 
Al escoger al mejor imitador como compañero, las mujeres difundirían los genes necesarios para copiar los rituales religiosos, los vestidos llenos de colorido, las canciones, los bailes, la pintura, etc..
Por esa vía, el legado de la evolución memética del pasado se integraría en nuestra estructura cerebral, para convertirnos en personas con criterios musicales, artísticos o religiosos.
Nuestros grandes cerebros son ingenios de imitación selectiva construidos por y para los memes lo mismo que para los genes. 

Figura 4

POR TOMOGRAFIA se determinará si el cerebro humano ha evolucionado hacia la imitación y difusión de los memes.

En estos mapas se ha cartografiado la actividad nerviosa asociada a un movimiento específico de la mano.
Las mismas zonas revelan si el sujeto actuaba por voluntad propia ( en rojo ), por mera observación ( en azul ) o por imitación de otra persona ( en verde ).
La imitación producía la actividad más intensa.
Los resultados sugieren que la región de Broca controla un "sistema neuronal especular" que ha evolucionado en el sentido de imitar las acciones.
Los monos poseen, sin embargo, un sistema similar.

El origen del lenguaje

El lenguaje pudo haber sido otra exquisita creación del mismo proceso de coevolución entre el meme y el gen. 
Las disputas relativas al origen y función del lenguaje adquirieron tal acritud, que, en 1866, la Sociedad Lingüística de París decidió dejar de examinar toda hipótesis sobre el asunto.
Ni siquiera hoy existe acuerdo, si bien las teorías que gozan de mayor favor apelan a la ventaja genética.
Robin Dunbar, de la Universidad de Liverpool, sostiene que el lenguaje es un sustituto del mutuo acicalamiento en el mantenimiento cohesionado de grupos sociales. 
Terrence Deacon, de la Universidad de Boston, opina que el lenguaje hizo posible la comunicación simbólica, que, a su vez, permitió mejorar la destreza cazadora reforzó los lazos sociales y facilitó la defensa en grupo.

La teoría de la guía memética justifica el lenguaje por la ventaja que otorga a la supervivencia de los memes.
Para comprender la razón de ello, hemos de preguntarnos qué tipo de memes sobrevivirían mejor y se difundirían en el acervo emergente de memes en los comienzos de la historia humana.
Para cualquier replicador, la respuesta general serían los memes dotados de fecundidad, fidelidad y longevidad muy altas, vale decir, los memes que obtienen muchas copias precisas y perdurables de sí mismos.

Los sonidos son más fértiles que los gestos, en particular los de tipo "¡oiga!" o "¡mira!".
Cualquiera que se halle en el campo de audición, mire o no al profiriente, se girará.

La fidelidad de los memes hablados es mayor para los que están formados por unidades discretas de sonidos (fonemas) y divididos en palabras; se trata de una suerte de digitalización que reduce los errores de copia.
A medida que las distintas acciones y vocalizaciones fueron compitiendo por asentarse en el acervo memético, las palabras prosperarían y desplazarían de la comunicación a los memes peor adaptados.
Luego, encadenando palabras en órdenes diferentes, por adición de prefijos e inflexiones, se crearían nichos fértiles para nuevos memes hablados más complejos.
En suma, los sonidos replicables de alta calidad arrasaron a los más pobres.

Al observar el efecto de este proceso sobre los genes, vemos que, una vez más, los mejores imitadores (los individuos que se expresaran con mayor claridad) adquirirían un estatuto superior, se aparearían con las mejores parejas y tendrían mayor descendencia.
En consecuencia, los genes que determinaran capacidad de imitar los sonidos más inteligibles y reproducibles aumentarían en el acervo génico.
En mi opinión, a través de este proceso los sonidos vencedores -la base del lenguaje hablado-instaron a los genes a crear un cerebro que no sólo fuera grande, sino que además se adaptara para copiar estos sonidos articulados.
Resultó de ello la especial aptitud humana para el lenguaje.
Se pergeñó a través de una competición memética y una coevolución de memes y genes.

El proceso de la guía memética constituye un ejemplo de replicadores (memes) que evolucionan de forma concurrente con su máquina copiadora (cerebro).
No fue en la aparición de los memes cuando se dio el primer caso de evolución concurrente;
debió acontecer algo similar en las primeras etapas de la vida en la Tierra, cuando las primeras moléculas replicativas se desarrollaron y evolucionaron hasta constituir la molécula de ADN y todos sus mecanismos de reproducción celular asociados.

Igual que en la evolución de esa depurada máquina copiadora de genes, cabría esperar que apareciera una máquina copiadora de memes mejor.

Fue lo que sucedió.
El lenguaje escrito supuso un gran salto adelante en lo que se refiere a longevidad y fidelidad. 
La imprenta realzó la fecundidad.
Desde el telégrafo al teléfono celular, del lentísimo correo de postas al correo electrónico, del fonógrafo al DVD y desde el ordenador hasta Internet, la máquina copiadora se ha ido refinando, difundiéndose con celeridad y cantidad crecientes multitud de memes.
La revolución informática es el resultado esperado de la evolución memética.

Esta teoría memética depende de varias conjeturas que pueden someterse a contrastación, en particular la hipótesis de que la imitación requiere una gran dosis de potencia cerebral pese a lo fácil que nos resulta. 
La investigación, mediante técnicas de imagen, podría comparar entre los individuos que acometen una acción y los que los copian.
Contrariamente a lo que indica el sentido común, esta teoría postula que la imitación se lleva la parte más dura y, también, que deben hallarse implicadas en su ejecución zonas cerebrales de evolución más reciente.
Además, dentro de cualquier grupo de especies animales emparentadas, los dotados de una mayor capacidad para la imitación deberían tener cerebros más grandes.
La pobreza mimética de los animales limita el número de datos disponibles, pero pueden analizarse y compararse, de acuerdo con este criterio, ciertas especies de aves, ballenas y delfines.

La teoría de los memes trivializa el desarrollo de la cultura

Los genes son replicadores.
Se transmiten fielmente de padres a hijos y dirigen el funcionamiento de los seres vivos. 
Esta fiel transmisión posibilita la intervención de la selección natural:
los genes que ofrecen a sus portadores una mejor supervivencia o que éstos puedan reproducirse más rápidamente que los portadores de otros genes, terminarán por imponerse.
Otros mecanismos, así la mutación, desempeñan papeles determinantes en la evolución, pero la adaptación resulta en buena medida de los genes que se repliquen con celeridad mayor.
Esta regla tan simple encierra una potencia explicativa sorprendente y ha permitido a los biólogos comprender fenómenos tan dispares como los que han llevado a la formación de la pelvis humana o al ritmo de los cambios sexuales en los peces hermafroditas.

Según Susan Blackmore, las creencias e ideas-los memes en su terminología-son replicadores que, transmitidos fielmente por imitación, dirigen el comportamiento de las personas que los adquieren.
Siguiendo su tesis, la evolución de las ideas estaría también conformada por la selección natural, y los cambios culturales se deberían a memes de veloz replicación.

Concedámosle, a medias por lo menos, la razón. 

Los conceptos tomados prestados de la biología son útiles para el estudio de la evolución cultural: 
consiste la cultura en ideas almacenadas en una población de cerebros humanos, y hay mecanismos análogos a la selección natural que pueden intervenir en qué ideas deben propagarse y cuáles desaparecer.
Pero Blackmore comete probablemente un error al pensar que la evolución cultural puede explicarse por selección natural sólo.
Muy al contrario, se requiere el concurso de la psicología, la antropología y la lingüística para poner en claro los múltiples procesos que convergen en la creación de la cultura.

A diferencia de los genes, las ideas no se transmiten en su puridad.
La información contenida en el cerebro de una persona genera un comportamiento que otra persona trata de imitar tras inferir dicha información.
En el proceso transmisor se introducen errores porque las diferencias genéticas, culturales y sociales entre dos personas provocan que el imitador llegue a hipótesis falsas sobre los motivos de la conducta ajena.

Por tanto, los memes sufren una transformación sistemática en el paso de un sujeto a otro, un proceso totalmente distinto de la selección natural, que resulta de las diferencias en la velocidad de propagación de los memes.
Por culpa de esa transformación de los memes, los miembros de una generación adquirirían en ocasiones un meme diferente del compartido por el conjunto de la generación precedente.

David Wilkins, del Instituto Max Planck de Psicolingüística de Nimega, descubrió un ejemplo sencillo de transformación memética.
Observó que los estadounidenses de diferentes generaciones daban una explicación distinta del sufijo -gate .
Las personas de más de 40 años, asociaban -gate a un escándalo de Estado, con el matiz de encubrimiento o tapadera.
Conocieron, va adultos.
La presidencia de Richard Nixon e interpretaron constructos tales como Travelgate en el mismo sentido que el famoso Watergate.
Por el contrario, los americanos más jóvenes, aunque oyeran alguna vez el sufijo -gate vinculado a un escándalo federal, lo asociaban ya a cualquier tipo de escándalo. 
Esta transformación, adviértase, pudo ocurrir sin competencia de los memes alternativos.

Los genes se transforman también mediante mutaciones espontáneas.
Pero las mutaciones genéticas ocurren raramente, alrededor de una por cada millón de replicaciones.
Ejercen, pues, un efecto despreciable en las adaptaciones directas.
Si las mutaciones se presentaran más a menudo, cada diez replicaciones por ejemplo, su efecto se manifestaría sobre la selección de los genes más frecuentes.
Esa es la situación que se produce con las ideas, que se transforman rápidamente en su difusión.
En esta hipótesis, la evolución cultural conjuga los efectos de la transformación con los de la selección natural.

En la evolución de las ideas intervienen a su vez procesos no selectivos de diverso tipo.
Acostumbra suceder, por ejemplo, que una persona tome de otra una idea y la depure o modifique luego; o que sincretice varias creencias tras entrar en contacto con otras personas.
Para ofrecer una interpretación correcta de la evolución cultural hemos de considerar todos esos múltiples procesos que orientan los diversos aspectos de dicha evolución.
Debemos a los sociólogos avances notables al respecto.
William Labov, de la Universidad de Pennsylvania, ha descrito procesos sociales y psicológicos que han provocado cambios dialectales en el transcurso de las generaciones.
Albert Banduara, de la Universidad de Stanford, ha estudiado la influencia de la imitación en la adquisición de las ideas.

A lo largo los últimos cien años, los biólogos han desarrollado conceptos y herramientas matemáticas que contribuyen a esclarecer la interacción entre los numerosos aspectos que pergeñan la evolución de las poblaciones.
Mediante la combinación de estas ideas con estudios empíricos, podría explicarse la evolución de la cultura.

Figura b

LAS IDEAS suelen sufrir una transformación sistemática al pasar de una persona a otra o de una generación a la siguiente.

Las personas no se limitan a imitar

La teoría memética que presenta Susan Blackmore plantea dos problemas importantes.
El primero, su afirmación de que la cultura no es más que una colección de memes.
En el término cultura lo engloba todo, desde la sencillez del uso de una laja lítica hasta instituciones complejas, un banco por ejemplo.
El segundo problema lo entraña su idea de que todos los memes, y por ende todos los aspectos de la cultura, se difunden por imitación. 
Desde la psicología, ninguna de esas dos hipótesis es admisible.

A principios del siglo XX, Edward Thorndike definía la imitación como el aprendizaje de un gesto a partir de la observación de su ejecución; sigue vigente ese significado en la actual investigación psicológica. 

Si la palabra imitación se usa en esa acepción, las afirmaciones de Blackmore son triviales, ya que las acciones miméticas no transmiten nada con valor cultural.
Atarse los zapatos o lanzar una pelota no constituyen gestos de interés para la evolución cultural del hombre.

Pero si adopta la definición de imitación dada por Blackmore (cualquier forma de comunicación entre personas, desde transmitir la parte esencial de una historia hasta recordar las instrucciones de un manual leído hace una semana), entonces esta palabra se desdibuja y vacía de sentido.
Además, esta definición laxa de imitación no explica ni la existencia ni la evolución de la cultura, que supone mucho más que la simple repetición mecánica de acciones físicas.
La cultura humana entraña compartir conocimientos, creencias y valores. 

Toda cultura descansa sobre un conjunto de interpretaciones compartidas acerca del funcionamiento del universo, denominadas a menudo esquemas.
Las reglas que operan en un restaurante responden a un esquema clásico:
lugares donde se prepara la comida, se sirve en la mesa y luego se limpia todo a cambio de dinero.
Los niños adquieren la mayoría de los esquemas característicos de su cultura a través de una mezcla de gula informal, procedente de los adultos y compañeros, y de una serie de complejos mecanismos psicológicos que habilitan a la persona para el pensamiento abstracto.
La imitación, definida en rigor, no interviene en absoluto en tales mecanismos.

Las creencias y valores compartidos, llamados también constructos sociales, nos llegan de la misma forma compleja y mal entendida.
A diferencia de los esquemas, que describen entidades tangibles (los restaurantes), los constructos sociales sólo existen porque las personas se ponen de acuerdo en que lo sean.
El dinero es un constructo social.
La justicia también.
Algunos de ellos poseen carnalidad física, como el billete de banco o la moneda pico, pero todos ellos trascienden su ropaje físico para designar un acuerdo mental respecto a su significado.
Sin un convenio sobre el valor de los billetes y las monedas, el dinero carece de valor.
Muchas creencias y valores regulan también las interacciones sociales.
En la mayor parte de la cultura occidental, por ejemplo, la justicia se basa en conceptos de equidad y propiedad.
Otras culturas definen a la justicia partiendo de ideas de servicio o de venganza.
En ningún caso se reduce a los juzgados, jueces y prisiones.

No abundan las investigaciones sobre la adquisición del pensamiento abstracto por los niños.
Obviamente, el lenguaje tiene algo que ver.
Es también muy significativa nuestra capacidad para darnos cuenta de que otras personas tienen intenciones y deseos, una capacidad que los psicólogos llaman "teoría de la mente".

La respuesta a la presión social-otro rasgo psicológico exclusivo de nuestra especie-constituye un poderoso motivo adicional para adherirse a las creencias y valores.
Una vez más, la imitación no tiene nada que ver con todo esto.
No podemos, aunque queramos, imitar a la justicia.
Llegamos a entender su significado paso a paso, a través de conversaciones, estudios, lecturas, películas y similares.

Blackmore sostiene que esta lenta acumulación de conocimientos depende de la imitación.
Pero las cosas no son tan lineales.
La investigación neurobiológica reciente indica que la imitación requiere mensajes específicos que deben computarse en zonas especializadas del cerebro.
Cuando un niño aprende el significado de la palabra restaurante o justicia, atraviesa por etapas psicológicas totalmente distintas de las que le permiten atarse los zapatos.

Los esquemas y los constructos sociales surgen del funcionamiento de la memoria y de la abstracción.
No tienen nada que ver con "aprender a ejecutar una acción mediante la observación de los hechos».
La aceptación y difusión de las ideas a través de la sociedad -especialmente de las nociones tan complejas como la de justicia- son lentas, impredecibles y difíciles de medir y, definitivamente, no pueden encuadrarse en la restrictiva teoría de los memes.
La cultura, entendida como una colectividad de cerebros y mentes, es el fenómeno más complejo de la Tierra.
Jamás lograremos entenderla si la abordamos con simpleza reduccionista.

Figura c

LOS CONOCIMIENTOS COMPARTIDOS, como las reglas que operan en los restaurantes, no pueden ser imitados.

Pruebas experimentales

Si el lenguaje se desarrolló en los humanos como consecuencia de una coevolución de memes y genes, los lingüistas deberían descubrir signos de que el lenguaje transmite memes dotados de fecundidad, fidelidad y longevidad notables mejor que vehicula la información sobre caza, formalización de contratos y otros asuntos específicos. 
Los experimentos propios de la psicología social deberían mostrar que los individuos prefieren imitar a las personas más elocuentes y las hallan más seductoras que las personas menos expresivas.

Se pueden someter a prueba otras predicciones de la teoría memética por medio de modelos matemáticos y simulaciones numéricas, que machos investigadores han utilizado para crear modelos de los procesos evolutivos.

La incorporación en el sistema de un segundo replicador más rápido debería introducir un cambio radical, análogo a la aparición de los memes y la expansión del cerebro humano.
Este segundo replicador debería controlar e incluso detener la evolución del primero.
Se podría recurrir a esos modelos para comprender con mayor detalle la coevolución de los memes y los genes.
Además, mediante simulación con robots imitadores de ruidos debería demostrarse que el lenguaje pudo haber surgido de forma espontánea en el seno de una población de imitadores.

La memética es una teoría nueva que lucha por encontrar su sitio rodeada de críticas.
Alguna de éstas no ha entendido la idea de replicador.

Debemos recordar que los memes, al igual que los genes, son meros fragmentos de información que pugnan por hacerse copiar o no se difundirán.
Sólo en este sentido, los memes pueden llamarse "egoístas" y tienen poder replicador.
Los memes no son entidades mágicas ni ideas platónicas flotantes, sino información contenida en recuerdos, acciones y artefactos humanos específicos. 
Tampoco abarcan todo el contenido mental, pues no todos los memes se han copiado de alguien.
Si se eliminasen nuestros memes, seguiríamos experimentando percepciones, emociones, imaginaciones y habilidades que nos son propias, no adquiridas de otras personas, ni podemos compartir.

Se objeta a menudo que los memes difieren por entero de los genes.
Y así es.
Evolucionan más deprisa y su dinámica no se halla encerrada en un sistema tan rígido como la replicación del ADN y la síntesis de proteínas.
En su naturaleza se parecen, más que a los genes, a unos nuevos replicadores, dotados de sus propios mecanismos de supervivencia y copia.
Los memes pueden copiarse en cualquier medio, de la palabra al papel, al libro, al ordenador o a otra persona.

Todavía persisten muchas voces críticas y queda macho trabajo por hacer.
A la postre, la memética sólo merecerá triunfar si aporta explicaciones mejores que sus teorías rivales y ofrece predicciones válidas y comprobables.
El gran complejo memético de la ciencia incluye métodos que descartan ideas vacuas, sin sentido o erróneas.
De acuerdo con tales criterios deberá enjuiciarse su valor.

Figura 5

LAS EMOCIONES despertadas por símbolos ideológicos dan idea de la importancia de los memes y de su poder sobre nuestro comportamiento.

[1] Nota: Muchas conductas humanas son complicadas mezclas de elementos innatos, aprendidos o imitados; por ejemplo, ir en bicicleta.


Herencia autosómica dominante y recesiva

Gran número de enfermedades genéticas importantes y bien conocidas son el resultado de una mutación en un único gen.
La edición de 1994 del Mendelian Inheritance in Man de McKusick enumera 6.678 rasgos por un único gen o monogénicos , definidos hasta la fecha en el ser humano.
De ellos, 6.188 se localizan en cromosomas autosómicos y 412 en el cromosoma X.
Los rasgos por un único gen constituyen el elemento central de gran parte de los avances que han permitido el rápido progreso actual de la genética médica.
En muchos casos se ha trazado el mapa de estos genes en localizaciones cromosómicas específicas, se ha obtenido su clonación y se ha determinado su secuencia. 

En este capítulo nos centraremos en las enfermedades por un único gen causadas por mutaciones en los cromosomas autosómicos.
Las enfermedades por un único gen causadas por mutaciones en los cromosomas sexuales constituyen el tema del capítulo 5.
Comentaremos los patrones de herencia de estas enfermedades en las familias, así como los factores que complican estos patrones.
En los casos apropiados, también se expone la base molecular de ciertas enfermedades genéticas.

También trataremos de los riesgos de transmisión de las enfermedades por un único gen a la descendencia, ya que suelen ser una preocupación importante para las parejas de riesgo.

CONCEPTOS BÁSICOS DE LA GENÉTICA FORMAL

Contribuciones de Gregor Mendel

Los rasgos monogénicos también se denominan rasgos mendelianos , en honor a Gregor Mendel, un monje austríaco del siglo XIX que dedujo varios principios genéticos importantes a partir de sus experimentos diseñados en guisantes.
Mendel estudió siete rasgos en los guisantes, cada uno de ellos determinado por un único gen.
Estos rasgos incluían atributos como la altura (plantas altas respecto a plantas bajas) y forma de la semilla (redondeada respecto a arrugada).
La variación de cada uno de estos rasgos está causada por alelos diferentes en loci individuales.

De la investigación de Mendel surgieron dos principios fundamentales.
El primero es el principio de segregación , que establece que los organismos que se reproducen sexualmente poseen genes que se presentan en parejas N que sólo un miembro de este par se transmite a la descendencia (es decir, se segrega).
El pensamiento predominante durante la época de Mendel era que los factores hereditarios de los dos padres se «fusionaban» en el descendiente.
En contraste con ello, el principio de segregación establece que los genes permanecen intactos y distintos.
Un alelo de la forma redonda de la semilla puede transmitirse a un descendiente que, a su vez, transmite el mismo alelo a su propia descendencia.
Si en vez de permanecer distintos los genes se mezclaran de algún modo en la descendencia, sería imposible trazar la herencia genética de una generación a la siguiente.
Así, el principio de segregación constituye un elemento clave en el desarrollo de la genética moderna. 

El principio de transmisión independiente de Mendel es su segunda gran contribución a la genética. 

Este principio establece que los genes de loci diferentes se transmiten de forma independiente.
Consideremos los dos loci antes mencionados. 
Un locus puede tener el alelo «redondeado» o el «arrugado» y el otro puede tener el alelo «alto» o el «bajo».
En un episodio reproductivo un progenitor transmitirá un alelo de cada locus a su descendencia.
El principio de transmisión independiente establece que el alelo transmitido en un locus («redondeado» o «rugoso») no tiene efecto sobre que alelo se trasmite en el otro locus («alto» o «bajo»).

Los experimentos de Mendel también demostraron que los efectos de un alelo pueden ocultar los del otro.

Llevó a cabo cruces (apareamientos) entre plantas de guisantes homocigotas para el gen «alto» (es decir, que tenían dos copias idénticas de un alelo que marcó como H ) y plantas homocigotas para el gen «bajo» (que tenían dos copias de un alelo denominado h ).
Este cruce, que sólo puede producir descendientes heterocigotos ( Hh ), se ilustra en la cuadrícula de Punnett de la figura 4.1.

Mendel descubrió que todos los descendientes de estos cruces, aun cuando fueran heterocigotos, serían altos.

Ello ilustra el hecho de que el alelo H es dominante .

Mientras que el alelo h es recesivo (se ha convenido en designar al alelo dominante en letras mayúsculas y al alelo recesivo en minúsculas). 
El término recesivo procede de una raíz latina que significa «ocultar».
Ello describe con claridad el comportamiento de los alelos recesivos; en los heterocigotos, las consecuencias de un alelo recesivo permanecen ocultas.
Mientras que un alelo dominante ejerce su efecto tanto en un homocigoto ( HH ) como en un heterocigoto ( Hh ), la presencia del alelo recesivo se detecta únicamente cuando aparece en forma homocigota ( hh ).
Por este motivo, las plantas cortas de guisantes se engendrarán sólo cuando las plantas progenitoras cruzadas sean portadoras al menos de un alelo ( h ).
Un ejemplo sería un cruce heterocigoto x heterocigoto, que se presenta en la figura 4.2.

El principio de segregación describe el comportamiento de los cromosomas en la meiosis.
Los genes de los cromosomas se segregan durante la meiosis y se transmiten como entidades distintas de una generación a la siguiente.
Cuando Mendel llevó a cabo sus experimentos fundamentales, no conocía los cromosomas, la meiosis, ni los genes (de hecho, este término no fue acuñado hasta 1909, mucho después del fallecimiento de Mendel).
Aunque su investigación se publicó en 1865 y se citó en algunas ocasiones, su importancia fundamental no fue reconocida hasta varias décadas después.
Sin embargo, las investigaciones de Mendel, que finalmente fueron reproducidas por otros autores al comenzar el siglo XX, constituyen el fundamento de gran parte de la genética moderna.

Las contribuciones fundamentales de Mendel fueron los principios de segregación y de transmisión independiente, así como su definición de dominancia y recesividad.

Figura 4-1

Cuadrícula de Punnett que ilustra un cruzamiento entre progenitores homocigotos HH y hh

Figura 4-2

Cuadrícula de Punnett que ilustra un cruzamiento entre dos heterocigotos Hh 

Conceptos básicos de probabilidad

La valoración del riesgo constituye una parte importante de la genética médica.
Por ejemplo, el médico o el consejero genético suelen informar a las parejas acerca de su riesgo de engendrar un hijo con un trastorno genético. 
Para comprender el modo en que se estiman estos riesgos deben comentarse algunos conceptos básicos de probabilidad. 
La probabilidad se define como la proporción de veces que ocurre un resultado específico en una serie de sucesos.
Así, podemos hablar de la probabilidad de obtener un 4 cuando se lanza un dado o la probabilidad de que una pareja tenga un hijo en vez de una hija.
Puesto, que las probabilidades son proporciones se distribuyen entre cero y uno, ambos inclusive.

Durante la meiosis, un miembro de un par de cromosomas se transmite a un determinado espermatozoide u óvulo. 
La probabilidad de que se transmita un miembro del par es de 1/2 y la probabilidad de que se transmita el otro miembro del par también es de 1/2 (obsérvese que para cualquier experimento, las probabilidades de todos los sucesos posibles deben sumar 1).
Puesto que esto es directamente análogo al lanzamiento de una moneda, en el que las probabilidades de que salga cara o cruz es de 1/2 para cada una, emplearemos el lanzamiento de una moneda como ejemplo ilustrativo.

Cuando una moneda se lanza al aire repetidas veces, el resultado de cada lanzamiento no tiene ningún efecto sobre los resultados siguientes.
Por este motivo, se considera que los sucesos son independientes .
Incluso si obtuviéramos una serie de 10 caras seguidas, la probabilidad de obtener cara o cruz en la siguiente tirada continuaría siendo de 1/2.
De modo similar, la probabilidad de que un progenitor transmite uno de los dos alelos de un locus es independiente de un episodio reproductor al siguiente.

El principio de independencia nos permite deducir dos conceptos fundamentales de la probabilidad: la norma de multiplicación y la norma de adición .
La norma de multiplicación establece que si dos ensayos son independientes, entonces la probabilidad de obtener un resultado determinado en ambos ensayos es el producto de las probabilidades de cada estudio.
Por ejemplo, podemos desear conocer la probabilidad de que un individuo obtenga dos caras en los lanzamientos consecutivos de una moneda.
Puesto que los lanzamientos son sucesos independientes, esta probabilidad está determinada por el producto de las probabilidades de obtener cara o cruz en cada uno de los lanzamientos: 1/2 x 1 2 = 1/4.
De modo similar.
La probabilidad de obtener dos cruces seguidos es 1/2 x 1/2 = 1/4.

La norma de multiplicación puede extenderse a cualquier número de ensayos.
Supongamos que una pareja quiera conocer la probabilidad de que sus tres hijos planificados sean de sexo femenino.
Ya que la probabilidad de engendrar una hija es aproximadamente de 1/2 y puesto que se trata de sucesos, en esencia, independientes (aunque pueden existir raras excepciones), la probabilidad de tener tres hijas es de 1/2 x 1/2 x 1/2 = 1/8. 
Sin embargo, si la pareja ya ha tenido dos hijas y entonces quiere conocer la probabilidad de engendrar otra hija, ésta es simplemente de 1/2.
Ello se debe a que los dos sucesos previos ya no son probabilidades sino que han ocurrido en realidad.
Debido a la independencia, estos sucesos no tienen ningún efecto sobre el resultado del tercer suceso.

La norma de adición establece que si se quiere conocer la probabilidad de un resultado u otro, simplemente podemos sumar las probabilidades respectivas.
Por ejemplo, la probabilidad de obtener dos caras en una serie (1/2 x 1/2 = 1/4) o la probabilidad de obtener dos cruces en una serie (1/4) se determina sumando las dos probabilidades: 1/4 + 1/4 = 1/2. 
En otro ejemplo, imaginemos una pareja que planea tener tres hijos y tiene una intensa aversión a que los tres sean del mismo sexo.
Pueden tranquilizarse en cierto modo sabiendo que la probabilidad de tener tres chicas (1/8) o tres chicos (1/8) es sólo de 1/4 (1/8 + 1/8).
La probabilidad de que engendren alguna combinación de chicos y chicas es, por lo tanto, de 3,4. ya que la suma de las probabilidades de todos los resultados posibles debe ser 1.

La probabilidad básica nos permite comprender y calcular los riesgos genéticos.
La norma de multiplicación se utiliza para estimar la probabilidad de que dos sucesos ocurran juntos.

La norma de adición se emplea para estimar la probabilidad de que ocurra un suceso u otro.

Frecuencias génicas y genotípicas 

La prevalencia de muchas enfermedades genéticas puede variar de forma considerable de una población a otra.
Por ejemplo, la fibrosis quística (CF), una grave enfermedad respiratoria (v. el comentario clínico 4.1), es común entre individuos de raza blanca, afectando aproximadamente a 1 de cada 2.500 nacimientos.
Es rara entre las poblaciones asiáticas, afectando sólo a 1 de cada 90.000 nacimientos.
Como se ha discutido en el capítulo 3, la anemia de células falciformes es común entre afroamericanos y afecta de modo aproximado a 1 de cada 600 nacimientos, y por ahora todavía no se observa casi nunca en descendientes de individuos de Europa septentrional. 
Los conceptos de frecuencia genotípica y de frecuencia génica nos ayudan a medir y comprender la variación en la población de los genes patológicos.

Imaginemos que efectuamos la tipificación del grupo sanguíneo MN de 200 individuos de la población. 
Este grupo sanguíneo, codificado por un locus en el cromosoma dos, tiene dos alelos principales, denominados M y N .
Se trata de un sistema en que pueden observarse los efectos de ambos alelos en el heterocigoto.

Por este motivo se considera que M y N son codominantes ; el heterocigoto puede diferenciarse de ambos homocigotos.
Cualquier individuo de la población puede poseer uno de los tres genotipos posibles (recuérdese que en el capítulo 3 se definió el genotipo como la constitución genética de un individuo en un locus ).

El individuo en cuestión puede ser homocigoto para M (genotipo MM ), heterocigoto ( MN ) u homocigoto para N ( NN ).
Tras efectuar la tipificación en todas las personas de nuestra muestra, encontramos la siguiente distribución de genotipos: MM , 64; MN , 126, y NN , 16.
A continuación se obtiene la frecuencia genotípica simplemente dividiendo cada recuento de genotipo por el número total de individuos.
La frecuencia de MM es 64/200 (= 0,32), la frecuencia de MN es 120/200 (= 0,60), y la frecuencia de NN es 16/200 (= 0,08).
La suma de estas frecuencias debe ser, naturalmente, igual a 1.

La frecuencia génica para cada alelo, M y N , puede obtenerse mediante el proceso de recuento del gen .

Cada homocigoto MM tiene dos alelos M , mientras que cada heterocigoto tiene un alelo M .
De forma similar, los homocigotos NN tienen dos alelos N y los heterocigotos tienen un alelo N .
En la muestra medida en este caso existen: (64 x 2) + 120 = 248 genes M (16 x 2) + 120 = 152 genes N .
En total, existen 400 genes en el locus MN (es decir, dos veces el número de individuos, ya que cada uno tiene dos alelos) Para obtener la frecuencia de M , efectuamos a continuación 248/400 = 0,62.
La frecuencia de N, 152/400, es de 0,38.
La suma de las dos frecuencias debe ser igual a uno.

Las frecuencias génicas y genotípicas especifican la proporción de cada uno de los alelos y de cada genotipo, respectivamente, en una población.
En condiciones sencillas es posible calcular estas frecuencias por recuento directo.

Comentario clínico 4-1 Fibrosis quística

La fibrosis quística (CF: Custic Fibrosis ) es uno de los trastornos por un único gen más frecuentes en Norteamérica y afecta aproximadamente a 1 de cada 2.500 recién nacidos de raza blanca.
En otros grupos étnicos es menos frecuente.
La prevalencia entre afroamericanos es de 1 de cada 17.000 y entre asiáticos se observa sólo en 1 de cada 30.000. 
Un número aproximado de 30.000 americanos sufre esta enfermedad.

En 1938 se identificó la CF por primera vez como una entidad patológica distinta y su primer nombre fue «fibrosis quística del páncreas».
Esta denominación se refiere a las lesiones fibróticas desarrolladas en el páncreas, uno de los principales órganos afectados por este trastorno.

Cerca del 85 % de los pacientes con CF tienen insuficiencia pancreática (es decir, el páncreas es incapaz de secretar enzimas digestivas, provocando desnutrición crónica). 

También se afecta el intestino y aproximadamente el 10 % de los recién nacidos con CF presentan íleo meconial (un espeso tapón de materia fecal que obstruye el colon).

Las glándulas sudoríparas de los pacientes con CF son anormales, lo que origina niveles elevados de cloro en el sudor.
Ésta es la base de la prueba de cloro en el sudor , que suele utilizarse en el diagnóstico de esta enfermedad.

Más del 95 % de los varones con CF son estériles debido a la ausencia u obstrucción del conducto deferente. 
El problema más importante que afrontan los pacientes con CF es la obstrucción de los pulmones por moco espeso y abundante.
Dada la imposibilidad de eliminar con eficacia este moco, los pulmones son muy susceptibles a las infecciones por bacterias como Staphylococcus aureus y de forma más grave Pseudomonas aeruginosa .
La infección crónica da lugar a la destrucción del parénquima pulmonar ocasionando por último la muerte por insuficiencia respiratoria.
Como resultado del empleo de antibióticos más potentes, de la fisioterapia torácica agresiva y de la terapia de sustitución de enzimas pancreáticas, durante las últimas tres décadas se han mejorado de modo notable las tasas de supervivencia de los pacientes con CF.
La supervivencia media en la actualidad es de 29 años. 
Esta enfermedad tiene una expresión muy variable; algunos pacientes experimentan dificultad respiratoria relativamente leve y supervivencia próxima a la normal.
Otros pacientes pueden padecer problemas respiratorios mucho más graves y sobrevivir sólo unos pocos años.

Investigadores de Londres, Toronto y Salt Lake City trazaron el mapa del gen de la CF en el cromosoma 7q en 1985 y 4 años más tarde se efectuó su clonación por investigadores en Michigan y Toronto.
Se trata de un gen de gran tamaño, con 250 kb y que incluye 27 exones.
La mayor parte de la investigación se ha dedicado a conocer este gen y su producto proteínico.
El producto proteínico, denominado «regulador de transmembrana de la fibrosis quística» (CFTR) interviene de manera evidente en el transporte de los iones de cloro por la membrana de las células epiteliales especializadas (como las que revisten el intestino y los pulmones). 
Existen pruebas de que el CFTR es el mismo canal del ion cloro.
La similitud del CFTR con las proteínas transportadoras de membrana, sugiere que éste puede intervenir en el transporte de los iones por la membrana celular.

El hecho de que el CFTR intervenga en el transporte de cloro nos ayuda a comprender los múltiples efectos de las mutaciones en el locus de la CF.
El transporte defectuoso de cloro provoca un desequilibrio salino, originando el moco obstructivo en pulmones y páncreas. 

También explica las anormalmente elevadas concentraciones de cloro en las secreciones sudoríparas de los pacientes con CF.

El análisis de la secuencia de DNA ha descubierto más de 300 mutaciones diferentes en el locus de CF.
La más común es la delación de tres bases que origina la pérdida de una fenilalanina en la posición 508 de la proteína CFTR.
Esta mutación se denomina « AF508 » (es decir, delación de la fenilalanina en la posición 508). 
Aunque existen muchas mutaciones diferentes de CF, la AF508 es responsable del 70 % de las mutaciones observadas entre los pacientes con ancestros de Europa septentrional.
Esta mutación, junto con otras relativamente comunes, es útil para el diagnóstico genético de la CF (v. cap. 11).

La identificación de la mutación especifica responsable de la CF en un paciente puede ser útil para predecir la evolución de la enfermedad.
Por ejemplo, los pacientes homocigotos para la mutación AF508 casi siempre presentan insuficiencia pancreática y suelen tener un grado relativamente grave de afectación respiratoria. 
Sin embargo, existen excepciones que indican la posibilidad de que factores adicionales (quizá genes en otros loci ) influyan en la expresión de la enfermedad. 

Además de aumentar nuestro conocimiento de la fisiopatología de la CF, la clonación del gen de la CF ha abierto la posibilidad de la terapia génica (v. cap. 11).
La investigación en sistemas experimentales ha demostrado que la inserción de genes normales en células con un transporte defectuoso de ion de cloro puede corregir esta alteración.
Ésta y otras investigaciones han suscitado el inicio de análisis clínicos en que se insertan genes normales en Adenovirus , que a continuación se introducen en los pulmones de pacientes con CF.
Se espera que los Adenovirus inserten el gen normal en las células epiteliales de las vías respiratorias, induciendo una función normal de los canales del cloro en estas células.
De este modo se podría finalmente lograr la curación de esta enfermedad, por lo general fatal.

Principio de Hardy Weinberg

El ejemplo antes empleado para el locus MN presenta una situación ideal para la estimación de la frecuencia génica; debido a la codominancia, es posible efectuar con facilidad el recuento y la diferenciación de los tres genotipos.
¿Qué sucede cuando uno de los homocigotos es indiferenciable del heterocigoto (es decir, cuando existe dominancia)?
En este caso se pueden utilizar los conceptos básicos de la probabilidad para especificar una relación predecible entre las frecuencias del génicas y las frecuencias genotípicas.

Imaginemos un locus que tiene dos alelos, denominados A y a .
Supongamos que, en una población determinada, conocemos la frecuencia del alelo A , que denominaremos p , y la frecuencia del alelo a , que denominaremos q .
A partir de estas premisas, deseamos determinar las frecuencias esperadas en la población de cada genotipo AA , Aa y aa .
Suponemos que los individuos de la población se emparejan al azar con independencia de su genotipo en este locus (el emparejamiento aleatorio también se denomina panmixia ).
Así, el genotipo no tiene ningún efecto sobre la selección de la pareja.
Si hombres y mujeres se emparejan de forma aleatoria se cumple la presunción de independencia.
Ello nos permite aplicar las normas de adición y multiplicación para calcular las frecuencias genotípicas.

Supongamos que la frecuencia p del alelo A en nuestra población es de 0.7.
Entonces, el 70% de los espermatozoides de la población deben tener el alelo A .
del mismo modo que el 70% de los óvulos.
Dado que p y q deben sumar 1, el 30 % de los óvulos y los espermatozoides deben ser portadores del alelo a (es decir, q = 0,30).
En condiciones de panmixia.
La probabilidad de que un espermatozoide portador de A se una a un óvulo portador de A está determinada por el producto de las frecuencias de genes: xxx (norma de multiplicación). 
Ésta es la probabilidad de engendrar descendencia con el genotipo AA .
Utilizando el mismo razonamiento, la probabilidad de engendrar descendencia con el genotipo aa es de xxx .

¿Qué sucede con la frecuencia de los heterocigotos en la población?
Los heterocigotos pueden formarse de dos modos distintos.
Un espermatozoide portador de A puede unirse a un óvulo portador de a , o bien un espermatozoide portador de a puede unirse a un óvulo portador de A .
La probabilidad de cada uno de estos sucesos está determinada por el producto de las frecuencias de genes, pq .
Puesto que deseamos conocer la probabilidad global de obtener un heterocigoto (es decir, el primer suceso o el segundo), podemos aplicar la norma de adición, sumando las probabilidades para obtener la frecuencia de heterocigotos, que es 2pq .
En la figura 4-3 se resumen estas operaciones.
Esta relación entre frecuencias génicas y frecuencias genotípicas fue establecida, de forma independiente, por dos autores, Godfrey Hardy y Wilhelm Weinberg, y se denomina principio de Hardy-Weinberg .

Como ya se ha mencionado, podemos utilizar este principio para estimar las frecuencias génicas y genotípicas cuando heterocigotos y homocigotos dominantes son indiferenciables. 
A menudo, éste es el caso para enfermedades recesivas como la fibrosis quística.
Únicamente se pueden diferenciar los homocigotos afectados, con genotipo aa .
El principio de Hardy-Weinherg nos dice que la frecuencia de aa debe ser q2 , Para la fibrosis quística en la población de raza blanca. xxx (es decir, la prevalencia de la enfermedad en recién nacidos).

Para estimar q efectuamos la raíz cuadrada en ambos lados de esta ecuación: xxx .
Puesto que xxx .
A continuación podemos calcular las frecuencias genotípicas de AA y Aa .
Este último genotipo, que corresponde a los portadores heterocigotos de la enfermedad, tiene un interés especial.
Calculamos que 2pq = 1/25 (obsérvese que ya que p es casi 1.0. podemos simplificar el cálculo redondeando p a 1,0: entonces, sin una pérdida significativa de precisión, xxx .
Ello nos dice algo bastante notable sobre la fibrosis quística y las enfermedades recesivas en general. 
Mientras que la incidencia de los homocigotos afectados es sólo de 1 por cada 2.500, los portadores heterocigotos de la enfermedades son mucho más comunes (uno de cada 25 individuos).
Por lo tanto, la gran mayoría de alelos de enfermedades recesivas permanecen, efectivamente, «ocultos» en los genomas de los heterocigotos.

En condiciones de panmixia, el principio de Hardy-Weinberg especifica la relación entre frecuencias génicas y frecuencias genotípicas.

Resulta útil en la estimación de las frecuencias génicas a partir de los datos de prevalencia de la enfermedad y en la estimación de la incidencia de portadores heterocigotos de genes de enfermedades recesivas.

Figura 4-3

Principio de Hardy-Weinberg 

Las frecuencias de población de los genotipos AA, Aa y aa se predicen según las frecuencias génicas y q).
Suele asumirse que las frecuencias génicas son las mismas en hombres que en mujeres.

Concepto de fenotipo

Se ha definido el genotipo como la constitución genética del individuo en un locus .
El fenotipo es lo que en realidad observamos clínicamente o físicamente.
Los genotipos no se corresponden de forma única con los fenotipos. 
Dos genotipos diferentes, un heterocigoto y un homocigoto dominante, pueden presentar el mismo fenotipo.
La fibrosis quística sería un ejemplo.
A la inversa, el mismo genotipo puede producir fenotipos diferentes en entornos distintos.
Un ejemplo de ello es la fenilcetonuria (PKV), una enfermedad recesiva observada aproximadamente en 1 de cada 10.000 recién nacidos de raza blanca.
Las mutaciones en el locus codificador de la enzima metabólica fenilalanín-hidroxilasa provocan que los homocigotos sean incapaces de metabolizar el aminoácido fenilalanina.
Aunque los niños con fenilcetonuria son normales en el nacimiento, su deficiencia metabólica produce una acumulación de fenilalanina y otros metabolitos tóxicos.
Esto es muy destructivo para el sistema nervioso central y, finalmente, provoca un retraso mental grave.
Se ha calculado que los niños con fenilcetonuria no tratados pierden, por término medio, uno o dos puntos de CI por semana durante el primer año de vida.
Así, la fenilcetonuria puede producir un fenotipo patológico grave.

Sin embargo, resulta fácil detectar la fenilcetonuria en el nacimiento (v. cap. 11) y la enfermedad puede evitarse iniciando una dieta baja en fenilalanina antes de transcurrido un mes desde el nacimiento.
Los individuos continúan teniendo el genotipo de la fenilcetonuria, pero el fenotipo de retraso mental ha sido alterado profundamente mediante modificaciones ambientales.

Este ejemplo demuestra que el fenotipo es el resultado de la interacción entre el genotipo y los factores ambientales. 
Debe destacarse que el «entorno» puede incluir el entorno genético (es decir, genes en otros loci , cuyos productos pueden interactuar con un gen específico o con sus productos).

El fenotipo, que es observable físicamente, es el resultado de la interacción entre el genotipo y el entorno.

Estructura del árbol genealógico básico

El árbol genealógico es uno de los instrumentos más utilizados en genética médica.
Ilustra las relaciones entre miembros de la familia y muestra quiénes son los familiares afectados por una enfermedad genética y quiénes son los no afectados.
De forma típica, una flecha señala el probando , el primer individuo diagnosticado en el árbol genealógico.
A veces, al probando se le denomina caso índice o propósito ( propósita para las mujeres).
La figura 4-4 describe las características de la notación del árbol genealógico.

Cuando se examina el parentesco en las familias, a menudo se refieren los grados de relación.
Los familiares de primer grado están relacionados como padres-descendencia o hermanos (hermano y hermana).
Los parientes de segundo grado son los separados por una «etapa» adicional (p. ej., abuelos y nietos, tíos/tías y sobrinos/sobrinas). 
Continuando con esta lógica los parientes de tercer grado incluirían por ejemplo, los primos hermanos y los sobrinos nietos, entre otros.

Figura 4-4

Notación del árbol genealógico básico

Citogenética clínica: bases cromosómicas de la enfermedad en el ser humano

En los dos capítulos anteriores se ha tratado de las enfermedades por un único gen.
Ahora se centra la atención en las enfermedades causadas por alteraciones cromosómicas lo bastante grandes para ser observables con el microscopio.
Estas alteraciones se denominan anomalías cromosómicas .
El estudio de los cromosomas y de sus anomalías se denomina citogenética .

Las anomalías cromosómicas son responsables de una proporción significativa de las enfermedades genéticas, presentándose aproximadamente en 1 de cada 150 nacidos vivos.
Son la principal causa conocida de retraso mental y de pérdida de gestación.
Se observan anomalías cromosómicas entre el 50 y el 20 % de los abortos espontáneos en el primer y segundo trimestre, respectivamente.
Por lo tanto, son una causa importante de morbididad y mortalidad. 

Como en otras áreas de la genética médica, los avances en la genética molecular han contribuido con muchas nuevas informaciones en el campo de la citogenética. 
Por ejemplo, las técnicas moleculares han permitido la identificación de anomalías cromosómicas, como las deleciones, que afectan regiones muy pequeñas. 
En algunos casos, se han identificado con precisión genes específicos responsables de síndromes citogenéticos. 
Así, se está difuminando la distinción entre «anomalías cromosómicas» y «enfermedades» por un único gen.
Además, la capacidad de identificar los RFLP y otros polimorfismos en progenitores y descendientes ha hecho posible que los investigadores averigüen si un cromosoma anormal procede de la madre o del padre.
De este modo ha aumentado nuestro conocimiento de la base biológica de los errores en la meiosis y de las anomalías cromosómicas. 

En este capítulo se estudiarán las anomalías del número y la estructura de los cromosomas y se revisará la base genética y cromosómica de la determinación del sexo.
Se examinará la función de las alteraciones cromosómicas y se discutirán varias enfermedades causadas por la inestabilidad cromosómica. 
Se destacarán las nuevas contribuciones que la genética molecular ha prestado a la citogenética. 

TECNOLOGÍA CITOGENÉTICA Y NOMENCLATURA

Aunque ya a mediados del siglo XIX era posible observar los cromosomas con el microscopio, era difícil observar los cromosomas individuales.
Así, era muy difícil efectuar el recuento del número de cromosomas en una célula o examinar las anomalías estructurales.
A comienzos de la década de los cincuenta se desarrollaron varias técnicas que mejoraron nuestra capacidad para observar los cromosomas.
Estas técnicas incluían: 
1' rend="il la utilización de venenos para el huso mitótico, como la colquicina y el colcemid, que detienen la división de las células somáticas en la metafase, cuando los cromosomas se encuentran en condensación máxima y son más fáciles de observar;
2' rend="il la utilización de una solución hipotónica (baja concentración salina), que provoca la hinchazón de las células, la rotura del núcleo y mayor separación entre los cromosomas individuales, y
3' rend="il el uso de medios de tinción, que se absorben de forma diferente por partes distintas de los cromosomas (produciendo las características bandas que facilitan la identificación de los cromosomas individuales)


Nuestra capacidad para estudiar los cromosomas ha mejorado gracias a la observación de los cromosomas en la metafase, a las soluciones hipotónicas que causan hinchazón de los núcleos y a las técnicas de tinción que producen bandas de los cromosomas.

El análisis de los cromosomas se efectúa del siguiente modo:
se recogen muestras de tejido vivo (en general, sangre) y se cultiva el tejido durante el intervalo apropiado de tiempo (en general, de 48 a 72 horas para los linfocitos periféricos), añadiendo colcemid para provocar la detención de la metafase; después se recogen las células y se coloca el sedimento celular en un portaobjetos, se rompen los núcleos celulares con una solución hipotónica, se efectúa la tinción con el colorante nuclear designado y se fotografía la «diseminación» en metafase de los cromosomas sobre el portaobjetos.

A continuación se cortan las imágenes de los cromosomas de la fotografía y se distribuyen los 22 pares de cromosomas autosómicos según su longitud, situando los cromosomas sexuales en la esquina inferior derecha. 

Esta presentación ordenada de los cromosomas se denomina cariotipo (fig. 6-1).
Más recientemente se utilizan, en algunas ocasiones, analizadores de imagen computarizada para presentar los cromosomas.

Tras la clasificación por tamaño, se realiza una clasificación adicional según la posición del centrómero.
Cuando éste se sitúa cerca del centro del cromosoma.
se considera que el cromosoma es metacéntrico (figura 6-2).
Un cromosoma acrocéntrico tiene su centrómero junto al extremo y los cromosomas submetacéntricos tienen los centrómeros entre el centro y el extremo.
El extremo de cada cromosoma denomina telómero .
Obsérvese que el brazo corto de un cromosoma se denomina p (de petite ), mientras que el brazo largo se designa como q .
En los cromosomas metacéntricos, donde los brazos tienen unas longitudes casi iguales los brazos p y q se denominan así por convención.

Un cariotipo es una presentación de los cromosomas ordenados según su longitud.
Dependiendo de la posición del centrómero, un cromosoma puede ser acrocéntrico, submetacéntrico o metacéntrico. 

Un cariotipo de una mujer normal se designa como 46,XX; 
un cariotipo masculino normal se designa como 46,XY. 
En la tabla 6-1 se resume la nomenclatura de las diversas anomalías cromosómicas, indicando la que corresponde a los trastornos estudiados más adelante.

Figura 6-1

Cariotipo bandeado de una mujer normal

Los cromosomas en metafase bandeados se disponen en orden de tamaño descendente.

Figura 6-2

Cromosomas metacéntricos, submetacéntricos y acrocéntricos

Obsérvense los pedúnculos y los satélites presentes en los brazos cortos de los cromosomas acrocéntricos.

Patrón de bandas del cromosoma 

Los primeros cariotipos facilitaron el recuento del número de cromosomas, pero las anomalías estructurales, como las deleciones cromosómicas, a menudo eran indetectables. 
En la década de los setenta se desarrollaron técnicas de tinción que generaban las bandas cromosómicas características de los cariotipos modernos.

La determinación del patrón de bandas (o bandeo) del cromosoma contribuye en gran medida a la detección de deleciones, duplicaciones y otras anomalías estructurales, y facilita la correcta identificación de los cromosomas individuales.
Se numeran las bandas mayores de cada cromosoma (fig. 6-3).
Así, 14q32 se refiere a la segunda banda de la tercera región del brazo largo del cromosoma 14.
Las subbandas se designan por comas decimales después de la banda (p. ej., 14q32, 3 es la tercera subbanda de la banda 2).

En los laboratorios de citogenética se emplean varias técnicas de bandeo cromosómico.
El bandeo de quinacrina (bandeo Q) fue el primer método de tinción utilizado para producir patrones de banda específicos. 

Este método requiere un microscopio de fluorescencia y ya no se utiliza tanto como el bandeo Giemsa (bandeo G) (la tinción de Giemsa se aplica tras la digestión parcial de las proteínas cromosómicas por tripsina).
El bandeo invertido (bandeo R) requiere tratamiento con calor e invierte el patrón usual blanco-y-negro observado en las bandas G y en las bandas Q.
Este método es especialmente útil para la tinción de los extremos dístales de los cromosomas.
Otras técnicas de tinción incluyen el bandeo C y las tinciones NOR (región de organización nucleolar).
Estos últimos métodos tiñen de forma específica determinadas porciones del cromosoma.
El bandeo C fine la heterocromatina constitutiva localizada en el centrómero y en su proximidad, y la tinción NOR resalta los pedúnculos y los satélites de los cromosomas acrocéntricos (fig. 6-2).

El bandeo de alta resolución implica la tinción de los cromosomas durante la profase o inicios de la metafase (prometafase), antes de que alcancen su máxima condensación.
Ya que los cromosomas en profase y en prometafase están más extendidos que los cromosomas en metafase, el número de bandas observables aumenta de 300 (como en la fig. 6- 3) hasta 800.
Ello permite la detección de anomalías menos evidentes que no suelen observarse mediante las técnicas de bandeo convencionales.
El bandeo de alta resolución es bastante tedioso y requiere más tiempo, por lo que suele utilizarse sólo cuando el médico está buscando una anomalía cromosómica específica y sutil. 
Un ejemplo sería la delación de las bandas l5qll-13 observada en el síndrome de Prader-Willi.

El bandeo cromosómico contribuye a identificar los cromosomas individuales y las anomalías estructurales de los cromosomas.
Las técnicas de bandeo incluyen los bandeos de quinacrina, Giemsa, invertido, C y NOR.
El bandeo de alta resolución, utilizando cromosomas en profase o en prometafase, aumenta el número de bandas observables.

Hibridación in situ con fluorescencia

La hibridación in situ con fluorescencia (FISH) es una técnica recientemente desarrollada en que un segmento marcado de DNA específico de cromosoma (sonda) se hibrida con cromosomas en metafase, profase o interfase y después se observan en un microscopio por fluorescencia.
Una utilización habitual de la FISH estriba en la determinación de una posible deleción en un cromosoma en un paciente.
En un individuo normal, la sonda se hibridará en dos lugares, reflejando la presencia de dos cromosomas homólogos en el núcleo de la célula somática.
Si una sonda del segmento cromosómico en cuestión se hibrida únicamente con uno de los cromosomas del paciente, es probable que el paciente presente la deleción en el otro cromosoma.
También se puede detectar el exceso de material cromosómico utilizando la técnica FISH. 
En este caso la sonda se hibridará en tres lugares, en vez de dos.
La figura 6-4, A ilustra un resultado de FISH en un hombre normal N en su hija, que tiene una pérdida de una pequeña pieza distal del brazo corto del cromosoma 4 (fig. 64- B ).
En la hija sólo se observa una mancha fluorescente, indicando que la niña presenta el síndrome de deleción de 4p o síndrome de Wolf-Hirschhorn.

La hibridación in situ con fluorescencia (FISH) es una técnica en que una sonda marcada se hibrida con cromosomas en metafase, profase o interfase.
La FISH puede utilizarse paradetectar material cromosómico perdido o adicional.

Tabla 6-1

Nomenclatura estándar de los cariotipos cromosómicos

Cariotipo.
Descripción.


Constitución cromosómica de hombre normal.


Mujer con trisomía 21, síndrome de Down.


Mosaico de células con trisomía 21 y de células normales en hombre.


Hombre con delación distal del brazo corto de la banda 14 del cromosoma 4.


Mujer con duplicación del brazo corto del cromosoma 5 .


Hombre con una translocación robertsoniana equilibrada de cromosomas 13 y 14.


El cariotipo muestra la pérdida de un 13 normal y un 14 normal.


Hombre con una translocación recíproca equilibrada entre los cromosomas 11 y 22.


Los puntos de rotura se localizan en 11q23 y 22q22 .


Una inversión en el cromosoma 3 que se extiende de p21 a ql3 .
Puesto que incluye el centrómero, se trata de una inversión pericéntrica.


Una mujer con un cromosoma normal y un cromosoma X anular.


Una mujer con un cromosoma X normal y un isocromosoma del brazo largo del cromosoma X.


Figura 6-3

Representación esquemática del patrón de banda de un cariotipo de bandeo G

En este ideograma se representan 300 bandas. 
Se designan los brazos cortos y largos de los cromosomas, y los segmentos se numeran según la nomenclatura estándar adoptada en la conferencia de París, de 1971.
En esta ilustración, se muestran ambas cromátides hermanas de cada cromosoma.

ANOMALÍAS DEL NÚMERO DE CROMOSOMAS

Poliploidía

Las células que contienen un múltiplo de 23 cromosomas en su núcleo son euploides (del griego eu = bueno y ploid = serie).
Por lo tanto, los gametos haploides y las células somáticas diploides son euploides.
La poliploidía o presencia de una serie completa de cromosomas adicionales en una célula se observa frecuentemente en las plantas y a menudo mejora su valor en la agricultura.
La poliploidía también ocurre en el ser humano, aunque con una frecuencia mucho menor.
Los procesos poliploides observados en humanos son la triploidía (69 cromosomas en el núcleo de cada célula) y la tetraploidía (92 cromosomas en cada núcleo celular).
Los cariotipos de estas dos entidades se designarían como 69XXX y 92XXX respectivamente (suponiendo que todos los cromosomas sexuales fueran X; se pueden observar otras combinaciones de cromosomas X e Y).
Puesto que el número de cromosomas presentes en cada una estas entidades es múltiplo de 23, las células son euploides en cada caso.
Sin embargo, los cromosomas adicionales codifican gran cantidad de producto genético excedente provocando múltiples anomalías, como defectos del corazón y del sistema nervioso central.
Tanto la triploidía como la tetraploidía son letales. 

La triploidía se observa únicamente en 1 de cada 10.000 nacidos vivos, pero se calcula que es responsable del 15% de las anomalías cromosómicas producidas en la concepción.
Por lo tanto, la inmensa mayoría de las concepciones triploides sufren un aborto espontáneo. 

La causa más frecuente de triploidía es la fertilización de un óvulo por dos espermatozoides ( dispermia ).
El cigoto resultante recibe 23 cromosomas del óvulo 23 cromosomas de cada uno de los dos espermatozoides.
La triploidía puede deberse también a la fusión de un óvulo y un cuerpo polar cada uno con 23 cromosomas, y la posterior fecundación por un espermatozoide.
El error meiótico , en que se produce un óvulo o un espermatozoide diploide también puede originar un cigoto triploide.

La tetraploidía es mucho más rara que la triploidía, tanto en las concepciones como entre los recién nacidos vivos.
Se ha descrito únicamente en unos pocos nacidos vivos y éstos han sobrevivido sólo breves períodos de tiempo.
La tetraploidía también puede estar causada por un fallo mitótico al inicio del desarrollo embrionario, en el que todos los cromosomas duplicados emigran a una de las dos células hijas.
También puede ser el resultado de la fusión de dos cigotos diploides.

Las células diploides tienen un múltiplo de 23 cromosomas.
La triploidía (69 cromosomas) y la tetraploidía (92 cromosomas) son entidades poliploides encontradas en seres humanos.
La mayoría de concepciones poliploides sufren un aborto espontáneo y todas ellas son incompatibles con la supervivencia a largo plazo.

Figura 6-4

Resultado de una hibridación in situ con fluorescencia (FISH)

Las flechas más grandes señalan una sonda hibridada con el centrómero del cromosoma 4 y las flechas más pequeñas señalan una sonda que se hibrida en 4p.
A, en un hombre normal se observan dos manchas de la sonda que hibrida con el cromosoma 4p.
B, esta sonda revela sólo una mancha en su hija, que presenta una deleción del cromosoma 4p que produce el síndrome de Wolf-Hirschhorn. 

Aneuploidía autosómica 

Las células que no contienen un múltiplo de 23 cromosomas se denominan aneuploides .
Estas células pierden algún cromosoma o contienen cromosomas adicionales.
Por lo general, sólo se afecta un cromosoma, pero es posible que se pierda o duplique más de un cromosoma.
Las aneuploidías (o aneusomías ) de los cromosomas autosómicos se encuentran entre las anomalías cromosómicas más importantes clínicamente.
Pueden ser principalmente, monosomía (presencia de una única copia de un cromosoma en una célula, por lo demás diploide)y trisomía (tres copias de un cromosoma).
Las monosomías autosómicas casi siempre son incompatibles con la gestación a término, de modo que únicamente se ha observado un pequeño número de casos entre recién nacidos vivos.
En contraste, algunas trisomías se observan con frecuencias apreciables entre los nacidos vivos. 
El hecho de que las trisomías provocan consecuencias menos graves que las monosomías ilustra un principio importante: 
el organismo puede tolerar un exceso de material genético con más facilidad que un déficit de material genético.

La causa más común de aneuploidía es la falta de separación, que ocurre cuando los cromosomas no se separan normalmente durante la meiosis (fig. 6-5).
La no separación ocurre durante la meiosis I o meiosis II.

El gameto resultante carece de un cromosoma o bien tiene dos copias de éste, produciendo un cigoto monosómico o trisómico, respectivamente.

Los trastornos aneuploides constan, principalmente, de monosomías y trisomías, y suelen estar causados por la falta de separación.
Las monosomías autosómicas son, casi siempre, letales, mientras que algunas trisomías autosómicas son compatibles con la supervivencia. 

Figura 6-5

En las faltas de separación meiótica, dos cromosomas homólogos emigran a la misma célula hija en vez de separarse de forma normal o emigrar a células hijas diferentes

Esto provoca descendencia monosómica y trisómica.

Trisomía 21

La trisomía 21 ( xxx ) [1] se observa aproximadamente en 1 de cada 800 nacidos vivos, siendo el trastorno aneuploide más común compatible con la gestación a término. 
Este trisomía produce el síndrome de Down, un fenotipo descrito por primera vez por John Langdon Down en 1866.
Transcurrieron casi 100 años entre la descripción de este síndrome por Down y el descubrimiento (en 1959) de que este síndrome está causado por la presencia de un cromosoma 21 adicional.

Aunque existe una notable variación en el aspecto de los individuos con síndrome de Down, presentan un conjunto de signos que facilitan el diagnóstico al médico. 
Los signos faciales incluyen puente nasal bajo, fisuras palpebrales oblicuas hacia arriba, orejas pequeñas y, a veces, con pliegues excesivos, y regiones maxilares y malares aplanadas, dando a la cara un aspecto característico (fig. 6-6).
Algunas de estas características suscitaron el empleo del término "mongolismo" en la bibliografía inicial, pero ya no es apropiado.
Las mejillas son redondas y las comisuras de la boca se dirigen hacia abajo.
El cuello es corto y la piel es redundante en la nuca, especialmente en los recién nacidos.
El occipucio es plano y las manos y los pies tienden a ser bastante anchos y cortos. 
Aproximadamente, el 50 % de los individuos con síndrome de Down tienen un único y profundo pliegue de flexión en sus palmas (denominado pliegue simiesco ).
La disminución del tono muscular (hipotonía) también es un signo muy constante y útil para el diagnóstico.

Varios problemas médicamente significativos se presentan con frecuencia creciente en los lactantes y niños pequeños con síndrome de Down.
Cerca del 3 % de estos lactantes desarrollarán obstrucción del duodeno o atresia (cierre o ausencia) del esófago, duodeno o ano.

Las infecciones respiratorias son frecuentes y el riesgo de desarrollar leucemia entre los pacientes con síndrome de Down es de 15 a 20 veces superior al de la población general.
El problema médico más importante reside en que cerca del 40 % de los pacientes afectados nacen con defectos cardíacos estructurales.
El más frecuente es la persistencia del conducto AV (auriculoventricular), un defecto en que los tabiques interauricular e interventricular no se cierran normalmente durante el desarrollo fetal.
El resultado es que la sangre fluye desde el corazón izquierdo hacia el corazón derecho y después hacia la vasculatura pulmonar, originando hipertensión pulmonar. 
Los defectos del tabique ventricular (VSD: Ventricular septal defects ) también son comunes entre los recién nacidos con síndrome de Down.
En la mayoría de individuos con síndrome de Down se observa retraso mental entre moderado y grave (CI entre 25 y 60).
El síndrome de Down es responsable del 10 % de todos los casos de retraso mental en Estados Unidos.

Los lactantes y los niños pequeños con síndrome de Down presentan además otros problemas médicos. 
Los más importantes y frecuentes son sordera de conducción y, a veces, neurosensorial, hipotiroidismo y diversas anomalías oculares.
En el comentario clínico 6-1 se esboza un plan de cuidados médicos habituales para los lactantes y los niños con síndrome de Down.

Debido a los problemas médicos observados en los niños con síndrome de Down, estos niños presentan una reducción significativa de sus tasas de supervivencia.
Los defectos cardíacos congénitos representan la causa aislada más importante de esta disminución de la supervivencia.
A principio de los años sesenta, únicamente la mitad de los niños con este trastorno sobrevivían hasta los 5 años de edad.
Como consecuencia de la mejoras alcanzadas en la cirugía correctora y en el tratamiento de la leucemia, las tasas de supervivencia han aumentado de forma considerable durante los últimos 30 años.
Se calcula que en la actualidad cerca del 75 al 80 % de los niños con síndrome de Down sobreviven hasta los 10 años de edad.
Existen pruebas concluyentes de que un medio ambiente favorable puede producir mejoras significativas de la función intelectual.

Los hombres con síndrome de Down son, casi siempre, estériles;
sólo se ha descrito un caso de un hombre con este síndrome que haya tenido descendencia. 

Algunas mujeres afectadas pueden tener hijos, aunque aproximadamente el 40 % no ovulan.
Puesto que la reproducción es tan poco frecuente, casi todos los casos de trisomía 21 pueden considerarse nuevas mutaciones.
El riesgo de que una mujer con síndrome de Down produzca un gameto con dos copias del cromosoma 21 (que después produciría un cigoto trisómico) es del 50 %.
Sin embargo, ya que aproximadamente el 75 % de las concepciones con trisomía 21 sufren aborto espontáneo, el riesgo de la mujer con síndrome de Down de tener hijos afectados nacidos vivos es considerablemente inferior al 50 %.

Cerca del 95 % de los casos de síndrome de Down están causados por falta de separación cromosómica; 
la mayoría de los casos restantes se deben a translocaciones cromosómicas.

109).
De acuerdo con las comparaciones de la morfología cromosómica en la descendencia afectada y en sus progenitores, se creía que casi el 80 % de las faltas de separaciones del cromosoma 21 se producían en la madre.
En la actualidad es posible una valoración más precisa comparando los RFLP y otros polimorfismos del cromosoma 21 en progenitores y descendientes.

Este método demuestra que el cromosoma adicional es aportado por la madre en aproximadamente el 95 % de los casos de trisomía 21.

[1] Por razones de brevedad, el resto de designaciones del cariotipo para las anomalías que no afectan los cromosomas sexuales indicarán hombre afectado.

Detección genética, diagnóstico genético y terapia génica

Como se ha descrito en capítulos anteriores, se han producido avances importantes en la tecnología del DNA, en el trazado del mapa génico, en la citogenética y en muchas otras áreas de la genética médica. 
Estos avances han preparado el camino para la obtención de diagnósticos más precisos y eficientes de los trastornos genéticos.
En especial, se ha generalizado en gran medida la detección en la población y el diagnóstico prenatal de los trastornos genéticos. 
Este capítulo se centra en los principios y aplicaciones del diagnóstico genético en este contexto.

Otro tema central es el tratamiento de la enfermedad genética. 
Muchos aspectos del tratamiento de la enfermedad implican diversas áreas de la medicina, como la cirugía y la farmacoterapia, que se encuentran fuera de los objetivos de este libro.
Sin embargo, la terapia génica, mediante la cual se insertan genes normales en las células de pacientes para combatir enfermedades específicas, está incluida en los propósitos de este texto y se estudiarán con cierto detalle.

DETECCIÓN DE LAS ENFERMEDADES GENÉTICAS EN LA POBLACIÓN

Las pruebas de detección representan una parte fundamental de los cuidados médicos habituales.
Estas pruebas suelen estar diseñadas para detectar enfermedades humanas en su fase preclínica.
Las pruebas de Papanicolaou para identificación de la displasia del cuello del útero y la detección de la hipercolesterolemia son ejemplos conocidos de esta estrategia de la salud pública.
Se ha definido la detección en la población como «la identificación de presunción de defecto o enfermedad todavía no diagnosticada, mediante el empleo de pruebas, exploraciones u otros procedimientos que pueden aplicarse con rapidez con el objetivo de diferenciar, en personas aparentemente sanas, entre los individuos que probablemente sufren una enfermedad y aquellos que probablemente no la padecen » (Mausner y Bahn, 1974). 
Las pruebas de detección no están diseñadas para proporcionar un diagnóstico definitivo, sino que se proponen la identificación de una subserie de la población en la que se han de realizar pruebas diagnósticas adicionales. 

1 ya están asociados con una enfermedad o con la predisposición a esta enfermedad
2 pueden transmitir la enfermedad a sus descendientes» ( National Academy of Sciences , 1975) 

La detección en recién nacidos en busca de enfermedades metabólicas hereditarias constituye un buen ejemplo del primer tipo de detección genética, y la detección de heterocigotos de la enfermedad de Tay-Sachs (estudiada después) ejemplifica el segundo.
Mientras que estos dos ejemplos implican la detección de poblaciones, la detección genética también puede aplicarse a los miembros de familias con antecedentes positivos de un trastorno genético.
Un ejemplo sería la realización de pruebas encaminadas a averiguar la presencia de una translocación recíproca equilibrada en las familias en las que uno o más miembros han sufrido un trastorno cromosómico (v. cap. 6).
En el recuadro 11-1 se enumeran los diversos tipos de detección genética, incluyendo varias formas de diagnóstico prenatal, que se estudian en este capítulo.

Principios de la detección

Los principios básicos de la detección se desarrollaron en los años sesenta y todavía se aplican ampliamente.
Cuando se considera si es apropiado efectuar una detección en la población deben tenerse en cuenta los siguientes puntos:

1 Características de la enfermedad .
El trastorno debe ser importante y relativamente frecuente.
De este modo nos aseguramos de que los beneficios derivados del programa de detección justifican sus costes.
Debe conocerse con claridad la historia natural de la enfermedad.
También debe existir un tratamiento aceptable y efectivo o, en el caso de determinados trastornos genéticos, el diagnóstico prenatal debe ser factible.

2 Características de la prueba .
La prueba de detección debe ser aceptable para la población, fácil de efectuar y relativamente barata.
Las pruebas de detección deben ser válidas y fiables.

3 Características del sistema .
La información acerca del diagnóstico y el tratamiento debe ser accesibles.
Ha de existir una estrategia de comunicación eficiente y efectiva de los resultados.


Como se describe en el comentario clínico 11-1, el programa de detección de la fenilcetonuria (PKU) cumple muy bien todos estos criterios.

Los programas de detección suelen utilizar pruebas muy aplicables y de coste no excesivo para identificar la población de riesgo.
A continuación, los miembros de esta población se someten a pruebas adicionales, más precisas, pero también más caras y lentas.
En este contexto, debe prestarse atención a la validez de una prueba de detección.
El término validez se refiere a la capacidad que tiene una prueba para diferenciar entre los individuos que tienen la enfermedad y los que no la tienen.
Ello implica dos componentes: sensibilidad y especificidad. 
Se define la sensibilidad como la capacidad para identificar correctamente los individuos que padecen la enfermedad.
Se mide como la proporción de individuos afectados en quienes la prueba es positiva (es decir, positivos verdaderos ).
La especificidad es la capacidad para identificar correctamente a quienes no tienen la enfermedad.
Se mide como la proporción de individuos no afectados en quienes la prueba es negativa (es decir, la tasa de negativos verdaderos ).
La sensibilidad y la especificidad se determinan comparando los resultados de la detección con los de una prueba diagnóstica definitiva (tabla 11-1).

Rara vez, o nunca, las pruebas de detección tienen una sensibilidad del 100 % y una especificidad del 100 %.

Ello se debe a que el margen de los valores de las pruebas en la población enferma se solapa con los de la población no afectada (fig. 11-1).
Por lo tanto, una prueba de detección (en oposición a la prueba diagnóstica definitiva) diagnosticará erróneamente a algunos miembros de la población.
Por lo general se establece un punto de corte que separe los grupos enfermos de los y no enfermos de la población.
Existe un equilibrio entre el impacto de la falta de detección o baja sensibilidad (es decir, un aumento de la tasa de falsos negativos ) y la baja especificidad (es decir, un aumento de la tasa de falsos positivos ).
Si la consecuencia para los individuos afectados no detectados es grave (como ocurre en la fenilcetonuria no tratada), entonces el nivel de corte se reduce de modo que casi todos los casos patológicos sean detectados (sensibilidad superior), mientras que aumenta el número de casos no patológicos (especificidad inferior) que deberán someterse a pruebas diagnósticas adicionales.
Si la confirmación de una prueba positiva es cara y peligrosa, entonces se reducen las tasas de falsos positivos (es decir, se eleva el punto de corte, suscitando una especificidad elevada a expensas de la sensibilidad).

Los elementos básicos de la validez son la sensibilidad (proporción de positivos verdaderos detectados) y la especificidad (proporción de falsos negativos detectados). 
Cuando aumenta la sensibilidad, disminuye la especificidad y viceversa.

Un tema fundamental en el marco clínico es la precisión que tiene una prueba de detección positiva.
Es necesario conocer la proporción de personas que presentan una prueba positiva y que padecen realmente la enfermedad estudiada [es decir, a/a+b ) en la tabla 11-1).
Esta cantidad se define como el valor predictivo positivo .

Un ejemplo ilustrará los conceptos de sensibilidad, especificidad y valor predictivo positivo.
La hiperplasia suprarrenal congénita (CAH: congenital adrenal hyperplasia ) por una deficiencia de 21-hidroxilasa es un error congénito de la biosíntesis de esteroides que puede provocar genitales ambiguos en mujeres y crisis suprarrenales en hombres y mujeres.
La prueba de detección, análisis de 17-hidroxiprogesterona, tiene una sensibilidad próxima al 95 % y una especificidad del 99 % (tabla 11-2).
La prevalencia de la CAH es de 1 de cada 10.000 individuos en la mayoría de las poblaciones de raza blanca, pero aumenta hasta 1 de cada 400 en la población esquimal Yupik.

Supongamos que se ha desarrollado un programa de detección de CAH en ambas poblaciones.
En una población de 500.000 blancos se registra una tasa de falsos positivos (1-especificidad) del 1 %.
Por lo tanto, alrededor de 5.000 individuos normales presentarán un prueba positiva.
Con una sensibilidad del 95 %, 47 de los 50 individuos con CAH serán detectados mediante una prueba positiva.
Obsérvese que la gran mayoría de personas que presentan una prueba positiva no padecerían en realidad una CAH; el valor predictivo positivo es 47/(47 + 5.000)o menos del 1%. 
Supongamos que se practica la detección de CAH en 10.000 miembros de la población Yupik.
Como se muestra en la tabla 11-2, 24 de 25 individuos con CAH presentarán una prueba positiva, y 100 sin CAH también presentarán una prueba positiva.
En este caso, el valor predictivo positivo es muy superior que en los blancos: 24/(24 + 100) = 19 % .
Este ejemplo ilustra un importante principio: el valor predictivo positivo de una prueba se incrementa cuando aumenta la prevalencia de la enfermedad. 

El valor predictivo positivo se define como la proporción de pruebas positivas que en realidad son positivas verdaderas.
Se incrementa cuando aumenta la prevalencia de la enfermedad estudiada.

Comentario clínico II-I

Detección neonatal de fenilcetonuria(PKU) 

Características de la enfermedad 

La detección en la población de recién nacidos con fenilcetonuria representa el mejor ejemplo de las aplicaciones del modelo de detección de las enfermedades genéticas.
Como se describe en el capítulo 3, la prevalencia de este trastorno autosómico recesivo del metabolismo de la fenilalanina se aproxima a 1 de cada 10.000-15.000 neonatos blancos.
La historia natural de la PKU es totalmente conocida.
Más del 95 % de los pacientes con PKU no tratados manifiestan un retraso mental entre moderado y grave. 
Este trastorno no es identificable clínicamente durante el primer año de vida, ya que los signos clínicos pueden ser leves y la PKU tan sólo se manifiesta como un retraso del desarrollo.
La restricción de fenilalanina de la dieta, iniciada antes de las 4 semanas de edad, es muy eficaz para modificar el curso de la enfermedad.
Casi todos los individuos con una PKU clásica tendrán una inteligencia normal si son tratados (una importante excepción es la del grupo formado por quienes presentan un defecto en el metabolismo de la biopterina y en quienes se utiliza un tratamiento diferente).

Características de la detección 

La fenilcetonuria se detecta mediante la prueba de Guthrie o medición de la fenilalanina en sangre utilizando un análisis de inhibición bacteriana.
Se obtiene una muestra de sangre en el período neonatal, en general por punción en el talón y se coloca en papel de filtro.
La sangre desecada se coloca en una placa de agar y se incuba con una cepa bacteriana ( Bacillus subtilis ) cuyo crecimiento requiere la presencia de fenilalanina.
La medición del crecimiento bacteriano permite la cuantificación de la fenilalanina en la muestra de sangre.
Si los resultados de la prueba son positivos, suele repetirse la prueba y después se practica un análisis cuantitativo de fenilalanina y tirosina en plasma.

Si la prueba se realiza a los 2 días de edad y después de una alimentación regular con una dieta con proteínas, el índice de detección (sensibilidad) se aproxima al 98 %.

Si se practica antes de transcurridas 24 horas desde el nacimiento, la sensibilidad es del 84 %.
Por lo tanto, el alta precoz de la sala de neonatos obliga a una repetición de la prueba al cabo de unas semanas tras el nacimiento.
La especificidad se aproxima al 100 %.

Características del sistema 

Debido a los requerimientos de proteínas normales en la dieta, en muchos estados se repite la prueba entre las 2 y las 4 semanas de edad, cuando la sensibilidad se aproxima al 100 %.
Debido al impacto que puede tener un diagnóstico erróneo, es deseable un elevado nivel de sensibilidad. 

Los niveles de fenilalanina en niños con fenilcetonuria clásica suelen superar los 20 mg/dl.
Por cada 20 detecciones de PKU positivos, tan sólo un lactante presentará una fenilcetonuria clásica.
Los otros serán falsos positivos (por lo general, debido a una tirosinemia reversible transitoria) o presentarán una forma de hiperfenilalaninemia (nivel elevado de fenilalanina) no causada por una PKU clásica. 

El coste de una prueba de Guthrie es de 1,25 dólares. 

Varios estudios han demostrado que el coste de la detección nacional de PKU es significativamente menor que el ahorro que se obtiene al evitar los costes de hospitalización y la pérdida de productividad.

Recuadro 11-I

Detección genética y diagnóstico prenatal

a Fenilcetonuria, todos los estados de Estados Unidos
b Galactosemia, todos los estados de Estados Unidos 
c Hipotiroidismo, 45 estados de Estados Unidos
d Otros: hemoglobinopatías, fibrosis quística 

2 Orina: aminoacidopatías

1 Enfermedad de Tay-Sachs, población judía askenazi 
2 Anemia de células falciformes, población afroamericana
3 Talasemias, grupos étnicos de riesgo
4 Fibrosis quística, sólo programas piloto 

1 Amniocentesis
2 Biopsia de vellosidades coriónicas
3 Obtención percutánea de muestra de sangre umbilical (PUBS)

1 Ecografía
2 Radiografía

1 Edad materna superior a los 35 años
2 Historial familiar de trastorno diagnosticable por técnicas prenatales
3 alfa-fetoproteína sérica materna anormal 
4 Triple detección, alfa-fetoproteína sérica materna, estriol, gonadotropina coriónica humana

A Historial familiar de redistribución cromosómica (p. ej., translocación)
B Detección en mujeres parientes en un árbol genealógico ligado al cromosoma X (p. ej., distrofia muscular de Duchenne o síndrome del cromosoma X frágil) 
C Detección de heterocigotos en familias de riesgo (p. ej., fibrosis quística)
D Detección presintomática (p. ejemplo, enfermedad de Huntington, carcinoma mamario o cáncer de colon)




Tabla 11-1

Definiciones de sensibilidad y especificidad

Tabla 11-2

Resultados hipotéticos de la detección de hiperplasia suprarrenal congénita (CAH) en una población blanca de baja prevalencia y una población Yupik de alta prevalencia

Figura 11-1

Distribución de creatincinasa (CK) en mujeres normales y en mujeres heterocigotas portadoras de una mutación en el gen de distrofia muscular de Duchenne.

Obsérvese la superposición en la distribución entre los dos grupos: cerca de dos tercios de portadores presentan niveles de CK superiores al percentil 95 en mujeres normales.
Si se utiliza el percentil 95 como punto de corte para identificar a las portadoras, entonces la sensibilidad de la prueba es del 67 % (es decir, se detectará a dos tercios de portadoras) y la especificidad es del 95 % (el 95 % de las mujeres normales serán identificadas correctamente). 

Detección en recién nacidos 

Los programas de detección en neonatos representan una oportunidad ideal para la detección presintomática y la prevención de la enfermedad genética.
En la actualidad, en todos los estados de Estados Unidos se practica a los recién nacidos la detección de fenilcetonuria e hipotiroidismo.
En la mayoría de estados se efectúa la detección de la galactosemia, un trastorno autosómico recesivo del metabolismo de los hidratos de carbono.

Todos estos trastornos cumplen los criterios antes mencionados para la detección de población.
Todos los individuos afectados corren un riesgo importante de retraso mental; la detección precoz permite la intervención y la prevención eficaces.

En fecha reciente, en varios estados de Estados Unidos y en otras naciones se han instaurado programas de detección para la identificación de neonatos con hemoglobinopatías (p. ej., anemia de células falciformes).

El motivo que justifica la realización de estos programas es que incluso el 15% de los niños con anemia de células falciformes fallece por infecciones antes de los 5 años de edad (v. cap. 3).
Se dispone de un tratamiento eficaz, en forma de antibioticoterapia profiláctica. 

Algunas comunidades han iniciado la detección de la distrofia muscular de Duchenne (DMD) midiendo los niveles de creatincinasa en recién nacidos.
El objetivo no consiste en la instauración de tratamiento presintomático, sino en la identificación de las familias que deben recibir consejo genético para la toma de decisiones informadas sobre la reproducción.
En la tabla 11-3 se resumen trastornos en que se suele practicarse una detección en recién nacidos.

La detección en recién nacidos es una estrategia efectiva de salud pública para enfermedades susceptibles de tratamiento, como la fenilcetonuria, el hipotiroidismo, la galactosemia y la anemia de células falciformes.

Tabla 11-3

Características de los programas de detección en neonatos

Enfermedad.
Herencia.
Prevalencia .
Prueba de detección.
Coste.
Tratamiento.


Fenilcetonuria.
Autosómica recesiva.
Prueba de Guthrie.
Restricción de fenilalanina.


Galactosemia autosómica recesiva.
Análisis de transferasa.
Restricción de transferasa .


Hipotiroidismo congénito.
En general, esporádica.
Medición de T4 o TSH.
Sustitución hormonal.


Anemia de células falciformes.
Autosómica recesiva.
afroamericanos.
Foco isoeléctrico o diagnóstico de DNA.
Profilaxis con penicilina .


Detección de heterocigotos

Los principios antes mencionados de detección en la población son aplicables a la detección de portadores no afectados de genes causantes de enfermedad.
La población diana es un grupo conocido de riesgo.
La «intervención» consiste en la presentación de cifras de riesgo y en la opción de diagnóstico prenatal.
Las enfermedades genéticas adecuadas para la detección en heterocigotos son, típicamente, las enfermedades autosómicas recesivas en las que el diagnóstico prenatal y el consejo genético están disponibles, son factibles.

Un ejemplo de detección en heterocigotos efectuado con gran éxito es el programa de detección de la enfermedad de Tay-Sachs en Norteamérica.
La enfermedad de Tay-Sachs infantil es un trastorno autosómico recesivo en la que existe una deficiencia de la enzima lisosomal 3-hexosaminidasa (HEX A).
Esta deficiencia provoca una acumulación del sustrato, gangliósido xxx , en los lisosomas neuronales. [2] 
La acumulación de este sustrato lesiona las neuronas y provoca ceguera, convulsiones, hipotonía y muerte hacia los 5 años de edad.

La enfermedad de Tay-Sachs es especialmente común entre los judíos askenazis, con una frecuencia de heterocigotos de 1:30.
Por lo tanto, esta población es un candidato razonable para la prueba de detección en heterocigotos. 
Disponemos de una prueba precisa de detección de portadores (análisis de HEX A o, en algunas poblaciones, la prueba de demostración directa de mutaciones).
Puesto que la enfermedad siempre es fatal, la mayoría de parejas aceptan opciones como la interrupción del embarazo o la inseminación artificial procedente de donantes no portadores.
Se llevó a cabo un esfuerzo planificado para educar a los miembros de la población diana acerca de los riesgos, las pruebas y las opciones disponibles.
Como resultado de la detección, el número de nacimientos con enfermedad de Tay-Sachs en Estados Unidos y Canadá ha disminuido el 90 %, desde 40 o 50 por año antes de 1970 hasta 3-5 por año en la década de los noventa. 

La fibrosis quística es otro trastorno autosómico recesivo en que es posible la detección de portadores. 
Sin embargo, los costes y los beneficios relativos de este programa de detección son menos concluyentes que los de la enfermedad de Tay-Sachs (comentario clínico 11-2).
En la tabla 11-4 se presenta una lista de trastornos para los que se han diseñado, en los países industrializados, programas de detección en heterocigotos.

Junto a los criterios necesarios para establecer un programa de detección en la población de trastornos genéticos, se han desarrollado normas relativas a los aspectos éticos y legales de los programas de detección en heterocigotos. 
Resumen en el recuadro 11-2.

La detección en heterocigotos consiste en examinar (en cuanto a de fenotipo o genotipo) una población diana para identificar a los portadores no afectados de un gen patológico. 
A continuación es posible proporcionar a los portadores información sobre los riesgos y las opciones de reproducción. 

Comentario clínico 11-2.

Detección en la población de fibrosis quística

En la actualidad se conocen más de 400 mutaciones causantes de fibrosis quística (CF) del locus CFTR.
Es evidente que no sería práctico tecnológicamente probarlas todas en un programa de detección en la población de portadores.
Sin embargo, entre las mutaciones que pueden causar CF en blancos, un 70% son una mutación de 3 bases denominada xxx (v. cap. 4). 
La detección en portadores utilizando sólo la detección mediante PCR de esta mutación detectaría aproximadamente el 90 % de las parejas de raza blanca en las que uno o ambos miembros sean portadores de esta mutación ( xxx , donde xxx representa la frecuencia de parejas portadoras en las que ningún miembro de la pareja es portador de la mutación xxx ). 
Probando varias mutaciones adicionales relativamente comunes, todas ellas con una frecuencia de gen xxx , en la actualidad la detección en la población detectaría entre el 75 y el 85 % de todas las mutaciones de CF.
Entonces se identificarían entre el 94 y el 98 % de las parejas con uno o ambos miembros portadores de una mutación CF (es decir, de xxx a xxx ).
La sensibilidad de la prueba de detección sería elevada.

Estos portadores definirían una población a la que podría ofrecerse el diagnóstico prenatal de CE Dada la frecuencia de heterocigotos CF cercana a 1 de 23 (cap. 4),únicamente una de cada 500 parejas estaría formada por dos heterocigotos ( xxx ).
Sólo algunas de estas parejas engendrarían un hijo afectado con CF.
Considerando el coste de las pruebas de detección de parejas, se ha calculado que el coste de identificar un feto homocigoto afectado sería de un millón de dólares. 

Además, sería necesaria una enorme cantidad de tiempo de asesoramiento para explicar las complejidades de estos resultados a las familias con riesgo identificadas mediante detección (en este contexto, es importante destacar que un estudio de padres de hijos con CF ha demostrado que sólo se podría establecer con precisión el riesgo de recurrencia para su descendencia futura en el 64 % de los casos). 
Se calcula que la mitad los genetistas y consejeros genéticos colegiados en Estados Unidos deberían dedicar todo su tiempo a la detección de la CF para satisfacer esta necesidad.
Es evidente la imposibilidad de esta práctica. 

Aunque la detección de portadores en individuos con antecedentes familiares positivos de CF es apropiado, es menos cierto que la detección de población sea necesario o factible.
Varios centros han iniciado programas piloto de detección en la población de CF.
La decisión final relativa a la detección de CF requiere los datos ulteriores procedentes de estos centros y un desarrollo tecnológico adicional.

Recuadro 11-2

Normas públicas para la detección de heterocigotos

Normas recomendadas
1 La detección debe ser voluntaria, asegurándose la confidencialidad
2 Para la realización de la detección es necesario el consentimiento firmado por escrito
3 Los profesionales que proporcionan los servicios de detección tienen la obligación de incluir en el programa una educación y un consejo adecuados
4 Es necesario un control de calidad de todos los aspectos de las pruebas de laboratorio, incluyendo una prueba de eficiencia sistemática y debe llevarse a cabo tan pronto como sea posible
5 De conseguirse el acceso de todos los individuos a la prueba

Diagnóstico presintomático 

Gracias al desarrollo del diagnóstico genético por análisis de ligamiento y detección directa de mutaciones ha sido factible el diagnóstico presintomático de algunas enfermedades genéticas. 
Se puede examinar a los individuos del trastorno con riesgo conocido de padecer una enfermedad y averiguar, antes de que manifiesten los síntomas clínicos del trastorno, si han heredado una mutación causante de enfermedad.
Disponemos, por ejemplo, de diagnóstico presintomático de la enfermedad de Huntington, nefropatía poliquística del adulto, neurofibromatosis de tipo 1 y carcinoma mamario autosómico dominante.
Al informar a los individuos que son portadores de un gen causante de enfermedad, el diagnóstico presintomático puede ayudar en la toma de decisiones sobre la reproducción.
También se puede tranquilizar a quienes no sean portadores del gen patológico. 
En algunos casos, el diagnóstico precoz puede mejorar la supervisión sanitaria.
Por ejemplo a las personas que heredan un gen de carcinoma mamario autosómico dominante se les pueden practicar mamografías a una edad precoz para detectar los tumores que aún tienen un tamaño reducido.
Los individuos que heredan genes causales de ciertas formas de cáncer de colon familiar (APC y HNPCC) también pueden beneficiarse del diagnóstico y el tratamiento precoces.

En algunas ocasiones se puede practicar el análisis genético para identificar a los individuos que han heredado un gen causante de enfermedad antes de que manifiesten los síntomas. 
Este método se denomina diagnóstico presintomático. 

Tabla 11-4

Ejemplos seleccionados de programas de detección en heterocigotos en grupos étnicos específicos

Enfermedad.
Grupo étnico.
Frecuencia de portadores.
Frecuencia de parejas.
Incidencia de la enfermedad.


Anemia de células falciformes.


Enfermedad de Tay-Sachs.


B-talasemia.


a-talasemia.


Fibrosis quística .


Implicaciones psicosociales de la detección genética

La detección de las enfermedades genéticas conlleva muchas implicaciones sociales y psicológicas. 
Es necesario comparar el impacto de ansiedad, coste y estigmatización potencial que rodea a una prueba positiva, con la necesidad de detección.
A menudo se cree, de forma errónea, que las pruebas de detección proporcionan un diagnóstico definitivo.
Debe insistirse, a quienes se someten a la detección, en que una prueba de detección positiva no indica necesariamente enfermedad. 

Los primeros programas de detección del rasgo drepanocítico que se efectuaron en los años setenta estaban plagados de creencias erróneas acerca del estado de portador.
A veces, la detección de un portador provocaba la cancelación de su seguro médico.
Estas experiencias subrayan la necesidad de consejo genético y de una educación pública eficaces.
Otros temas incluyen el derecho a no ser examinado y la posible invasión de la intimidad.

Los aspectos sociales, psicológicos y éticos de la detección genética se complicarán progresivamente a medida que sean accesibles técnicas diagnósticas de DNA más modernas.
Por ejemplo, aun cuando disponemos del diagnóstico presintomático de la enfermedad de Huntington, tan sólo el 20 % de los individuos con riesgo eligen esta opción.
En gran parte, ello se debe a que no existe en la actualidad ningún tratamiento eficaz posible de esta enfermedad.

En otras enfermedades genéticas, como algunos síndromes neoplásicos autosómicos dominantes, el diagnóstico precoz permite un pronóstico mucho mejor.
Sin embargo, cuando los programas de detección de estas enfermedades sean más comunes, también se deberán afrontar los temas de intimidad y confidencialidad, así como la necesidad de una comunicación precisa de la información sobre el riesgo.

INSTRUMENTOS MOLECULARES PARA DETECCIÓN Y DIAGNÓSTICO

Hasta hace poco tiempo, la detección genética solía basarse en análisis del fenotipo patológico, como el análisis de B-hexosaminidasa para la enfermedad de Tay-Sachs o un análisis de la creatincinasa para la distrofia muscular de Duchenne (DMD).
Los avances en la tecnología del DNA han conducido al diagnóstico en el genotipo.
En algunos casos, el análisis de ligamiento puede determinar si un individuo ha heredado un gen patológico y en otros casos se ha desarrollado el análisis directo de las mutaciones causantes de enfermedad.

En la actualidad, el diagnóstico genético en cuanto al de DNA está complementando, y a veces reemplazando, por las pruebas de detección basadas en exámenes del fenotipo.

El análisis de ligamiento y/o el diagnóstico directo de la mutación han sido utilizados en el examen diagnóstico en las familias, en el diagnóstico prenatal de trastornos genéticos y, más recientemente, en la detección de población.
Los avances efectuados en la tecnología, junto al aumento de la demanda de pruebas, han dado lugar a la creación de laboratorios moleculares clínicos en muchos centros médicos de Norteamérica y Europa.
A continuación se describen las técnicas moleculares utilizadas en estos laboratorios.

[1] Igual que el síndrome de Hurler (v., cap., 4), la enfermedad de Tay-Sachs es un ejemplo de trastorno de almacenamiento lisosomal


Tijeras y pinzas de láser

En el laboratorio se emplean ya láseres para atrapar células, una a una, o sus componentes .
Con un haz para fijar y otro para cortar, los investigadores proceden a manipulaciones sutilísimas.

Los haces de luz intensos y puros, los láseres, son componentes rutinarios de impresoras y lectores de discos compactos.
Lo que no significa que los láseres limiten su aplicación a tareas triviales.
Imagine el lector que enfoca un haz sobre tal orgánulo del interior celular.
Considere, además, que el haz bloquea en su sitio la estructura en cuestión, a modo de pinzas fijadoras.
Mientras ese haz minúsculo retiene al componente celular, un segundo haz actúa de escalpelo o tijeras y ejecuta una tarea de cirugía fina en el orgánulo. 

Incluso en un mundo como el nuestro, acostumbrado a los láseres, representaciones semejantes suenan a fantasía científica.
Nada más falso.
Lo mismo que el cirujano guía las micropinzas y las microtijeras a través del endoscopio para acometer una intervención quirúrgica sin apenas agredir el órgano, el biólogo celular puede utilizar "pinzas de láser" y "tijeras de láser" para sus propias manipulaciones en células y orgánulos, sin provocar lesiones.

Primero aparecieron las tijeras de láser.
Hará unos 30 años, Donald E., Rounds y el autor sugirieron la posibilidad de emplear láseres para explorar la estructura y función de las células y sus orgánulos. 
Empezamos por definir los parámetros de nuestros haces (longitudes de onda y duración de las exposiciones, entre otros), para seguir determinando qué orgánulos podrían manipularse con haces luminosos que alterasen regiones intracelulares mínimas, de sólo 0,25 micrómetros de diámetro. 
(El diámetro de un cabello humano se cifra en unos 100 micrómetros.)
Andando el tiempo, descubrí con mi grupo que las tijeras de láser podían aplicarse a la investigación de los cromosomas y el huso mitótico, que guía la separación de los cromosomas durante la división celular.
Amén de tales orgánulos nucleares, los haces facilitaron el estudio de mitocondrias (centrales celulares de energía), así como de estructuras diversas: microfilamentos, microtúbulos y centrosomas (involucrados en el mantenimiento de la arquitectura celular y el transporte de moléculas dentro de las células). 

Aunque no siempre entendamos con exactitud el mecanismo en cuya virtud el láser desencadena los cambios que se operan en los componentes celulares, podemos provocar determinadas alteraciones que quedan fijas, pero sin comprometer la estructura o el entorno de la diana.
Lo corroboran la microscopía óptica y electrónica, herramientas habituales en biología; ponen éstas de manifiesto que las tijeras de láser pueden producir cambios específicos en un cromosoma, en el propio interior celular.
Con anterioridad, mi grupo había demostrado que las tijeras inactivaban una parte determinada de un cromosoma de células en división, en particular, una región que contiene genes que regulan la formación del nucléolo.
Y lo que revestía mayor interés: la alteración persistía en la progenie de esas células, ya que todas ellas poseían versiones inactivas de los genes en la misma región.

La alteración producida en el cromosoma, una lesión de menos de un micrómetro, aparece en forma de región clara si observamos la célula con un microscopio óptico de contraste de fases.
Con microscopía electrónica de transmisión, que permite de 10.000 a 100.000 aumentos, se percibe que esa región corresponde a una alteración estructural definida se advierte, además, que el material cromosómico que hay a cada lado de la lesión y el citoplasma que rodea al cromosoma parecen incólumes. 
El aspecto que ofrece esta región al microscopio de contraste de fases obedece a un cambio en el índice de refracción, no a la remoción física de material (el láser cambia las propiedades químicas y físicas del cromosoma sin destruirlo).

Otro tanto cabe decir a propósito de la membrana celular; para estudiarla basta una ligera perturbación de su fluidez.
Con láser pueden producirse incisiones de un micrómetro en la membrana, que se sellan en una fracción de segundo.

Mediante esta técnica de optoporación (producción de poros por métodos ópticos), se introducen moléculas en la célula al abrir los poros sin rasgar la membrana de forma permanente.

La optoporación podría aplicarse sobre todo a la manipulación genética de las plantas, cuyas paredes celulares son rígidas e impenetrables si las comparamos con la membrana de la célula animal.
En Irvine, he recurrido, con Hong Liang, a la optoporación para insertar genes en células de arroz.
De esas células genéticamente modificadas se desarrollaron plantas enteras, donde cada célula portaba y expresaba los genes introducidos. 
Este trabajo, junto con el de la inactivación de genes del nucleolo, demuestra que las tijeras de láser sirven para insertar o para delecionar genes.

En Europa se han utilizado esas tijeras para manipular gametos humanos (espermatozoides y óvulos) en el marco de la implantación asistida.
Las tijeras reducen o eliminan una parte pequeña de la zona pelúcida de óvulos fecundados in vitro .
Los embriones se colocan entonces en el útero, donde la reducción en el espesor de la zona pelúcida facilita, así parece, la implantación.
Esta reducción en el espesor también puede lograrse por técnicas tradicionales, pero el láser no requiere productos tóxicos que dañan el embrión. 

Los estudios más completos en humanos realizados hasta el momento (por el grupo de Severino Antinori en Roma) estiman un aumento de un 50 por ciento en la tasa de embarazos entre más de 200 mujeres cuyos embriones habían sido sometidos a reducción lasérica de la zona respecto a mujeres no sometidas al tratamiento.
En Estados Unidos, han comenzado los ensayos de reducción de la zona pelúcida por láser.
En Israel se está a la espera de la aprobación gubernativa.

En colaboración con el grupo de Nancy L., Allbritton en Irvine, mi equipo se sirve de las tijeras para abrir células, de suerte que sus componentes químicos puedan analizarse a voluntad.
Aprovechamos lo que en otras circunstancias constituye un efecto secundario negativo del uso del láser. 
Nos referimos a la formación de una nube minúscula de gas ionizado, al microplasma que se genera cuando se focaliza el láser en el portaobjeto de vidrio sobre el que descansa la célula.
La expansión y contracción del microplasma provoca tensiones mecánicas que, a su vez, pueden rasgar la célula. 
(Los médicos aprovechan un efecto similar, aunque a escala mayor, para pulverizar cálculos renales y cataratas mediante una onda de choque emitida por el láser.)
Colocando un tubito capilar encima de la célula, podemos recoger sus componentes y analizarlos en una suerte de instantánea de su bioquímica en ese momento.
Esta técnica posibilita un sinfín de aplicaciones de química analítica celular.
Se pretende, sobre todo, determinar la identidad y concentración exacta de las proteínas oncológicas.

En todas las aplicaciones de las tijeras, debe buscarse precisión y selectividad.
La precisión remite al direccionamiento del láser sobre el objetivo; la selectividad alude a la alteración controlada de la diana, que deja intactas las zonas circundantes.

Gracias a la calidad de los elementos ópticos de los microscopios actuales no resulta difícil conseguir la precisión deseada.
Los objetivos tienen un acabado de fabricación perfecto y presentan corrección cromática en el rango del espectro visible, para que todas las longitudes de onda enfoquen casi el mismo punto en el espacio; así, cuando se utilizan múltiples haces con diferentes longitudes de onda, podemos estar seguros de que enfocan sobre el mismo punto.

Con un objetivo corriente de aceite de inmersión de 100 aumentos, la física que gobierna el tamaño de una mancha focal de láser predice que dicho tamaño es un poco menor que la longitud de onda del láser.
Así, un haz producido a partir de un granate de neodimio, itrio y aluminio ( Nd:YAG ) que opere a una longitud de onda de 532 nanómetros puede producir una mancha focal de 499 nanómetros.
La precisión puede ser incluso mejor de lo que parece.
El láser que hemos elegido produce una distribución gaussiana de la energía, es decir, la energía que forma la mancha focal se ajusta a una curva acampanada.
Puesto que sólo el máximo de la curva puede tener energía suficiente para alterar un orgánulo particular, el punto efectivo puede ser bastante menor que el diámetro de la mancha focal medida.

También se puede alcanzar la selectividad (o alteración controlada).

Lo prueban las aplicaciones desarrolladas hasta ahora. 
Sin embargo, por culpa de un conocimiento incompleto de las interacciones entre el láser y la diana biológica, se ignora cómo garantizar la selectividad en las nuevas aplicaciones (salvo la mostración empírica de su funcionamiento).
No se trata de ningún fenómeno insólito.
Durante un siglo se tomaron aspirinas sin que los expertos conocieran su desenvolvimiento molecular. 
Con ello no se niega que el conocimiento exacto del mecanismo de operación de un fármaco o una técnica conduzcan a una mejor aplicación.
Por eso mismo, se trabaja con ahínco para desentrañar la compleja interacción entre láser y célula.

Diversos factores se oponen a la mejor comprensión de los efectos ejercidos por los haces.
Medir y registrar lo que ocurre en regiones celulares de menos de un micrómetro representa un reto imponente, lo mismo que controlar la energía de un láser en volúmenes tan sutiles.
Pero no andamos a ciegas.
Por estudios macroscópicos sabemos que, cuando el láser interacciona con tejido orgánico, se activan varios procesos físicos y químicos. 
Tales procesos pueden dispararse por la absorción de fotones individuales o por la absorción, casi simultánea, de múltiples fotones.

La absorción de un solo fotón podría limitarse a calentar la diana.
O iniciar reacciones químicas que produzcan radicales libres u otros productos lesivos para las células.
Los fotones de alta energía (como los del rango ultravioleta) pueden incluso romper enlaces moleculares y disgregar las moléculas en un proceso de fotoablación.
Además, la absorción de múltiples fotones en un tiempo muy corto puede resultar equivalente a la absorción de un solo fotón de alta energía.
Este tipo de absorción múltiple de fotones puede desencadenar reacciones químicas o conducir a la disociación molecular que se aprecia cuando se absorben fotones ultravioleta uno a uno.
Cualquiera de estas situaciones, o su combinación, pudiera darse en una diana celular de menos de un micrómetro.

Dejemos aparte las propiedades de absorción de la diana.
Otro factor clave que condiciona el efecto ejercido por el haz reside en la irradiancia de la luz incidente (energía que llega a la superficie de la diana en un período de tiempo dado, medido en watt por centímetro cuadrado). 
Con pulsos de láser que expongan la diana a luz durante intervalos de microsegundos a femtosegundos ( xxx segundos), la irradiancia puede ser enorme.
Se conocen los efectos que ejerce el láser en tejidos expuestos durante períodos de tiempo largos (cientos de segundos a minutos); por recordar algunos: calentamiento- capaz de desnaturalizar, coagular o vaporizar las moléculas- y reacciones químicas que engendran radicales libres, entre otros fotoproductos nocivos. 

Mucho menos se ha profundizado en los efectos producidos por irradiancias elevadas cuando se usan los volúmenes con los que trabajamos intracelularmente (una esfera de una micrómetro de diámetro tiene un volumen menor de un femtolitro). 

Entre los desafíos a plantearse destaca uno: definir el umbral a partir del cual una mayor irradiancia modificar el efecto ejercido en la diana; por ejemplo, descubrir a qué nivel el láser pasa de calentar la diana a generar ondas de choque inducidas por el microplasma.
Aunque queda mucho por conocer sobre los efectos y umbrales de la irradiancia, no cabe duda de que se ven afectados por la duración del pulso de láser y por las propiedades de absorbancia de la diana y su entorno.

La investigación sobre umbrales de irradiancia se halla en plena efervescencia.
Mi grupo se ha concentrado en el dominio por debajo del micrómetro.
De ellos cabe esperar un pronto avance sustantivo en el desciframiento de los mecanismos de interacción entre láser y diana, que agilice el desarrollo de aplicaciones subcelulares del láser.
Pese a las cuestiones pendientes, con láser se pueden realizar cortes de suma precisión en casi cualquier componente celular que pueda observarse al microscopio óptico; podemos destruir o inactivar regiones específicas de las dianas.

No tardaremos en superar incluso esa limitación visual.
Pienso en el empleo de moléculas que absorban luz (MAL) y estén diseñadas para descubrir y unirse a una secuencia de ADN del gen a manipular.
De esta secuencia, demasiado pequeña para dejarse ver, podría ignorarse el lugar que tiene asignado en el cromosoma.
Al irradiar la célula con luz de la longitud de onda correcta aportaría energía a las MAL, que a su vez inactivarían el gen unido.
(Este proceso también podría activar el mismo u otros genes, mediante transferencia de energía capaz de desencadenar una cascada de reacciones químicas o inactivar un gen supresor.)
El resto de la célula, aunque expuesta a la luz, no mostraría secuelas de la irradiación. 

Las tijeras de láser, con sus pulsos de luz intensos y cortos, abrieron el surco que llevó a la cirugía microscópica de células y moléculas.
Desde hace poco se les han unido las pinzas de láser. 
A primera vista resulta impropio el empleo de luz para atrapar y arrastrar objetos.
De la naturaleza de la luz es calentar, quemar, medir o calibrar.
No parece que le corresponda ejercer una fuerza capaz de sujetar o mover un objeto. 
Ahora bien, la cantidad de movimiento de la luz puede transmitirse a una diana.
Pero cuando el sol brilla sobre nuestra cabeza y ejerce un empuje sutil contra nosotros, escapan a nuestra consciencia sensorial las fuerzas liliputienses resultantes. 
En cambio, esas fuerzas bastan para influir en los procesos subcelulares, donde las masas de los objetos son mínimas. 

A mediados de los años ochenta, Arthur Ashkin descubrió que un haz láser de onda continua y baja potencia (menos de un watt ) podía «atrapar ópticamente» bacterias y protozoos, uno a uno.
Con su grupo demostró (primero con un láser verde-azul de iones de argón y después con el láser infrarrojo de Nd:YAG , al que las células son más transparentes) que podían atrapar y desplazar células enteras y sus orgánulos.

Steven Cha, Nobel de física, comprobó, con su grupo de la Universidad de Stanford, que las pinzas de láser atrapaban también moléculas.
Conjugaron perlitas transparentes de poliestireno con los extremos de ADN enrollado y desnudo; emplearon fuerzas ópticas para empujar las cuentas y desplegar la molécula entera de ADN.
Steven M. Block, de Princeton, y Michael P. Sheetz, de Duke, han utilizado el atrapamiento óptico de moléculas para motores de quinesinas, las estructuras proteínicas responsables de los latigazos de los flagelos o de la cola de los espermatozoides; esos motores agitan también las partículas y los orgánulos intracelulares. 

Un objeto pequeño y transparente al láser de una frecuencia particular refracta los rayos incidentes, curvando la luz.
En virtud de la refracción, la cantidad de movimiento se transmite de la luz a la diana.
Cuando los haces y la diana aparecen en geometría correcta, la cantidad de movimiento transferida a la diana tira de ésta en la dirección del haz incidente; de ese modo, el rayo puede sujetar a la diana en su sitio.
Moviendo el haz, el operador de láser puede arrastrar la diana de un punto a otro.

Los haces de las tijeras y de las pinzas difieren en duración e intensidad.
Mientras que las tijeras emplean pulsos cortos de alta irradiancia, las pinzas utilizan haces continuos de baja irradiancia.
La diana debe ser transparente a las pinzas, de manera que el haz atraviese el objeto sin que se absorba energía y se convierta en calor destructivo o produzca un daño fotoquímico.

Con un haz podríamos aprehender una célula o un orgánulo, que podrían todavía moverse; con un segundo rayo, se bloquea al objeto en su sitio.
Las pinzas de láser acostumbran usar luz cuya longitud de onda oscile entre 0.7 y 1.06 micrómetros y cuya potencia varíe de 25 a 500 miliwatt sobre una mancha focal de entre 0,5 y 1,0 micrómetro de diámetro.
El haz de tales características genera fuerzas en el rango del piconewton, suficiente para atrapar células y mover orgánulos. 

Las pinzas de láser permiten nuevos tipos de experimentos. 
Mis colegas de Irvine Michael Cahalan, Bruce J., Tromberg, Xunbin Wei, Tatiana Krasieva y Paul Negulescu se sirvieron de ellas para analizar la relación entre la forma y la función de las células T del sistema inmunitario.
La presentación de moléculas extrañas, o antígenos, por las células H del sistema inmunitario desencadena una cascada de reacciones, con un aumento en la concentración de iones calcio en las células T .
Ese incremento promueve, a su vez, la especialización y proliferación de las células T , procesos esenciales de la inmunidad.

Las células T muestran una morfología polarizada, definida por su forma y dirección en que reptan.
Con pinzas de láser de zafiro-titanio se atraparon células B y se situaron en distintos puntos de la superficie de la célula T . 
Cuando la célula B se colocó en el extremo posterior de la célula T , no se produjo ninguna respuesta; dos minutos después, aquélla se separaba de ésta.
Pero la colocación de la célula B en el extremo anterior de la célula T provocó un inmediato incremento en la concentración intracelular de iones calcio, señal de que la respuesta en cascada se había disparado.
Este descubrimiento se corresponde bien con la noción de que las células T y otros leucocitos emigran en direcciones específicas, en respuesta ante señales recibidas por receptores situados en sus extremos anteriores.

Las pinzas de láser atrapan incluso células de inusitada motilidad.
Como demostraron Yona Tadir, Gregory J., Sonek y William H., Wright, a la sazón en Irvine, se puede aprehender un espermatozoide humano y manipularlo a voluntad.
Y para investigar las fuerzas natatorias del espermatozoide, fuimos reduciendo gradualmente la fuerza que lo atrapaba hasta determinar en qué nivel podía escaparse.
Del análisis de esta «fuerza de escape relativa» ha surgido un método para examinar la relación entre fuerza natatoria, velocidad y patrón de movimiento.
El descubrimiento de que los espermatozoides que nadan en zigzag lo hacen con mayor vigor que los que siguen una línea recta, puede explicar ciertas observaciones clínicas; por ejemplo, los hombres con una proporción más alta de espermatozoides que naden en zigzag podrían ser más fértiles que los que producen espermatozoides que naden en línea recta.

A tenor de otra línea de investigación en que se recurrió al empleo de pinzas de láser, los espermatozoides extraídos del epidídimo (donde maduran y se recogen antes de la eyaculación) de individuos incapaces de eyacular nadan con una fuerza igual a sólo un tercio de la fuerza de los espermatozoides normales.
(Parece ser que la adquisición del vigor entero requiere la maduración completa que ocurre durante su paso por el epidídimo.)
Este resultado ayuda a explicar por qué se optimiza la fecundación cuando se inyecta directamente en los óvulos el esperma de estos pacientes, en vez de limitarse a la fecundación de óvulos en una placa de petri, donde el éxito depende de la fuerza natatoria.

El análisis de la fuerza escapatoria relativa debería, por tanto, ser un método valioso a la hora de determinar si la esterilidad se debe a una motilidad baja.
Asimismo, podría servir para ensayar tratamientos encaminados a mejorar la motilidad. 
En resumen, las pinzas podrían resultar útiles en el tratamiento clínico de la esterilidad y en el estudio de la motilidad de los espermatozoides.

En la aplicación de las pinzas a tales funciones debe procederse con delicadeza, porque la exposición a las pinzas podría dañar la motilidad por culpa de efectos térmicos y fotoquímicos difíciles de evitar.
En verdad, y ante el uso de un haz continuo, con las pinzas hay que tener más cuidado con el calor que en el caso de las tijeras.

Tromberg, Sonek y Yagang Lia, al medir el calor generado por las pinzas, detectaron aumentos de temperatura en las regiones irradiadas de las células.
Unieron una molécula sensible al calor, el laurdano, a membranas celulares artificiales (liposomas) o células cultivadas de ovario de hámster y registraron los cambios operados en la intensidad de la fluorescencia y en la emisión de longitudes de onda.

Con ese microtermómetro celular se detectó un aumento de 1,15 a 1,45 grados centígrados por cada 100 miliwatt de potencia láser en la mancha focal.
Las pinzas podían decuplicar dicha potencia; sin una oportuna disipación del calor, podrían producirse alteraciones fatales de las velocidades de las reacciones bioquímicas, inactivarse proteínas sensibles al calor (enzimas incluidas) e incluso precipitarse la muerte celular.
Gracias a estas mediciones con laurdano se ha enriquecido el banco de datos sobre las interacciones entre el láser y su diana.

Si cada una por su lado, tijera y pinza, sirven de muchísimo, las técnicas que utilizan las dos, de forma secuencial o combinada, facilitan una mejor manipulación de las células. 
La primera aplicación conjunta de tijera y pinza de láser fue realizada por Romy Wiegand-Steubing, en mi laboratorio.
Se sirvió de pinzas de láser infrarrojo de Nd:YAG para juntar dos células de mieloma humanas; con tijeras de láser ultravioleta de nitrógeno cortó las membranas celulares colindantes. 
De ese modo, las dos células se fusionaron en una híbrida, que contenía los genomas de ambas. 
Las células fusionadas pueden combinar atributos de interés en una sola entidad; piénsese, por ejemplo, en la reunión de la capacidad de producir un compuesto útil de una célula con la de dividirse indefinidamente de la otra.

Yendo más allá en el trabajo de atrapar espermatozoides, el grupo de Karin Schütze recurrió a las pinzas infrarrojas para aprehender y guiar espermatozoides bovinos hasta el óvulo por un surco que había abierto en la zona pelúcida con las tijeras.
Merced a esta técnica se obtuvo cierto porcentaje de fecundaciones, igual que en un ensayo similar con espermatozoides y óvulos de ratón.
Podría pensarse en un planteamiento parecido, si se demuestran su seguridad y eficacia, para su aplicación en manipulación de gametos en humanos, por no hablar de sus posibilidades en mejora animal.
Aunque la precaución y la observancia rigurosa de las leyes deben guiar a los investigadores del campo de la reproducción asistida, éstos han mostrado interés en las pinzas y las tijeras de láser.

La combinación de tijera y pinzas permite manipulaciones sin precedentes en orgánulos y células enteras. 
En colaboración con Edward D., Salmon recurrimos a las tijeras de láser para proceder al corte de cromosomas que se encontraban en pleno proceso mitótico (o división celular).
Con las pinzas de láser fuimos luego desplazando los fragmentos por el interior celular al objeto de estudiar las fuerzas ejercidas por el huso mitótico (la maquinaria celular que tira de los cromosomas replicados hacia los extremos opuestos de una célula en división). 

Para nuestra sorpresa, no nos costó nada mover los fragmentos que estaban fuera del huso, pero no pudimos llevarlos al interior del huso.
Se confirmaba la existencia de una «jaula» en el huso que bloquea la entrada de material extraño en su interior.
Y los fragmentos cromosómicos recién producidos lo eran.
Si consideramos que el material genético se transmite de una célula a la siguiente generación mediante la mitosis, entenderemos que la evolución haya desarrollado un sistema de bloqueo contra la entrada de material indeseado en la zona del huso. 

El equipo de Rieder le está sacando más partido a ese utillaje.
Han demostrado que las pinzas inhiben el movimiento de fragmentos de cromosomas cortados dentro del huso. 
Se puede, además, con tales técnicas, abordar sin agredir las fuerzas existentes en el huso mitótico. 
Habida cuenta del papel que éste desempeña en la división celular, poder desentrañar su funcionamiento habrá de conducirnos a un conocimiento más hondo de las enfermedades asociadas la división celular, cáncer y malformaciones congénitas entre ellas.

En el marco del programa sobre empleo de microhaces de láser en biomedicina promovido por el norteamericano Instituto Nacional de la Salud, hemos construido un equipo de trabajo que abarca dos pinzas y unas tijeras de láser incorporadas en un microscopio confocal de fluorescencia.
Los haces de láser son ajustables, vale decir, pueden programarse a cualquier longitud de onda.
Con la conjunción de todo ese instrumental en un mismo equipo se satisfacen a la vez múltiples exigencias del biólogo celular.
El investigador puede observar células u orgánulos fluorescentes con el microscopio confocal durante y después del período de actuación de las pinzas y las tijeras. 
De ese sistema de captura y corte confocal controlado por palancas de mando (CATS) pueden extraer sumo provecho infinitos trabajos celulares y subcelulares, sin olvidar los encaminados a refinar los procedimientos de secuenciación de ADN. 

En Irvine, Barbara Hamkalo y Al Jasinkas recurren a esa técnica para remover el centrómero de un cromosoma, la región donde los microtúbulos del huso se afirman. 
Se disputa si esta región interviene o no en procesos genéticos, ante las enormes dificultades que rodean el aislamiento y análisis del centrómero en busca de secuencias génicas activas.
Las tijeras y pinzas de láser deberían ayudar a resolver la controversia.

Han pasado más de ochenta años desde que Albert Einstein sentó las bases teóricas del láser. 
A principios de los sesenta, los haces láser se convirtieron en realidad.
Hoy los equipos de laboratorio que incorporan láseres permiten que el biólogo se transforme en cirujano celular, explorando y manipulando células y orgánulos.
El empleo de ese nuevo utillaje habrá de repercutir en medicina, biología del desarrollo, citogenética y manipulación del genoma humano.
Con el láser se adivina un futuro brillante para la investigación biológica y sus aplicaciones.

Figura 1

MEDIANTE EL USO CONJUNTO de tijeras y pinzas de láser podemos realizar sutiles manipulaciones en orgánulos celulares 

Antes de que pasen diez años, debería convertirse en rutinario lo que aquí se ilustra: dos haces, en función de pinzas ( rosa ), sujetan una célula firmemente en su sitio.
Otro, en función de tijera ( azul claro ), atraviesa la célula para extraer un gen defectuoso ( rojo ).
Unas segundas tijeras ( azul oscuro ) abren un pasillo en la membrana celular por donde pueda pasar una secuencia genética funcional ( puntos negros ).
A continuación, y con un propósito terapéutico, podrían producirse clon es de la célula alterada genéticamente y transplantarla al organismo.

Figura 2

ENTRE LOS LOGROS ALCANZADOS por las tijeras de láser se incluye la creación de surcos ( izquierda ) en la zona pelúcida que rodea a un óvulo fecundado de ratón (recuadro) para facilitar la implantación 

Otro logro ha sido su utilización para eliminar una porción cromosómica en V (derecha) de una célula viva procedente de una rata canguro.
La porción mide alrededor de un micrómetro en su parte más ancha.

Figura 3

LA ENERGÍA transmitida a las células (dosis de energía), medida en joule por centímetro cuadrado ( xxx ), depende de la irradiancia de los haces láser y del tiempo que las células permanecen expuestas a la luz

(Las líneas diagonales discontinuas representan dosis constantes de energía).
Se indican algunos de los efectos que se pueden conseguir en las células con diferentes combinaciones de irradiancia y tiempo.
Al menos un ligero calentamiento (rojo) acostumbra producirse en un amplio rango de energías.
En rojo oscuro, se indica la zona donde el calentamiento constituye el mecanismo dominante.

Figura 4

QUE EL LÁSER pueda aprehender objetos se funda en el principio físico de la conservación del momento

La refracción de cualquier parasimétrico de rayos láser produce fuerzas.
Un par típico (haces A y B) produce las fuerzas Fa y Fb , resultantes de la respuesta de la diana al cambio operado en la cantidad de movimiento de los haces luminosos.
Si el punto focal se encuentra antes del centro de la diana, la suma de cualquier par de tales fuerzas es una fuerza total (la flecha vertical F)que tira de la diana en la dirección del haz. 
Un punto focal situado allende el centro de la diana empujaría a ésta.
Los puntos focales situados a la izquierda o la derecha del centro moverían la diana hacia la izquierda o la derecha.

Figura 5

POLARIDAD DE LAS CÉLULAS T , confirmada en ensayos con pinzas de láser 

Las células B , que provocan la liberación de calcio por las células T , se colocan, sirviéndose de pinzas, en la trayectoria de las células T . 
Si instalamos la célula B en uno de los extremos de la célula T quiescente, no se induce ningún cambio; la mancha fluorescente roja en la célula T permanece roja ( arriba ).
Pero cuando la célula B se coloca en el otro extremo de la célula T , se produce una liberación de calcio que se pone de manifiesto por la fluorescencia amarilla ( abajo ). 

Figura 6

EL SISTEMA DE MICROHAZ de tijeras de láser incorpora un haz pulsante y un microscopio de contraste de fases

Los pulsos, que duran 10 nanosegundos, son visibles cuando la luz discreta brilla a lo largo de la trayectoria del láser.
El sistema que aparece en la foto es el predecesor directo del sistema confocal de ablación-captura, cuyos haces, encerrados, no se dejan ver.


TRATAMIENTO DE LA ENFERMEDAD GENÉTICA 

En las próximas décadas, el impacto de la biología molecular y la ingeniería proteica en el tratamiento de la enfermedad genética puede ser tan grande como el que produjeron los polimorfismos de DNA y la clonación posicional en la genética médica general durante la década anterior.
En este capítulo se revisan las terapias estándar empleadas en el tratamiento de la enfermedad genética y se delinean algunas de las nuevas estrategias que pueden utilizarse en el futuro.
En particular se subrayan las terapias que reflejan el enfoque genético en medicina.
Como con toda terapia, el objetivo de tratar una enfermedad genética es eliminar o reducir los efectos del trastorno.
No sólo en el paciente, sino también en su familia.
Además. 
La familia debe ser informada sobre los riesgos de que la enfermedad ocurra en otros miembros.
Esta última responsabilidad, el consejo genético, constituye un componente principal del tratamiento de trastornos hereditarios y se aborda en el capítulo 18.
El tratamiento de muchas enfermedades monogénicas consistirá finalmente en la terapia de transferencia génica, siempre que sea posible realizar el procedimiento con seguridad y efectividad.
Sin embargo, incluso cuando puedan transferirse copias de un gen normal al paciente para realizar una curación permanente, la familia necesitará consejo genético, pruebas de portador y diagnóstico prenatal, en muchos casos a lo largo de varias generaciones.
Actualmente es posible en algunos casos reemplazar la proteína defectuosa, mejorar su función o reducir las consecuencias de su Deficiencia.
Se han publicado varias revisiones excelentes del tratamiento de la enfermedad genética, incluidas las de Valle (1987), Beaudet y cols. (1989), Rosenberg (1990) y Desnick 1991).

ESTADO ACTUAL DEL TRATAMIENTO DE LA ENFERMEDAD GENÉTICA

Enfermedades multifactoriales

Para la mayor parte de las enfermedades multifactoriales, los componentes genético y ambiental de la etiología se comprenden de manera deficiente.
Sin embargo, cuando se reconoce una contribución ambiental, se dispone de una oportunidad para intervenir efectivamente, porque la exposición al factor ambiental a menudo puede modificarse.
Así, el tabaquismo constituye un factor ambiental que todos los pacientes con enfisema deben evitar.
Al menos un mecanismo por el que el humo de los cigarrillos genera enfisema se ha revelado con el estudio de la deficiencia de xxx -antitripsina, xxx ; un trastorno monogénico. 
Como se describió en el capítulo anterior, el humo del cigarrillo oxida el residuo de metionina fundamental en el lugar activo de la xxx , xxx , lo que reduce 2.000 veces su capacidad para inhibir la elastasa. 
De este modo, el tabaquismo produce una pérdida sustancial adquirida de la función de xxx . 

La impresión general de que las enfermedades genéticas son invariablemente difíciles de tratar es desmentida por los datos sobre trastornos de herencia multifactorial en neonatos que pueden curarse con cirugía, una forma de modificación Fenotípica.
Estas alteraciones incluyen cardiopatías congénitas.
Labio leporino y paladar hendido, y estenosis pilórica.
Estas tres anomalías estructurales afectan a cerca del 1,5% de todos los recién nacidos vivos, lo que constituye aproximadamente el 30 % de todos los recién nacidos con enfermedad genética.
En alrededor de la mitad de estos pacientes, las entidades son curables mediante una sola operación.
Es posible curar al menos del 10 al 15 % de los niños con un trastorno genéticamente determinado.
Sin embargo, el tratamiento de la enfermedad heredada casi nunca resulta tan beneficioso, aunque con frecuencia mejora la calidad de vida.

Para trastornos multifactoriales que de modo característico se presentan en la adolescencia o la vida adulta, como la hipertensión esencial, la diabetes, la enfermedad de arteras coronarlas, la esquizofrenia y otras psicosis principales las imperfecciones del tratamiento reflejan la ignorancia de la etiología. 

Enfermedades monogénicas

El tratamiento de las enfermedades monogénicas es muy deficiente hasta el momento.
Un estudio reciente de 351 trastornos mostró que con la terapia actual la duración de la vida puede hacerse normal sólo en alrededor del 15% de las alteraciones monogénicas' la capacidad reproductiva en el 11% y de la adaptación social en el 6%.
No obstante, el análisis indicó que el éxito del tratamiento es mayor para trastornos en los que el defecto bioquímico básico se conoce (fig. 13- 1).
Incluso en este grupo favorecido, sin embargo, la terapia fue completamente correctiva de la disfunción fenotípica sólo en el 12%, lo fue parcialmente en el 40 % y no produjo ningún efecto en el resto.
Así, la investigación para averiguar la base genética y bioquímica de la enfermedad hereditaria tiene un impacto en el paciente, aunque incluso en trastornos bioquímicamente definidos el tratamiento actual no restablece la salud normal en la gran mayoría de los pacientes.


El actual estado insatisfactorio del tratamiento de la enfermedad genética se debe a numerosos factores' incluyendo los tres siguientes:
1 El locus mutante es desconocido en más del 80 % de las enfermedades genéticas y el conocimiento de la fisiopatología en los trastornos en los que se conoce el gen afectado o la anomalía bioquímica es inadecuado. 
En la fenilcetonuria ( PKU ), por ejemplo, a pesar de los años de estudio, todavía no se conocen los mecanismos por los que la elevación de la fenilalanina deteriora el desarrollo y la función cerebrales.
Para enfermedades como la fibrosis quística y la distrofia muscular de Duchenne, en las que la proteína afectada se ha identificado recientemente, se requiere mucho trabajo antes de que se determine la función normal del polipéptido y se comprenda por completo el proceso de la enfermedad

2 Algunas mutaciones actúan en un momento temprano del desarrollo o causan patología irreversible antes de que se diagnostiquen. 
Estos problemas pueden anticiparse en algunos casos si existen antecedentes familiares de enfermedad genética o si el cribado de portadores identifica parejas en riesgo.
En este último caso, el tratamiento prenatal algunas veces es posible para alteraciones médicas y quirúrgicas.

3 Con frecuencia, los casos iniciales de una enfermedad por diagnosticar están gravemente afectados y muchas veces son menos susceptibles al tratamiento que aquellos con expresión más leve.
En estos últimos, la proteína mutante puede retener alguna función residual que puede incrementarse mediante una de varias estrategias, como se describe más adelante.


Figura 1

Efecto del tratamiento de enfermedades genéticas en tres manifestaciones de la enfermedad. 

Aunque la respuesta al tratamiento es pobre tanto en enfermedades de causa desconocida como en las que se conoce el defecto bioquímico, los resultados son mejores en este último grupo.

CONSIDERACIONES ESPECIALES SOBRE EL TRATAMIENTO DE LA ENFERMEDAD GENÉTICA

Necesidad de valoración a largo plazo del tratamiento

En la enfermedad genética, quizá más que en otras áreas de la medicina, un tratamiento en un principio juzgado como eficaz puede finalmente resultar imperfecto. 
Existen al menos tres facetas en este problema.
En primer lugar, un tratamiento en principio eficaz puede mostrar en observaciones a más largo plazo ciertas insuficiencias sutiles.
Así, aunque los niños con PKU bien tratados no presentan un retraso grave y tienen cocientes intelectuales normales o casi normales (v. más adelante).
A menudo manifiestan trastornos de aprendizaje sutiles y anomalías del comportamiento que deterioran sus resultados académicos.
De manera similar, en pacientes con galactosemia se han reconocido anomalías del aprendizaje semejantes a las que se observan en individuos con PKU tratados.

En segundo lugar, tras el tratamiento eficaz de los cambios patológicos en un órgano pueden surgir problemas inesperados en tejidos que previamente no se identificaron como implicados clínicamente, ya que los pacientes no sobreviven lo suficiente.
Por ejemplo. a pesar de un tratamiento adecuado, la mayoría de las mujeres con galactosemia presentan insuficiencia ovárica que parece causada por una toxicidad continua producida por galactosa.
La detección de manifestaciones tardías puede requerir muchos años de observación después de la terapia inicial.
Otra enfermedad que ilustra este fenómeno es la cistinosis , que se debe a acumulación de cistina en el lisosoma por un defecto en su segregación.
El almacenamiento de cistina provoca inicialmente insuficiencia renal.
Sin embargo, a medida que los pacientes que reciben trasplantes renales crecen, se genera la morbididad por hipotiroidismo, enfermedad de los islotes que causa diabetes y varias anomalías neurológicas Un último ejemplo lo proporcionan las mutaciones en el gen del retinoblastoma (v. cap. 16).
Los pacientes tratados con éxito por tumor ocular en los primeros años de vida presentan un riesgo incrementado de desarrollar un cáncer independiente, un osteosarcoma, después de la primera década.
Por lo tanto, irónicamente, un tratamiento que prolonga con éxito la vida proporciona una nueva oportunidad para la expresión clínica del defecto básico, en particular en condiciones en las que el gen mutante se expresa normalmente en muchos tejidos, proporcionando así más dianas potenciales para el desarrollo de la patología.

En tercer lugar, una terapia que no produce problemas a corto plazo puede inducir serios efectos secundarios a largo plazo.
Por ejemplo, la administración del factor de coagulación en la hemofilia genera algunas veces la formación de anticuerpos para la proteína suministrada, y la transfusión sanguínea en la talasemia causa invariablemente una sobrecarga de hierro que puede ser tratada, pero con dificultad.

Heterogeneidad genética y tratamiento 

El tratamiento óptimo de defectos monogénicos a menudo requiere un grado inusual de precisión diagnóstica, a nivel de la molécula afectada.
Como se señaló repetidamente en capítulos anteriores, la heterogeneidad genética (heterogeneidad alélica o de locas) constituye una característica común de las enfermedades genéticas. 
Para un tratamiento adecuado, con frecuencia resulta fundamental no sólo tratar una anomalía bioquímica que a veces puede ser secundarla, como las señaladas en la tabla 12-9, sino identificar con precisión el defecto bioquímico básico.
Por ejemplo, las anomalías de la fenilalanín-hidroxilasa y de las enzimas del metabolismo de la biopterina producen hiperfenilalaninemia, pero el tratamiento de los dos tipos de defectos es muy diferente.
Incluso las mutaciones alélicas pueden requerir tratamientos distintos: la talasemia y la enfermedad drepanocítica, dos trastornos de la B-globina clínicamente diferentes, ilustran este concepto.

El estudio de las variantes alélicas de los defectos enzimáticos ha mostrado que los alelos que retienen pequeñas cantidades de actividad enzimática residual casi siempre causan una enfermedad mucho menos grave que los alelos nulos. 
La diferencia entre pacientes con PKU clásica (prácticamente sin actividad enzimática residual) y los que presentan hiperfenilalaninemia benigna (con alrededor del 5 % de actividad) ilustra este principio.
El corolario de esta observación es que el tratamiento efectivo de PKU clásica mediante transferencia génica o enzimática requerirá la administración de sólo pequeñas cantidades de fenilalanín-hidroxilasa.

La heterogeneidad alélica tiene implicaciones adicionales para la terapia.
Algunos alelos producen menos cantidad de una proteína que, sin embargo, tiene función residual.
Las Estrategias diseñadas para incrementar la expresión o la estabilidad de la proteína parcialmente funcional pueden resultar eficaces para corregir el defecto bioquímico. 
En contraste, no resulta efectivo incrementar la cantidad de una proteína mutante sin función residual.
De hecho, la expresión aumentada de una proteína mutante sin función puede ser deletérea, porque es posible que ejerza un efecto dominante negativo si interactúa con el producto del alelo normal o con otras proteínas deteriorando su función.
Esta consideración también es importante en los procedimientos de transferencia de un gen normal a un paciente con una enfermedad genética. 
Por ejemplo, en la osteogénesis imperfecta, los pacientes con alelos nulos pueden ser más fáciles de tratar mediante transferencia génica que los que producen cadenas de colágeno cualitativamente anormales que reducen la contribución efectiva del gen transferido.

ESTRATEGIAS DE TRATAMIENTO

La enfermedad genética puede tratarse a muchos niveles, en los diferentes pasos desde el gen mutante.
En el resto de este capítulo se describe el fundamento utilizado o propuesto para el tratamiento en cada uno de estos niveles.
En general, las enfermedades ya descritas en este libro se emplean como ejemplos, aunque también se presentan nuevos trastornos cuando es necesario para ilustrar un enfoque específico. 
Ninguno de los tratamientos actuales son por necesidad mutuamente excluyentes, aunque la terapia de transferencia génica eficaz hará superfluas otras terapias.
El tratamiento «a nivel del fenotipo clínico» constituye una categoría que intenta incluir todos los tipos de intervenciones médicas o quirúrgicas que no son específicos del tratamiento de la enfermedad genética.
Con frecuencia, ésta es la única terapia disponible y, en algunos casos, puede ser todo lo que se requiere, como en el caso, por ejemplo, de algunas malformaciones corregibles. 
Por último, debe subrayarse mucho la importancia de educar quirúrgicamente al paciente -no sólo lograr que comprenda la enfermedad.
Sus implicaciones genéticas y el tratamiento, sino también asegurar su conformidad con una terapia que puede resultar incómoda y durar toda la vida.

Figura 2

Diversos niveles de tratamiento relevantes para la enfermedad genética, con las Estrategias correspondientes empleadas en cada nivel

Tabla 2

Nivel de intervención Estrategia terapéutica

Nivel de intervención.
Estrategia terapéutica.


Gen Mutable.
Modificación del genotipo somático (trasplante; terapia de transferencia génica en el futuro cercano) .
Modulación farmacológica de la expresión génica.


mRNA mutante.


Proteína mutante.
Sustitución proteica .
Incremento de la función proteica residual .


Disfunción metabólica o bioquímica de otro tipo.
Compensación especifica de enfermedad (dietaria o farmacológica en pacientes seleccionados).


Fenotipo clínico .
Intervención médica o quirúrgica;educación del enfermo.


La Familia.
Consejo genético; detección sistemática de portadores; diagnóstico presintomático.


Tratamiento de anomalías metabólicas

El procedimiento específico más eficaz para el tratamiento de la enfermedad genética ha sido a nivel de la anomalía metabólica.
De hecho, este concepto es familiar para quienes están al tanto de las necesidades de compensación dietética por la incapacidad de los seres humanos y otros primates superiores para sintetizar el ácido ascórbico (vitamina C).
Las estrategias principales empleadas para manipular el metabolismo en el tratamiento de errores congénitos se indican en la tabla 13-1.
La necesidad de evitar ciertos fármacos y sustancias químicas en pacientes con enfermedades farmacogenéticas, como la deficiencia de glucosa -6- fosfato-deshidrogenasa, se describe en el capítulo 12.

Tabla 1

Tratamiento de la enfermedad genética mediante manipulación metabólica

Tipo de intervención metabólica.
Sustancia o técnica.
Ejemplo.


Anulación.
Fármacos antipalúdicos .
Deficiencia de G6PD.
Isoniacida.
Acetiladores lentos.


Restricción dietaria.
Fenilalanina.
PKU .
Galactosa.


Galactosemia.


Sustitución.
Tiroxina.
Hipotiroidismo congénito.
Biotina.
Deficiencia de biotinidasa.


Desviación.
Benzoato de sodio.
Trastornos del ciclo de la urea .
Resinas orales.
Hipercolesterolemia familiar .


Inhibición.
Lovastatina .
Hipercolesterolemia familiar.


Depleción.
Terapia de cambio plasmático .
Hipercolesterolemia familiar.


Restricción dietética

La restricción dietética es uno de los métodos más antiguos y más efectivos para tratar la enfermedad genética.
Su ventaja es que puede resultar muy eficaz:
su inconveniente es que generalmente requiere cumplimiento de por vida. con una dietética restringida y a menudo artificial.
Esta obligación dietética es molesta para la familia tanto como para el enfermo en especial en la adolescencia.
Muchas de las enfermedades que es posible tratar de esta manera comprenden vías catabólicas de aminoácidos y. por consiguiente, generalmente es necesaria una severa restricción de proteína dietaria.
Sin embargo, los nutrientes esenciales como los aminoácidos no pueden eliminarse por completo:
su ingestión debe ser la suficiente para cubrir las necesidades anabólicas. 
Los individuos con defectos enzimáticos leves (p. ej. , los alelos mutantes <<de mala calidad>>) pueden tolerar mayor cantidad del compuesto agresor, por lo que la dieta está menos restringida y el cumplimiento puede ser mejor.
En otros casos, el precursor dietario del sustrato agresor no es un nutriente esencial (p. ej., la galactosa) y es posible eliminarlo por completo de la dieta.
En la actualidad hay más de 24 enfermedades que comprenden otros tantos loci que se tratan de esta manera.

Una dieta restringida de fenilalanina evita en gran medida el daño neurológico en la PKU clásica .
Los niños fenilcetonúricos son normales al nacimiento, porque la enzima materna los protege durante la vida prenatal.
Los resultados del tratamiento son mejores cuando el diagnóstico se realiza poco tiempo después del nacimiento y el tratamiento se comienza con rapidez.
Si el niño recibe una dieta normal en los primeros meses de vida, se produce retraso mental irreversible:
el grado de déficit intelectual se relaciona directamente con la demora en el inicio de la dieta baja en fenilalanina.
El estado mental normal de pacientes con hiperfenilalaninemia benigna demuestra que el tratamiento efectivo de la PKU clásica puede realizarse si los niveles de fenilalanina se mantienen por debajo de aproximadamente 0,7 mmol.
Sin esta guía natural se hubieran requerido muchos estudios clínicos para establecer un nivel plasmático de fenilalanina «seguro» en la enfermedad clásica.
Hoy se recomienda que los pacientes con PKU se mantengan con una dieta baja de fenilalanina durante toda su vida, ya que se desarrollan anomalías neurológicas y de conducta en muchos de ellos (aunque no en todos) si la dieta se interrumpe.
Sin embargo, incluso en pacientes que se han tratado a lo largo de toda su vida, hoy se sabe que, aun cuando su cociente intelectual es normal o casi normal, existen déficit neuropsicológicos (p. ej., incapacidades conceptuales, espaciovisuales y de lenguaje), si bien el tratamiento produce resultados muy superiores a lo que ocurre sin éste.

Sustitución

El suministro de metabolitos esenciales, cofactores u hormonas cuya Deficiencia se debe a una enfermedad genética es conceptualmente simple y con frecuencia lo es también su aplicación.
Algunos de los defectos monogénicos tratados con mayor éxito pertenecen a esta categoría. 
Un ejemplo importante es el hipotiroidismo congénito causado por diversos defectos (10 a 15 % de los cuales son genéticos) en la formación de la glándula tiroides o su producto principal, la tiroxina. 
Debido a que el hipotiroidismo congénito es frecuente (alrededor de 1/4.000 neonatos) y a que el tratamiento puede prevenir el retraso mental asociado, muchos países realizan un cribado neonatal, de manera que la administración de tiroxina puede iniciarse inmediatamente después del nacimiento para prevenir los graves defectos intelectuales que de otro modo son inevitables.
En la deficiencia de biotinidasa , la enzimopatía impide la recuperación de biotina a partir de proteínas que la contienen, evitando así el reciclaje de este cofactor enzimático.
La administración de grandes cantidades de biotina resulta completamente correctiva si se suministran antes de que se desarrollen graves secuelas neurológicas.

GENÉTICA DEL CÁNCER

El cáncer es uno de los problemas más frecuentes y graves de la medicina clínica.
Las estadísticas muestran que alguna forma de esta enfermedad afecta a más de una tercera parte de la población, provoca más del 20% de todas las muertes y, en países desarrollados, genera más del 10% del coste total de la atención médica.

El cáncer no es una sola enfermedad, sino un nombre aplicado a una gran variedad de tumores malignos que se forman por el mismo proceso básico de crecimiento descontrolado. 
La proliferación celular genera una masa (neoplasia o tumor) que invade tejidos vecinos (de ahí el nombre de cáncer que significa cangrejo) y puede producir también metástasis en layares más distantes.
El crecimiento es autónomo, crecientemente maligno, y si no se trata, invariablemente fatal.
El diagnóstico y tratamiento precoces son vitales, y la identificación de personas con riesgo incrementado de cáncer antes del desarrollo de la enfermedad constituye un objetivo importante de la investigación en esta área.

Un tumor está compuesto de un parénquima de células proliferativas, con una estroma de tejido conectivo y vasos sanguíneos.
Existen tres formas principales, sarcomas , en los que el tumor se ha formado en el tejido mesenquimatoso: carcinomas , que se originan en el tejido epitelial, y enfermedades malignas hematopoyéticas y linfoides , como la leucemia y los linfomas.
Dentro de los grupos principales, los tumores se clasifican por su lugar, tipo de tejido y grado de malignidad.
La mayor parte de los cánceres son trastornos de la época tardía de la vida, pero algunos son característicos de la infancia o pueden mostrar un grado mayor de malignidad si ocurren en personas jóvenes. 

NATURALEZA GENÉTICA DEL CÁNCER 

El concepto de cáncer como una enfermedad genética es relativamente nuevo.
Quizás el 5% de todos los cánceres parecen seguir un patrón familiar; en la mayor parte, sin embargo, la herencia parece desempeñar un papel pequeño o nulo.
No obstante, machos cánceres, como otras enfermedades que muestran características de herencia multifactorial, poseen un importante componente genético en cuanto a que ciertas personas son más susceptibles de desenrollar una malignidad particular como resultado de defectos genéticos que predisponen a cáncer.
Además, es evidente que prácticamente todo cáncer es esencialmente -incluso en ausencia de componentes heredados aparentes- el resultado de mutaciones en células somáticas y su progresión también implica la expresión de una serie de genes. 

En el pasado, los virus y la exposición a agentes ambientales como la radiación ionizante se señalaron como causas de la mayor parte de los cánceres.
Hoy se reconoce que la causa de base es la mutación génica y. cuando participan agentes carcinógenos, operan produciendo mutación.
Las mutaciones que provocan cáncer afectan los genes responsables de la proliferación celular, el desarrollo de las células y otras actividades celulares fundamentales.
Cuando la regulación normal está alterada, se inicia el crecimiento descontrolado y se desarrolla un tumor maligno.

La comprensión de la base genética del cáncer que ha comenzado a surgir después de muchas décadas de investigación se ha producido en gran medida por la aplicación de la genética molecular al análisis de varias formas de enfermedad maligna.
Como ocurre a menudo en la genética médica, lo anormal ha proporcionado datos con respecto a lo normal:
aprender lo que está mal en el cáncer también ha ayudado a clarificar el control genético de muchos aspectos del crecimiento celular normal.

El campo de la genética del cáncer es uno de los de mayor movimiento en la biología y medicina actuales, y la comprensión de los mecanismos del cáncer se está incrementando con rapidez.
Aunque aún queda mucho por aprender, están comenzando a aparecer los contornos de una hipótesis unificadora.
En este capítulo se considera el estado actual del conocimiento de la base genética del cáncer incluyendo el papel de genes específicos como causa de cánceres tanto hereditarios como esporádicos.

Naturaleza clónica del cáncer 

Existen firmes pruebas de que un tumor es un clon de células derivadas de una sola célula ancestral, en la que ha ocurrido el evento iniciador (una mutación somática).
La naturaleza clónica de los tumores se conoce desde hace muchos años.
La prueba original se obtuvo del estudio de tumores en mujeres heterocigotas para la enzima ligada al X glucosa -6- fosfato-deshidrogenasa (GóPD).
Como se señaló previamente, debido a la inactivación del X, sólo uno del par de alelos ligados al X en una mujer heterocigota se expresa en una célula somática. 
Las líneas celulares derivadas de tumores en estas mujeres expresaron uno u otro alelo de GóPD, pero no ambos, lo que indicó que cada tumor había crecido a partir de una sola célula.
Los reordenamientos cromosómicos característicos en muchos tipos de cáncer, como las translocaciones que se observan en el linfoma de Burkitt y en la leucemia mieloide crónica (que se describe más adelante), también indican que estas enfermedades malignas tienen origen en una sola célula.

DOS CLASES DE GENES DE CÁNCER 

Los genes que provocan cáncer son de dos tipos distintos: oncogenes y genes supresores de tumor .
Ambos tipos tienen efectos opuestos en la carcinogénesis. 
Los oncogenes facilitan la transformación maligna, mientras que los genes supresores de tumor, como su nombre indica, bloquean el desarrollo del tumor reculando los genes que participan en el crecimiento celular.

Figura 1

Esquema general de los mecanismos de oncogénesis por activación y mutación de protooncogén o pérdida de genes supresores de tumor

El crecimiento y proliferación celulares son estimulados por los productos de protooncogenes y se encuentran bajo el control negativo de genes supresores de tumor que evitan el hipercrecimiento.
La acción de los oncogenes es dominante y requiere sólo una mutación simple, lo que provoca crecimiento celular descontrolado.
La acción de los genes supresores de tumor es recesiva; cuando ambos alelos están matados o se pierden, el crecimiento celular no está regulado y genera formación de tumor.

Oncogenes

Los oncogenes son genes que afectan el crecimiento y desarrollo celular normal.
Si un oncogén está alterado o sobreexpresado, a causa de una mutación en el gen mismo o por una alteración del control externo, la célula en la que ocurre el cambio puede sufrir crecimiento descontrolado y finalmente convertirse en maligna.
La mayor parte de los oncogenes son formas mutadas («activadas») de genes normales, denominadas protooncogenes , que participan en el control de la proliferación y diferenciación celular.

Los oncogenes pueden identificarse experimentalmente en estudios de transferencia de DNA por su capacidad para transformar una línea celular de ratón no oncogénica en cultivo para que genere focos de células con propiedades oncogénicas.
Hasta la fecha se han identificado más de 50 oncogenes humanos (y. por lo tanto, sus protooncogenes normales), sobre todo gracias a los estudios de transfección de DNA con DNA genómico de tumores humanos.

Es digno de medición que muchos de los oncogenes identificados en tumores humanos (pero no todos) han llegado a relacionarse con oncogenes víricos previamente aislados de virus tumorales de RNA.
La demostración de que la información genética de un virus podía cambiar una célula normal en maligna estableció la posibilidad de que los cenes actuaran como controladores centrales de la conversión maligna.
Los virus de RNA conocidos como retrovirus poseen la propiedad singular de transcribir RNA en DNA, utilizando la enzima transcriptasa inversa.
El DNA vírico puede entonces integrarse en el DNA cromosómico del huésped y expresarse. 
Cuando se caracterizó el llamado oncogén xxx (secuencias transformadoras del virus del sarcoma de Rous), se halló que éste no era un verdadero gen vírico, sino un gen huésped que había sido recogido por un ancestro del virus a través de un proceso denominado transducción . 
El gen huésped equivalente fue el primer protooncogén que se reconoció.
Ahora, como se mencionó antes, se sabe que muchos protooncogenes están relacionados con virus tumorales de RNA específicos.
(A diferencia de los oncogenes de retrovirus, los oncogenes de virus tumorales de DNA, como el virus de simio xxx y el virus del polioma, no son protooncogenes transducidos, sino genes víricos.)
Los protooncogenes han permanecido muy conservados en la evolución; por ejemplo, el protooncogén xxx (denominado así debido a que se identificó originalmente en el virus del sarcoma de la rata de Harvey) y su proteína correspondiente se han encontrado en organismos tan alejados en la escala de la evolución como los seres humanos y las levaduras, lo cual sugiere que estas proteínas poseen papeles biológicos esenciales.
Los diferentes papeles de las distintas clases de protooncogenes en la regulación del crecimiento se ilustran en la figura 2.

Tabla 1
<head type=main rend= "bo">Ejemplos seleccionados de oncogenes

Oncogén.
Fuente.
localización del protooncogén en el mapa.
Propiedad bioquímica .


Proteínas de transducción de señal.


Virus de la leucemia marina de Abelson.
Virus del sarcoma de Roas (pollo).
Carcinoma de colon humano.
Virus del sarcoma de la rata de Harvey.
Varios tumores humanos (neuroblastomas) .


Proteínas nucleares de ligamiento de DNA.


Sarcoma del pollo.
Neuroblastoma humano.


Factores de crecimiento secretados.


Virus del sarcoma del simio .


Receptores de superficie celular del factor de crecimiento.


Eritroblastosis en aves.


Enlaza DNA.


Desconocida.


Cadena B del factor de crecimiento derivado de plaquetas (PDGF).


Receptor de esteroides.


Figura 2

Transducción de señal y regulación de crecimiento por productos de protooncogenes, que se clasifican por su localización y función en la célula

La desregulación de un protooncogén puede provocar transformación maligna
1 Factores de crecimiento secretados, como el EGF (factor de crecimiento embrionario) y el PDGF (factor de crecimiento derivado de plaquetas)
2 Receptores específicos para los factores de crecimiento secretados
3 Proteínas de transducción de señal citoplasmáticas como xxx y xxx , que son cinasas proteicas y proteínas de ligamiento de GTP
4 Proteínas nucleares como xxx interactúan con genes mediante enlace con DNA

Activación del protooncogén 

Un importante hallazgo relacionado con los protooncogenes surgió del análisis molecular de un oncogén ras derivado de una línea celular de carcinoma de vejiga.
El oncogén y su contrapartida, el protooncogén, diferían sólo en un par de bases.
La alteración, una mutación puntual en una célula somática del tumor, propició la síntesis de un producto génico anormal que pudo estimular el crecimiento de la línea celular y convertirla en un tumor.
Los oncogenes tienen un efecto dominante a nivel celular es decir, cuando se activan, un solo alelo mutante es suficiente para cambiar el fenotipo de una célula de normal a maligno. 
Las mutaciones puntuales ras se observan en muchos tumores, y experimentalmente se ha mostrado que los genes ras constituyen la diana de carcinógenos conocidos' un hallazgo que apoya el papel de los genes r as matados en el desarrollo de muchos cánceres.

La mutación estructural es sólo uno de los varios mecanismos que pueden inducir activación de protooncogenes (tabla 16-2).
Las translocaciones cromosómicas son un mecanismo frecuente para la activación de un protooncogén en diversos cánceres.
En la leucemia mieloide crónica (LMC), que se expone con más detalle en una sección posterior, la expresión incrementada y la transformación maligna se relacionan con la translocación del protooncogén abl de su posición normal en el cromosoma 9 al punto de rotura en el cromosoma 22 en una célula madre hematopoyética. 
La translocación 9; 22 contribuye directamente al desarrollo del fenotipo maligno y también resulta un buen indicador diagnóstico de la LMC, al igual que lo son otras translocaciones características en otros tipos específicos de cáncer.

Los cambios citogenéticos son típicos del cáncer y mucho más frecuentes en estadios tardíos o más malignos o invasivos que en estadios tempranos del desarrollo tumoral.
Hasta ahora, la mayor parte de los estudios citogenéticos de la progresión del tumor se han relacionado con las leucemias.
En la LMC, por ejemplo, en la etapa de crisis de blasto, pueden existir varias anomalías citogenéticas adicionales, incluyendo cambios numéricos o estructurales, como una segunda copia del cromosoma 9;22 translocado o un isocromosoma para 17 q.
En las etapas avanzadas de otras formas de leucemia, las translocaciones son frecuentes.
Un aspecto de la investigación del cáncer es la definición citogenética y molecular de estas anomalías, muchas de las cuales ya se sabe que están relacionadas con protooncogenes y posiblemente permiten el incremento de la expresión de éstos.

Otro mecanismo importante para la sobreexpresión es la amplificación génica , un proceso raro o inexistente en las células normales, pero que algunas veces resulta frecuente en las células cancerosas.
Los segmentos amplificados de DNA a menudo se detectan como dos tipos de cambio citogenético, diminutos dobles (cromosomas accesorios muy pequeños) y regiones homogéneamente tenidas (HSR), que no se bandean de forma normal y contienen múltiples copias amplificadas de un segmento particular de DNA.
Aún no se entiende cómo y por qué se producen los diminutos dobles y la HSR, pero se sabe que las regiones amplificadas incluyen copias adicionales de protooncogenes con efectos en el crecimiento celular.
Por ejemplo, el protooncogén xxx está amplificado más de 200 veces en el 40% de los neuroblastomas.
Se considera que este fenómeno se relaciona con la progresión del tumor. 

Necesidad de más de un gen activado 

Un aspecto importante del inicio y progresión de la carcinogénesis por protooncogenes activados es que la mutación de un solo gen parece incapaz de provocar la transformación: más bien, parece que se requieren genes separados con efectos complementarios.
Aunque aquí no se describen los detalles de los experimentos que demuestran que las células normales requieren cooperación entre diferentes oncogenes activados para que se produzca transformación, la conclusión es importante:
la transformación maligna constituye un proceso de eventos múltiples que no se logra con un solo paso.

Tabla 2

Mecanismos de activación de protooncogenes

Mecanismo.
Tipo de gen activado.
Resultado.


Mutación reguladora.


Mutación estructural.


Translocación, inserción retrovírica, amplificación génica .


Genes del factor de crecimiento.


Receptores del factor de crecimiento, proteínas transductoras de señal.


Oncogenes nucleares.


Expresión o secreción incrementadas.


Permite autonomía de expresión.


Sobreexpresión.


Tabla 3

Translocaciones cromosómicas características en enfermedades malignas humanas seleccionadas

Neoplasia.
Translocación cromosómica .
Casos(%).
Protooncogén afectado.


Linfoma de Burkitt.


Leucemia mieloide crónica .


Leucemia linfocítica aguda.


Leucemia linfoblástica aguda.


Leucemia promielocítica aguda.


Leucemia linfocítica crónica.


Linfoma folicular.


Genes supresores de tumor

Mientras los productos de protooncogenes promueven el crecimiento, los de los genes supresores de tumor normalmente bloquean el crecimiento anormal y la transformación maligna, y contribuyen al proceso maligno sólo cuando se pierde la función de ambos alelos.
En otras palabras, en contraste con las mutaciones en protooncogenes, que son dominantes en su acción. 
Las mutaciones en los genes supresores de tumor son recesivas. 

Origen de «dos impactos» del cáncer

En la década de los años sesenta, DeMars propuso la existencia de mutaciones recesivas que generaban cáncer. 
Este autor sugirió que algunas formas de cáncer podían iniciarse cuando una célula en un individuo heterocigótico para una mutación recesiva en la línea germinal sufría una segunda mutación somática, convirtiendo la célula en homocigota y generando un tumor.
Esta hipótesis de «dos» impactos, fue ampliada por Knudson (1971), quien propuso que algunas formas de cáncer como el retinoblastoma , que ocurren tanto hereditaria como esporádicamente, podían explicarse sobre la base de que en la forma hereditaria la primera mutación se transporta en la línea germinal mientras que en la forma esporádica ambas mutaciones son somáticas y ocurren en la misma célula somática. 
Hoy se reconocen los genes normales como genes supresores de tumor, que desempeñan papeles reguladores en la proliferación, diferenciación y otras funciones celulares básicas. 
La inactivación de genes supresores de tumor por mutación o algún otro mecanismo produce pérdida de regulación y, por tanto, resulta oncogénica. 

Genes supresores de tumor en muchas formas de cáncer

Como se describe con detalle más adelante.
Los genes supresores de tumor se han implicado en varias formas mendelianas graves de cáncer, incluidos el retinoblastoma, el tumor de Wilms, la poliposis familiar, la neurofibromatosis tipo I y la rara forma de cáncer familiar conocida como síndrome de Li-Fraumeni.
Si bien en cada uno de estos trastornos la herencia autosómica dominante es la regla o al menos se observa en cierta proporción de casos, se requiere la pérdida o alteración de ambas copias del gen supresor de tumor para el desarrollo tumoral.
La explicación de esta aparente paradoja es que una sola copia funcional de un gen supresor de tumor es suficiente para proporcionar un fenotipo celular normal; sin embargo, una célula en la que una copia ya está alterada o perdida, por mutación en la línea germinal o somática, perderá su capacidad de suprimir el desarrollo tumoral si por casualidad sufre una mutación somática en el alelo restante.
Debido a que los tumores son clónicos por naturaleza, este evento puede provocar un tumor. incluso si sucede sólo en una de las numerosas células de un tejido.

La importancia de los genes supresores de tumor no se limita a las formas autosómicas dominantes de cáncer. 
Como se describe posteriormente, los genes supresores de tumor también participan en la progresión de varias formas de cáncer frecuente no hereditario, como el cáncer colorrectal.

Progresión tumoral por evolución clónica

El esquema recién delineado, donde el desarrollo de enfermedad maligna sólo requiere la activación de un protooncogén o la pérdida de función de ambos alelos de un gen supresor de tumor, está muy simplificado.
Por el contrario, la formación de tumor constituye un proceso de múltiples etapas que incluye una sucesión de cambios genéticos en la población celular del tumor en desarrollo.

Uno de los hallazgos sorprendentes con respecto a la progresión tumoral es que la pérdida o inactivación del mismo gen puede contribuir al desarrollo de varios cánceres diferentes frecuentes.
Por ejemplo, el gen p53 en el brazo corto del cromosoma 17 falta o es anormal en células de cánceres esporádicos de pulmón y mama.
El gen del retinoblastoma es defectuoso o falta en casi todos los casos de carcinoma de células pequeñas del pulmón y en muchos otros cases de cáncer pulmonar, pero no en cánceres como el de colon.
Aunque aún no está clara la sucesión completa de cambios genéticos secundarios y su papel en la progresión tumoral, es obvio que se requieren varias mutaciones en diferentes loci para que un tumor alcance su potencial completo de malignidad.

Se han aislado los productos de varios genes supresores de tumor.
Debido a que los genes supresores de tumor y sus productos son por naturaleza protectores frente al cáncer, se espera que su comprensión finalmente lleve a mejorar métodos de terapia anticancerosa.

Tabla 4

Productos de genes supresores de tumor seleccionados

Gen supresor de tumor.
Producto génico y funciones posibles.


Trastornos en que el gen está afectado.


Familiares .
Esporádicos.


Retinoblastoma .
Carcinomas pulmonares de células pequeñas, cánceres pulmonares.


Tumor de Wilms .
Cáncer pulmonar.


Síndrome de Li-Fraumeni.
Cáncer pulmonar, cáncer de mama.


Neurofibromatosis tipo 1.
Desconocido.


Desconocido .
Cáncer colorrectal.



Neoformación de órganos 

Se han dado los primeros pasos hacia la creación de órganos semisintéticos que sirvan de recambio de los naturales.

Miles de personas de todas las edades ingresan cada día en hospitales, obligadas por el malfuncionamiento de algún órgano vital.
Muchas morirán al faltar donaciones de trasplantes.
Por referir un ejemplo, sólo en los Estados Unidos, durante el año 1997, de los 40.000 necesitados de trasplante de corazón, lo recibieron sólo 2300, según datos oficiales.
También escasean hígados y riñones que podrían salvar vidas, como insuficiente es la piel para los quemados y otros cuyas heridas no sanan debidamente.
A menudo es más fácil reparar el coche que al conductor; para el primero se dispone de piezas de recambio, un lujo que el ser humano no ha podido permitirse, hasta ahora.

Pero un nuevo planteamiento, fascinante, podría revolucionar el tratamiento de pacientes urgidos de estructuras vitales nuevas: la creación de tejidos u órganos artificiales, los llamados neoórganos.
Un ingeniero de tejidos inyecta o deposita en una herida o un órgano que requiera regeneración una molécula, por ejemplo, un factor de crecimiento.
Esta molécula moviliza las células del paciente hacia la herida e insta su transformación en un tipo celular concreto, que regenere el tejido.
Más ambicioso es el trasplante de células, del propio paciente o de un donante, cultivadas de antemano e incorporadas en una urdimbre tridimensional de polímeros biodegradables, como los que se utilizan para hacer suturas resorbibles.
La estructura entera, células y urdimbre, se trasplanta a la herida; aquí, las células se dividen y reorganizan para formar tejido nuevo.
Al propio tiempo, se van desintegrando los polímeros artificiales, hasta que sólo queda formado un producto completamente natural, un neoórgano. 

La creación de neoórganos ha surgido de la aplicación de los avances de los últimos decenios en nuestros conocimientos de biología fundamental a la reconstrucción de tejidos, igual que el progreso experimentado en ciencia de materiales permite proyectos de nuevo cuño en arquitectura.

En la televisión y en el cine aparecen relatos fantasiosos de formación de órganos, de creación de individuos incluso, a partir de un extraño cultivo celular alimentado por un poderoso nutriente.
La ingeniería de los tejidos no llega a tanto.
El futuro que nos espera se adivina tras lo ya conseguido, a saber, la aplicación médica de tejido de síntesis en hospitales de los países avanzados.
En ese dominio revolucionario se incluyen piel, cartílago, hueso, ligamento y tendón artificiales.
No parece disparatado pensar en órganos al alcance de todos, algún día.

Al menos en línea de principio, pueden producirse hígados, riñones, mamas o intestinos, órganos grandes y complejos dotados, cada uno, de tipos celulares distintos. 
No ocurre otra cosa en el vientre de la mujer embarazada, en cuyo seno un puñado de células indiferenciadas se desarrollan hasta formar un individuo complejo, constituido por múltiples órganos y tejidos, con propiedades y funciones muy dispares.
El día en que se desentrañe el mecanismo en cuya virtud se forma el hígado o el pulmón, la ciencia se hallará capacitada para replicar el proceso. 

Las células responden de manera predictible si se las expone a factores bioquímicos determinados.
Para estimular el crecimiento de tejido nuevo, lo más directo consiste en dejar la herida o el órgano lesionados en contacto con factores que estimulen la curación o regeneración. 
La técnica se apoya en un par de observaciones clave realizadas en huesos y en vasos sanguíneos.

En 1965, Marshall R. Urist comprobó que, en los animales que habían recibido implantes de hueso pulverizado, se estimulaba la formación de nuevo tejido óseo. 
Tal observación motivó el aislamiento de las proteínas responsables de dicha función.
Eran las proteínas morfogenéticas óseas (" bone morphogenetic proteins ", BMP).
Se determinó la secuencia de ADN de los genes que las cifraban.
Más tarde, varias compañías empezaban a producir BMP humanas recombinantes en cantidades industriales mediante inserción de dichos genes en líneas de células de mamífero. 

Se están realizando ensayos clínicos para comprobar la capacidad de regeneración tisular que poseen estos promotores del crecimiento óseo.
Y se está analizando su posible aplicación en la curación de fracturas óseas agudas causadas en accidentes o en el reforzamiento de la regeneración de tejido periodontal enfermo.
En la fase de ensayo clínico terminada, BMP-7, fabricada por Creative BioMolecules, ha demostrado que ayuda a curar fracturas óseas severas.
Se ha seguido la evolución de 122 pacientes cuyas fracturas de la pierna no se habían soldado, pasados nueve meses.
Los resultados fueron parecidos en los pacientes cuya curación se estimuló con BPM-7 y en los que recibieron un injerto quirúrgico de hueso cultivado de otra parte de su cuerpo. 

La ingeniería de neoórganos ha de hacer frente a un reto decisivo, la alimentación célula por célula.
Los tejidos de un grosor superior a unos milímetros precisan vasos sanguíneos que crezcan en su interior y que permitan el aporte de nutrientes.
Debemos a Judah Folkmann saber que cabe estimular a las células para que produzcan nuevos vasos sanguíneos.
Folkmann descubrió tamaña posibilidad hace casi treinta años, mientras estudiaba cómo impedir la multiplicación de las células en los tumores malignos. 

Folkmann observó que los tumores en crecimiento necesitaban formar sus propios vasos sanguíneos para recibir nutrientes.
En 1972 propuso utilizar moléculas específicas para inhibir esa angiogénesis, al objeto de matar por inanición a los tumores.
Este enfoque de la lucha contra el cáncer dio sus primeros frutos en 1998.
Tras caer en la cuenta de que otras moléculas estimulaban también la angiogénesis, los investigadores han venido descubriendo nuevos factores, unos inhibidores y otros promotores.

Éxito del que comienzan a sacar partido los ingenieros de tejidos.
Han llegado a los anaqueles del comercio moléculas recombinantes que estimulan la angiogénesis.
Por experimentación animal se ha comprobado que estas moléculas promueven el crecimiento de nuevos vasos que sortean las obstrucciones de la arteria coronaria, por ejemplo.
Se preparan también ensayos a pequeña escala para someter a prueba este planteamiento en la terapia de patologías similares en humanos.

Antes de que los fármacos que estimulan la formación de tejidos y órganos alcancen un carácter rutinario, habrá que vencer algunos obstáculos.
De momento sólo se han determinado los factores responsables del crecimiento de los huesos y de los vasos sanguíneos. 
Para regenerar el hígado u otros órganos, tendremos que identificar primero y producir luego con la máxima fiabilidad las moléculas específicas de su desarrollo. 

Además, queda mucho por avanzar en la administración de las sustancias que deben dirigir la regeneración de los órganos.
¿Qué concentración específica de moléculas se necesita para producir el efecto deseado?
 
¿Cuánto tiempo deben estar expuestas las células a un factor dado?
 
¿Hasta cuándo persiste la eficacia de los factores en el cuerpo?
Se precisarán, sin duda, factores múltiples para la formación de órganos complejos; pero, ¿en qué momento exacto del desarrollo debe un factor sustituir a otro?
El dominio de la técnica de administración controlada de fármacos -pensemos en los parches transdérmicos desarrollados por la industria farmacéutica- habrá de hacer más fácil la solución del problema.

En particular, puede recurrirse a polímeros inyectables para colocar las moléculas bioactivas en su punto de operación, con una mínima intervención quirúrgica.
Michael J. Yaszemsky, Allan W. Yasko y Mikos (firmante de este artículo) trabajan en el desarrollo de nuevos polímeros biodegradables e inyectables para aplicaciones ortopédicas.
Gracias a su maleabilidad, los polímeros colman irregularidades, se endurecen en diez o quince minutos y proporcionan a la región esquelética reconstruida propiedades mecánicas parecidas al hueso que sustituyen. 
Luego, en un intervalo de semanas a meses, los polímeros se degradan según una pauta controlada mientras que el hueso neoformado ocupa su lugar.

Hemos venido estudiando las posibilidades de regeneración ósea dirigida que ofrecen los hidrogeles biodegradables e inyectables en el tratamiento de afecciones dentales, como la unión precaria del diente y el hueso.
Estos polímeros hidratados gelatinosos incorporan moléculas que modulan la función celular e inducen la formación ósea.
Tejen una urdimbre sobre la que pueda crecer el hueso nuevo y minimizan la formación de tejido cicatrizal en la región regenerada.

Los grupos de Jeffrey F. Bonadio y Steven A. Goldstein avanzaron una variedad interesante de la administración más frecuente; combinan conceptos de terapia génica e ingeniería de tejidos.
En vez de administrar directamente factores de crecimiento, insertan los genes que determinan la síntesis de esas proteínas.
Se valen de un plásmido, una molécula de ADN circular en la que se insieren dichos genes.
Las células incorporan el ADN y lo tratan como propio, convirtiéndose en fábricas en miniatura que producen los factores cifrados en el plásmido.
El ADN insertado no se incorpora en el ADN celular, sino que flota libremente y termina por degradarse, con lo que termina la síntesis.
La inserción de plásmidos ha favorecido la regeneración ósea en animales, pero habrá que ahondar más todavía en la duración de sus efectos.

David J. Mooney, junto con Lonnie D. Shea y nuestros colaboradores de Michigan han observado en animales que el engarce de polímeros biodegradables tridimensionales con plásmidos libera ADN durante largos períodos; al propio tiempo, los polímeros sirven de urdimbre para el nuevo tejido en formación.
El ADN se abre camino en las células adyacentes conforme éstas van penetrando en la urdimbre.
Las células expresarán entonces las proteínas deseadas.
A través de esta técnica se gobierna con mayor precisión la formación del tejido.
Quizá llegue el día en que los médicos regulen la dosis y el momento de producción de la molécula por las células que incorporan el ADN, activando múltiples genes a su debido tiempo para promover la formación de tejidos en diferentes fases. 

Ni que decir tiene que la estimulación del desarrollo de tejidos y órganos, vía factores de crecimiento, constituye un espectacular paso al frente.
Pero palidece cuando se le compara con la meta del ingeniero de tejidos:
la creación ex novo de neoórganos enteros.
La idea de "recambios prefabricados" empieza a adquirir forma en el buscado trasplante directo de células en el cuerpo donde deban luego transformarse en componente específico.
La mejor manera de producir órganos y tejidos hay que fiarla todavía en la sabiduría bioquímica del organismo.
Se transfieren las células apropiadas, en una matriz tridimensional, hasta el lugar deseado;
el crecimiento se produce dentro de la persona o del organismo, no en un medio externo y artificial. 
Se sigue este método ya en pacientes con lesiones cutáneas o alteraciones en cartílagos.
Establecieron sus bases Ioannis V. Yannas, Eugene Bell, Robert S. Langer, Joseph P. Vacanti y otros, en los años setenta y ochenta.

El procedimiento habitual exige la multiplicación de las células en cultivo.
De éstas se siembra una matriz de polímeros sintéticos o de colágeno (proteína ésta que forma la urdimbre de sostén natural de la mayoría de los tejidos).
Amén de aportar células, la matriz crea y mantiene un espacio para la formación del tejido; guía, también, su desarrollo estructural.
Cuando se conozcan de forma completa los mecanismos que gobiernan el desarrollo de un órgano o tejido, podrán fabricarse a partir de una pequeña muestra inicial de células. (El conocimiento suficiente de las vías de desarrollo debería permitir trasladar al laboratorio los proceso del organismo;
nos hallaríamos así ante órganos prefabricados genuinos.
El cirujano podría implantarlos inmediatamente en caso de urgencia -una situación de sumo interés si tenemos en cuenta que el fallo de una víscera puede llevar a la muerte fulminante-, sin esperar semanas o meses a que el nuevo órgano crezca en el laboratorio, ni recurrir a factores de crecimiento que induzcan el desarrollo de los tejidos en el interior del paciente).

Por lo que respecta a la piel, el proyecto se ha hecho ya realidad.
La norteamericana administración de alimentos y medicamentos (FDA) ha aprobado un producto de piel viva, y otros esperan turno.
Existe una apremiante necesidad de piel.
Se cuentan por millones los que sufren úlceras diabéticas, particularmente difíciles de sanar. 
A muchos otros se les extirpa piel durante la terapia de cáncer de piel, por no hablar de los injertos en quemados. 

Otro tejido cuyo uso se difundirá en humanos será el cartílago, en intervenciones ortopédicas, craneofaciales y urológicas.
El cartílago disponible hoy es insuficiente para el número de operaciones anuales destinadas a reparar articulaciones dañadas y para la cirugía de reconstrucción de cara y cabeza.
El cartílago, sobrio en el requerimiento de nutrientes, no precisa de la formación de nuevos vasos sanguíneos, lo que facilita su empleo directo en ingeniería de tejidos. 

Genzyme Tissue Repair, de Cambridge, ha recibido la aprobación de la FDA para el desarrollo de tejidos derivados de células del propio paciente cuando se trata de reparar lesiones traumáticas del cartílago de la rodilla.
El procedimiento abarca el cultivo de células, extraídas del paciente y a poder ser de la misma rodilla lesionada, y su posterior implante en la lesión.
En función del paciente y de la extensión del defecto, la regeneración completa tarda de 12 a 18 meses. 
En estudios en animales, Charles A. Vacanti, Joseph Vacanti, Langer y colaboradores han comprobado la posibilidad real de cultivar cartílago para su ulterior desarrollo adoptando formas reconocibles de orejas, narices y otras.

La relativa facilidad de cultivo y crecimiento del cartílago ha permitido a Anthony J. Atala una aproximación nueva al tratamiento de ciertas alteraciones urológicas, como la incontinencia.
La empresa Reprogenesis financia la investigación de Atala, que estudia la posibilidad de extraer células cartilaginosas del paciente, dejar que se multipliquen en el laboratorio y utilizarlas para añadir masa a la uretra o los uréteres y así mejorar la incontinencia urinaria en adultos y el reflujo vesical en niños. 
Estas enfermedades están causadas a menudo por una falta de tono muscular que permite que la orina fluya de forma inesperada hacia adelante en adultos o hacia atrás en el síndrome infantil.
A quienes padecen incontinencia severa o reflujo vesical se les somete a procedimientos varios, incluida una intervención quirúrgica delicada. 
Los adultos reciben a veces colágeno, que proporciona la misma masa que el implante de cartílago; pero termina por degradarse.
La nueva solución conlleva sólo un mínimo de cirugía invasiva para la instalación de las células y el crecimiento de nuevo tejido.

Walter D. Holder Jr., Craig R. Halberstadt y Mooney han empezado a aplicar los principios generales de la ingeniería de tejidos a una cuestión médica del máximo interés entre la población femenina. 
Nos referimos al empeño puesto en la fabricación de nuevo tejido mamario, extraído de piernas y nalgas, que sustituya al extirpado mediante mastectomía o tumorectomía. 
Se procede a una biopsia de tejido de la paciente; de ésta se aíslan células, para que crezcan y se multipliquen fuera del cuerpo.
Más tarde, estas células de la paciente podrían reimplantarse mediante una matriz de polímeros biodegradables.
En el organismo, el crecimiento de las células y la degradación de la matriz conducirían a un tejido completamente nuevo y natural.
De este proceso sólo resultaría una masa de tejido blando, no el complejo sistema de tipos celulares que encontramos en una mama verdadera, pero podría proporcionar una alternativa a las actuales prótesis o implantes de mama.

Al éxito obtenido con enfermedades humanas en modelos animales se debe el optimismo que se respira en el dominio de neoórganos con uno o varios tipos celulares.
Mikos ha demostrado la posibilidad del desarrollo de tejido óseo nuevo, sobre polímeros biodegradables, a partir de trasplantes de células de la médula ósea. 
El trasplante de células al propio lugar de deficiencia esquelética posibilita la síntesis celular in situ de los factores requeridos y, con ello, abre la puerta a una vía inédita de administración de fármacos promotores del crecimiento.

En cualquier sistema, el tamaño plantea sus propias exigencias.
Los tejidos de un tamaño sustancial dado precisan aporte de sangre.
Para satisfacer esa demanda, habrá que trasplantar los tipos celulares apropiados, junto con sustancias que estimulen la angiogénesis; así, enriqueciendo el polímero de la urdimbre con sustancias promotoras del crecimiento de los vasos.
Otros autores, nosotros entre ellos, nos inclinamos por la creación de una red de vasos sanguíneos en el órgano formado antes de su trasplante, mediante la incorporación de células que se transformen en vasos sanguíneos dentro de la matriz polimérica.
Bastaría luego con conectarlos a los vasos sanguíneos próximos al tejido para la llegada de la circulación.

En colaboración con Peter J. Polverini, Mooney ha demostrado que las células de los vasos sanguíneos trasplantados establecen tales conexiones y que los nuevos vasos están formados por células implantadas y del propio huésped.
La técnica podría resultar un fracaso si el tejido se trasplantara a una región donde los vasos sanguíneos hubieran sufrido la agresión de una terapia anticancerosa o un traumatismo.
Si así ocurriera, convendría promover el desarrollo del tejido en otro lugar, donde los vasos se formaran mejor en el interior de la nueva estructura.
Mikos aplica este enfoque con Michael J. Miller para la fabricación de hueso vascularizado en cirugía de reconstrucción.
Por indicar un ejemplo: en el tratamiento de un paciente con cáncer de la cavidad oral que haya recibido radioterapia en la región de la boca que haya comprometido el aporte de sangre a la mandíbula, podría desarrollarse nueva mandíbula, conectada a un hueso de la cadera bien vascularizado.

Algunos biomateriales, como el colágeno, están disponibles en la naturaleza o pueden adaptarse desde otros usos biomédicos.
Nosotros nos contamos entre quienes preparan nuevos materiales poliméricos biodegradables. 
En su especificidad, podrían determinar el tamaño y la forma del tejido fabricado, así como controlar la función de las células en contacto con el material y degradarse a un ritmo que optimice la generación de tejido.

La piel, el hueso y el cartílago son tejidos estructurales. 
Con toda probabilidad recogerán la primera cosecha de éxitos, gracias a su relativa sencillez.
El horizonte de la ingeniería de tejidos seguirá cifrado en la elaboración de órganos internos.
Al hígado corresponde realizar múltiples reacciones químicas que son necesarias para la vida; por insuficiencia hepática mueren decenas de miles de personas cada año. 
Desde el mítico Prometeo se sabe de una capacidad exclusiva del hígado, la de regeneración parcial tras una lesión;
los ingenieros de tejidos se esfuerzan por extraerle partido a esa propiedad de las células hepáticas. 

Joseph Vacanti, Achiles A. Demetriou y otros investigadores han demostrado la posibilidad de crear nuevos tejidos parecidos al hígado en animales, a partir de hepatocitos trasplantados.
Nosotros hemos desarrollado biomateriales donde progresen tejidos parecidos al hígado y hemos comprobado que la administración de fármacos a estas células hepáticas trasplantadas potencia su crecimiento.
Los nuevos tejidos que medran en estos estudios sirven para reemplazar alguna que otra operación química del hígado en animales, pero no se ha conseguido replicar la función completa del órgano.

H., David Humes y Atala trabajan con células renales para fabricar neoórganos que adquieran la capacidad filtradora del riñón.
Por su parte, los estudios en animales del grupo de Joseph Vacanti nos han enseñado que el intestino puede crecer, dentro de la cavidad abdominal, para luego anexionarse al tejido intestinal existente.
Aplicados a humanos, tales neointestinos podrían ser de ayuda para pacientes con síndrome del intestino corto, una enfermedad de origen congénito o traumático. 
Quienes sufren esa patología ven afectado su desarrollo físico general por un aporte insuficiente de nutrientes secundario a los problemas digestivos.
No hay más tratamiento que el trasplante intestinal; pero escasean el número de donantes.
Apoyado en esa misma técnica, Atala acaba de demostrar en animales la formación de una vejiga completa, capaz de sustituir a la congénita.

Ni siquiera el corazón escapa al tesón de los expertos en regeneración.
Un grupo liderado por Michel V. Sefton ha iniciado un ambicioso proyecto para fabricar corazones nuevos que libre de la muerte, por insuficiencia cardíaca, a muchas personas.
Se tardará de diez a veinte años en aprender a fabricar un corazón completo. 
Antes, sin embargo, podremos disponer de tejidos como el que forma las válvulas cardíacas y los vasos sanguíneos.
Advanced Tissue Sciences y Organogenesis son empresas que se han embarcado en la búsqueda de procedimientos comerciales para su fabricación.

¿Qué nos deparará el futuro?
Si se nos dice que la medicina contará, de aquí a cinco años, con extractos de piel funcional lo consideraremos razonable.
Nos mostraremos escépticos si se nos dice eso mismo de hígados implantables funcionales.
Pero tal vez sea posible si se amplía el plazo a unos treinta años.


El cromosoma de la masculinidad

Los cromosomas X e Y de los humanos forman una extraña pareja.
El X se parece a cualquier otro cromosoma, pero el Y resulta bastante peculiar.
¿Qué evolución han seguido?

Los cromosomas que determinan el sexo - X e Y- constituyen una pareja extravagante.
Los 22 cromosomas restantes de nuestras células tienen tan idénticas sus parejas correspondientes, que parecen estructuras gemelares.
De cada par, un cromosoma procede de la madre y otro del padre; en condiciones normales los dos tienen, sin embargo, el mismo tamaño y llevan los mismos genes.
(Los genes son los planos de ADN sobre las proteínas, encargadas de realizar la mayor parte del trabajo del organismo.)
Por eso resulta llamativo el contraste entre el Y y el X.
El cromosoma Y es mucho menor que el X; en realidad es muy canijo.
Aloja unas docenas de genes, frente a los 2000 o 3000 que encontramos en el X.
Algunos de los genes del cromosoma Y carecen del acostumbrado correspondiente en el X.
Por si fuera poco, está cargado de ADN «morralla», secuencias de nucleótidos que no encierran instrucciones para la síntesis de moléculas útiles.

Hasta hace poco, los biólogos se veían incapaces de explicar los mecanismos que le habían conducido a semejante estado de penuria.

Algunas teorías había, pero no sabían cómo someterlas a contrastación.
La situación ha dado un giro copernicano. gracias en buena medida al Proyecto Genoma Humano y otros esfuerzos parecidos encaminados a descifrar la secuencia completa de los nucleótidos de ADN en los 24 cromosomas del ser humano: X, Y y los 22 autosomas (los cromosomas no involucrados en la determinación del sexo).
A la manera en que los paleontólogos trazan la evolución seguida por una especie apoyados en el estudio de los esqueletos de los animales actuales y de los fósiles, los biólogos moleculares recorren la evolución de cromosomas y genes mediante las huellas dejadas en las secuencias de ADN . 

Los hallazgos recientes demuestran una historia de los cromosomas sexuales sorprendentemente dinámica, marcada por una serie de perturbaciones drásticas en el Y y cambios compensatorios en el X.
Esa relación se sigue dando también hoy.

Además, el cromosoma Y -durante largo tiempo considerado un Desbarajuste, capaz de hacer poco más que poner en marcha el programa de la masculinidad- resulta que esconde posibilidades insospechadas para la mayoría de los biólogos.
A lo largo de más de 300 millones de años se las ha arreglado para conservar un puñado de genes importantes en la supervivencia del macho y para adquirir otros necesarios en el proceso de fecundación.
Pese a su aspecto modesto, tiene en sus manos un sorprendente poder.

La investigación sobre la evolución de los cromosomas sexuales humanos partió de la curiosidad científica. 
Pero no sólo de ésta.
Con una aspiración más pragmática se buscaba explicar y revertir la esterilidad del varón.
El descubrimiento de genes del cromosoma Y que influyen sobre la capacidad reproductora podría llevar a tratamientos innovadores en el hombre privado de tales genes o portador de versiones defectuosas de los mismos.

Los avances recientes hunden sus raíces en ideas asentadas 100 años atrás.
Antes del siglo XX, los biólogos, observando lo que acontecía en los reptiles, pensaban que el ambiente determinaba el sexo también en el ser humano y en otros mamíferos.
Por lo que concierne a los reptiles, la temperatura del embrión en un momento precoz del desarrollo insta la intervención de un sistema, poco conocido todavía, que primará la formación de un macho o una hembra.
A comienzos del siglo XX, sin embargo, se dieron cuenta de que en ciertas especies los cromosomas mediaban en la determinación del sexo.
Unos veinte años después se vio que los mamíferos estaban entre los que se servían de cromosomas - el X y el Y- para determinar el sexo en el desarrollo embrionario.

Figura 1

LOS CROMOSOMAS X e Y fueron, hace millones de años, parejas equivalentes

Pero el cromosoma Y se encogió, en tanto que X mantuvo su integridad.
La investigación genética ha empezado a aclarar los pases que condujeron a la notable disparidad de hoy.
Las micrografías muestran los cromosomas tal como aparecen en la metafase de la división celular.

Rosario de pruebas

En los decenios siguientes, se halló que el cromosoma Y constituía el marcador del sexo masculino. 
La investigación dedujo, además, que el X y el Y habían evolucionado a partir de autosomas emparejados de un antepasado común. 
Por azar, poco antes o inmediatamente después de que surgieran los mamíferos, se produjo una mutación en una pequeña zona de la copia del autosoma que originó el Y, determinando que los embriones heredasen ese cromosoma cambiado (junto con su compañero, el futuro cromosoma X) y nacieran machos.
Los embriones que heredasen los dos cromosomas X serían hembras.

En 1990 los genéticos identificaron la zona del cromosoma Y que confiere masculinidad.
Se trata del gen SRY (de « sex-determining region Y»).
La proteína cifrada por SRY insta la formación de los testículos, según parece por la activación de genes de diversos cromosomas.
Luego, la testosterona y otras sustancias sintetizadas en los testículos toman las riendas de la configuración de la masculinidad.

Para llegar a la conclusión de que los cromosomas sexuales humanos comenzaron su vida de pareja acoplada, ]a ciencia se fundaba en los extremos de X e Y, que son un tanto gemelares y aptos para participar en un proceso de recombinación.
Durante la meiosis (la división celular que da lugar a espermatozoides y óvulos), los cromosomas emparejados se alinean juntos e intercambian segmentos; después, una copia de cada autosoma más un cromosoma sexual se distribuyen por igual en cada gameto.
Aunque ahora la mayor parte del Y se parece poco al X, los extremos de ambos cromosomas se alinean durante la meiosis en los machos e intercambien fragmentos como si X e Y siguieran siendo una pareja.
(Esa alineación resulta crucial para la distribución adecuada de los cromosomas en el espermatozoide).

Que el cromosoma X y el Y fueron un tiempo iguales se corrobora con una prueba más, que atañe a la parte de Y que no se recombina con X.
Muchos de los genes distribuidos en la región no recombinante siguen teniendo su equivalente en el cromosoma X.

La existencia de la región no recombinante, que constituye el 95 por ciento de Y, nos ofrecía una pista de cómo ese cromosoma se convirtió en sombra de su entidad originaria.
En la naturaleza y en el laboratorio, la recombinación ayuda a mantener la integridad de los cromosomas.
Por el contrario, su ausencia provoca que los genes de las regiones no recombinantes acumulen mutaciones destructivas que terminan por degradarlos, si no borrarlos.
Parecía razonable pensar, pues, que hubo algo que detuvo el intercambio de partes de X e Y, con el desplome consiguiente de los genes de la región no recombinante del cromosoma Y.
Pero transcurrieron decenios sin saberse cuando y cómo cesó la recombinación después de que surgiera dicho cromosoma.

Figura 2

LOS CROMOSOMAS de una célula de varón ( fotografía ) comprenden 22 pares de autosomas (no implicados en la determinación del sexo), más un cromosoma X y otro Y

De cada par, un miembro procede de la madre y el otro del padre.
Los genes de la NRY , región no recombinante del cromosoma Y ( azul en el diagrama ), han permitido trazar la historia evolutiva de X e Y.
Como nos dice su nombre, se trata de una región que no puede recombinarse, o intercambiar ADN , con el cromosoma X.
Sólo se indican los genes operativos.
Aproximadamente la mitad tienen su pareja correspondiente en X ( rojo ); algunos de estos genes cumplen funciones de «administración y cuidado», y son necesarios para la supervivencia de la mayoría de las células.
Ciertos genes de la NRY son activos sólo en los testículos ( púrpura ), donde probablemente participan en la fecundidad del varón.

Conocimiento gradual

El trabajo realizado a lo largo de los últimos cinco años ha venido cubriendo muchos vacíos.
En 1999 uno de los autores (Lahn) y David C., Page, del Instituto Whitehead de Investigaciones Biomédicas de Cambridge, demostraron que el cromosoma Y perdió su capacidad de intercambio con el X de una manera gradual e inesperada: 
primero implicó un trozo de ADN circundante al gen SRY y, luego, se expandió, en bloques separados, por el cromosoma.
Pero sólo Y se deterioró en respuesta a la pérdida de la recombinación X- Y; el cromosoma X prosiguió con su recombinación entre sus dos copias en la meiosis de las hembras.

¿A qué se debió el fracaso de la recombinación entre X e Y?
Cuando el cromosoma X y el cromosoma Y primitivos se aprestaban a intercambiar segmentos durante la meiosis en un antepasado lejano de los mamíferos modernos, parte del ADN del cromosoma Y sufrió, a buen seguro, una inversión o un giro total con respecto a su parte equivalente en el cromosoma X.
Puesto que la recombinación requiere que las dos secuencias de ADN similares se alineen juntas, la inversión suprimiría en adelante cualquier interacción entre las zonas de emparejamientos primitivas de X e Y.

Descubrimos que la recombinación cesó en distintos episodios cuando examinamos las secuencias nucleotídicas de 19 genes que aparecen en la región no recombinante de X y de Y. (Algunas de las copias de Y han dejado de funcionar).
En general, si las copias emparejadas de un gen perdieron su posibilidad de recombinación, la disparidad de las secuencias crecer con las generaciones.
Ante una cifra limitada de diferencias entenderemos que la recombinación cesó recientemente; si el número es elevado, se detuvo hace mucho tiempo. 

En su mayoría, los pares X- Y caían en cuatro grupos.
Dentro de cada grupo, las copias de X e Y diferían en la misma cuantía, prueba de que la recombinación cesó casi al mismo tiempo.
Los grupos se distinguían entre sí con claridad.
Las copias de Y que empezaron a divergir de su correspondiente en el cromosoma X casi al tiempo en que surgió el gen SRY eran las que más diferían de sus parejas; los otros grupos mostraban una divergencia progresivamente menor entre las copias de X e Y.

Mediante la comparación interespecífica de las secuencias de ADN , los biólogos calculan con bastante aproximación el momento en que los genes emparejados (y, por tanto, las regiones que los alojaban) comenzaron a seguir caminos separados.
Aplicado el método a nuestro ámbito se reveló que los precursores autosómicos de X e Y seguían emparejados y persistían intactos en los reptiles que vivieron antes de que la línea de los mamíferos iniciara su proceso de frondosa ramificación.
Pero los monotremas (así el ornitorrinco y el equidna), que se numeran entre los primeros en ramificarse de otros mamíferos, poseen el gen SRY y una región adyacente no recombinante.
De tales diferencias se infería que el gen SRY surgió, y se detuvo la recombinación de una región cercana, en un momento cercano a la aparición de los mamíferos, hace unos 300 millones de años.

Nuestro conocimiento acerca de la cronología de los hechos aumentó cuando aplicamos el método del «reloj molecular».
En biología se calcula la llamada tasa de variación de fondo, es decir, la velocidad o tasa de variación de las secuencias de ADN cuando no se encuentran sometidas a una presión especial que les obligue a permanecer inalteradas.
Multiplicando la amplitud de la disparidad de la secuencia en los pares X- Y por la tasa calculada, dedujimos que la primera inversión suspensora de la recombinación aconteció hace de 240 a 320 millones de años.

Análisis parecidos señalan que la siguiente inversión ocurrió hace entre 130 y 170 millones de años, poco antes de que surgieran los marsupiales, ramificados de la línea que dio lugar a los mamíferos placentarios.
El tercer episodio de inversión se registró hace entre 80 y 130 millones de años, antes de la diversificación de los mamíferos placentarios. 
Por último, la inversión final se produjo hace entre 30 y 50 millones de años, después de que los simios iniciaran su propia senda evolutiva, aunque antes de que lo hicieran primates y homínidos.

Apeándose de la tendencia general de los pares X- Y, algunos genes de la región no recombinante del cromosoma Y determinan proteínas que difieren sorprendentemente poco de las proteínas cifradas por sus equivalentes en X, incluso en zonas que sufrieron la inversión en momentos muy tempranos.
La explicación de la conservación de los mismos se apoya, probablemente, en una ley evolutiva harto simple. según la cual si un gen es crucial para la supervivencia, tenderá a conservarse. 
En efecto, los genes de Y que menos cambios han experimentado coinciden con los «encargados de la administración de la casa», vale decir, los responsables de mantener la integridad de las células del organismo.

Figura 3

LA DEGENERACIÓN DEL CROMOSOMA Y conoció cuatro episodios distintos

Comenzó hace unos 300 millones de años, después de que un antepasado reptiliforme adquiriera un gen nuevo ( SRY ) en uno de sus cromosomas autosómicos.
En cada uno de los episodios se produjo un fallo de la recombinación (intercambio de ADN ) entre el cromosoma X y el Y durante la meiosis, la división celular que origina el espermatozoide y el óvulo.
Si la recombinación se bloquea, los genes de las regiones afectadas dejan de funcionar y se degradan.

La secuencia que se muestra es muy esquemática. 
En realidad, el cromosoma Y se expendió temporalmente a veces (enfilando ADN autosómico en zonas todavía capaces de recombinación), antes de que los fracasos de la recombinación condujeran a un acortamiento neto.

Figura 4

EVOLUCIÓN DE LA INACTIVACIÓN DE X o silenciamiento de la mayoría de los genes del cromosoma X en las células de una hembra

Según parece, procedió de una forma gradual - un gen o algunos genes a la vez- para compensar la pérdida de genes del cromosoma Y ( diagrama ).
En el gato moteado encontramos un efecto de la inactivación de X ( fotografía ).
El gen determinante del color de la piel, naranja o negro, se halla en el cromosoma X.
Las hembras que llevan la versión naranja en un cromosoma X y la versión negra en el otro X tendrán algunas zonas naranja y otras negras, dependiendo de qué X esté inactivo en cada célula.
El responsable de las zonas blancas es un gen diferente.

Compensación de pérdidas 

La lógica- y un cuerpo sólido de resultados de investigación- nos induce a afirmar que a la recombinación fallida entre el cromosoma X y el Y, con la degradación consiguiente de muchos genes en Y, hubo de sucederle un tercer proceso que compensara la degeneración.
¿Por qué?
No todos los genes están activos en todas las células. 
Ahora bien, cuando una célula necesita determinadas proteínas, activa las copias, maternas y paternas, de los genes correspondientes.
La cantidad de proteína sintetizada a partir de cada copia se ajusta con finura a las exigencias de desarrollo del organismo y a su desenvolvimiento diario.
Por tanto, cuando los genes del cromosoma Y comenzaron a desaparecer, la producción de las proteínas asociadas habría disminuido catastróficamente en los machos, de no haber logrado las especies afectadas desarrollar algunas mañas compensadoras.

La mosca del vinagre y muchas especies más resuelven ese desequilibrio doblando la actividad de las versiones X de los genes Y perdidos en el macho.
Otros animales emplean una estrategia más compleja.
Primero refuerzan la actividad de los genes del cromosoma X, lo mismo en el macho que en la hembra; con ello se colman los niveles de proteína en el macho, aunque se crea un exceso en la hembra.
Algunos, como los nemátodos, reducen a la mitad la actividad de los genes de X en la hembra.
Otros, mamíferos incluidos, recurren a un proceso de inactivación de X, en cuya virtud las células de embriones tempranos de hembras silencien al azar la mayoría de los genes en uno de sus dos cromosomas X.
Las células vecinas podrían silenciar copias de X diferentes, pero todas las descendientes de una determinada célula se acomodar n al mismo patrón de inactivación de X .

Aunque desde hacia tiempo se veía en la inactivación de X una respuesta al deterioro de los genes de Y, carecíamos de pruebas tajantes.
Si la degeneración de los genes de Y conducía a la inactivación del cromosoma X, entonces los genes de X dotados de pareja funcional en la región no recombinante de Y continuarían, cabía esperar, operando en las hembras (para evadir la inactivación), de suerte que así se mantuvieran en las hembras niveles de proteína equivalentes a los de los machos.
Al analizar los niveles de actividad de los pares X- Y en dos docenas de especies de mamíferos, uno de los autores (Jegalian) y Page descubrieron años atrás que las copias en X de genes funcionales en Y escapaban a la inactivación. 
Su investigación reveló también que la inactivación de X, aunque hoy sucede en un instante durante el desarrollo del animal, distó mucho de surgir de una forma repentina.
Apareció trozo a trozo. quizá gen a gen dentro de un segmento, no de golpe en el cromosoma.

Esterilidad del varón

Además de revelar la historia de los cromosomas sexuales, los estudios genéticos del cromosoma Y nos permiten conocer las causas de algunos casos de esterilidad. 
En casi la mitad de las parejas afectadas, el problema reside total o parcialmente en el varón, que produce un número insuficiente de espermatozoides o ninguno en absoluto. 
A menudo las raíces de estas anomalías permanecen oscuras.
Los nuevos hallazgos revelan, sin embargo, que el cromosoma Y contiene genes de fertilidad y que la alteración de uno o varios explica por qué alrededor del 10 por ciento de los hombres producen escasos espermatozoides o incluso ninguno.

En los años setenta, tras observar al microscopio que muchos varones estériles carecían de pequeños fragmentos del cromosoma Y, presentes por norma en los fértiles, se dedujo la responsabilidad de dicho cromosoma en la infecundidad.
Sabemos hoy que las delaciones en una cualquiera de tres regiones especificas del Y provocan esterilidad.
Se trata de las regiones AZF (por factor de azoospermia) a, b y c.
Cada una de ellas contiene múltiples genes.

En su mayoría, tales genes se muestran muy activos en los testículos, donde se producen los espermatozoides.
(Los genes fabrican en cantidades abundantes las proteínas que codifican.) 
De semejante comportamiento se desprende la importancia de los genes de las regiones AZF para la formación de fluido seminal, aunque se desconoce su contribución exacta, así como la interacción de los mismos con genes de la fecundidad de otros cromosomas.

Algunos andrólogos introducen ya las deleciones del cromosoma Y en valoración diagnóstica. 
Si los varones que sufren esas delaciones producen al menos algunos espermatozoides, podrían someterse a una terapia de inyección espermática citoplasmática ( ICSI ), en que se recoge semen de los testículos y se inyecta en el ovocito en el laboratorio.
Pero los hijos así concebidos heredarán los cromosomas Y defectuosos y padecer n probablemente los mismos problemas de esterilidad.

Una vez se descifren las funciones exactas de las proteínas codificadas por los genes de la zona AZF , podría quizá repararse la esterilidad de los varones con delaciones en el cromosoma Y, mediante la restauración de los genes perdidos y la aportación consiguiente de las proteínas necesarias.
Vistas las cosas desde la otra cara. algunos se proponen aprovechar esa información para el diseño de fármacos que alteren la maquinaria de producción de espermatozoides y convertirlos en nuevos anticonceptivos del varón.

Figura 5

LA INYECCIÓN del espermatozoide (visible en la microaguja) directamente en un óvulo puede vencer la esterilidad en varones afectados por mutaciones en el cromosoma Y

Temas nuevos

Resulta cuando menos llamativo que la región no recombinante del cromosoma Y posea, además de un puñado de genes valiosos, reflejados en el cromosoma X, otra docena que promueve la fecundidad en el macho.
Estos genes de la masculinidad determinan sólo proteínas sintetizadas exclusivamente en los testículos, presumiblemente para intervenir en la producción de espermatozoides. 
Algunos parecen haber saltado hasta el cromosoma Y desde otros cromosomas.
De otros genes pudiera decirse que estuvieron en Y desde el comienzo, aunque cumpliendo en su inicio una misión diferente; con el tiempo adquirirían las nuevas funciones.
De donde se desprende que la degeneración constituye un asunto mayor en el curso evolutivo seguido por el cromosoma Y.
Otra segunda cuestión, apenas atisbaba hasta hace poco, se refiere a la adquisición de los genes de fertilidad.

Los teóricos discrepan a propósito de las fuerzas que convierten al cromosoma Y en un imán para esos genes.
La especie, en cuanto tal, podría beneficiarse del secuestro en los machos de genes nocivos o inútiles para las hembras.
Es también posible que al estar en el Y los genes de la fertilidad se aseguraría su paso de macho a macho sin tener que desviarse a través de las hembras (que podrían desecharlos sin sufrir consecuencias directas).

¿Cómo pueden los genes de la fertilidad prosperar en ausencia de recombinación, bajo las mismas condiciones que arruinaron a la mayoría de los restantes genes del cromosoma Y?
La respuesta podría esconderse tras la observación siguiente:
la inmensa mayoría de los genes de fertilidad presenta múltiples copias en el cromosoma Y.
Semejante multiplicidad puede servir de tampón contra los efectos de mutaciones destructivas, que acostumbran afectar sólo a una copia por vez.
Algunas copias acumulan mutaciones y terminan por fracasar, en tanto que las restantes siguen cuidando la capacidad reproductora del hombre y sirven de simiente para su propia multiplicación. 

La evolución de los cromosomas sexuales se ha estudiado a fondo en el hombre.
De su comparación con los trabajos acometidos en otras especies han aflorado una serie de principios generales que operan incluso en animales cuyos cromosomas sexuales siguieron un curso evolutivo distinto del expuesto a propósito de los mamíferos.
Aves y mariposas usan el sistema W-Z de determinación del sexo.
Cuando la herencia de una sola copia de un cromosoma específico induce la formación de un macho, a ese cromosoma se le llama Y, y a su pareja X .
Cuando la herencia de una sola copia de un cromosoma promueve la formación de una hembra, a ese cromosoma se le denomina W, y a su pareja Z.

Un principio importante es que los cromosomas sexuales proceden de los autosomas.
Pero los autosomas concernidos pueden variar.
En las aves los cromosomas W y Z surgieron de autosomas distintos de los que dieron lugar a X e Y en los mamíferos. 
Por su parte, los cromosomas X e Y de la mosca del vinagre se originaron a partir de autosomas diferentes de los que dieron lugar a esos cromosomas en los seres humanos.

En la mayoría de las especies que se reproducen por vía sexual, una vez que surgieron los cromosomas sexuales, éstos comenzaron a adquirir rasgos muy peculiares a medida que sufrieron uno o más ciclos de tres pasos secuenciales: supresión de la recombinación, degeneración de las regiones no recombinantes del cromosoma específico del sexo (el Y o el W) y, por último, la compensación por parte del otro cromosoma.
Al mismo tiempo, el cromosoma específico del sexo devino importante para la fecundidad, como sucedió con el cromosoma Y en el hombre y en los insectos.

Es razonable preguntarse por el futuro que le aguarda a nuestra especie.
¿Podría continuar el ciclo hasta desaparecer toda recombinación entre el cromosoma X y el Y, hasta la destrucción final de Y, quizá dentro de millones de años? 
Los descubrimientos recientes insinúan que los machos están capacitados para proteger sus genes del cromosoma Y que resultan decisivos para la supervivencia y la fecundidad del varón.
Ello no obstante, la degradación total del cromosoma Y sigue ahí como una posibilidad teórica.

Muy a menudo, se emprende una investigación génica con la mirada puesta en alguna enfermedad, para conocerla a fondo y repararla si se puede.
Así se acometieron varios trabajos sobre el cromosoma Y, que buscaban comprender el desarrollo y la corrección de la esterilidad.
A un buen número de estudios, sin embargo, les preocupaba menos la posible terapéutica.
Conforme se iban identificando más genes de los cromosomas X e Y, gracias a la investigación médica y a una secuenciación sistemática, los científicos no se resistieron a plantear una cuestión más básica, la de si esos genes aportaban alguna novedad sobre el pasado remoto de ese extraño desemparejamiento entre el cromosoma X y el Y.
Ahora vemos, en efecto, que los genes tenían una maravillosa historia que contarnos. 


Invasión de células cancerosas y metástasis

La amenaza más seria del cáncer para la vida es la diseminación inadvertida de las células tumorales por todo el cuerpo.

Una mejor comprensión de cómo estas células invaden los tejidos ayuda a plantear nuevos tratamientos

Supongamos que, en el curso de un examen rutinario, una mujer de 55 años se entera de que en la mamografía aparecen signos de tumoración sospechosa en una mama. 
Con la ayuda de un microscopio y una muestra de la masa del tejido, un anatomopatólogo puede diagnosticar fácilmente si el tejido es maligno.
Más difícil resulta determinar el pronóstico de esa paciente.
Depende de un fenómeno celular que no puede observarse directamente: la metastatización, la diseminación de las células malignas por todo el cuerpo que pone en marcha el desarrollo de tumores secundarios.

Cuando el tratamiento de un cáncer fracasa, la metástasis es la causa primaria de muerte.
Si un tumor primario se detecta en una fase precoz y se elimina antes de que aparezcan metástasis, el cáncer quedará erradicado. 
En cambio, si ya han aparecido metástasis, aunque éstas sean microscópicas, el pronóstico es grave.

Abandonadas a su suerte, las metástasis crecerán y acabarán con la vida del enfermo.

Aunque la naturaleza ominosa de la metástasis es algo conocido, el carácter complejo del proceso mediante el cual se instaura ha impedido a lo largo de la historia el progreso investigador.
Afortunadamente, el análisis tenaz de cada uno de los pasos del desarrollo de la metástasis ha proporcionado una nueva y riquísima información. 
Uno de los hallazgos más importantes ha sido el hecho de que, en contra de algunas de las primeras hipótesis, la metastatización es un proceso activo y no una consecuencia accidental del crecimiento del tumor.
Por supuesto, la invasión de los tejidos sanos por las células tumorales -un paso crítico en el desarrollo de los tumores secundarios- constituye un fenómeno complejo que implica la respuesta combinada de células cancerosas y de células normales. 

Nuestro laboratorio, adscrito al Instituto Nacional del Cáncer (NCI), se ha adentrado en el estudio de las bases fundamentales de la invasión del cáncer y del proceso metastático a dos niveles:
el de la maquinaria bioquímica de la invasión que opera en la superficie celular, y el de los genes propios de la célula tumoral que hacen posible la metástasis.
El enfoque que hemos aplicado conjuga la observación directa del comportamiento de la célula tumoral, la extracción y purificación de las proteínas sintetizadas por las células tumorales y el aislamiento de los genes expresados predominante o exclusivamente en células con actividad metastática alta o baja. 
Hemos identificado un conjunto de genes y proteínas que parecen regular los aspectos de la invasión y de la producción de metástasis.

Estos descubrimientos nos permiten ahora buscar nuevos tratamientos y adoptar estrategias terapéuticas capaces de detener el crecimiento del tumor antes de que metastatice. 
Se están desarrollando marcadores nuevos, muy prometedores, que predicen la existencia de metástasis pequeñas antes de que den síntomas.
De todo esto habrán de beneficiarse los pacientes de cáncer en los estados iniciales de la enfermedad.
Más acuciante, sin embargo, es la necesidad de conseguir técnicas que permitan erradicar los tumores secundarios ya existentes. 
El esclarecimiento de los mecanismos biomoleculares operativos en el proceso de instauración de la metástasis parece que nos conduce también a la consecución de ese objetivo.

Fue Joseph Claude Récamier, un médico francés, quien introdujo el término "metástasis" en su trabajo Recherches du Cancer , publicado en 1829. 
Fue también él quien aportó los primeros datos anatómicos indicativos de que las metástasis se producen como consecuencia de la penetración de células cancerosas en la circulación, que así viajan hasta lugares distantes del organismo.
Antes de Récamier, cirujanos y anatómicos habían reconocido que los tumores podían extenderse más allá de los límites de su ámbito propio de crecimiento local y colonizar tejidos cercanos y ganglios linfáticos, pero estaban convencidos de que las colonias de células tumorales en órganos más distantes tenían un origen independiente.
Récamierdescribió la infiltración local, la invasión de las venas por el tejido canceroso y la aparición de tumores secundarios en el cerebro de pacientes con cáncer de mama.

Sus aportaciones a la biología de la metástasis ocupan su lugar en la historia pero no le llevaron al diseño de tratamientos mejores.
Concibió el vendaje compresivo como tratamiento del cáncer de mama-presumiblemente con la idea de frenar la extensión del tumor.
Hoy sabemos que ese procedimiento facilitaría más bien el vertido de células cancerosas a la circulación.
Afortunadamente, el tratamiento por compresión nunca llegó a hacerse popular.

Estudios más recientes sobre las metástasis han puesto de manifiesto que se trata de una dura carrera de obstáculos con múltiples etapas en la que sólo un pequeñísimo porcentaje de las células que abandonan el tumor primario -menos de una de cada 10.000- consiguen sobrevivir y dar lugar a nuevas colonias del tumor. 
La competición comienza cuando las células tumorales abandonan el tumor primario infiltrándose a través de las paredes de los canales en el sistema circulatorio vascular o linfático.
Los tumores provocan la formación de nuevos vasos sanguíneos (proceso que recibe el nombre de angiogénesis) para nutrir la masa de rápido crecimiento expansivo.
Los vasos sanguíneos recién formados en el tejido tumoral son algo más permeables, lo que permite que las células malignas pasen a través de sus paredes con bastante facilidad.
Cualquiera de las ramificaciones del árbol vascular dentro de la masa tumoral es un lugar en potencia para el vertido de células a la circulación.

Los vasos linfáticos son puertas de entrada para las células malignas, aunque los tumores no promueven la generación de una red linfática propia.
Como los vasos linfáticos drenan de ordinario el exceso de líquido entre las células, la carencia de linfáticos en los tumores podría contribuir a una elevación de la presión hidrostática en el interior de la masa tumoral, como demostró Pietro Gullino, del Instituto de Anatomía Patológica de Turín.
Esta presión interna podría dañar el tumor y representar una ventaja para el huésped al ocluir canales vasculares y dejar zonas de la masa tumoral desprovistas de oxígeno y nutrientes.
La ausencia de linfáticos en el interior del tumor significa también que las células cancerosas pueden penetrar en el sistema linfático sólo en la interfase entre el tumor y los tejidos circundantes.

Las células tumorales aprovechan la circulación sanguínea o linfática hasta que, llevados por la corriente, encallan en un lecho capilar o en un ganglio linfático, ya sea obstruyendo uno de los vasos estrechos o adhiriéndose al revestimiento interior de un vaso.
En la mayoría de los casos, una célula potencialmente metastática pasa por el corazón antes de encontrar su lugar de asentamiento. 
Si se toma en consideración exclusivamente la anatomía del sistema circulatorio, puede predecirse el lugar en que harán su aparición el 60 por ciento de las metástasis provenientes de un tumor primario.
Por ejemplo, los pulmones son lugar frecuente de metástasis para muchos cánceres, porque el corazón bombea toda la sangre a través de sus capilares antes de enviarla a cualquier otro sitio del organismo.
Las metástasis del cáncer de colon aparecen con frecuencia en el hígado, porque el hígado recibe directamente la sangre venosa procedente del intestino grueso.

Cuando las metástasis surgen en otros órganos distintos de los que cabía esperar, se debe de ordinario a que las células del tumor han encontrado un "suelo" especialmente apto para su supervivencia y crecimiento.
El ambiente favorable de esos órganos puede incluir hormonas o factores promotores del crecimiento que estimulan de manera selectiva las células tumorales.
El gradiente de concentración de ciertas proteínas que emanan de los órganos podría ejercer un efecto de atracción sobre las células tumorales e inducirles a salir de la circulación.

La inmensa mayoría de las células tumorales que se detienen en los capilares o en los ganglios linfáticos mueren víctimas de la turbulencia mecánica de la circulación o como consecuencia de los ataques de las defensas del huésped.

Transcurridas entre ocho y 24 horas, sin embargo, algunas células tumorales, pocas, que han conseguido sobrevivir comienzan a invadir la pared del vaso y acaban por fin fuera de la circulación.

No todas las células tumorales que penetran en un órgano sobreviven y crecen.

Sólo las que comienzan a proliferar iniciarán una nueva colonia.
Son diversos los estímulos que pueden promover la proliferación, y entre ellos se encuentran factores de crecimiento locales, hormonas segregadas por el huésped y factores de crecimiento autoestimulantes producidos por las mismas células tumorales.
Una colonia metastática incipiente llegará a expandirse en el caso exclusivo de que la angiogénesis del tumor asegure el aporte de nutrientes. 
El tumor secundario totalmente desarrollado tendrá entonces su propio suministro vascular y podrá así constituirse en nueva fuente de células tumorales circulantes. 
Por tanto, las colonias metastáticas pueden dar origen a su vez a nuevas metástasis y acelerar de este modo el empeoramiento del paciente.

En varios momentos del proceso metastático-durante la entrada en la circulación, durante la salida de la circulación a través de la pared vascular y durante la penetración en el tejido normal circundante-,las células tumorales deben poner de manifiesto sus propiedades invasivas. 
Hasta finales de los años setenta, debatíase si la invasión de la célula cancerosa era mera consecuencia de la presión en el interior del tumor a medida que éste crecía y a la tendencia disminuida de las células tumorales a permanecer unidas entre sí. 
Vista así la invasión, como un proceso pasivo, sería la propia presión del crecimiento la que empujaría las células hacia la circulación, donde se produciría su disociación y diseminación. 
Esta teoría, sin embargo, no puede explicar cómo algunos tumores benignos (como el leiomioma de útero) crecen hasta alcanzar un gran tamaño y producir una elevada presión interna y, en cambio, no invaden los tejidos adyacentes y no dan lugar a metástasis.

Nuestro grupo dedicado al estudio de la metástasis en el NCI se propuso comprobar experimentalmente la hipótesis del crecimiento-presión.
Comenzábamos por tratar las células metastáticas del tumor con un agente que bloqueaba su capacidad de dividirse, y analizábamos entonces su capacidad migratoria e invasiva a través de las barreras de los tejidos.
El bloqueo del crecimiento no tuvo ningún efecto sobre el resultado.
Llegamos a la conclusión de que no se requiere la presión del crecimiento tumoral para empujar las células y ayudarlas a cruzar la barrera.
La invasión tumoral es decididamente un proceso activo.

La malignidad, la potencialidad de un tumor para dar metástasis, requiere la invasión.
El patólogo puede identificar esta propiedad examinando una muestra de la zona limitante del tumor.
Un tumor benigno, que carece de propiedades invasivas, tiene un borde característico, resultado de la compresión y del desplazamiento del tejido adyacente. 
Un tumor maligno, por el contrario, posee límites muy poco definidos, que dan lugar a una zona difusa que se conoce con el nombre de borde de invasión y es precisamente ahí donde las células abandonan de manera activa la masa primaria. 
En los complicados fenómenos bioquímicos que se desarrollan en el borde de invasión participan ambas células, las tumorales y las del huésped. 

Para entender bien el mecanismo de la invasión hace falta valorar la existencia de las barreras físicas que c~ interponen entre las células tumorales circulantes y los tejidos extravasculares.
La primera barrera es la capa de células endoteliales que revisten el interior de vasos sanguíneos y linfáticos.
Los experimentos de Garth Nicholson en el Hospital M. D. Anderson de Houston han demostrado que las células tumorales poseen una afinidad adhesiva especial por la superficie del endotelio. 
La adherencia de las células tumorales a la capa del endotelio provoca la retracción de éste y como consecuencia queda expuesto el tejido subyacente.
De este modo la célula tumoral recluta las células normales del endotelio para que cooperen con ella durante el proceso de invasión.

La barrera situada debajo de las células endoteliales, la matriz extracelular, es más abigarrada y se requiere una mayor dosis de ingenio para superarla.
La matriz es un entramado denso de diversas moléculas de proteínas e hidratos de carbono.
En el organismo de los mamíferos vallas de matriz extracelular dividen los tejidos en una serie de compartimentos.
Una forma especializada de la matriz es la membrana basal, que recubre vasos sanguíneos, células musculares y el sistema nervioso.
Junto a la membrana basal existe otro tipo de matriz, el estroma intersticial que engloba otras células del tejido y vasos linfáticos. 

La matriz extracelular actúa en parte como un andamiaje para el crecimiento de los tejidos.
Proporciona una permeabilidad selectiva para el transporte de proteínas y otras moléculas entre las células a través de las paredes de los vasos sanguíneos y durante la filtración renal. 
Más aún, la matriz extracelular sirve también de barrera mecánica frente a la invasión de la célula tumoral.

Normalmente, las poblaciones celulares de ambos lados de la matriz extracelular no se mezclan entre sí, ni siquiera durante la cicatrización de una herida ni cuando se desarrollan los órganos del embrión.
Las células malignas, sin embargo, atraviesan fácilmente la matriz extracelular, cruzan las fronteras de los tejidos y acaban en un lugar que les es ajeno.
La rotura de las membranas basales constituye una señal característica del borde de invasión en todo cáncer humano.
Una definición de "orden social" de la célula tumoral metastática es su tendencia a disgregar las fronteras de la matriz extracelular y mezclarse con células de tipos diferentes de las que se encuentran en el lecho del tumor original. 

La membrana basal continua que rodea los vasos sanguíneos no contiene ordinariamente poros o canales cuya luz posibilite el tránsito pasivo por ella de las células tumorales. 
Las células de un tumor metastatizante deben, por tanto, encontrar otra vía de penetración en la membrana basal para escaparse de la circulación.
Con el propósito de estudiar este proceso, mis colaboradores y yo extrajimos membranas basales de tejidos animales.
Cuando las células metastáticas del tumor se colocaron sobre las membranas basales aisladas, se unieron a ellas con avidez.
Por debajo de las células tumorales a las que se habían unido, se formó una zona de lisis donde se fragmentaron las proteínas de la membrana basal. 
Finalmente, las células tumorales penetraron a través de la región de la membrana basal alterada. 

Con estos resultados llegamos a la conclusión de que la invasión de la membrana basal seguía un proceso de tres pasos.
El primer paso era la adhesión de la célula tumoral a la membrana basal.
La adhesión está mediada por receptores específicos en la superficie de la célula tumoral que reconocen ciertos componentes de la membrana basal.
El segundo paso consistía en la activación de enzimas destructivas que escindían o desenredaban moléculas de la membrana basal situadas por debajo mismo de la célula tumoral.
El tercer paso estribaba en la introducción de pseudópodos, que la misma célula tumoral emitía, en la zona de lisis, a lo que seguía la emigración de la célula tumoral completa.

Estos tres pasos de la invasión de la célula tumoral -adhesión, modificación enzimática de la membrana y emigración-deben estar perfectamente coordinados y sincronizados.
A medida que avanza el frente de la superficie celular tumoral y activa las enzimas destructivas para romper las moléculas de las proteínas que obstruyen su paso, la retaguardia tumoral debe permanecer firmemente asida a la matriz extracelular.
Una vez que el camino por delante queda expedito, la célula tumoral debe dejar en suspenso la actividad destructiva de sus enzimas y permitir así su propio avance.
Este cambio es necesario, porque, para proseguir en su avance, la célula tumoral invasora debe asirse a la matriz en el sentido de la marcha, contraerse hacia delante y desenganchar cualquier tipo de uniones en su retaguardia.
En otras palabras, una célula tumoral invasora debe simultáneamente perforar un túnel, agarrarse a las paredes de ese túnel y autopropulsarse hacia delante.

Este comportamiento tan llamativo no es algo privativo de las células cancerosas metastáticas:
de vez en cuando, células normales deben invadir también otros tejidos del organismo.
El comportamiento invasivo normal se manifiesta, por ejemplo, durante la implantación de la placenta en la pared del útero y durante la formación de los órganos en el embrión.
Los leucocitos circulantes han de atravesar las paredes de los vasos sanguíneos para llegar al lugar de la infección.
Se establecen redes de vasos sanguíneos mediante la emigración e invasión de células endoteliales que cruzan las barreras de la matriz extracelular y penetran en regiones de los tejidos que necesitan los nutrientes y el oxígeno que sólo un nuevo sistema vascular puede proporcionar. 

En todos estos casos, el mecanismo de invasión que utilizan las células normales probablemente sea el mismo que el de las células tumorales.
Existe, no obstante, una diferencia esencial en la regulación de esos procesos: 
cuando desaparece el estímulo en una invasión normal, las células se detienen.
Las células malignas pueden moverse de manera implacable y cruzar las barreras de los tejidos en momentos y lugares que serían totalmente inadecuados para las células normales.

Toda una serie de preguntas acerca de cómo se regulan las células normales y las células tumorales metastáticas nos animaron a mí y a mis colaboradores a buscar los genes y las proteínas característicamente asociados con la invasión.
Un resultado sorprendente de nuestros estudios fue que las proteínas reguladoras negativas -las que inhiben el comportamiento invasor de las células normales y cancerosas- podían importar tanto como los factores positivos que promueven la agresividad.
Ese descubrimiento nos llevó a proponer la existencia de genes supresores de metástasis:
genes que codifican proteínas capaces de suprimir los pasos decisivos conducentes a la implantación de la metástasis.

El concepto de genes con efecto supresor sobre las metástasis está en consonancia con el trabajo reciente de Robert Weinberg, del Instituto Whitehead, y Bert Vogelstein, de la Universidad Johns Hopkins, en el que demuestran el interés de los genes que suprimen el crecimiento de las células cancerosas. 

Weinberg y Vogelstein han propuesto que la pérdida o la mutación de ciertos genes reguladores, piénsese en el gen pS3 y el gen retinoblastoma, conducirán a un crecimiento anormal, descontrolado, lo que representa el paso primero en el desarrollo de un cáncer.
El siguiente paso crítico será la transición del tumor de su fase de mero crecimiento a la de invasión y metástasis. 

Durante esa transición, algunos acontecimientos genéticos hacen que las células tumorales aumenten la síntesis de proteínas que estimulan la migración y la de enzimas que degradan la matriz extracelular.
Esas mismas células podrían detener la expresión de proteínas supresoras de metástasis que suelen bloquear las células de los tumores benignos y evitan que se conviertan en metastáticas.

Para identificar las enzimas implicadas en la invasión, hemos comparado las enzimas producidas en cultivos de células invasoras metastáticas con las de células de tumores no metastáticos.
Hemos hecho uso de líneas de células cancerosas aisladas por Isaiah Fidler, del Hospital M.D. Anderson, quien de manera sistemática ha producido líneas de células tumorales de ratón con propensiones metastáticas diversas.
Hemos observado que una tendencia invasora elevada está asociada con una síntesis mayor de metaloproteinasas, una clase de enzimas proteolíticas.
Después de ampliar nuestros estudios a cánceres humanos, hemos visto que niveles elevados de metaloproteinasas en los tumores se correlacionan con el desarrollo de invasión y metástasis en cánceres de mama, colon, estómago, tiroides, pulmón e hígado. 

Al menos ocho miembros de la familia de genes de metaloproteinasas se han encontrado hasta la fecha.
Todas las metaloproteinasas tienen estructuras semejantes, aunque difieren de manera significativa en lo que se refiere al tipo de proteína que hidrolizan. 
Una proteína importante atacada por las metaloproteinasas es, por ejemplo, el colágeno, una molécula filamentosa constituida por una hélice triple que forma la urdimbre de la matriz extracelular.
Una de sus variedades, la que se conoce con el nombre de colágeno de tipo IV, constituye el esqueleto de la barrera representada por la membrana basal. 
Otros tipos de colágeno son característicos de otros tejidos.
Es, por tanto, muy probable que las células tumorales necesiten hacer uso de más de una metaloproteinasa, así como de otras clases de enzimas destructivas, para atravesar las barreras que encuentran a su paso por los tejidos.

Para demostrar que las metaloproteinasas eran en efecto necesarias para la invasión, tratamos las células tumorales con anticuerpos que bloqueaban selectivamente la actividad de ciertos miembros de la familia de las metaloproteinasas.
Los anticuerpos abolían la acción invasora de las células tumorales en nuestras pruebas.

Mucho de lo que ya se sabe sobre la estructura de las metaloproteinasas podrá algún día contribuir al descubrimiento de fármacos que eviten o detengan la invasión y la aparición de metástasis mediante un bloqueo enzimático selectivo.

Por ejemplo, sabemos que todas las metaloproteinasas se sintetizan bajo una forma inactiva completamente incapaz de hidrolizar cualquier molécula de proteína.
La inactividad de esa forma se debe a una secuencia de nueve aminoácidos pertinaz ("muy conservada") y situada en uno de los extremos de todas las moléculas de metaloproteinasas.

Esta secuencia contiene un resto muy activo correspondiente al aminoácido cisteína.
En la forma inactiva, el extremo de la molécula está plegado y el resto de la cisteína interacciona con el ion metálico alojado en el centro activo de la enzima.
Con su centro activo así bloqueado, la enzima es incapaz de atacar la proteína que constituye su blanco específico.
Por tanto, las metaloproteinasas se producen con sus propios inhibidores incorporados.

La metaloproteinasa se toma activa cuando la disposición de su molécula cambia y el péptido (o fragmento proteico) que contiene la cisteína se ve forzado a retirarse del ion del centro activo.
La misma metaloproteinasa secciona entonces el péptido que lleva la cisteína y se convierte en permanentemente activa.
Junto con mis colaboradores hemos estudiado los cambios conformacionales de las metaloproteinasas en muestras de tumores humanos, y nos hemos encontrado con que a través de esos cambios las enzimas adoptan una forma que es plenamente activa.

Esta información sobre el comportamiento de las metaloproteinasas podría encerrar interés práctico. 
Una posibilidad muy sugestiva es la de que un fármaco que imite el péptido que lleva la cisteína bloquease la actividad de la metaloproteinasa y, por consiguiente, detuviese o inhibiese la invasión y el desarrollo de metástasis. 
Sin embargo, quedan todavía cuestiones pendientes sobre qué es lo que dispara los cambios de la enzima. 

Cabe la posibilidad de que otros tipos de enzimas degradativas presentes en la célula tumoral activen una metaloproteinasa latente mediante la escisión de su péptido inhibidor. 
Cabe también, lo acaba de sugerir William G. Stetler- Stevenson, uno de mis colaboradores en nuestro grupo de investigación sobre metástasis, que proteínas inmovilizadas en la superficie de una célula tumoral activen las enzimas.
Estas proteínas activadoras de la superficie celular podrían ofrecer a la célula tumoral un exquisito control sobre la actividad local de las metaloproteinasas.

Incluso ya activada, la metaloproteinasa podría fracasar en la hidrólisis de la molécula que constituye su objetivo específico, si está presente un inhibidor tisular de la metaloproteinasa (ITMP).
De igual manera que existe una familia de metaloproteinasas, existe también una familia de ITMP, con dos miembros por lo menos: el ITMP-I original y el nuevo ITMP-2, identificado por Stetler-Stevenson. 

Ambos ITMP están capacitados para inhibir todas las metaloproteinasas, aunque el ITMP-2 muestre una afinidad especial por la forma latente, o inactiva, de la colagenasa de tipo IV, la metaloproteinasa que escinde el colágeno de tipo IV.

Los ITMP producidos en tejidos normales, tales como cartílago y hueso, podrían desempeñar un papel importante en la defensa de la matriz extracelular de una degradación excesiva.
Podrían evitar lesiones en los nervios al proteger la membrana basal que rodea las fibras nerviosas. 
Los ITMP actúan también como reguladores potenciales del crecimiento celular en vasos sanguíneos y en la médula ósea.

Las células tumorales segregan también ITMP: por ejemplo, muchas células tumorales producen el ITMP-2. 
La misma célula maligna que sintetiza una metaloproteinasa podría, por tanto, producir un inhibidor de ella.
La función de la enzima se pondrá en marcha sólo si el número de moléculas enzimáticas es mayor que el número de moléculas del inhibidor ITMP.
Así pues, en este como en otros aspectos de la invasión de la célula cancerosa, el resultado depende del equilibrio entre proteínas reguladoras positivas y negativas.

Los ITMP son, por consiguiente, proteínas supresoras de la metástasis.
Varios laboratorios han demostrado que el ITMP-I y el ITMP-2 pueden detener el proceso de la invasión por parte de la célula tumoral.
El ITMP-2 puede también bloquear la formación de nuevos vasos sanguíneos que las metástasis en crecimiento necesitan para su nutrición. 
Estos resultados tan alentadores significan que los ITMP, o fármacos que actúen como ellos, podrían constituir un medio para evitar la invasión o para tratar las metástasis.

Patricia S. Steeg, colaboradora de nuestro grupo de investigación sobre metástasis, ha descubierto recientemente una nueva proteína potencialmente supresora de la metastatización.
Al realizar un estudio sistemático para ver las diferencias en actividad genética entre células tumorales metastáticas y no metastáticas procedentes de ratón, Steeg advirtió que, en las metastáticas, faltaba un gen o estaba inactivo.
La proteína correspondiente a este gen estaba siempre ausente o al menos sus niveles eran muy bajos en muchas líneas de células tumorales metastáticas; en cambio, abundaba siempre en las células tumorales no metastáticas.
Esa proteína recibió el nombre de nm23 (no metastática 23). 

Los estudios clínicos realizados por nuestro grupo, Colm Hennesy, de la Universidad de Newcastle, y Narimichi Kimura, del Instituto Metropolitano de Gerontología de Tokio, han concluido que, en una muestra de cánceres primarios de mama, niveles bajos de nm23 se asociaban de manera llamativa con la aparición de metástasis y una supervivencia baja.
Y a la inversa, niveles elevados de nm23 se correlacionaban con la ausencia de metástasis y un pronóstico muy favorable para las pacientes.
Más aún, en más de la mitad de los casos de cánceres de mama que pudieron analizarse buscando alteraciones genéticas faltaba una de las dos copias del gen nm23 .
Existe, pues, una correlación muy estrecha entre las alteraciones del nm23 y la transición hacia la invasión y la metástasis. 
Este mismo patrón se ha observado no sólo en el cáncer de mama, sino también en otros tipos de cáncer.

Si unos niveles adecuados de la proteína nm23 confieren protección frente al cáncer de mama, la determinación de este indicador en dicho tumor podría tener utilidad clínica.
Un número significativo de mujeres con cáncer de mama recién diagnosticado no presenta ningún signo de existencia de metástasis.
Y, sin embargo, se sabe que entre el 25 y el 30 por ciento de estas pacientes sufren metástasis ocultas demasiado pequeñas para producir síntomas.
La determinación de nm23 junto con la de otros indicadores bioquímicos, objeto hoy de estudio, podrían servir para que los patólogos identificaran las pacientes con riesgo más alto de que tengan metástasis ocultas, y beneficiarse así del tratamiento oportuno.

La proteína nm23 podrá tal vez utilizarse en el futuro como parte del diagnóstico y del tratamiento.
Steegha conseguido en el laboratorio insertar el gen nm23 en células metastáticas cultivadas, y con ello aumentar la expresión de la proteína nm23 . 
Cuando estas células se inyectaron en ratones no formaron metástasis.
Podemos especular con la idea de que si el gen nm23 pudiera introducirse en las células tumorales del organismo mediante alguna terapia genética aún no desarrollada, estaríamos en condiciones de interrumpir el proceso metastático.

¿De qué modo la proteína nm23 inhibe las metástasis?
Sabemos que la proteína nm23 ejerce una actividad enzimática que permite unir grupos fosfato a moléculas de proteína.
Tales grupos fosfato pueden modificar las actividades de las proteínas, incluidas las de aquellas que regulan señales de crecimiento y la diferenciación.
Sabemos también que la célula no segrega la nm23 al exterior.

Fuera de eso, seguimos, sin embargo, sin conocer la función precisa de la proteína nm23 .

Quizá la función normal de la nm23 llegue a revelarse cuando conozcamos los detalles de su fascinante permanencia a través de millones de siglos de evolución.
La nm23 humana es casi idéntica a la awd, una proteína de la mosca de la fruta que ha estudiado Allan Shearn, de la Universidad Johns Hopkins.
En la mosca de la fruta la awd es necesaria para la formación correcta de todos los órganos epiteliales adultos: el cerebro, los ojos, las alas, las patas y los órganos de la reproducción. 
Por analogía, podemos especular que la nm23 tiene una función importante en la organización normal y en la comunicación entre células humanas.
Durante el desarrollo, las células de los tejidos normales se comunican entre sí para producir un órgano con la forma y el tamaño normales.
La pérdida de la nm23 , o su regulación aberrante, podrían contribuir a crear un estado celular inestable favorecedor del comportamiento autonómico y metastático de las células tumorales.

La predicción y la prevención de metástasis constituyen un objetivo clínico de primer orden.
De parejo interés, aunque más urgente, es la erradicación de las metástasis ya establecidas en los pacientes de cáncer.
Los estudios moleculares de las vías de comunicación que regulan la invasión y colonización de las células tumorales han permitido a Elise Kohn reconocer la utilidad de una nueva clase de compuestos sintéticos, los carboxiamidoaminoimidazoles (CAI).

Cuando se administran por vía oral, los CAI detienen en animales el crecimiento de metástasis ya establecidas. 
Como han demostrado Chris Felder, del Instituto Nacional de Salud Mental, y Donald Hupe, de la firma Merck & Co., los compuestos CAI alteran el flujo de los iones calcio hacia el interior de la célula.

Nosotros pensamos que este cambio podría velar las señales que estimulan el crecimiento de las colonias metastáticas. 
Los experimentos realizados en el laboratorio han demostrado que los CAI pueden detener el crecimiento de varios tumores sólidos, incluidos los melanomas (tumores muy metastáticos de las células pigmentarias de la piel), cánceres de colon, mama y próstata.

Cuando este artículo vea la luz, ya habrá dado comienzo un estudio de pruebas clínicas de fase I de este nuevo tratamiento con los CAI en el Instituto Nacional del Cáncer.
Es éste un momento apasionante para todos los estudiosos de la metástasis y de la invasión celular.
Después de más de siglo y medio de esfuerzo investigador, la ciencia médica puede al fin convertir nuestros conocimientos sobre la metástasis en algo útil para los pacientes.
Nunca hubo tantos enfoques para resolver este problema, y nunca se vio tan esperanzador el futuro.

METASTASIS, o tumores secundarios, que vemos en esta gammagrafia isotópica en forma de pequeñas "manchas radiactivas" coloreadas (zonas hipercaptantes) sobre un fondo azul en la mama de una paciente con cáncer.
Son el resultado del asentamiento de células tumorales que se han separado de la masa primaria y han invadido tejidos más alejados.
La ciencia empieza a vislumbrar los mecanismos que permiten a las células emigrar;
los expertos se esfuerzan por encontrar medios con que saber al paso de ese proceso.


Genética de poblaciones

CONCEPTOS DEL CAPÍTULO

Los individuos pueden llevar sólo dos alelos diferentes de un gen dado.
Un grupo de individuos puede llevar un gran numero de alelos diferentes, dando lagar a un reservorio de diversidad genética.
La diversidad que tiene una población se puede medir con la ley de Hardy-Weinberg. 
La mutación es la única fuente de variabilidad genética, y otros factores como la deriva, la migración y la selección pueden alterar la cantidad de variación genética en poblaciones.

Cuando Charles Darwin publicó El origen de las especiesen 1859 después de la publicación conjunta con Alfred Russel Wallace sobre los mecanismos probables de la selección natural, establecieron los fundamentos de la interpretación moderna de la evolución.
Aunque los organismos son capaces de reproducirse de forma exponencial, Wallace y Darwin observaron que no se lleva a cabo este potencial de crecimiento de las especies.
Por el contrario, en la naturaleza el número de individuos de una población permanece relativamente constante.
Por consiguiente, tanto Wallace como Darwin dedujeron que debía darse alguna forma de lucha competitiva por la supervivencia.
Darwin observó que entre los individuos de una especie hay una variación natural; sobre esta observación basó su teoría de la selección natural:
"... que cualquier organismo, si varía, aunque sea ligeramente, de una manera beneficiosa para el mismo... tendrá una mayor probabilidad de supervivencia". 
Darwin incluyó en su concepto de supervivencia "no solo la vida de los individuos, sino también el éxito en dejar descendientes".

Aunque Gregor Mendel estuvo familiarizado con el trabajo de Darwin, Darwin no conoció ningún mecanismo para explicar la variación morfológica que había observado.
Sin embargo, con el desarrollo del concepto de genes y alelos, se establecieron las bases genéticas de la variación hereditaria.

Cuando otros prosiguieron el estudio de la evolución, quedó claro que en este proceso, la unidad de estudio eran las poblaciones y no los individuos.
Por consiguiente, para estudiar el papel de la genética en la evolución, fue necesario considerar las frecuencias alélicas de las poblaciones en lugar de los descendientes de cruces concretos. 
Así surgió la disciplina de la genética de poblaciones .

A principios del siglo XX una serie de investigadores, como Gudny Yule, William Castle, Godfrey Hardy y Wilhelm Weinberg, formularon los principios básicos de esta disciplina. 
En los primeros años de la genética de poblaciones, el énfasis se centró en la teoría y en el desarrollo de modelos matemáticos para describir la estructura genética de las poblaciones.
Entre los investigadores en este campo se incluyen a Sewall Wright, Ronald Fisher y J. B. S. Haldane.
Siguiendo sus trabajos, investigadores de campo y de laboratorio han utilizado técnicas bioquímicas y moleculares para medir directamente la variación a los niveles proteicas y de DNA para comprobar estas teorías y modelos.
Las frecuencias alélicas y las fuerzas que alteran estas frecuencias, como la mutación, la migración, la selección y la deriva genética al azar, se han examinado y se están examinando.
En este capitulo consideraremos algunos aspectos generales de la genética de poblaciones y también discutiremos otras áreas de la genética relacionadas con la evolución.

Poblaciones y acervos génicos 

Los miembros de una especie se distribuyen a menudo en un rango geográfico amplio.
Una población es un grupo local que pertenece a una especie, dentro de la que ocurren apareamientos real o potencialmente.
La serie de informaciones genéticas que llevan todos los miembros que se cruzan de una población se denomina acervo génico .
Para un locus dado, este acervo incluye todos los alelos de dicho gen que están presentes en la población.
En genética de poblaciones la atención se centra en grupos en lugar de en individuos, y en la cuantificación de las frecuencias alélicas y genotípicas en generaciones sucesivas en lugar de en la distribución de los genotipos que aparecen en un solo cruce.
A lo largo del capítulo se utilizará el término frecuencia alélica v se representarán las frecuencias alélicas en relación con las frecuencias genotípicas.
Los gametos producidos en una generación dan lugar a los zigotos de la generación siguiente.

Esta nueva generación tiene un acervo génico reconstituido, que puede diferir del de la generación anterior.

Las poblaciones son dinámicas; pueden crecer y expansionarse o disminuir y contraerse mediante cambios en las tasas de nacimientos o fallecimientos, o por migración o por fusión con otras poblaciones.
Esto tiene consecuencias importantes y, con el tiempo, puede dar lugar a cambios en la estructura genética de poblaciones.

Cálculo de las frecuencias alélicas

Un método utilizado para estudiar la estructura genética de las poblaciones es medir la frecuencia de un alelo dado.
Esto es posible una vez que se ha establecido el modo de herencia y el número de alelos diferentes de dicho gen en la población.
No siempre se pueden determinar directamente las frecuencias alélicas, debido a que en muchos casos sólo se pueden observar fenotipos y no genotipos.
Sin embargo, si se consideran alelos que se expresan de un modo codominante hay una relación directa entre los fenotipos y los genotipos, teniendo cada fenotipo un único genotipo.
Tal es el caso del grupo sanguíneo MN , que en la especie humana se hereda de manera autosómica.

En este caso el gen L del cromosoma 2 tiene dos alelos, LM y LN (a menudo denominados M y N, respectivamente) [0] .
Cada alelo controla la producción de un antígeno distinto de la superficie de los glóbulos rojos.

Así, el genotipo de un individuo de una población puede ser del tipo M ( LMLM ), N ( LNLN ) o MN ( LMLN ).

En la Tabla 24.1 se presentan los genotipos, fenotipos y reacciones inmunológicas del grupo sanguíneo MN .
Debido a que son codominantes (y que los genotipos se pueden deducir de los fenotipos), se puede determinar la frecuencia de los alelos M y N en una población simplemente contando el número de individuos de cada fenotipo.
Como ejemplo, consideremos una población de 100 individuos, de los que 36 son del tipo M, 48 del tipo MN y 16 del tipo N.
Los 36 individuos del tipo M tienen 72 alelos M y los 49 heterozigotos MN tienen otros 48 alelos M. 
Es decir, en la población hay 72 + 48 o 120 alelos M, de un total de 200 alelos de este locus (100 individuos, cada uno con dos alelos).

Por consiguiente, la frecuencia del alelo M en esta población es 0,6 ( 120/200 = 0,6 = 60% ).
La frecuencia del alelo no se puede estimar de manera similar ( 80/200 = 0,4 = 40% ) y es 0,4.

Tabla 24.1

Grupos sanguíneos MN 

Genotipo.
Tipo sanguíneo.
Anti-M.
Anti-N.


LMLM .
M.
+ .
- .


LMLN .
MN .
+ .
+ .


LNLN .
N.
- .
+ .


En la Tabla 24.2 se presentan dos métodos para calcular la frecuencia de los alelos M y N en una población hipotética de 100 individuos, y en la Tabla 24.3 se da una lista de las frecuencias de los alelos M y N obtenidas realmente en varias poblaciones humanas.

Tabla 24.2

Métodos para determinar las frecuencias alélicas para alelos codominantes

Cálculo del número de alelos .


Genotipo/Fenotipo.
MM .
MN .
NN .
Total.


Número de individuos.
36.
48.
16.
100.


Número de alelos M.
72.
48.
0.
120 .


Número de alelos N.
0.
48.
32.
80.


Número total de alelos.
72.
96.
32.
200.


Frecuencia de M en la población: 120/200 = 0.6 = 60%.


Frecuencia de N en la población: 80/200 = 0.4 = 40%.


De los genotipos .


Genotipo/Fenotipo .
MN .
MN .
NN .
Total .


Número de individuos.
36.
48.
16.
100.


Frecuencia de los genotipos.
36/100 = 0.36.
48/100 = 0.48.
16/100 = 0.16.
1.00.


Frecuencia de M en la población: 0.36 + (1/2)0.48 = 0.36 + 0.24 = 0.60 = 60%.


Frecuencia de N en la población: 0.16 + (1/2)0.48 = 0.16 + 0.24 = 0.40 = 40%.


Tabla 24.3

Frecuencia de los alelos M Y N en distintas poblaciones

Esquimales (Groenlandia).
83.48.
15.64.
0.88.
0.913.
0.087.


Indios de los EE.UU.
60.00.
35.12.
4.88.
0.776.
0.224.


Blancos de los EE.UU.
29.16.
49.38.
21.26.
0.540.
0.460.


Negros de los EE.UU.
28.42.
49.64.
21.94.
0.532.
0.468.


Ainus (Japón).
17.86.
50.20.
31.94.
0.430.
0.570.


Aborígenes (Australia).
3.00.
29.60 .
67.40.
0.178.
0.822.


Ley de Hardy-Weinberg

En el caso del grupo sanguíneo MN , M y N son alelos codominantes.
Por otro lado, si un alelo hubiera sido recesivos, los heterozigotos serían fenotípicamente idénticos a los individuos homozigotos dominantes y no se hubieran podido determinar directamente las frecuencias de los alelos.
Sin embargo, en este caso se puede usar un modelo matemático, desarrollado independientemente por el matemático inglés Godfrey H. Hardy y el físico alemán Wilhelm Weinberg, para calcular las frecuencias alélicas.

La ley de Hardy-Weinberg (LHW) es uno de los conceptos fundamentales en genética de poblaciones. 
La LHW tiene tres propiedades importantes:
1.' rend="bo Las frecuencias alélicas predicen las frecuencias genotípicas;
2. en el equilibrio, las frecuencias alélicas y genotípicas no cambian de generación en generación; y
3. el equilibrio se alcanza con una sola generación de apareamiento al azar.

Suposiciones de la ley de Hardy-Weinberg

La ley de Hardy-Weinberg supone las siguientes condiciones: 
1.' rend="bo La población es infinitamente grande, que en término prácticos significa que la población es lo suficientemente grande como para que los errores de muestreo y efectos aleatorios sean despreciables. 
2. E] apareamiento dentro de la población se da al azar.
3. No hay ventaja selectiva de ningún genotipo; es decir, todos los genotipos producidos por el apareamiento al azar son igualmente viables y fértiles.
4. No hay otros factores, como mutación, migración y deriva genética al azar.

Suponga que en tal población ideal un locus tiene dos alelos A y a.
La frecuencia del alelo A, tanto en óvulos como en espermatozoides, está representada por p, y la frecuencia de a en los gametos está representada por q.

Debido a que la suma de p y q representa el 100 por ciento de los alelos de dicho gen en la población, p + q = 1 .
Se puede utilizar un esquema para representar la combinación al azar de los gametos que llevan estos alelos y de los genotipos resultantes (Figura 24.1).

En la combinación al azar de los gametos de la población, la probabilidad de que tanto el esperma como el óvulo tengan el alelo A es p X p = p2 .

Igualmente, la probabilidad de que los gametos lleven alelos diferentes es (p x q) + (p x q) = 2pq , y la probabilidad de que se obtenga un individuo homozigoto recesivo es q x q = q2 .
Advierta que estos términos también describen una de las características de la LHW:
las frecuencias alélicas determinan las frecuencias genotípicas. 
En otras palabras, mientras que el valor p, es la probabilidad de que en la fecundación ambos gametos lleven el alelo A, es también una medida de la frecuencia de los homozigotos AA en la generación siguiente.
De modo similar, 2pq describe la frecuencia de los heterozigotos Aa y q2 es una medida de la frecuencia de los homozigotos recesivos ( aa ).
Así, la distribución de los genotipos homozigotos y heterozigotos en la siguiente generación se puede expresar como, p2+2pq+q2= 1 (24-1) Consideremos a continuación una población en la que el 70 por ciento de los alelos de un gen dado son A, y el 30 por ciento a.
Así, p = 0,7 y q = 0,3 , y p(0,7) + q(0,3) = 1 . 
En la Figura 24.2 se presenta la distribución de los genotipos que se producen por apareamiento al azar.
En la nueva generación, el 49 por ciento ( p2 ) de los individuos será homozigótico dominante, el 42 por ciento ( 2pq ) será heterozigótico y el 9 por ciento ( q2 ) será homozigótico recesivo. 
Se puede calcular la frecuencia del alelo A en la nueva generación como p2 + 1/2 (2pq) 0,49 + 1/2 (0,42) 0,49 + 0,21 = 0,70 (24-2) .
La frecuencia de a es q2 + 1/2 (2pq) 0,09 + 1/2 (0,42) 0,09 + 0,21 = 0,30 (24-3) .
Ya que p + q = 1 , el valor de a se podría calcular como, q = 1 - P q = 1 - 0,70 = 0,30 (24-4) .
Las frecuencias de A y a en la nueva generación son las mismas que en la generación anterior.

Si se asumen las condiciones de Hardy-Weinberg, se pueden calcular las frecuencias alélicas y las frecuencias de otros genotipos conociendo sólo la frecuencia de un genotipo (como el del homozigoto recesivo).
En genética humana esta relación se utiliza para calcular la frecuencia de los heterozigotos que llevan un alelo recesivo para una enfermedad genética (véase mas abajo).

Se dice que una población, en la que la frecuencia de un alelo dado permanece constante de generación en generación, está en equilibrio genético .
En este caso, las frecuencias de A y a permanecen constantes.

En el ejemplo anterior hay varios puntos importantes.
Primero, aunque los alelos hipotéticos que hemos considerado estuvieran en equilibrio, no todos los alelos de la población lo están.
Esto es particularmente cierto cuando no se cumplen las suposiciones hechas en la ley de Hardy-Weinberg. 
Segundo, el ejemplo ilustra por qué los caracteres dominantes no tienden a aumentar su frecuencia a lo largo de las generaciones.
Finalmente, los ejemplos demuestran que en una población se pueden mantener el equilibrio y la variabilidad genética .
Una vez establecida en la población, la frecuencia alélica se mantiene constante en el equilibrio - un factor importante en la evolución.

Comprobación del equilibrio 

La ley de Hardy-Weinberg también se puede utilizar para determinar si los genotipos de una población dada están en equilibrio.
En una población natural, cualquiera de las condiciones de la LHW (tamaño, apareamiento al azar, ausencia de selección) puede no ser válida.
Para hacer esto, tiene que ser posible identificar fenotípicamente a los heterozigotos.
Si esto es posible, entonces se debe comprobar si la población cumple la relación p2 + 2pq + q = 1 .
Si no [1] algún factor (probablemente selección, mutación, migración) está dando lugar a que las frecuencias alélicas varíen en las generaciones siguientes.

La distribución de las frecuencias alélicas del grupo sanguíneo MN en los aborígenes australianos es un buen ejemplo de esta aplicación de la ley de Hardy-Weinberg.
A partir de los valores de las frecuencias alélicas de la Tabla 24.3 (0,178 para M y 0,822 para N), se pueden calcular las frecuencias esperadas de los tipos sanguíneos M, MN y N para determinar si la población está en equilibrio:

Frecuencia esperada del tipo M = p2 = (0,178)- = 0,037 = 3,2% Frecuencia esperada del tipo MN= 2pq = 2(0,178)(0,822) = 0,292 = 29,2% Frecuencia esperada del tipo N= q2 = (0,822)2 = 0,676 = 67,6% (24-5) .
Estas frecuencias esperadas son casi idénticas a las frecuencias observadas que se presentan en la Tabla 24.3, confirmando que la población está en equilibrio. 
Si hubiera alguna duda de sí las frecuencias observadas varían significativamente de las frecuencias esperadas, se puede realizar un análisis de ji-cuadrado (véase el Capítulo 3).

Por otro lado, si la prueba de Hardy-Weinberg demuestra que la población no está en equilibrio, alguna de las condiciones necesarias no se estará cumpliendo. 

Utilizando los valores de las frecuencias de M y N que se muestran en la Tabla 24.3, se puede construir una hipotética población mezclada con 500 aborígenes australianos y 500 indios americanos.
Las frecuencias de M y N de esta hipotética población se muestran en la Tabla 24.4. 
En tal población la frecuencia de M (p) sería, 0,315 + 1/2(0,324) = 0,477 (24-6) y la de N (q) sería, 0,361 + 1/2(0,324) = 0,523 (24-7) Si estas frecuencias alélicas provinieran de una población con apareamiento al azar, entonces se esperaría que las proporciones fenotípicas fueran MM ( p2 ), 22,8 por ciento, MN ( 2pq ), 49,8 por ciento y NN ( q2 ), 27,4 por ciento.
Debido a que las frecuencias genotípicas esperadas no concuerdan con las observadas (y la prueba de ji cuadrado confirmaría esto [2] ), concluiríamos que la población no está en equilibrio, debido, obviamente, a la ausencia de apareamiento al azar en esta población hipotética. 

Sin embargo,
¿qué sucedería si tal población mezclada se estableciera en una isla solitaria y tuvieran lugar apareamientos al azar?
¿Cuánto se tardaría en alcanzar el equilibrio?
Aplicando la ecuación de Hardy- Weinberg, podemos predecir que después de una generación las frecuencias genotípicas observadas y esperadas convergirían, ilustrando la tercera característica importante de la LHW.
Utilizando los valores de p y q que hemos calculado, y recordando que las frecuencias alélicas representan a las del conjunto global de los genes de la primera generación de la población mezclada, las frecuencias después de una generación de apareamiento al azar se muestran en la Figura 24.3.
Podemos ver que, después de una generación, las frecuencias genotípicas observadas del 22,8 por ciento para MM(p2) , del 49,8 por ciento para MN(2pq) y del 27,4 por ciento para NN(q2) son las que se esperan en una población en equilibrio.
Las frecuencias alélicas de M = 0,477 y de N = 0,523 también son las esperadas para una población en equilibrio.
Sin embargo, para demostrar rigurosamente el equilibrio de la LHW , tenemos que calcular las frecuencias de los apareamientos.

Tabla 24.4

Frecuencias de los alelos M y N y de los genotipos en poblaciones naturales y artificiales 

Población.
MM .
MN .
NN .
p.
q.


Aborígenes australianos.
0.03.
0.296.
0.674.
0.178.
0.822.


Indios americanos.
0.60.
0.351.
0.049.
0.776.
0.224.


Población mezclada (500 + 500):.


Observadas.
0.315.
0.324.
0.361 .
0.477.
0.523.


Esperadas.
0.228.
0.498.
0.274 .


Extensiones de la ley de Hardy-Weinberg

A1 considerar las frecuencias genotípicas y alélicas para loci autosómicos utilizando la ecuación de Hard-Weinberg, asumimos que la frecuencia de.] es la misma en espermatozoides y en óvulos.
¿Qué pasa con los genes ligados al X?
En especies que tienen dos cromosomas X, como en la especie humana y en Drosophila , las hembras llevan dos copias de todos los genes del cromosoma X mientras que los machos tienen sólo una copia de todos los genes ligados al X. Por consiguiente, los genes del cromosoma X se distribuyen desigualmente en la población, y cuando hay igual número de machos que de hembras, las hembras llevan los dos tercios de todos los genes ligados al X y los machos llevan un tercio del número total.
¿Se puede aplicar la ley de Hardy-Weinberg para calcular las frecuencias alélicas y genotípicas en tal circunstancia?

Genes ligados al X

Es fácil determinar la frecuencia de los genes ligados al X en los machos.

Debido a que tienen una sola copia de todos los genes de este cromosoma, el fenotipo nos revela tanto los alelos dominantes como los recesivos.
Por consiguiente, en los machos, la frecuencia de un alelo ligado al X es la misma que la frecuencia fenotípica.
En Europa occidental se da una forma de daltonismo ligado al X con una frecuencia del 8 por ciento en los varones ( q = 0,08 ).
Debido a que las mujeres tienen dos dosis de todos los genes del cromosoma X, las mujeres daltónicas presentarán las frecuencias fenotípicas esperadas para dominancia completa, y se pueden calcular las frecuencias genotípicas y alélicas utilizando la ecuación típica de Hardy-Weinberg. 
Por ejemplo, como la frecuencia del daltonismo en los varones tiene una frecuencia de 0,08, la frecuencia esperada en las hembras, en el equilibrio, será q2 , es decir, 0,0064. 
Esto significa que en una muestra de 10.000 varones se esperaría que 800 fueran daltónicos, pero de 10.000 mujeres sólo 64 presentarán este carácter. 
En la Tabla 24.5 se comparan los valores esperados de las frecuencias para caracteres ligados al X en varones y en mujeres.

Si la frecuencia de un alelo ligado al X difiere entre varones y mujeres, entonces la población no está en equilibrio.
En contraste con los alelos de los loci autosómicos, el equilibrio no se alcanzará en una sola generación, sino que se irá aproximando a lo largo de generaciones sucesivas.
Debido a que los varones heredan el cromosoma X de su madre, las frecuencias alélicas en las mujeres determinarán las frecuencias en los varones de la generación siguiente.

Las hijas heredarán un cromosoma X materno y otro paterno, y su frecuencia alélica será la frecuencia promedio de sus padres.
Aun cuando las frecuencias alélicas del conjunto de la población permanezcan constantes, hay una oscilación en las frecuencias para los dos sexos en cada generación, siendo las diferencias la mitad de la generación anterior, hasta que se alcanza el equilibrio. 
Este concepto se ilustra en la Figura 24.4 para el caso simple en donde un alelo se ha iniciado en las mujeres con una frecuencia igual a 1 y en los varones igual a 0.

Tabla 24.5

Frecuencia relativa esperada en machos y en hembras para caracteres ligados al X

Frecuencia de machos con el carácter.
Frecuencia esperada en las hembras.


90/100 .
81/100.


50/100.
25/100.


10/100.
1/100.


1/100.
1/10.000.


1/1000.
1/1.000.000.


1/10.000.
1/100.000.000 .


Figura 24.1

Los gametos se suponen provenientes del acervo génico para formar los genotipos de la siguiente generación

En este ejemplo.
Los machos y las hembras tienen la misma frecuencia tanto para el alelo dominante A (p) como para el alelo recesivo a (q).
Después del cruzamiento los tres genotipos AA, Aa y aa tienen las frecuencias p2 , 2pq y q2 respectivamente.

Figura 24.2

En esta población, la frecuencia del alelo A es p = 0,7, y la frecuencia del alelo a es q = 0,3

Utilizando la fórmula p2 + 2pq + q2 , se puede calcular la frecuencia de los genotipos de la siguiente generación como M = 0,49, Aa = 0,42 y aa = 0,09 . 

Las frecuencias de A y de a permanecen constantes de generación en generación.

Figura 24.3

Si el apareamiento en una población, con diferencias en las frecuencias alélicas, sigue las expectativas de Hardy- Weinberg, se alcanzará el equilibrio en una sola generación

Compare las frecuencias genotípicas después de una generación de apareamiento con las de la Tabla 24.4 para una población en equilibrio.

[0] L se refiere a Karl Landsteiner, el genético en cuyo honor se denominó el locus

[1] Toda población cumple con la relación p2 + 2pq + q2 = 1 , esté o no esté en equilibrio

Como se verá más adelante cuando una población no está en equilibrio lo que no concuerda son las frecuencias observadas para cada uno de los genotipos con las correspondientes frecuencias esperadas de esos genotipos, de acuerdo con la ley de Hardy-Weinberg.

[2] Por supuesto, la prueba de ji-cuadrado habría que efectuarla no con frecuencias o porcentajes, sino con valores absolutos, número de individuos observados y esperados para cada uno de los genotipos


Ratones expertos

Mediante ingeniería genética se ha conseguido reunir ciertos componentes moleculares del aprendizaje y la memoria y obtenido un ratón bastante experto.

Cuando decidí dedicarme a la investigación científica no me imaginaba que algún día mi trabajo sería objeto de un programa de entretenimiento en televisión.
Pero a finales de septiembre, después de anunciar que en mi laboratorio habíamos conseguido, por manipulación génica, mejorar la capacidad de aprendizaje y memoria de una cepa de ratones, vi con sorpresa el jugo que le sacaba un programa disparatado de humor.

Nuestra investigación se incardinaba, sin embargo, en una dilatada trayectoria de trabajos consagrados a descubrir qué pasa en el cerebro durante el aprendizaje y de qué están hechos los recuerdos que guarda la memoria.
Al generar estos ratones listos - una cepa a la que dimos el nombre de Doogie - confirmábamos una vieja teoría de más de 50 años sobre los mecanismos de aprendizaje y memoria e ilustrábamos el papel de una molécula particular en el proceso de la formación de la memoria.
Esa molécula podría algún día constituirse en blanco de fármacos para tratar la enfermedad de Alzheimer, entre otros trastornos cerebrales, y quizá para impulsar la capacidad de aprendizaje y de memoria de las personas sanas.

Importa entender las bases moleculares del aprendizaje y de la memoria porque lo que aprendemos y lo que recordamos determinan, en buena medida, lo que somos.
La memoria, y no sólo la cara y otros rasgos físicos, ayudan a definir a un individuo.
Lo saben muy bien quienes conocen de cerca a los pacientes de Alzheimer.
Además, el aprendizaje y la memoria trascienden al individuo para transmitir nuestra cultura y civilización de generación en generación.
Son fuerzas decisivas de la evolución cultural y social, así como del comportamiento.

Nociones básicas

El cerebro humano contiene unos cien mil millones de neuronas, trabadas en redes sobre las que se asientan diversidad de atributos mentales y cognitivos, desde la memoria hasta la personalidad, pasando por la inteligencia y las emociones.
Las bases para entender los mecanismos moleculares y genéticos del aprendizaje y la memoria se pusieron en 1949.
Donald O., Hebb propuso entonces una idea profunda, aunque sencilla, para describir la representación y almacenamiento cerebral de la memoria.

En lo que ahora se conoce por regla del aprendizaje de Hebb, éste propuso que un recuerdo se produce cuando dos neuronas conectadas se activan simultáneamente de suerte tal, que se refuerza la sinapsis, lugar donde las dos células nerviosas entran en contacto.
En la sinapsis, la información vehiculada por los neurotransmisores, sustancias químicas, fluye desde la célula presináptica hacia la postsináptica. 

En 1973 Timothy V., P., Bliss y Terje Lomo descubrieron un modelo experimental con el sello característico de la teoría de Hebb.
Observaron que las neuronas del hipocampo (región del cerebro con forma de caballito de mar) se acoplaban con mayor firmeza si las estimulaban con una serie de pulsos eléctricos de alta frecuencia.
El aumento de esta intensidad sináptica - fenómeno denominado potenciación a largo plazo (PLP)- puede durar horas, días o incluso semanas.
El hecho de que la PLP se encuentre en el hipocampo resulta fascinante, pues esta estructura cerebral desempeña un papel decisivo en la formación de la memoria del hombre y los animales. 

Estudios posteriores de Mark F., Bear y otros demostraron que la aplicación de una estimulación de baja frecuencia a la misma vía del hipocampo producía un descenso de larga duración de la intensidad de las conexiones en ese lugar.
La reducción, también duradera, recibe el nombre de depresión a largo plazo (DLP); no hemos de confundirla con la depresión clínica. 

La intensificación y la relajación de las conexiones sinápticas a través de procesos de PLP y de DLP se han convertido en los principales candidatos para explicar el mecanismo cerebral de almacenamiento y desaparición de la información aprendida.
Sabemos que la PLP y la DLP adquieren formas plurales.
Los fenómenos ocurren también en diversas regiones del cerebro, además del hipocampo, incluidos el neocórtex - la sustancia gris - y la amígdala, una estructura implicada en las emociones. 

¿Qué mecanismos moleculares controlan estas formas de cambios sinápticos, o plasticidad?
Estudios de los años ochenta y noventa acometidos por Graham L., Collingridge, Roger A., Nicoll, Robert C., Malenka, Gary S., Lynch y otros hallaron que los cambios dependían de un mismo tipo de molécula.
Estos investigadores demostraron que la inducción de las formas principales de PLP y DLP requería la activación de los receptores de NMDA , instalados en las membranas celulares de las neuronas postsinápticas.

Los receptores de NMDA son poros minúsculos, constituidos por cuatro subunidades proteicas que controlan la entrada de iones calcio en las neuronas.
(El nombre de estos receptores deriva del N-metil-D-aspartato , un compuesto químico artificial que se une a ellos.)
Son los candidatos perfectos para operar los cambios sinápticos de la regla de aprendizaje de Hebb, pues requieren que se abran dos señales distintas, a saber, la unión del glutamato neurotransmisor y la despolarización de membrana, un cambio eléctrico. 
Nos hallamos así ante los interruptores moleculares ideales para funcionar como «detectores de coincidencia» y ayudar a que el cerebro asocie dos sucesos.

Aunque la PLP y la DLP dependen de receptores de NMDA , la asociación de procesos de tipo PLP y DLP con el aprendizaje y la memoria ha resultado más difícil de lo esperado.
El equipo encabezado por G., M., Morris ha observado el fenómeno siguiente:
a las ratas cuyos cerebros eran tratados con fármacos - que bloquean el receptor de NMDA les costaba aprender a resolver la prueba del laberinto acuático de Morris mucho más que a otras ratas.
Este hallazgo es, en buena medida, congruente con la predicción relativa a la misión que cumple la PLP en el aprendizaje y la memoria.
Mas, con frecuencia, los fármacos producen alteraciones de comportamiento, sensoriales y motores; existe, por tanto, una línea muy sutil entre la eficacia del fármaco y su toxicidad.

Hace cuatro años, cuando yo trabajaba en el laboratorio de Susumu Tonegawa en el Instituto de Tecnología de Massachusetts, desarrollé una nueva técnica genética para estudiar el papel del receptor de NMDA en el aprendizaje y la memoria.
La técnica refinaba el método para crear los ratones «knock-out» , individuos en los que se ha inactivado de intento un gen.
Los ratones knock-out tradicionales carecen de un gen particular en cada célula y tejido.
Del estudio de la salud y comportamiento de esos animales se deduce qué función desempeña el gen de marras.

Muchos tipos de ratones knock-out mueren al nacer o antes del parto, privados de los genes necesarios para el desarrollo normal.
Los genes que cifran las subunidades de los receptores de NMDA resultaban esenciales en un grado parecido; los ratones knock-out cuyo receptor de NMDA se hallaba afectado morían en los primeros momentos de la vida.
Y así fue como vislumbré un camino para borrar una subunidad del receptor de NMDA en sólo una región especifica del cerebro.

UN RATÓN LLAMADO DOOGIE

Sobre la destreza de los ratones «expertos» .

¿En qué difieren los ratones Doogie de otros ratones? 
Se les ha sometido a manipulación génica para que produzcan por encima del nivel normal una subunidad clave del receptor del N-metil-D-aspartato (NMDA), una proteína.

¿Qué misión cumple el receptor de NMDA? 
Contribuye a reforzar la conexión entre dos neuronas que están activas al mismo tiempo. 
Se supone que en esa intensificación reside el fundamento del aprendizaje y la memoria.

¿Hasta dónde llega la pericia de los ratones Doogie? 
Nada que tenga que ver con la inteligencia matemática u otros tipos de creatividad humana.
Pero los Doogie distinguen entre objetos que han visto antes y recuerdan cómo encontrar una plataforma en un tanque de agua turbia mucho mejor que los ratones normales. 

¿De qué modo la manipulación génica logra que sean más listos? 
El receptor de NMDA de los ratones Doogie permanece abierto el doble de tiempo que en los ratones normales.
Ese tiempo extra les posibilita una formación más eficiente del recuerdo.

¿Podría usarse la misma técnica para potenciar la capacidad humana de aprender y recordar? 
En principio, la posibilidad existe.
Pero el aprendizaje y la memoria en el hombre son mucho más complejos que el reconocimiento de objetos o el recuerdo de un laberinto acuático. 
Además de las barreras científicas y técnicas, deberían abordarse la seguridad y los problemas éticos que rodean la ingeniería genética en humanos.
Parece más probable que los laboratorios farmacéuticos intentarán antes desarrollar fármacos que interaccionen con el receptor de NMDA para potenciar la capacidad de memoria en personas con déficit de ésta.

El estudio de un ratón knock-out 

Con la nueva técnica preparé un ratón que carecía, en la región CA1 del hipocampo, de una parte crítica del receptor de NMDA , la subunidad NR1 .
Nos acompañó la fortuna en eliminar el gen en la región CA1 porque era ahí donde se había centrado la mayoría de los estudios sobre PLP y DLP y porque las personas con lesiones en esa área presentaban déficits de memoria.
En colaboración con Matthew A., Wilson, Patricio T., Huerta, Thomas J., McHugh y Kenneth I., Blum, del MIT , observé que los ratones knock-out habían perdido la capacidad de modificar la intensidad de las conexiones nerviosas en las regiones CA1 .
Estos animales tenían una representación espacial anormal y una memoria espacial pobre:
no podían recordar el camino andado en un laberinto de agua.
Investigaciones ulteriores en mi laboratorio de la Universidad de Princeton han revelado que los ratones se resentían también en otras tareas relacionadas con memorias no espaciales.

Aunque estos experimentos apoyaban la hipótesis de que los receptores de NMDA eran esenciales para la memoria, no resultaban definitivos del todo.
Los fármacos utilizados para bloquear los receptores podrían haber ejercido su efecto a través de otras moléculas, además de los receptores de NMDA .
Y los déficits de memoria de los ratones knock-out podrían deberse a otra anomalía inesperada e independiente de los déficits en PLP/DLP .

Para resolver estas cuestiones, decidí hace un par de años intensificar la función de los receptores de NMDA en ratones y comprobar si dicha alteración mejoraba el aprendizaje y la memoria.
En caso afirmativo, el resultado-combinado con los obtenidos previamente- nos diría que el receptor de NMDA constituía un agente principal de los procesos de la memoria.

Me centré en las subunidades NR2A y NR2B del receptor de NMDA .
Sabemos hoy que los receptores de NMDA de aves, roedores y primates permanecen abiertos más tiempo en los individuos jóvenes que en los adultos.
La diferencia podría, quizás, explicar por qué los animales jóvenes aprenden más fácilmente- y recuerdan durante más tiempo lo que han aprendido- que otros individuos más viejos.

A medida que los individuos maduran, comienza un cambio que consiste en sintetizar receptores de NMDA con subunidades NR2A en vez de NR2B .
Los estudios de laboratorio han demostrado que los receptores con subunidades NR2B permanecen abiertos más tiempo que los que tienen NR2A .
Pensé que ese cambio vinculado a la edad podría justificar la mayor dificultad de los adultos en aprender información nueva.

Tomé entonces una copia del gen que dirige la producción de NR2B y lo uní a un segmento especial de ADN que servía de interruptor para aumentar específicamente la capacidad sintetizadora de la proteína en el cerebro adulto.
Inyecté el gen en óvulos de ratón fecundados; se incorporó en los cromosomas y produjimos ratones genéticamente modificados que portaban una copia extra del gen NR2B .

En colaboración con Guosong Liu y Min Shuo, mi grupo encontró que los receptores de NMDA de los ratones genéticamente modificados permanecían abiertos durante unos 230 milisegundos, casi el doble del tiempo característico en ratones normales.
Determinamos también que las neuronas del hipocampo de ratones adultos establecían conexiones sinápticas más intensas que los ratones normales de la misma edad.
Y, además, esas conexiones se asemejaban a las trabadas por ratones jóvenes. 

Figura 1

PARA PRODUCIR ratones tontos y listos hay que manipular el receptor de NMDA 

una proteína involucrada en el aprendizaje y la memoria.
Sabido que el receptor de NMDA desempeña papeles clave en distintas partes del organismo, el equipo del autor empleó fragmentos de ADN para manipular los genes de diversas subunidades del receptor sólo en el cerebro.
Los ratones listos, o Doogie , presentan subunidades extra en su cerebro; el cerebro de los ratones tontos, o knock-out condicionales, carece de una subunidad del receptor de NMDA .

EL DOOGIE A PRUEBA

El ratón experto vuelve sobre sus pasos.

En las pruebas iniciales de los ratones Doogie , observamos la mayor probabilidad de que reconocieran antes un objeto familiar que otro inédito, como el juguete rolo de la fotografía superior.
Pero esa tarea de reconocimiento de objetos, así se llama la prueba, valora sólo un tipo de memoria.

Para averiguar si los Doogie gozaban de unas capacidades de aprendizaje y memoria reforzadas, recurrimos al test del laberinto acuático de Morris.
En esta prueba de laboratorio más compleja, introdujimos un ratón en una pileta circular de 1,2 metros de diámetro y llena de agua turbia.
Colocamos en la piscina una plataforma de plexiglás claro, punto menos que invisible, que era casi- no totalmente- de la misma altura que el agua, de manera que quedara oculta debajo de la superficie.
Rodeamos la piscina con una cortina negra de ducha que tenia ciertas marcas, como el punto rojo que aparece en la fotografía.
A los ratones no les gusta mojarse; en las pruebas nadan hasta encontrar la plataforma, donde se colocan casi fuera de la superficie del agua y descansan.

Observamos que los Doogie localizaban la plataforma sumergida antes que los ratones normales.
Introdujimos una nueva complicación:
quitamos la plataforma para ver si los animales recordaban donde se había hallado ésta antes en relación con las marcas en la cortina como el punto rojo.
Cuando les pusimos de nuevo en la piscina, los Doogie pasaban más tiempo que los ratones normales en el rincón donde había estado la plataforma, signo de que recordaban dónde debería encontrarse.
¿Alguna recompensa?
Pues si. 
Un buen secado con toalla y un poco de comida bajo la lámpara de rayos infrarrojos.

¿Hasta dónde llega la habilidad de un ratón experto?

Luego, Ya-Ping Tang y otros integrantes de mi equipo se dedicaron a valorar el aprendizaje y las capacidades de memorización de nuestros ratones Doogie .

Empezamos por estudiar uno de los aspectos fundamentales de la memoria, la capacidad de reconocer un objeto.
Colocamos a los ratones Doogie en una caja abierta.
Dejamos que exploraran dos objetos durante cinco minutos.
Varios días después, reemplazamos uno de los objetos por otro nuevo; volvimos a introducir ratones en la caja.
Los genéticamente modificados recordaban el objeto viejo y dedicaban su tiempo a explorar el nuevo.
Los ratones normales, sin embargo, dedicaban igual tiempo a explorar ambos objetos, prueba de que el objeto viejo no les era más familiar que el nuevo.
Repitiendo los ensayos a intervalos diferentes, vimos que los ratones genéticamente modificados recordaban objetos a lo largo de un intervalo temporal cuatro o cinco veces superior que los ratones normales.

En una segunda fase de pruebas, Tang y yo examinamos la capacidad de los ratones de asociar una descarga suave en sus patas con el hecho de estar en un tipo particular de cámara o escuchar cierto tono.
Observamos que la probabilidad de que los ratones Doogie quedaran «paralizados» - signo de que recordaban el miedo-era mayor que la de los ratones normales, cuando volvían a la cámara o escuchaban el tono de la prueba varios días después.
Deducíase, pues, que los ratones Doogie tenían mejor memoria.
Pero ¿aprendían también más deprisa?

El aprendizaje y la memoria representan etapas diferentes de un mismo proceso gradual y continuo, cuyos pasos a menudo no son fáciles de distinguir.
Sin memoria no se puede medir el aprendizaje; sin aprendizaje no existe memoria que pueda valorarse.
Para determinar si la alteración genética de los ratones Doogie les ayudaba a aprender, recurrimos a un protocolo experimental clásico del comportamiento, el aprendizaje de la extinción del miedo.

En la prueba de la extinción del miedo condicionábamos los ratones igual que antes, en una cámara con descargas; después, colocábamos los animales de nuevo en el ambiente productor de miedo - pero sin las descargas en las patas- una y otra vez.
La mayoría de los animales necesitaba unas cinco repeticiones para «desaprender» el nexo entre estar en la cámara de las descargas y recibir una descarga.
Los ratones Doogie necesitaban sólo dos repeticiones para aprender a no asustarse.
También aprendieron a no asustarse con el tono antes que los ratones normales.

La última prueba de comportamiento fue el laberinto acuático de Morris.
Se requería que los ratones usaran claves visuales de una cortina del laboratorio para encontrar el lugar de una plataforma oculta en una pequeña pileta de agua lechosa.
Esta tarea, ligeramente más complicada, pone en juego muchos factores cognitivos, incluidas habilidades analíticas, aprendizaje y memoria, así como la habilidad para elaborar estrategias.
De nuevo los ratones genéticamente modificados ejecutaron mejor la tarea que los ratones normales.

Nuestros experimentos con los ratones Doogie confirmaron las predicciones de la regla de Hebb.
Sugirieron también que el receptor de NMDA era un interruptor molecular clave para muchas formas de aprendizaje y memoria.

Aunque nuestros experimentos demostraron el papel central de los receptores de NMDA en diversos procesos de aprendizaje y memoria, quizá no sea la única molécula que se halle involucrada.
Esperamos que muchas moléculas que cumplan algún papel en el aprendizaje y la memoria se identifiquen en los años venideros. 

A raíz de la publicación de nuestros resultados, machos han querido saber si esos hallazgos significan que pronto habrá niños más listos genéticamente modificados o píldoras que nos conviertan en genios.
La respuesta es sencillamente no.

La inteligencia suele definirse como la capacidad para resolver problemas.
Aunque aprendizaje y memoria son partes integrantes de la inteligencia, ésta es un rasgo complejo que abarca el razonamiento, habilidades analíticas y la capacidad para generalizar información previamente aprendida, entre otros factores.
Muchos animales tienen que aprender, recordar, generalizar y resolver diversos tipos de problemas, como negociar su territorio, prever la relación entre causa y efecto, y evitar alimentos venenosos.
El hombre posee muchos tipos de inteligencia, como la que hace a uno un buen matemático, un biólogo molecular o un extraordinario jugador de fútbol.

Por ser el aprendizaje y la memoria dos componentes fundamentales para la resolución de problemas, no debiera sorprendernos que una potenciación de dichas capacidades condujera a una mejora de la inteligencia.
Pero los diversos tipos de inteligencia significan que la clase y grado de potenciación dependen mucho de la naturaleza de las capacidades de aprendizaje y memoria involucradas en una determinada tarea.
Animales con una mayor facultad para reconocer objetos y resolver laberintos en el laboratorio, por ejemplo, podrían encontrar antes alimento y trasladarse de un lugar a otro si se encuentran en libertad.
Podrían tener también una mayor facilidad para huir de los predadores incluso evitar caer en una trampa.
Pero la ingeniería genética nunca podrá convertir un ratón en un genio capaz de tocar el piano.

Nuestro descubrimiento de que una manipulación genética menor convierte esa diferencia mensurable en todo un conjunto de tareas de aprendizaje y memoria apunta hacia otra posibilidad: 
que el NR2B se convierta en blanco de fármacos para tratar alteraciones de la memoria relacionadas con la edad.
Una aplicación inmediata sería la búsqueda de compuestos que mejoraran la memoria al estimular la actividad o cantidad de moléculas de NR2B en pacientes con un organismo saludable, aunque dotado de un cerebro que empieza a sentir con el envejecimiento los efectos de la demencia senil.
Ese tipo de fármacos podría mejorar la memoria en pacientes moderadamente afectados por la enfermedad de Alzheimer u otras demencias.
Se trataría de estimular la función de la memoria de las neuronas que quedan, modulando y potenciando la actividad NR2B de las células. 
Por supuesto, en la creación de tales medicinas se tardará un decenio, por lo menos, no sin antes resolver numerosas incógnitas.

Tendrán que sopesarse los posibles efectos secundarios de estos fármacos en el hombre, aunque el aumento de actividad NR2B en los ratones Doogie no parece haber sido causa de toxicidad, apoplejía o infarto.

Pero si el cerebro necesita más NR2B para que funcionen mejor el aprendizaje y la memoria, 
¿por qué ha dispuesto la naturaleza que su concentración disminuya con la edad?
Varias son las hipótesis en que se trabaja.
Para unos, el cambio de NR2B a NR2A evita que la capacidad de la memoria del cerebro se sobrecargue.
Para otros, el autor incluido, el cambio es evolutivamente adaptativo desde el punto de vista de la población, porque limita la probabilidad de que los individuos viejos - que supuestamente ya se han reproducido- compitan con éxito con los más jóvenes en la búsqueda de recursos como el alimento.

La idea de que la selección natural no prime una mejor capacidad de aprendizaje y memoria en los adultos tiene profundas implicaciones.
Significa que las modificaciones genéticas de atributos mentales y cognitivos como el aprendizaje y la memoria pueden abrir un nuevo camino a la evolución de la biología genética y quizás a la civilización, con una velocidad sin precedentes.

Figura 2

PUNTO DE ENCUENTRO entre dos neuronas, la sinapsis

La hipótesis predominante en torno a la formación del recuerdo implica la intervención de receptores de NMDA , que se asientan en la superficie de la célula postsináptica.
Los receptores de NMDA , poros diminutos por donde cursan iones de calcio, pueden vincular dos sucesos a un tiempo- un prerrequisito para fijar un recuerdo- porque se abren sólo cuando reciben las dos señales.
La primera señal es la unión del glutamato liberado por la célula presináptica; la segunda, la activación eléctrica por el estímulo emanante de otra neurona que expulsa magnesio del canal del receptor.
La entrada de calcio activa las cascadas bioquímicas que terminan por reforzar la sinapsis.

EN BUSCA DE UN FÁRMACO QUE SOSTENGA LA MEMORIA

Los ratones expertos son sólo el primer paso

¿Se fabricará pronto una píldora que nos ayude a recordar dónde hemos dejado las llaves del coche?
No.
Aunque se está buscando con tesón. 
Joe Z., Tsien se ha asociado con el empresario Charles Hsu para formar una compañía basada en el descubrimiento reseñado en el artículo.

La empresa, Eureka Pharmaceuticals , tiene su sede en la oficina de Han, en el grupo Walden, en San Francisco.
La compañía se dedica a la aplicación de la genómica, a la búsqueda de moléculas que son blancos potenciales de fármacos con que puedan tratarse alteraciones del sistema nervioso central, como la pérdida de memoria y la demencia.
«Pensamos que las herramientas que Joe y sus colaboradores han desarrollado pueden traducirse pronto en bases para descubrir tratamientos de enfermedades», dice Hsu, presidente de Eureka.

El primer objetivo de Eureka es el receptor de NMDA - que Tsien y sus colaboradores manipularon genéticamente para crear sus ratones Doogie , aunque la compañía buscará otras dianas.
El receptor, un poro que permite la entrada del calcio en las células nerviosas, constituye un prerrequisito para el fortalecimiento de la conexión entre dos células nerviosas.

A lo largo de los últimos diez años, varios laboratorios farmacéuticos han ensayado con fármacos potenciales contra apoplejía que hacen descender la actividad del receptor de NMDA .
Cuando se restringe la cantidad de sangre en el cerebro, algo que ocurre con el coágulo que bloquea la arteria, las neuronas liberan mucho glutamato, un compuesto que las células usan para comunicarse.
En un fenómeno de excitotoxicidad, el exceso de glutamato se une a receptores de NMDA en otras neuronas, permitiendo que una oleada de calcio fluya hacia el Interior de células distintas.
Sumado a la falta de oxigeno, que provoca la muerte celular.

Hasta ahora, sin embargo, la búsqueda de bloqueadores del receptor de NMDA que podrían constituir fármacos del tratamiento de la apoplejía ha sido «increíblemente desalentadora», apunta Robert C., Malenka, de la Universidad de Stanford.
El problema, explica, estriba en encontrar un compuesto que se una precisamente en el lugar correcto del receptor NMDA y sólo de la manera adecuada, sin causar otros efectos neurológicos. 
(Al fin y al cabo, la fenilciclidina, una droga alucinógena, se une a ese mismo receptor).

La ineficacia de los bloqueadores del receptor de NMDA contra el trombo cerebral- unido a la posibilidad de que los agentes que se enlacen con el receptor resulten tóxicos- ha apagado el entusiasmo de quienes soñaban con el desarrollo de fármacos que incrementaran la memoria y el aprendizaje mediante la activación del receptor.
«Que yo sepa, nadie está considerando seriamente regular hacia arriba la actividad del receptor de NMDA para elevar la memoria», dice Malenka.
«Pero puede ocurrir que alguien salga un día con el fármaco mágico que trabe al receptor como debe».

Una situación más plausible, la perseguida por Tsien, podría ser el desarrollo de fármacos que de una manera sutil modulen la actividad del receptor NMDA , sin unirse a él directamente, señala Ira B., Black, de la Universidad de Medicina y Odontología de Nueva Jersey.
Black estudia un compuesto natural, un factor neurotrófico derivado del cerebro (BDNF), que aumenta la probabilidad de que segmentos del receptor de NMDA tengan un grupo fosfato unido.
Los receptores de NMDA con grupos fosfato son, a buen seguro, más activos que los que carecen de ellos.

Sin embargo, la mayoría de los neurocientíficos coinciden en que la búsqueda de un fármaco que eleve el aprendizaje y la memoria sin efectos secundarios tardará en llegar.


Deshilando el tejido de la vida

Si el lector es varón que piense casarse, he aquí un consejo de amigo: optar por una mujer que sea más lista que uno.
A esa regla de oro me atuve cuando me casé con Michelle Tetreault, biofísica brillante.
Un auténtico volcán de sugerencias, jamás me permite seguir adelante si los pasos andados no está claros. 

Ella es experta en fotosíntesis y en su labor recurre a las técnicas más modernas de la bioquímica. 
Manipulando las unidades hereditarias mínimas, los pares de bases de una hebra de ADN, le es posible cambiar uno a uno los aminoácidos que integran una proteína fundamental, para estudiar a continuación hasta qué punto cumple su papel esa molécula alterada.

Me inoculó el gusanillo de la biología molecular. 
No me cabe duda de que las técnicas punta que ella emplea sobrepasan las posibilidades de cualquier fisgón como yo.
Pero la puerta no está cerrada.
Empezaremos a entreabrirla este mes remedando algo que para los biotecnólogos es hoy coser y cantar: extraer y purificar ADN.

El ADN es la mayor de las moléculas conocidas.
Una hebra entera de ADN consta de millones de átomos. 
Al separarse de una célula, el ADN se rompe en múltiples fragmentos.
En solución, esas hebras poseen una carga eléctrica levemente negativa, hecho que contribuye a crear una química fascinante.
Por ejemplo, los iones salinos son atraídos hacia las cargas negativas del ADN, con el efecto de neutralizarlas, y ese fenómeno impide que los fragmentos separados de ADN se adhieran entre sí.
Controlando las concentración de sales, los biólogos provocan que los fragmentos de ADN se dispersen o se aglomeren.
Y en esto reside el secreto de la remoción del ADN celular.

Se empieza por lisar las células, abriéndolas para verter sus entrañas moleculares en una solución tampón, donde se disuelve el ADN.
En ese momento, el tampón contiene ADN y todo un surtido de restos celulares: ARN, proteínas, carbohidratos y otras sustancias en menor cuantía.
Fijando las proteínas con detergente y reduciendo la concentración de sales, podemos sacar el ADN, obteniendo una muestra casi prístina de las moléculas portadoras de la herencia.

Mi profundo agradecimiento a Jack Chirikjian y Karen Graf de Edvotek ( www.edvotek.com ), empresa de biotecnología educativa de West Bethesda (Maryland), por mostrarme la manera en que cualquiera puede purificar el ADN de las células de un vegetal sin salir de la cocina.
Lo primero es preparar el tampón.
Se vierte 120 mililitros de agua en un recipiente de vidrio limpio, junto con 1,5 gramos (1/4 de cucharita de té) de sal de mesa, cinco gramos (una cucharita de té) de bicarbonato de sosa y cinco mililitros (una cucharita de té) de champú o de un detergente de lavandería líquido.
Estos productos portan menos aditivos que los jabones de tocador, aunque no hay que renunciar a probar con otros productos.

El detergente cumple una doble misión: lisar las paredes celulares y promover el fraccionamiento de las proteínas largas para que no salgan con el ADN.
En Edvotek recomiendan usar sal de mesa pura y agua destilada, pero yo he empleado con éxito sal yodada y agua embotellada, y hasta una vez en que olvidé añadir el bicarbonato obtuve buenos resultados.
Evítese el agua del grifo.
Para aminorar la velocidad de degradación del ADN, se enfría la solución tampón en un baño de hielo triturado y agua antes de proseguir.

En la despensa está la fuente de ADN.
Yo obtuve magníficos resultados con una cebolla;
Edvotek recomienda también ajo, plátanos y tomates.
Elegida la fruta o verdura, y tras cortarla a cuadraditos e introducirlos en la mezcladora, se añade un poco de agua. 
Mézclese bien, accionando las hojas de corte de la mezcladora a impulsos de 10 segundos.
O bien, más fácil, pásense los trozos por un prensa-ajos.
Se romperán algunas células y otras muchas expondrán sus paredes al ataque del detergente.

Introduciremos cinco mililitros de esas gachas de verdura picada en un recipiente limpio; los mezclaremos con 10 mililitros del tampón enfriado.
Agítese vigorosamente durante dos minutos al menos.
Se procede luego a separar la sustancia vegetal visible del caldo molecular concentrado. 
Si es posible, empléese una centrífuga.
(Para construirlas, léase la sección de marzo).
Centrifúguese el contenido a baja velocidad durante cinco minutos y, después, viértanse cinco mililitros del líquido en exceso en un recipiente estrecho: una pipeta o un frasco de plástico transparente.
A falta de centrífuga, cuélese el material por un filtro ordinario de café para eliminar la mayor parte del desecho vegetal. 
Con suerte, todo desecho que pase el filtrado se hundirá o flotará, por lo que será fácil vaciar los sólidos en el fregadero y luego introducir el líquido aclarado en un recipiente limpio.

La solución retendrá fragmentos de ADN, amén de un enjambre de moléculas indeseables.
Para extraer el ADN, enfriaremos un poco de alcohol isopropílico en el congelador hasta los cero grados.
En la mayoría de las farmacias se vende en concentraciones del 77 al 99 por ciento.
Optaremos por el de mayor concentración (sin colorantes ni perfumes).
Con una paja de sorber refrescos, depositaremos 10 mililitros del alcohol enfriado encima de la solución de ADN.
Para que no nos entre alcohol en la boca, se coloca la paja en la botella del alcohol y se pellica la superficie con la punta de la paja.
Dejaremos que el alcohol se escurra lentamente por la cara interna del recipiente inclinando éste ligeramente.
El alcohol, al ser menos denso que el tampón, flotará.
Se inserta una varilla estrecha atravesando la capa de alcohol. ( Edvotek recomienda un palillo de remover café o una varilla de cóctel de vidrio.).

Con parsimonia removeremos la varilla, hacia adelante y hacia atrás, con su punta justo debajo de la separación entre el alcohol y la solución tampón.
Entonces, los trozos mayores de ADN se arrollarán sobre la varilla, quedándose en la solución las moléculas menores.
Al cabo de un minuto sacaremos la varilla a través del alcohol, y así el ADN se adherirá al extremo de varilla con el aspecto de un lodo viscoso y transparente agarrado a la punta.

Aunque el resultado sea impresionante, es éste un procedimiento sencillo y barato que no rinde un producto puro. 
Los profesionales añaden enzimas que fragmentan las moléculas de ARN para asegurarse de que éstas no se entremezclan con el codiciado ADN.

Incluso tras las extracción más concienzuda, es normal que en el recipiente quede ADN residual que forma una telaraña invisible en el líquido.
Pero con un pequeño esfuerzo adicional también es posible ver ese material.
Algunos colorantes, como el azul de metileno, se adhieren a los fragmentos de ADN dotados de carga.
Una minúscula cantidad añadida a la solución remanente revela zarcillos de ADN no recogido.
Ignoro si los colorantes de cocina o los tintes de ropa o cabello funcionan también.
Sería cuestión de averiguarlo. 
Añádase sólo una gota;
nuestro deseo es que todas las moléculas del tinte se adhieran al ADN, sin que quede ninguna para manchar el agua.

La extracción del ADN de un organismo es sólo el primer paso en la mayoría de los experimentos biológicos. 
A los aficionados quizá les interese conocer hasta dónde pueden llegar en la experimentación.
De momento hasta clasificar los distintos fragmentos de ADN según sus longitudes.

La Society for Amateur Scientists y Edvotek han aunado fuerzas para crear un kit que contiene muestras de células, útiles de laboratorio, enzimas, tampones y detergentes que facilitan la creación de preparados de más calidad.
Para más información acerca de éste y otros proyectos científicos para aficionados, visite el foro de la sociedad en xxx .


Ensayo aleatorio y controlado con placebo del ritonavir en la enfermedad por VH-1 avanzada

Introducción

La zidovudina fue el primer tratamiento antirretroviral con el que se demostró una reducción significativa de la tasa de mortalidad debida al sida en un ensayo controlado con placebo en pacientes con enfermedad por el VIH-1 avanzada. 
Otros ensayos clínicos controlados con placebo posteriores han puesto de manifiesto su eficacia clínica en la prevención de las infecciones oportunistas menores en los pacientes con una inmunodeficiencia moderada y en los que presentan una infección por VIH-1 primaria.
En comparación con la monoterapia con zidovudina, otros análogos de los nucleósidos (solos o en combinación con zidovudina) han retardado la aparición de la progresión de la enfermedad por VIH-1 o la muerte en los pacientes con una inmunodeficiencia moderada en ensayos clínicos relativamente largos, en especial como tratamiento inicial.

El ritonavir ( Norvir, Abbott Laboratories, IL, EE.UU.) es un inhibidor específico y potente de la aspartil proteasa del VII-1.
El fármaco tiene una biodisponibilidad oral elevada y una semivida plasmática prolongada, que permite su administración dos veces al día.
En comparación con los análogos de los nucleósidos, el ritonavir tiene un efecto más potente y mantenido sobre la viremia plasmática, las cargas de infección celular por VIH-1 y los subgrupos de inmunofenotipos linfocitarios.
En ensayos clínicos sobre el VIH-1, algunas respuestas de marcadores indirectos predicen la eficacia clínica del fármaco.

Sin embargo, -no está clara -su utilidad en la valoración del tratamiento- con inhibidores de la proteasa.
En consecuencia, este estudio se diseñó como un ensayo clínico simple y a gran escala para investigar rápidamente, en una subpoblación específica de pacientes con enfermedad por VIH-1 (recuentos de linfocitos CD4 xxx ) la seguridad y eficacia clínica del ritonavir.
Estos resultados podrían ser generalizables a una población más amplia.

Métodos

El objetivo principal de este ensayo multicéntrico, controlado con placebo, doble ciego y aleatorio, fue determinar la eficacia del ritonavir en solución de administración oral (600 mg dos veces al día), para reducir la tasa de mortalidad o de cualquier enfermedad especificada definitoria de sida nueva o recidivante, en una población de adultos infectados por el VIH-1 que presentaban recuentos de linfocitos CD4 de xxx o inferiores, permitiendo al mismo tiempo el empleo simultáneo de tratamientos anti-VIH-1 autorizados. 
Los objetivos secundarios fueron evaluar el efecto del ritonavir sobre los recuentos de linfocitos CD4 y CD8 y las concentraciones plasmáticas de ARN del VIH-1 ( Roche Amplicor límite de detección, 200 copias/ml ).
La respuesta de la viremia plasmática se estudió en un subgrupo de pacientes (los primeros 159 a los que se incluyó en la distribución aleatoria con concentraciones basales de ARN de VIH1 en plasma superiores a 15.000 copias/ml [subgrupo de actividad antiviral]). 
El protocolo del estudio fue aprobado por los organismos reguladores apropiados y por los comités de revisión de los centros participantes.

Pacientes

Los participantes en el estudio dieron su consentimiento informado por escrito antes de que se les incluyera en las técnicas de selección para el mismo.
Los participantes elegibles debían tener como mínimo 12 años de edad, una prueba de anticuerpos sé ricos confirmada para el VIH-1, un recuento de linfocitos CD4 de xxx o inferior y haber recibido al menos 9 meses de tratamiento con al menos un nucleósido aprobado (zidovudina, zalcitabina, didanosina o estavudina).

Además, los participantes debían haber recibido al menos 6 semanas de tratamiento estable con una puntuación de Karnofsky de más de 70.
Los criterios de exclusión fueron la participación previa en este estudio o en otro estudio sobre un inhibidor de la proteasa del VIH-1, la enfermedad aguda, las anomalías hematológicas o bioquímicas predefinidas, el tratamiento actual con más de 2 inhibidores nucleósidos de la transcriptasa inversa, el consumo de drogas ilegales o fármacos experimentales, el tratamiento con fármacos contraindicados específicos, el embarazo y la lactancia materna.

Objetivos

El objetivo principal a valorar fue el tiempo transcurrido hasta la aparición de cualquier nueva enfermedad definitoria de sida o hasta la muerte no precedida por un episodio definitorio de sida.
Se consideró que era un nuevo episodio definitorio de sida cualquier enfermedad clínica especificada por los Centers for Disease Control and Prevention de los EE.UU.
Las recidivas de neumocistosis, candidiasis esofágica o úlcera herpética crónica se incluyeron también en el objetivo principal de valoración, pero no se consideraron en cambio, así las recidivas o progresiones de otros acontecimientos.
Un objetivo secundario de valoración, que se analizó por separado, fue la muerte por cualquier causa.
Cada episodio notificado como enfermedad correspondiente al objetivo de valoración era evaluado por 2 revisores independientes (DWC y SK) que no conocían la asignación de los tratamientos y que utilizaron unos criterios diagnósticos predefinidos.
Se exigió la documentación original para mantener un alto grado de confianza clínica a la hora de confirmar cada uno de los episodios como correspondientes al objetivo clínico.

Asignación aleatoria

A los pacientes elegibles se les asignó aleatoriamente la administración de ritonavir (600 mg en una solución líquida de administración oral dos veces al día) o un placebo indiferenciable (solución liquida- de administración oral dos veces al día) mediante un esquema generado por ordenador (preparado por Abbott Laboratories).
La asignación se realizó en bloques de 4 pacientes, estratificados según la región geográfica (Norteamérica, Europa y Australia):
Se garantizó el ocultamiento de la asignación mediante el empleo de un sistema de respuesta de voz automatizado.

Técnicas

Los pacientes que dieron su consentimiento informado fueron objeto de un examen de selección mediante una historia clínica completa y una revisión de las medicaciones simultáneas, una exploración física completa, un electrocardiograma, una radiografía de tórax, un recuento de linfocitos CD4 y CD8 y análisis hematológicos y bioquímicos estándar, a lo largo de un período de 28 días antes de la asignación aleatoria del tratamiento.
Los recuentos basales de linfocitos CD4 y CD8 y las concentraciones plasmáticas de ARN del VIH-1 se definieron con la media de dos determinaciones efectuadas durante los 10 días previos a la distribución aleatoria. 
Tras ésta, se repitieron la valoración de la historia clínica, la exploración física, los análisis de laboratorio para la detección de efectos tóxicos y marcadores indirectos, y las comprobaciones del cumplimiento del tratamiento (mediante el recuento de la cantidad de medicación, en estudio restante) a intervalos semanales durante el primer mes, cada 2 semanas durante los 2 meses siguientes y una vez al mes a partir de entonces.
La evaluación se repitió al suspender la medicación en estudio.
Todos los análisis de carácter hematológico y bioquímico se llevaron a cabo en laboratorios centrales.

Tratamiento y seguimiento

Hubo tres períodos de estudio del tratamiento farmacológico con la medición en estudio y tratamiento anti-VIH-1 simultáneo. 
Durante las primeras 16 semanas, los pacientes recibieron la medicación en estudio con enmascaramiento y el tratamiento anti-VIH-1 banal simultáneo que no podía modificarse según el protocolo (a menos que hubiera de ser interrumpido por intolerancia a éste).

Durante el segundo período, desde 16 semanas después de la asignación aleatoria hasta la administración de ritonavir en diseño abierto a todos los pacientes después de que se hubiera alcanzado el objetivo principal del estudio, los pacientes que presentaron un episodio definitorio de sida confirmado durante el primer o el segundo períodos fueron tratados con ritonavir en diseño abierto.

Los pacientes que no experimentaron ningún episodio definitorio de sida durante estos dos periodos continuaron con la medicación en estudio enmascarada.
Además, durante el segundo período, los médicos pudieron modificar la pauta de tratamiento simultáneo de hasta 2 nucleósidos autorizados mediante la inclusión de lamivudina, que se comercializó durante el estudio. 
En el tercer período, tras haberse alcanzado el objetivo principal, se ofreció a todos los pacientes la administración de ritonavir en diseño abierto fuera cual fuera su estado respecto a la enfermedad.
Los pacientes que suspendieron la medicación en estudio enmascarada continuaron con las visitas programadas durante todo el ensayo. 

Episodios adversos

Los episodios adversos se definieron como cualquier alteración de un signo o síntoma clínico, o anomalía de laboratorio significativa, excluyendo los trastornos asociados a la propia infección por el VIH-1.
Todos los episodios adversos fueron evaluados en cuanto a su gravedad y relación con el fármaco en estudio.

Análisis estadístico 

Los cálculos del tamaño muestral se basaron en la estimación de que la tasa anual de progresión de la enfermedad o de muerte sería de un 40%.
Partimos del supuesto de que hasta en un 10% de los pacientes se perdería el control evolutivo antes de que se produjera un episodio definitorio de sida, y establecimos el objetivo de una potencia estadística del 80% para detectar una reducción del 33% en la proporción de riesgos.
Con la inclusión rápida de los 350 pacientes que se calculó que eran necesarios por grupo de tratamiento, el análisis final de progresión de la enfermedad o muerte se planificó inicialmente para el momento en que 191 pacientes hubieran experimentado episodios de este tipo.
Sin embargo, dado que la incorporación fue más rápida de lo previsto, se aumentó el tamaño muestral hasta unos 500 pacientes por grupo de tratamiento, con un análisis final dé progresión de la enfermedad o muerte tras haber alcanzado el mismo umbral de número de episodios.

Todos los análisis se llevaron a cabo según intención de tratar, con pruebas bilaterales y un grado de significación estadística de 0,05.
Los cálculos se llevaron a cabo con el programa informático SAS (versión 6, cuarta edición).
El tiempo transcurrido hasta la progresión de la enfermedad o la muerte, así como el tiempo transcurrido hasta la muerte por cualquier causa, se analizaron con un modelo de riesgos proporcionales de Cox estratificado según la región geográfica. 
Las comparaciones entre los grupos se realizaron con la prueba de rangos logarítmicos.
Además, se utilizaron curvas de Kaplan-Meier sin estratificación para presentar estos datos.
Se puso a prueba la suposición de una proporción dé riesgos constante a lo largo del tiempo, mediante el cálculo del logaritmo negativo de las estimaciones de Kaplan-Meier para el paralelismo intergrupos dentro de cada región geográfica.
Se clasificó también a los pacientes según los datos demográficos basales (media de todos los valores para el recuento de linfocitos CD4 obtenido en un plazo de 10 días antes de la asignación aleatoria y número de análogos de nucleósidos tomados en la situación basal).
A continuación examinamos la posible interacción entre el grupo de asignación de tratamiento y el estado basal mediante una prueba de proporción de probabilidad comparando modelos de riesgos proporcionales con término de interacción y sin él.

Los informes sobre las muertes se enviaron a un comité de vigilancia de seguridad independiente (RW, RP y DD).
El protocolo especificaba que debía aplicarse la regla de detención de 0'Brien Fleming para la interrupción prematura del estudio como consecuencia de las muertes.
Se realizaron dos análisis preliminares antes del final de la parte de diseño doble ciego del estudio (basados en los resultados del análisis del tiempo hasta la progresión de la enfermedad o la muerte).

Resultados

Se seleccionó a los pacientes para el estudio entre abril y julio de 1995.
Se examinó a un total de 1.716 pacientes, de los que 626 fueron excluidos (fig. 1).

De los 1.090 pacientes incluidos en la distribución aleatoria (543 en el grupo de ritonavir y 547 en el grupo de placebo), dos de cada grupo no llegaron a recibir ninguna medicación del ensayo.
Estos pacientes fueron incluidos en el análisis principal según intención de tratar, pero se les excluyó del análisis de episodios adversos atribuibles a los fármacos en estudio.

Los pacientes (tabla 1) se incorporaron al estudio en 67 centros de Australia (42 pacientes), Europa (238 pacientes) y Norteamérica (810 pacientes).
Un 97% de los pacientes del grupo de ritonavir y un 95% de los del grupo placebo habían sufrido uno o varios trastornos asociados al VIH-1 antes del inicio del estudio.

Los episodios relacionados con el VIII-1 más frecuentes registrados antes del estudio fueron la candidiasis oral (64%), la infección por herpes simple (28%) o por herpes zoster (28%), la leucoplasia vellosa (27%), la neuropatía periférica (26%), la linfadenopatía generalizada persistente (25%), la diarrea (20%) y la neumonía por Pneumocystis carinii (25%).

Sin embargo, no existían diferencias aparentes entre los grupos de tratamiento en cuanto a los trastornos relacionados con el VIH-1 persistentes que se daban en el momento de la incorporación al estudio.
El empleo de fármacos simultáneos en los grupos de ritonavir y placebo era similar antes de la incorporación al estudio, con una mediana de 14 medicaciones en ambos grupos (tabla 1).
Los análogos de los nucleósidos utilizados con más frecuencia al inicio del tratamiento aleatorio eran la zidovudina (45%), la estavudina (26%), la zalcitabina (19%) y la didanosina (16%).

La mediana de duración de la medicación en estudio con enmascaramiento fue de xxx semanas en los pacientes a los que se asignó ritonavir y de xxx semanas en los pacientes a los que se asignó placebo; 15 ( xxx ) pacientes se perdieron para el seguimiento antes del periodo 3 (fig. 1, incluyendo los que no recibieron la medicación en estudio), con lo que la mediana de control evolutivo era de 28,9 semanas ( xxx ) hasta el final del período 2.
Durante el período 3, se ofreció a los pacientes un tratamiento con ritonavir en diseño abierto y se les siguió durante una mediana total de 51,3 semanas ( xxx ).
Se perdió el control evolutivo de 76 pacientes ( xxx ) antes de finalizar el período 3.

Un total de 119 ( xxx ) pacientes de grupo de ritonavir y 205 ( xxx ) del grupo placebo presentaron un evento definitorio de sida o fallecieron durante los períodos 1 y 2 de este estudio.
La proporción de riesgos era de 0,53 (IC del 95%, 0,42-0,66; rangos logarítmicos xxx ).
El episodio del objetivo de valoración principal que se dio con más frecuencia fue la muerte. 
El episodio de primera enfermedad más frecuente fue la candidiasis esofágica nueva o recidivante (tabla 2).
Las tasas de primeros episodios fueron, en general, más bajas en el grupo de ritonavir que en el grupo de placebo.
Un análisis secundario de las muertes que se produjeron durante los períodos 1 y 2 indicó una proporción de riesgos estimada de 0,60 (IC del 95%, xxx ; rangos logarítmicos xxx ).

Se analizó también la progresión de la enfermedad o la muerte teniendo en cuenta como factores de estratificación el recuento basal de linfocitos CD4 o el número de análogos de los nucleósidos basales (tabla 3).
No se observó ninguna interacción significativa entre el grupo de tratamiento y el estado basal de estos factores en ninguno de los análisis.
El ritonavir redujo de manera significativa el riesgo de progresión deja enfermedad o muerte en todas las categorías de recuento basal de linfocitos CD4 utilizadas en este análisis.

Además, los resultados obtenidos sugieren que el riesgo de progresión de la enfermedad o de muerte aumentaba a medida que disminuía el número basal de linfocitos CD4.
El ritonavir solo no redujo de forma significativa el riesgo de progresión de enfermedad o de muerte.
Sin embargo, se observó una disminución significativa del riesgo de progresión de la enfermedad o de muerte para el ritonavir utilizado en combinación con al menos un análogo de los nucleósidos empleado en la situación basal.

Una vez demostrado el beneficio clínico proporcionado por el ritonavir, los episodios dejaron de ser evaluados por revisores independientes.
En consecuencia, no pudimos incluir la progresión de la enfermedad en el análisis realizado al final del periodo 3.
Sin embargo, sí pudo llevarse a cabo un análisis de la supervivencia de los pacientes al finalizar el periodo 3.
Un total de 87 pacientes (16%) a los que se había asignado ritonavir y 126 (23%) a los que se había asignado placebo fallecieron por el conjunto de todas las causas.
La proporción de riesgos era de 0,69 (IC del xxx ; rangos logarítmicos xxx ).

En el grupo de ritonavir (para los pacientes en los que se disponía de datos correspondientes a todos los momentos de valoración), los recuentos de linfocitos CD4 se incrementaron respecto al valor basal medio de xxx (aumento máximo, xxx ), y los recuentos de linfocitos CD8 se incrementaron respecto a un valor basal medio de xxx (aumento máximo, xxx ; fig. 4).
Los recuentos de CD4 más elevados se mantuvieron en los pacientes tratados con ritonavir. 
Los recuentos de CD8 disminuyeron respecto al valor máximo en el período 1 pero se estabilizaron en los períodos 2 y 3, durante los cuales los efectos medios para los linfocitos CD4 y CD8 se igualaron en el grupo placebo.

En el subgrupo de estudio de la actividad antiviral, la carga viral plasmática disminuyó rápidamente en los pacientes a los que se asignó el tratamiento con ritonavir, con una reducción media máxima respecto al valor basal de xxx xxx a las 2 semanas, seguido de un posterior restablecimiento del valor basal (fig. 4).
A continuación, las concentraciones de ARN del VIH-1 se estabilizaron durante el período 2 en un valor de aproximadamente 0,6 log xxx por debajo del valor basal.
Durante los periodos 2 y 3, cuando los pacientes pasaron al ritonavir en diseño abierto, la viremia plasmática en los demás pacientes del grupo placebo disminuyó hasta valores inferiores a los observados en el resto de pacientes del grupo de ritonayir.

Efectos adversos

En total, 114 (21,1%) pacientes del grupo de ritonavir y 45 ( xxx ) del grupo placebo abandonaron prematuramente el tratamiento con la medicación en estudio enmascarada ( xxx ).
Sin embargo, muchos de los pacientes del grupo de ritonavir abandonaron el tratamiento durante las primeras 4 semanas del mismo;
un 9% de los pacientes del grupo de ritonavir y un 1% de los del grupo placebo suspendieron el tratamiento con la medicación enmascarada durante el período 1.
En la tabla 4 se indican los episodios adversos notificados con más frecuencia y las causas del abandono prematuro. 
Estos episodios se clasificaron la mayoría de las veces como síntomas gastrointestinales.

Las alteraciones analíticas fueron más frecuentes con el ritonavir que con el placebo:
se produjeron concentraciones elevadas de alanina aminotransferasa ( xxx ) en un xxx frente al 4,0%; gammaglutamil transpeptidasa superior a xxx en un 21,1% en comparación con el 11,9%; creatinfosfocinasa superior a 800 U/l en un 12,5% en comparación, con un 8,2%; y triglicéridos séricos en ayunas superiores a xxx en un 12,9% en comparación con el xxx .
En cambio, las anomalías hematológicas fueron menos frecuentes en el grupo de ritonavir que en el placebo: se produjo anemia (hematocrito xxx ) en un 20,3% en comparación con un xxx y neutropenia ( xxx ) en un 6,4 en comparación con un 10,2%.

Discusión

En este estudio, el ritonavir redujo el riesgo dé complicaciones del sida y prolongó la supervivencia en los pacientes con enfermedad por VIH-1 avanzada.

Los participantes en este ensayo clínico a gran escala eran representativos de muchos pacientes con enfermedad por VIH-1 avanzada.
La eficacia del tratamiento con ritonavir frente a la enfermedad se estimó de manera conservadora en relación a unas variables de valoración clínica rigurosamente confirmadas.
Los trastornos relacionados con el VIII-1 frecuentes fueron siempre menos frecuentes como primeros episodios en el grupo dé ritonavir que en el grupo placebo.

Existen dos factores principales que pueden haber ejercido un sesgo en el resultado global.
En primer lugar, hubo un número significativamente superior de pacientes del grupo ritonavir que del grupo placebo que suspendieron prematuramente la medicación en estudio en una fase inicial del mismo a causa de síntomas adversos relacionados con el fármaco; este factor podría haber causado una subestimación de la eficacia del tratamiento en el análisis según intención de tratar.
En segundo lugar, puesto que las respuestas de los linfocitos CD4 no se ocultaron a los investigadores, la administración de un tratamiento con ritonavir en diseño abierto después del periodo 1 a los pacientes que habían presentado episodios definitorios de sida puede haber motivado una sobrenotificación de episodios de este tipo en el grupo placebo, con lo que habría una tasa dé enfermedad aparente más elevada en este grupo.
Sin embargo, el episodio de valoración más frecuente fue la muerte: Además, el riesgo de aparición de una enfermedad definitoria de sida se redujo de manera uniforme para los diversos episodios de enfermedad esperados.
Si se hubiera producido un sesgo de notificación de este tipo para el objetivo de valoración principal de enfermedad, la administración de ritonavir durante el período 2 para los casos de aparición de episodios de enfermedad y durante el período 3 para todos los pacientes con independencia del estado de su enfermedad hubiera reducido el beneficio global de supervivencia observado para el ritonavir en el análisis de todas las muertes de cualquier causa.
Nuestros datos pueden corresponder al efecto de un tratamiento temprano frente a uno diferido con ritonavir en esta población de pacientes.

A pesar de la intolerancia inicial de algunos pacientes para la solución líquida de ritonavir, los episodios adversos graves fueron poco frecuentes.
Las elevaciones significativas de las concentraciones séricas de enzimas hepatocelulares y triglicéridos durante el tratamiento con ritonavir fueron en general reversibles y no se asociaron a síntomas clínicos.
El beneficio aportado por el ritonavir, con una mediana de 14 medicaciones simultáneas al incorporarse al estudio, apoya la seguridad global del fármaco en este contexto.
La interacción farmacocinética esperada u observada del ritonavir en el metabolismo de otros fármacos a través de la vía entérica y de la vía metabólica del citocromo P450 3A4 microsómico hepático sugiere la posibilidad de interacciones farmacológicas graves.
En este ensayo se excluyeron las medicaciones en función de los riesgos teóricos de posibles interacciones farmacológicas;
sin embargo, se permitió el tratamiento con rifabutina.
La rifabutina fue el único fármaco asociado a un aumento de los episodios adversos. 

La eficacia del tratamiento anti-VIH-1 se refleja en las respuestas de los linfocitos CD4 y de la viremia plasmática; 
las respuestas descritas con anterioridad al tratamiento con ritonavir se confirmaron en este ensayo.
La respuesta descrita de los linfocitos CD814 se confirmó también. 
Los linfocitos CD81a y los factores secretados por los CD819 pueden intervenir en la respuesta del huésped a la infección por el VIH-1.
Los cambios en los recuentos de linfocitos CD8 durante el tratamiento con ritonavir pueden indicar también un efecto directo o un efecto terapéutico en el que el aumento inicial y la posterior reducción respecto a la respuesta máxima con un tratamiento potente pueden corresponder a una menor replicación del VIH-1 y a una posterior disminución de la activación inmunitaria.

Tras la presentación inicial de los resultados de este estudio y la autorización del ritonavir en los EE.UU. han continuado o se han iniciado ensayos controlados para confirmar el beneficio clínico aportado por un inhibidor de las proteasas sobre la salud y la supervivencia de los pacientes con enfermedad por VIH-1 o sida.

La necesidad de demostrar sucesivamente la eficacia de los diversos fármacos de una misma clase; en vez de su seguridad, actividad y eficacia comparativas, debe contrapesarse con los derechos de los voluntarios que participan en ensayos clínicos.
Esta cuestión ética está adquiriendo una importancia creciente a medida que se establecen directrices sobre las normas de calidad de los tratamientos. 

Aunque el ritonavir fue el primer inhibidor de las proteasas con el que se demostró beneficio clínico, el conocimiento de la correlación clínica de los cambios de la concentración de ARN de VIH-1 y del recuento de linfocitos CD4 y CD8 requiere un análisis más detallado.
Se observó una supresión mayor y más sostenida del ARN VIH-1 que la observada en este estudio en pacientes con una inmunodeficiencia relacionada con el VIH-1 menos avanzada y cuando se administró zalcitabina y zidovudina conjuntamente con el ritonavir como tratamiento inicial.
Los resultados preliminares de los estudios que están en curso han evidenciado una supresión del ARN de VIH 1 por debajo de los valores de cuantificación habituales con el empleo de ritonavir junto con, saquinavir o junto con zidovudina y lamivudina en la mayoría de los pacientes.

En este estudio se utilizó el diseño de añadir ritonavir a un máximo de 2 agentes antirretrovirales nucleósidos autorizados, según lo tolerado por cada paciente.
Esta estrategia concordaba, pues, con la práctica clínica habitual.
En los pacientes con un tratamiento previo amplio y una inmunodeficiencia avanzada, el enfoque de añadir un solo fármaco simula la monoterapia con dicho fármaco. 
El patrón de supresión incompleta y transitoria del ARN de VIH-1 observado en este estudio es similar al de otros estudios de monoterapia con ritonavir, en los que aparecieron de manera escalonada resistencias farmacológicas del VIH-1. 
La eficacia aparentemente superior del ritonavir en pacientes con un recuento de linfocitos CD4 basal más elevado y en los pacientes que tomaban al menos otra medicación anti-VIH-1 en situación basal puede reflejar el mejor estado de salud general o la mejor tolerabilidad del ritonavir en estos pacientes.
Sin embargo, también es posible que corresponda a una actividad anti-VIH-1 mayor y más persistente o a una mayor eficacia del tratamiento con ritonavir establecido en combinación de forma más temprana, como resultado de la disminución o el retraso de las resistencias farmacológicas que puede proporcionar un tratamiento combinado. 

Este ensayo demostró la eficacia del ritonavir en el tratamiento dé pacientes con enfermedad por VIH-1 avanzada que habían recibido ya un tratamiento previo intenso. 
La táctica de añadir un solo fármaco anti-VIH-1 potente a una pauta de tratamiento estable o que fracasa, en un ensayo clínico, puede mejorarse en la práctica clínica.
Serán necesarios nuevos estudios para evaluar si el tratamiento combinado inicial de ritonavir y otros fármacos antirretrovirales o el cambio simultáneo de dos o más fármacos anti-VIH-1 confiere un beneficio clínico superior y más persistente que el identificado aquí, como parecen indicar los estudios de marcadores indirectos.

Tabla 1

Características demográficas y clínicas basales

Demografía.


V/M.


Mediana (límites) de edad en años.


Raza blanca/no blanca.


Factor de riesgo para HIV-1 (número de pacientes).


Varón homosexual o bisexual.


Heterosexual, compañero sexual HIV-1-positivo.


Usuario de drogas por vía parenteral.


Pareja sexual de usuario de drogas por vía parenteral .


Receptor de transfusión.


Hemofílico.


Otro o desconocido.


Puntuación funcional de Karnofsky.


Antecedentes de HIV-1.


Tiempo desde el diagnóstico HIV-1 (años).


Duración del tratamiento antirretroviral (años) .


Duración del tratamiento con zidovudina (años).


Número medio de fármacos antirretrovirales utilizados.


Recuento de linfocitos CD4 (número de pacientes).


Recuento de linfocitos (por xxx ).


CD4.


CD8.


Carga de RNA de HIV-1 en plasma (log10 xxx ).


Subgrupo de actividad antiviral.


Mediana (AIC).


Todos los pacientes estudiados.


Mediana (AIC).


Tratamiento con basal simultáneo tomado por al menos un 15% de los pacientes en cada grupo.


Zidovudina.


Stavudina.


Zalcitabina.


Didanosina.


Trimetoprima.


Sulfametoxazol.


Claritromicina.


Rifabutina.


Pentamidina.


Ciprofloxacino.


Fluconazol.


Aciclovir.


Paracetamol.


Ácido ascórbico .


Testosterona.


Loperamida.


Filgrastim.


Figura 1

Perfil del ensayo

En el periodo 2 existe un cierto solapamiento entre los pacientes con objetivos confirmados (enfermedad definitoria de sida o muerte) y número de muertes.

Figura 2

Progresión de la enfermedad o muerte (hasta el final del periodo 2)

Tabla 2

Primeros episodios evolutivos individuales hasta el final del periodo 2

Número de pacientes.


Ritonavir.


Placebo.


Muertes.


Eventos definitorio de sida.


Total.


Candidiasis esofágica.


Sarcoma de Kaposi.


Retinitis por citomegalovirus .


Neumocistosis.


Citomegalovirus, sin retinitis.


Complejo Mycobacterium avium .


Síndrome de emaciación.


Linfoma.


Otros.


Total de episodios.


Tabla 3

Episodios evolutivos principales estratificados según las características basales

Figura 2

Supervivencia global (hasta el final de periodo 3) Obsérvese que la escala temporal difiere de la de la figura 2

Figura 4

Cambios medios (y EE) en recuentos de células CD4, recuentos de células CD8, y concentración plasmática de RNA HIV- 1

El número de pacientes del grupo placebo que recibió ritonavir en diseño abierto después del período 1 se indica entre paréntesis para las visitas realizadas cada 2 meses a partir del mes 4.
ARN del VIH-1 en plasma para el subgrupo de actividad antiviral solamente. 

Tabla 4

Episodios adversos relacionados con el tratamiento (en más de 1.5% de los participantes) y tasas de abandono de prematuro de la medicación en este estudio

Síntoma.


Nausea.


Vómitos.


Diarrea.


Debilidad.


Gusto alterado.


Parestesias peribucales.


Total de abandonos .



División celular y cromosomas 

CONCEPTOS DEL CAPÍTULO

La continuidad genética entre las células y entre los organismos de cualquier especie con reproducción sexual se mantiene gracias a la mitosis y la meiosis.
Estos procesos, regulares y eficientes, dan lagar a células somáticas diploides y a gametos y esporas haploides, respectivamente. 

En estos estudios de división, el material genético se condensa, formando estructura visibles y discretos llamados cromosomas.
Estas estructuras adquieren diversas formas y apariencias y su estudio ha proporcionado importantes descubrimientos sobre la naturaleza del material genético.

En cada ser vivo hay una sustancia que se denomina el material genético .
Excepto en ciertos virus, este material esta compuesto del ácido nucleico DNA.
Una molécula de DNA tiene muchas unidades llamadas genes , cuyos productos dirigen todas las actividades metabólicas de las células.
El DNA, con su batería de genes, está organizado en cromosomas , estructuras que sirven de vehículo para la transmisión de la información genética.
El modo en el que los cromosomas se transmiten de una generación celular a la siguiente, y de los organismos a sus descendientes, es extraordinariamente preciso.
En este capítulo, al desarrollar el tema de la continuidad genética entre células y organismos, consideraremos cuan exactamente se realiza tal transmisión. 

En los eucariotas hay dos procesos muy importantes: la mitosis y la meiosis .
Aunque el mecanismo de ambos procesos es similar en muchos aspectos, los resultados son totalmente diferentes.
La mitosis conduce a la producción de dos células, cada una con un número de cromosomas idéntico al de la célula paterna.
Por el contrario, la meiosis reduce la cantidad de material genético y el número de cromosomas exactamente a la mitad.
Esta reducción es esencial a fin de que se dé la reproducción sexual sin doblar la cantidad de material genético en cada generación. 
Concretando, la mitosis es aquel periodo del ciclo celular durante el cual los c hereditarios se reparten de manera precisa e igual en las células hijas.
La meiosis es parte de un tipo especial de división celular que da lugar a la producción de células sexuales: los gametos y las esporas.
Este proceso es un paso esencial en la transmisión de la información genética de un organismo a sus descendientes.

En la mayoría de los casos, los cromosomas son visibles sólo cuando las células están realmente dividiéndose, es decir, en la mitosis o en la meiosis. 
Cuando las células no están en división, el material genético que constituye los cromosomas se despliega y desespiraliza, dando lugar en el interior del núcleo a una red difusa, que en conjunto se denomina cromatina .
En este capítulo examinaremos esta transición y consideraremos también dos casos especiales en los que los cromosomas son extraordinariamente grandes y adecuados para la investigación.
El estudio de estos ejemplos, los cromosomas politénicos y los cromosomas plumulados (en escobilla) , ha ampliado enormemente nuestro conocimiento sobre la organización genética y sus relaciones con la función génica.

Estructura de la célula

Antes de describir la mitosis y la meiosis, revisaremos brevemente la estructura de las células.
Veremos que muchos c celulares, como los nucleolos, ribosomas y centriolos, tienen que ver, directa o indirectamente, con procesos genéticos.
Otros c, como las mitocondrias o los cloroplastos, tienen sus propias unidades genéticas de información.
También es útil comparar las diferencias estructurales entre las células bacterianas procarióticas y las células eucarióticas.
La variación en la estructura y en la función de las células depende de la expresión génica específica para cada tipo celular.

El conocimiento de la estructura celular se basaba, antes de 1940, en la información obtenida con el microscopio óptico.
Hacia los años 40, el microscopio electrónico se encontraba en sus primeras fases de desarrollo y ya en los años 50 se pudieron desvelar muchos detalles ultraestructurales de la célula.
Con el microscopio electrónico se vio que las células eran estructuras particulares y altamente organizadas.
Apareció un nuevo mundo de membranas retorcidas, orgánulos diminutos, microtúbulos, gránulos y filamentos.
Estos revolucionarios descubrimientos invadieron todo el campo de la biología.
Nos ocuparemos de aquellos aspectos de la estructura celular relacionados con el estudio genético. 
Muchas de las partes de las células se describen en la Figura 2.1, en donde se representa a una célula animal típica.

Límites celulares

La célula está rodeada por una membrana plasmática , que es una cubierta externa que define los límites celulares y delimita a la célula de su ambiente externo inmediato.
No es una membrana pasiva, sino que regula el paso de materiales, como gases, nutrientes y productos de deshecho, desde el exterior y hacia el exterior de la célula.

Además de esta membrana, las células vegetales tienen una cubierta externa llamada pared celular .
Uno de los c principales de esta estructura rígida es un polisacárido denominado celulosa .
Las células bacterianas tienen también una pared celular, pero su composición química es completamente diferente de la composición de la pared celular vegetal.
En las bacterias el componente principal es una macromolécula compleja denominada peptidoglicano .
Como sugiere su nombre, la molécula consta de péptidos y azúcares.
Largas cadenas de polisacáridos permanecen unidas transversalmente por péptidos cortos, lo que confiere gran robustez y rigidez a la célula bacteriana.
Algunas bacterias todavía tienen otra cubierta, la cápsula .
Este material mucoso protege a estas bacterias, en su invasión patógena a organismos eucariotas, de la actividad fagocitaria del huésped.
Como veremos en el Capítulo 10, la presencia de la cápsula está regulada genéticamente. 
Su pérdida, debida a una mutación en la bacteria Diplococcus pneumoniae , responsable de la neumonía, sentó las bases para un importante experimento que confirmó que el DNA es el material genético.

La actividad en los límites celulares son procesos fisiológicos dinámicos.
Tanto el transporte hacia el exterior de las células como hacia el interior, o la comunicación entre éstas, son decisivos para su funcionamiento normal.
Debido a que los procesos fisiológicos son de naturaleza bioquímica, esperaríamos que muchos genes y sus productos fueran esenciales para estas actividades. 
Éste es realmente el caso, y por ello las mutaciones de estos genes pueden alterar, o interrumpir, las funciones fisiológicas normales, a menudo con consecuencias graves.
Por ejemplo, la enfermedad hereditaria distrofia muscular de Duchenne es el resultado de la pérdida funcional completa de la distrofina , que se cree actúa en las membranas de las células musculares.
Utilizando la técnica de localización por inmunofluorescencia, no se detectaría distrofina en el músculo estriado de un individuo afectado, comparado con el músculo de un sujeto control.

Muchas de las células animales, si no la gran mayoría, tienen una cubierta por encima de la membrana plasmática, denominada cubierta celular.
La composición química de la cubierta celular , formada por glicoproteínas (y por ello denominada a veces el glicocáliz ) y por polisacáridos, difiere de estructuras comparables de vegetales y de bacterias.

Una de las funciones de la cubierta celular es dar identidad bioquímica a la superficie celular.
Además de otras formas de reconocimiento molecular, en la cubierta celular se encuentran varios antígenos.
Todos los tipos de identidad bioquímica de la superficie celular se encuentran bajo control genético y muchos de ellos se han investigado a fondo.
Por ejemplo, los antígenos AB y MN , que pueden inducir una respuesta inmune en las transfusiones sanguíneas, [1] se encuentran en la superficie de los glóbulos rojos.
En otras células, los antígenos de la histoincompatibilidad, que pueden inducir una respuesta inmune en los trasplantes de tejidos y órganos, se encuentran en la cubierta celular. 
Además, cierto número de moléculas receptoraS altamente específicas, es integrante de la superficie celular.
Estos son lugares de reconocimiento que reciben señales químicas que a menudo se transfieren a la célula.
Tales señales pueden iniciar una serie de respuestas químicas y finalmente inducir la actividad de genes concretos.
Podemos concluir que la superficie celular conecta a la célula con el mundo exterior y está ligada de varias formas a procesos genéticos. 

El núcleo

La presencia del núcleo y de otros orgánulos membranosos es característica de las células eucarióticas.
El núcleo alberga al material genético, el DNA, asociado a un gran número de proteínas ácidas y básicas.
En el periodo del ciclo celular en el que no hay división, este complejo de DNA/proteínas se encuentra disperso y desespiralizado y se denomina cromatina . 
Como discutiremos más adelante, este material se espiraliza y se condensa en la mitosis y la meiosis, dando lugar a unas estructuras denominadas cromosomas .
En el núcleo también se encuentra el nucleolo , un componente amorfo, en donde se sintetiza el RNA ribosómico ( rRNA ) y en donde se producen las primeras fases del ensamblaje de los ribosomas.
Las regiones del DNA que codifican al rRNA constituyen en conjunto la región organizadora nucleolar o NOR .

La ausencia de cubierta nuclear y de orgánulos membranosos es característica de los procariotas.
En la bacteria E. coli el material genético es una larga molécula circular de DNA, compactada en un área que se denomina región nucleoide .
Parte del DNA puede estar unido a la membrana celular, pero en general el nucleoide abarca una extensa área de la célula. 
Aunque el DNA está compactado, no sufre la espiralización característica de los estadios mitóticos de los eucariotas, que es cuando los cromosomas se hacen visibles.

Tampoco el DNA de los procariotas está asociado tan ampliamente a proteínas como el DNA de los eucariotas. 
Las células procarióticas no tienen nucleolo diferenciado, aunque tienen genes que codifican moléculas de rRNA .

El citoplasma y sus orgánulos 

Excluyendo al núcleo, el resto de la célula eucariótica que queda rodeada por la membrana plasmática está compuesto por el citoplasma y por todos los orgánulos celulares asociados. 
El citoplasma es una sustancia coloidal, sin partículas, denominada citosol , que rodea y engloba a numerosos tipos de orgánulos celulares.
Además de estos c, hay un amplio sistema de túbulos y filamentos que forman el citoesqueleto , que proporciona un entramado de sostén para las estructuras citoplasmáticas.
Consta principalmente de microtúbulos, derivados de la tubulina, y de microfilamentos, derivados de la actina.
Este armazón estructural mantiene la forma de la célula, facilita la movilidad celular y sujeta a los distintos orgánulos.
La tubulina y la actina son proteínas muy abundantes en las células eucariotas. 

Uno de los orgánulos, el retículo endoplasmático membranoso ( ER ), compartimentaliza al citoplasma, incrementando enormemente la superficie disponible para las reacciones bioquímicas. 
El ER puede ser liso, en cuyo caso sirve como lugar para la síntesis de ácidos grasos y fosfolípidos, o puede ser rugoso, así llamado por encontrarse en él los ribosomas.
Los ribosomas, de los que hablaremos más adelante en detalle (véase el Capítulo 12), es el lugar para la traducción de la información genética, que se encuentra en el RNA mensajero ( mRNA ), en proteínas. 

Para la actividad de las células eucariotas hay otras tres estructuras citoplásmicas muy importantes: las mitocondrias , los cloroplastos y los centriolos .
Las mitocondrias se encuentran tanto en células animales como en vegetales y son los lugares para la fase oxidativa de la respiración celular .

Estas reacciones químicas generan grandes cantidades de adenosina trifosfato (ATP) , una molécula rica en energía.
El cloroplasto es un tipo de plastidio que se encuentra en vegetales superiores, algas y algunos protozoos. 
Este orgánulo está asociado con la fotosíntesis , el principal proceso de captación de energía de nuestra planeta.
Tanto las mitocondrias como los cloroplastos tienen un tipo de DNA distinto del que se encuentra en el núcleo.
Además, estos orgánulos pueden autoduplicarse, transcribir y traducir su información genética.
Es interesante advertir que la maquinaria genética de las mitocondrias y de los cloroplastos se parece mucho a la de las células procariotas.
Ésta y otras observaciones han llevado a proponer que estos orgánulos fueron tiempo atrás organismos primitivos de vida libre que establecieron relaciones simbióticas con una célula eucariótica primitiva.
Esta propuesta, que se refiere al origen evolutivo de estos orgánulos, se denomina la hipótesis endosimbiótica .

Las células animales y algunas vegetales tienen también un par de estructuras complejas llamadas centriolos .
Estos cuerpos citoplásmicos, que se encuentran en una región especializada denominada centrosoma , están asociados con la organización de las fibras del huso, que actúan en la mitosis y en la meiosis.
En algunos organismos el centriolo deriva de otra estructura, el cuerpo basal , que está asociado con la formación de cilios y flagelos.
Durante muchos años, se ha sugerido en muchas publicaciones que los centriolos y los cuerpos basales tienen DNA, que estaría implicado en la duplicación de estas estructuras, pero en la actualidad se está de acuerdo en que no es así.

La organización de las fibras del huso por los centriolos se lleva a cabo en las primeras fases de la mitosis y de la meiosis.
Estas fibras, formadas por un racimo de microtúbulos, juegan un papel importante en el movimiento de los cromosomas, cuando se separan en la división celular.
Los microtúbulos constan de polímeros de las subunidades alfa y beta de la proteína tubulina. 
La interacción de los cromosomas y de las fibras del huso se considerará más adelante en este capítulo. 

Cromosomas homólogos, haploidía y diploidía

Para discutir los procesos de la mitosis y de la meiosis es importante entender claramente el concepto de cromosomas homólogos .
Su comprensión será también muy importante cuando hablemos de la genética mendeliana.
Por ello, antes de seguir adelante, trabajaremos este concepto e introduciremos otros términos importantes.

Los cromosomas se visualizan mas fácilmente en la mitosis.
Cuando se examinan con cuidado, se observa que tienen un tamaño y una forma característicos.
Cada cromosoma tiene una región condensada, o constreñida, llamada centrómero, que confiere la apariencia general de cada cromosoma.
La Figura 2.2 ilustra cromosomas con centrómeros situados en posiciones diferentes a lo largo de los mismos.
A ambos lados del centrómero se sitúan los brazos cromosómicos.

Dependiendo de la posición del centrómero, los brazos tiene longitudes relativas distintas.
Como se indica en la Figura 2.2, los cromosomas se clasifican en metacéntricos, submetacéntricos y acrocéntricos o telocéntricos de acuerdo con la localización del centrómero [2] .
Por convención, el brazo más corto es el que se encuentra por encima del centrómero y se denomina brazo p (p de « petit »).
El brazo más largo se encuentra debajo y se denomina el brazo q (por ser q la letra siguiente en el alfabeto).

Estudiando la mitosis podemos hacer otras observaciones importantes.
En primer lugar, las células somáticas de los individuos de una misma especie tienen el mismo número de cromosomas.
Es el llamado número diploide (2n) .
Cuando se examina la longitud de tales cromosomas y la situación del centrómero, surge el segundo rasgo general.
En relación con estos dos criterios, casi todos los cromosomas se encuentran formando parejas.
Los miembros de cada par se denominan cromosomas homólogos .

Para cada cromosoma con una longitud y una situación del centrómero específicas, existe otro cromosoma con rasgos idénticos.
Desde luego, hay excepciones a esta regla, que se encuentran en organismos como las levaduras y los mohos, los cuales, durante la mayor parte de su ciclo biológico, se encuentran en estado haploide.

En la Figura 2.3 se presenta la apariencia física, casi idéntica, de los miembros de parejas de cromosomas homólogos.
En dicha figura se presenta una fotografía de cromosomas mitóticos humanos (en la parte superior de la figura), que se han recortado de la foto y se han emparejado, formando un cariotipo [3] (en la parte inferior de la figura).
Como se puede ver, la especie humana tiene un número 2n de 46, con gran diversidad de tamaños y posición del centrómero.
Hay que advertir también que cada uno de los 46 cromosomas tiene claramente una estructura doble, con dos cromátidas hermanas en paralelo unidas por un único centrómero.
Si se permitiera que estos cromosomas continuaran dividiéndose, cada pareja de cromátidas hermanas, que son idénticas entre sí, se separarían hacia dos nuevas células.

El número haploide (n) de cromosomas es igual a la mitad del número diploide.
En la Tabla 2.1 se demuestra el amplio rango de valores de n que se encuentra en 32 especies distintas de animales y vegetales.
La totalidad de los genes que se encuentran en el conjunto formado por uno de los dos cromosomas homólogos de cada una de las parejas constituye el genoma haploide de la especie.

Las parejas de cromosomas homólogos tienen una semejanza genética importante.

Tienen genes idénticos, situados en los mismos lugares a lo largo del cromosoma, que se denominan locus (en plural, loci).
Por ello, tienen idéntico potencial genético. 
En organismos con reproducción sexual, uno de los miembros de cada pareja proviene de la madre (a través del óvulo) y el otro del padre (a través del esperma). 
Por ello, y como consecuencia de la herencia biparental, cada organismo diploide tiene dos copias de cada uno de los genes. 
Como veremos en los siguientes capítulos sobre genética de la transmisión, los miembros de cada par de genes, aunque influyen en el mismo carácter, no son necesariamente idénticos.
Las formas alternativas del mismo gen se denominan alelos.
En una población de individuos de la misma especie pueden existir muchos alelos diferentes del mismo gen.

Tabla 2.1

Número haploide de cromosomas de diversos organismos

Nombre vulgar.
Nombre científico .
No haploide.


Alga verde.
Chlamydonas reinhardi .
18.


Algodón.
Gossypium hirsutum .
26 .


Ave de corral.
Gallus domesticus .
39.


Caballo.
Equus caballus .
32.


Cabeza de dragón .
Antirrhinum majus .
8.


Cebolla de jardín.
Allium cepa .
8.


Chimpancé.
Pan trogldytes .
24.


Estramonio.
Datura stramonium .
12.


Ganado vacuno.
Bos taurus .
30.


Gato.
Felis domesticus .
19.


Guisante de jardín.
Pisum sativum .
7.


Gusano de seda.
Bombyx mori .
28.


Haba.
Vicia faba .
6.


Moho rosa del pan.
Neurospora crassa .
7.


Humano .
Homo sapiens .
23.


Levadura.
Saccharomyces cerevisiae .
17 .


Maíz.
Zea mays .
10.


Nombre vulgar.
Nombre científico .
No haploide.


Moho del cieno .
Dictyostelium discoidium .
7.


Moho negro del pan.
Aspergillus nidulans .
8.


Mono rhesus .
Macaca mulatta .
21.


Mosca de vinagre.
Drosophila melanogaster .
4.


Mosca doméstica.
Musca domestica .
6.


Mosquito.
Culex pipiens .
3.


Nemátodo.
Caernorhabditis elegans .
6.


Nenúfar blanco .
Nymphaea alba .
80.


Patata.
Solanum tuberosum .
24.


Perro.
Canis familiaris .
39.


Primavera de noche.
Oenothera biennis .
7.


Rana.
Rana pipiens .
13.


Ratón.
Mus musculus .
20.


Saltamontes.
Melanoplus differentialis .
12.


Tabaco.
Nicotiana tabacum .
24.


Tomate.
Lycopersicon aculentum .
12.


Trigo.
Triticum aestrivum .
21.


Los conceptos de número haploide, número diploide y cromosomas homólogos pueden relacionarse con la meiosis. 
Durante la formación de los gametos, o de las esporas, la meiosis reduce el número diploide de cromosomas al número haploide.
Por ello, los gametos, o esporas, haploides tienen exactamente un miembro de cada una de las parejas de cromosomas, es decir, una dotación haploide completa. 
Después de la fusión de los dos gametos en la fecundación, se restablece el número diploide, es decir, el zigoto tiene dos dotaciones haploides completas de cromosomas.
De esa manera se mantiene la constancia del material genético de generación en generación. 

Hay una excepción importante en el concepto de parejas de cromosomas homólogos.
En muchas especies, los miembros de una pareja, los cromosomas que determinan el sexo , no tienen, normalmente, igual tamaño, igual situación del centrómero, la misma proporción entre los brazos, o el mismo potencial genético.
Por ejemplo, en la especie humana, los varones tienen un cromosoma Y, además de un cromosoma X (Figura 2.3), mientras que las mujeres tienen dos cromosomas X homólogos.
Los cromosomas X e Y no son estrictamente homólogos. 
El Y es considerablemente de menor tamaño y carece de la mayoría de los loci que se encuentran en el cromosoma X.
No obstante, en la meiosis se comportan como homólogos [4] , por lo que los gametos producidos por los varones recibe un cromosoma X o un Y.

Mitosis y división celular 

El proceso de la mitosis es básico para todos los organismos eucariotas.
En muchos organismos unicelulares, como los protozoos, y algunos hongos y algas, la mitosis, como parte de la división celular, proporciona el mecanismo básico para su reproducción asexual.
Los organismos pluricelulares diploides comienzan su ciclo biológico como óvulos fecundados unicelulares o zigotos .
La actividad mitótica del zigoto y de las células hijas posteriores es la base para el crecimiento y desarrollo del organismo.
En organismos adultos, la actividad mitótica asociada con la división celular es esencial en la cicatrización de las heridas y en otros tipos de sustitución de células en ciertos tejidos. 
Por ejemplo, las células epidérmicas en la especie humana se están desprendiendo y remplazando continuamente.
Se estima que cada individuo desprende diariamente
¡unos 100 mil millones de células! 
En los vertebrados, la división celular da lugar también a una producción continua de reticulocitos, que eliminarán finalmente sus núcleos y repondrán el número de glóbulos rojos.
En situaciones anormales, las células somáticas pueden presentar procesos de división celular incontrolada, originando un cáncer.

Figura 2.1

Esquema de una célula animal típica

Se destacan los celulares discutidos en el texto.

Figura 2.2

Tipos de cromosomas basados en la localización del centrómero

Adviértase que la forma del cromosoma en la anafase viene determinada por la posición del centrómero. 

Figura 2.3

Preparación metafásica de cromosomas humanos femeninos y el cariotipo que se deriva de ellos

Todos los cromosomas se encuentran formando pareja homólogas.
En realidad cada cromosoma es una estructura doble, formada por un par de cromátidas hermanas unidas por un centrómero común.

[1] No obstante, hay una diferencia básica entre los grupos sanguíneos A-BO y MN y sus correspondientes antígenos, y es que este último no tiene implicaciones clínicas debido a que no hay una reacción fuerte frente a los antígenos de este grupo, por lo que no se tiene en cuenta en transfusiones sanguíneas. 

[2] Existe un cuarto tipo los llamados cromosomas puntuales, de muy pequeño tamaño, en los que no se aprecian brazos.

[3] Propiamente, el cariotipo es el conjunto de cromosomas característico de una especie, mientras que su disposición ordenada, como en la parte inferior de la Figura 2.3, se denomina idiograma. 

[4] La homología entre el cromosoma X y el Y suele reducirse a un pequeño segmento, lo que permite a estos cromosomas emparejarse en la meiosis y segregarse correctamente.

Genética mendeliana

CONCEPTOS DEL CAPÍTULO

Las características se heredan bajo el control de factores discretos llamados genes, que se transmiten de generación en generación a través de los cromosomas de acuerdo con las reglas descritos por primera vez por Gregor Mendel.
Las proporciones genéticas, expresados como probabilidades, están sujetos a desviaciones al azar y pueden evaluarse utilizando el análisis estadístico. 

Aunque la herencia de los caracteres biológicos se ha reconocido hace miles de años, la primera idea importante sobre el mecanismo implicado se dio hace menos de siglo y medio. 
En 1866, Gregor Mendel, publicó los resultados de una serie de experimentos que sentaron las bases de la genética como disciplina formal.
En los años siguientes se estableció el concepto de gen como unidad hereditaria discreta y se clarificó el modo en el que los genes se transmiten a los descendientes y controlan los caracteres.
La investigación en este área se aceleró en la primera mitad del siglo XX.
Los hallazgos obtenidos sirvieron de base para el tremendo interés y esfuerzo investigador en genética desde los años 40.
Se puede afirmar sin ninguna duda que los estudios en genética, y más recientemente aquellos que se refieren al ámbito molecular, han permanecido continuamente en la frontera de la investigación biológica desde principios del siglo XX.

En este capítulo trataremos del desarrollo de los principios establecidos por Mendel, lo que denominamos genética de la transmisión o genética mendeliana .
Estos principios describen cómo los genes se transmiten de padres a hijos y derivan directamente de los experimentos de Mendel.

Cuando Mendel comenzó sus estudios de la herencia utilizando Pisum sativum , el guisante de jardín, no se sabía de la existencia de los cromosomas ni del papel ni mecanismo de la meiosis.
No obstante, Mendel pudo determinar la existencia de unidades de herencia discretas y predecir su comportamiento en la formación de los gametos.
Investigadores posteriores, con acceso a datos citológicos, pudieron relacionar sus observaciones sobre el comportamiento de los cromosomas en la meiosis con los principios de la herencia mendeliana.
Una vez establecida esta correlación, los postulados de Mendel se aceptaron como base para el estudio de la genética de la transmisión.
Incluso hoy constituyen la piedra angular de los estudios sobre la herencia.

Gregor Mendel

Johan Mendel Mendel nació en 1822 en una familia de campesinos en el pueblo de Heinzendorf, que ahora forma parte de la República Checa.
Excelente estudiante en la escuela superior, Mendel estudió filosofía durante varios años y fue admitido en el Monasterio Agustino de Santo Tomas de Brno, en 1843.
Allí tomó el nombre de Gregor y recibió apoyo en sus estudios e investigaciones durante el resto de su vida.
En 1849 fue relevado de sus obligaciones pastorales y fue nombrado para un puesto de enseñanza que ocupó durante varios años.
De 1851 a 1853 asistió a la Universidad de Viena, en donde estudio física y botánica.
En 1854 volvió a Brno en donde, durante los 16 años siguientes, enseñó física y ciencias naturales.

En 1856 Mendel realizó su primera serie de experimentos de hibridación con el guisante de jardín.
La fase investigadora de su carrera duró hasta 1868, año en el que fue elegido abad del monasterio.
Aunque su interés por la genética se mantuvo, sus nuevas responsabilidades ocuparon todo su tiempo.
En 1884 Mendel murió de una enfermedad renal.
El periódico local le rindió el siguiente tributo: 
«Su muerte ha privado a los pobres de un benefactor, y a la humanidad de un gran hombre de noble carácter, que fue cálido amigo, promotor de las ciencias naturales y sacerdote ejemplar».

Planteamiento experimental de Mendel

En 1865 Mendel publicó los primeros resultados de algunos cruces genéticos sencillos realizados entre ciertas variedades de guisante.
Aunque, como vimos en el Capítulo 1, no fue el primero que intentó obtener pruebas experimentales de la herencia, el trabajo de Mendel es un elegante modelo de diseño experimental y de análisis.

Mendel demostró una B perspicacia en elegir la metodología necesaria para una buena biología experimental.
Escogió un organismo fácil de cultivar y que podía hibridarse artificialmente.
En la naturaleza el guisante se autofecunda, pero experimentalmente es fácil realizar fecundaciones cruzadas.
Se reproduce bien y se hace adulto en una sola estación.
Mendel trabajó con siete caracteres, o rasgos visibles, representados por dos formas o caracteres alternativos.
Para el carácter «altura del tallo», por ejemplo, experimentó con plantas altas y enanas .
Seleccionó otros seis pares de caracteres alternativos que afectaban a la forma y el color de la semilla, a la forma, el color y la situación de las vainas, y al color de las flores.
Dispuso de variedades puras de los comerciantes de semillas locales.
Al autofecundar las plantas, cada carácter permaneció invariable generación tras generación; es decir, demostraron ser «variedades puras».

El éxito de Mendel en un campo en donde otros habían fracasado, puede atribuirse a varios factores, además de la elección del organismo adecuado.
En cada experiencia limitó su análisis a uno o muy pocos pares de caracteres alternativos.
Tuvo mucho cuidado en registrar los datos cuantitativamente, algo necesario en experimentos genéticos.
Del análisis de sus datos, Mendel dedujo ciertos postulados que se han convertido en los principios de la genética de la transmisión.

Los resultados de los experimentos de Mendel no fueron apreciados hasta comienzos del siglo XX, bastante después de su muerte.
Una vez redescubiertas las publicaciones de Mendel por genéticos que investigaban la función y el comportamiento de los cromosomas, las implicaciones de sus postulados fueron inmediatamente claras.
¡Había descubierto las bases de la transmisión de los caracteres hereditarios!

El cruce monohíbrido

El cruce más sencillo realizado por Mendel implicaba sólo a un par de caracteres alternativos.
Cada uno de tales experimentos de cruce implica un cruce monohíbrido .
Un cruce monohíbrido se realiza cruzando individuos de dos variedades paternas, cada una de las cuales presenta una de las dos formas alternativas del carácter en estudio.
Inicialmente examinaremos la primera generación de descendientes de tal cruce y luego consideraremos los descendientes de individuos autofecundados de esta primera generación.
A los padres se les llama P1 o generación paterna , sus descendientes son la F1 o primera generación filial y los individuos resultantes de la autofecundación de F1 constituyen la F2 o segunda generación filial.
Desde luego, podemos continuar con las generaciones siguientes, si fuera el caso.

El cruce entre guisantes de variedad pura con tallos altos y tallos enanos es representativo de los cruces monohíbridos de Mendel.
Alto y enano son formas o caracteres alternativos del carácter «altura del tallo».
A menos que plantas altas o enanas se crucen entre sí o con otra variedad, mantendrán su pureza por autofecundación, dando lugar a su respectiva característica generación tras generación. 
Sin embargo, cuando Mendel cruzó plantas altas con plantas enanas, el resultado de F1 fue sólo de plantas altas.
Cuando permitió que los miembros de F1 se autofecundaran, Mendel observó que 787 de las 1.064 plantas de F2 eran altas y que 277 eran enanas.
Adviértase que en este cruce (Figura 3.1) el carácter enano desaparece en F1, para después reaparecer en la generación F2.

Los datos genéticos normalmente se expresan y se analizan como proporciones.
En este ejemplo concreto se realizaron muchos cruces idénticos P1 y se obtuvieron muchas plantas F1, todas altas.
De los 1.064 descendientes de F2, 787 eran altos y 277 enanos, una proporción aproximada de 2,8:1,0, cercana a 3:1.

Mendel hizo cruces similares entre plantas de guisante que presentaban cada uno de los otros pares de caracteres alternativos. 
En la Figura 3.1 se presentan los resultados de estos cruces.
En cada caso el resultado fue similar al cruce alto/enano que acabamos de describir.
Todos los descendientes de F1 fueron idénticos a uno de los padres.
En F2 se obtenía una proporción aproximada de 3:1. 
Las tres cuartas partes eran como la F1 mientras que la cuarta parte presentaba el carácter alternativo que había desaparecido en F1.

Es conveniente señalar otro aspecto de los cruces monohíbridos.
En cada uno de ellos los patrones de herencia en F1 y en F2 fueron similares, independientemente de qué planta P1 hubiera sido el origen del polen, o del esperma, y de cual hubiera sido el origen del óvulo.
Los cruces pudieron realizarse en cualquier sentido, es decir, polen de la planta alta polinizando a plantas enanas, o viceversa. 
Estos se denominan cruces recíprocos .
Por ello, los resultados de los cruces monohíbridos de Mendel no dependerían del sexo.

Para explicar estos resultados, Mendel propuso la existencia de factores discretos para cada carácter. 
Sugirió que estos factores eran las unidades básicas de la herencia, que pasaban sin cambio de generación en generación, determinando los distintos caracteres que expresaba cada planta.

Utilizando estas ideas básicas, Mendel emitió hipótesis precisas de cómo tales factores podían explicar los resultados de los cruces monohíbridos.

Los tres primeros principios de Mendel

Teniendo en cuenta los patrones consistentes de los resultados de los cruces monohíbridos, Mendel dedujo los siguientes tres postulados o principios de la herencia.

FACTORES EN PAREJAS

Los caracteres genéticos están controlados por factores que se encuentran a pares en cada organismo. 

En el cruce monohíbrido entre plantas altas y enanas, cada carácter tiene un factor específico.
Debido a que los factores están a pares, son posibles tres combinaciones: dos factores para altura normal, dos factores para enanismo, o un factor de cada tipo.
Cada individuo posee una de estas tres combinaciones, lo que determina la altura del tallo.

DOMINANCIA/RECESIVIDAD

Cuando dos factores distintos, responsables de un carácter dado, se encuentran en un individuo, uno de los factores domina sobre el otro, que se denomina recesivo. 

En cada cruce monohíbrido, el carácter que se expresa en la generación F~ es consecuencia de la presencia del factor dominante.
El carácter que no se expresa en F~, pero que reaparece en F', se encuentra bajo la influencia genética del factor recesivo.
Adviértase que esta relación de dominancia/recesividad sólo se manifiesta cuando se encuentran juntos en el mismo individuo factores diferentes.
Los términos dominante y recesivo también se utilizan para designar a los caracteres.
En el caso anterior, el carácter tallo alto es dominante y el carácter tallo enano es recesivo.

SEGREGACIÓN

En la formación de los gametos, los factores emparejados se separan o segregan al azar, de tal manera que cada gameto recibe uno u otro con igual probabilidad.

Si un individuo tiene un par de factores iguales (por ejemplo, ambos especificando tallo alto), entonces todos los gametos reciben un factor para tallo alto.
Si un individuo tiene factores distintos (por ejemplo uno para tallo alto y otro para tallo enano), entonces cada gameto tiene un 50 por ciento de probabilidad de recibir un factor para alto 0 un factor para enano.

Estos principios proporcionan una explicación adecuada de los resultados de los cruces monohíbridos.
El cruce alto/enano se utilizará para ilustrar esta explicación. 
Mendel razonó que las plantas altas P1 tenían un par de factores idénticos, como también lo tenían las plantas enanas P1.
Todos los gametos de las plantas altas recibían un factor alto como consecuencia de la segregación.

De la misma manera, todos los gametos de las plantas enanas recibían un factor enano.
Una vez fecundadas, todas las plantas F1 recibían un factor de cada padre, un factor alto de uno y un factor enano del otro, restableciéndose la pareja.
Debido a que alto es dominante sobre enano, todas las plantas F1 eran altas.

Cuando las plantas F1 forman gametos, el principio de la segregación exige que cada gameto reciba, al azar, bien el factor alto, bien el enano.
Una vez se ha producido la fecundación al azar en la autofecundación de la F1, se formarán cuatro combinaciones en F2 con igual frecuencia:

(1) alto/alto
(2) alto/enano
(3) enano/alto
(4) enano/enano

Las combinaciones (1) y (4) darán lugar claramente a plantas altas y enanas, respectivamente.
De acuerdo con el principio de la dominancia/recesividad, las combinaciones (2) y (3) producirán plantas altas.
Por consiguiente, se predice que la F2 conste de tres cuartos altas y un cuarto enanas, una proporción de 3:1.
Esto es aproximadamente lo que Mendel observó en los cruces entre plantas altas y enanas.
Un patrón similar se observó en cada uno de los otros cruces monohíbridos.

Terminología genética actual

Para ilustrar el cruce monohíbrido y los tres primeros principios de Mendel, tenemos que introducir varios términos nuevos, así como una serie de símbolos para los factores.
Caracteres tales como alto o enano son expresiones visibles de la información que tienen los factores.
La apariencia física de un carácter se denomina el fenotipo del individuo.

Todos los factores son unidades de herencia denominados genes por los genéticos actuales.
Para cualquier carácter dado, como la estatura de la planta, el fenotipo viene determinado por la presencia de combinaciones diferentes de formas alternativas de un solo gen llamadas alelos .
Por ejemplo, los factores que condicionan alto y enano son alelos que determinan la estatura de la planta de guisante.

Dependiendo del organismo que se estudie, se utilizan muchas convenciones para asignar símbolos a los genes.
En el capítulo 4 revisaremos algunos de éstos, pero por ahora adoptaremos uno que podamos utilizar adecuadamente en los capítulos próximos.
Por convención se puede tomar la primera letra del carácter recesivo para simbolizar el carácter en cuestión.
La letra minúscula designa al alelo recesivo del carácter, y la letra mayúscula designa al alelo dominante del carácter. 
Por consiguiente, tomamos la e para designar al alelo enano, y la E representará al alelo alto.
Cuando se escriben los alelos en pareja para representar a los dos factores presentes en cualquier individuo (EE, Ee o ee ) estos símbolos se refieren al genotipo .
Este término refleja la constitución genética de un individuo, ya sea haploide o diploide.
Leyendo el genotipo, es posible saber el fenotipo de un individuo:
EE y Ee son altos, y ee es enano.
Cuando hay dos alelos idénticos (EE o ee ) se dice que el individuo es homozigoto u homozigótico ; cuando los alelos son diferentes (Ee), utilizamos el término heterozigoto o heterozigótico .
Estos símbolos y términos se utilizan en la Figura 3.2 para ilustrar un cruce monohíbrido completo. 

Enfoque analítico de Mendel

¿Qué le indujo a Mendel a deducir la existencia de factores en parejas?
Ya que había dos rasgos alternativos para cada carácter, parecía lógico que debieran existir dos factores distintos.
Sin embargo 
¿por qué uno de los rasgos o fenotipos desaparece en F1?
La observación de F2 ayuda a responder esta pregunta.
El carácter recesivo y su factor no desaparecen realmente en F1; simplemente están escondidos o enmascarados, para después reaparecer en la cuarta parte de los descendientes F2.
Por consiguiente Mendel concluyó que un factor para alto y otro para enano se transmitían a cada uno de los individuos F1, pero debido a que el factor, o alelo, alto, es dominante sobre el factor, o alelo, enano, todas las plantas F1 son altas.
Finalmente,
¿cómo se explica la proporción 3:1 en F2?
Como se muestra en la Figura 3.2, si los alelos alto y enano de los heterozigotos F1 segregan al azar en los gametos, y si la fecundación es también al azar, esta proporción es el resultado normal del cruce.

Debido a que actuaba sin los conocimientos previos que disfrutan los genéticos actuales, el razonamiento analítico de Mendel debe considerarse un logro científico realmente destacado.
Con la base de experimentos de cruce bastante simples, pero realizados de manera precisa, propuso la existencia de unidades discretas de la herencia y explicó de qué manera se transmiten de una generación a la siguiente.

Tablero de Punnett

Los genotipos N' fenotipos que resultan de la combinación de los gametos en la fecundación pueden visualizarse fácilmente construyendo un tablero de Punnett , así llamado por la persona que lo ideó, Reginald C. Punnett. 
En la Figura 3.3 se presenta este método de análisis para el cruce monohíbrido F1 xxx F1.
Cada uno de los posibles gametos se sitúa en una columna o en una fila, representando las columnas a los de la madre y las filas a los del padre.
Después de situar los gametos en filas y columnas, se predice la nueva generación combinando la información gamética masculina y femenina para cada combinación y situando los genotipos resultantes en los cuadrados.
Este proceso presenta todos los posibles sucesos de fecundación al azar.
Los genotipos y fenotipos de todos los descendientes potenciales se determinan leyendo las anotaciones de los cuadrados.

El método del tablero de Punnett es particularmente útil cuando se comienza a aprender genética y para resolver problemas.
Advierta lo fácil que es deducir la proporción fenotípica 3:1 y genotípica 1:2:1 de la generación F2 en la Figura 3.3.

El cruzamiento prueba: un carácter 

Se predice que las plantas altas que se producen en F2 presentan los genotipos EE o Ee.
Podríamos preguntarnos si hay algún modo de distinguir a los genotipos.
Mendel diseñó un método bastante simple que todavía se utiliza en experiencias de cría de animales y vegetales: el cruzamiento prueba .
El organismo de fenotipo dominante, pero de genotipo desconocido, se cruza con un individuo homozigoto recesivo .
Por ejemplo, como se muestra en la Figura 3.4(a), si una planta alta de genotipo EE se cruza con una planta enana, que tiene que tener el genotipo ee , todos los descendientes serán fenotípicamente altos y genotípicamente Ee.
Sin embargo, como se muestra en la Figura 3.4(b), si una planta alta es Ee y se cruza con una planta enana ( ee ), entonces la mitad de los descendientes serán altos (Ee) y la otra mitad enanos ( ee ).
Por consiguiente, una proporción 1:1 alto/enano demuestra la naturaleza heterozigótica de las plantas altas de genotipo desconocido.
El resultado del cruzamiento prueba refuerza la conclusión de Mendel de que factores discretos controlan los caracteres alto y enano. 

El cruce dihíbrido

Una ampliación natural de la realización de cruces monohíbridos fue para Mendel diseñar experimentos en donde se examinaban simultáneamente dos caracteres.
Tal cruce, que implica dos pares de caracteres alternativos, se denomina cruce dihíbrido .
También se denomina cruce de dos factores .
Por ejemplo, si plantas de guisante con semillas amarillas, que también son redondas, se cruzan con plantas que tienen semillas verdes, que también son rugosas, aparecerán los resultados presentados en la Figura 3.5.
Los descendientes F1 tendrán todos semillas amarillas y redondas.
Por consiguiente, es evidente que amarillo es dominante sobre verde y que redondo es dominante sobre rugoso.
En este cruce dihíbrido, si se permite que los individuos F1 se autofecunden, aproximadamente 9/16 de las plantas F2 expresarán amarillo y redondo, 3/16 amarillo y rugoso, 3/16 verde y redondo y 1/16 verde y rugoso.

Una variante de este cruce se presenta también en la Figura 3.5.
En lugar de cruzar un padre P1 con los caracteres dominantes (amarillo, redondo) con otro con los caracteres recesivos (verde, rugoso), se cruzan plantas con semillas amarillas y rugosas con plantas con semillas verdes y redondas.
A pesar del cambio de los fenotipos en P1, tanto la F1 como la F2 dan lo mismo que antes.
Quedará claro por qué esto es así en la sección siguiente. 

Figura 3.0

El jardín de Mendel

Figura 3.1

Resumen de los siete pares de caracteres alternativos y de los resultados de los siete cruces monohíbridos de Mendel

En cada caso, el polen proveniente de plantas que manifestaban uno de los caracteres alternativos se utilizó para fecundar al óvulo de plantas que manifestaban el otro carácter alternativo.
En la generación F1, uno de los dos caracteres, referido como dominante, se manifestaba en todas las plantas.
El carácter alternativo, referido como recesivo, aparecía de nuevo aproximadamente en la cuarta parte de las plantas de la F2.
En la fotografía se muestra el guisante de jardín ( Pisum sativum ).

Figura 3.2

Explicación de un cruce monohíbrido entre plantas de guisante altas y enanas

Los símbolos E y e se utilizan para designar a los factores alto y enano, respectivamente, en los genotipos de las plantas adultas y de los gametos.
Todos los individuas se muestran dentro de un rectángulo.
Todos los gametos se muestran dentro de un circulo.

Figura 3.3

Utilización de un tablero de Punnett para generar las proporciones F2 del cruce F1 x F1 que se muestra en la Figura 3.2

Figura 3.4

Cruzamiento prueba para un solo carácter

En (a) el descendiente alto es homozigoto.
En (b) el alto es heterozigoto.

Ligamiento, entrecruzamiento y mapas cromosómicos

CONCEPTOS DEL CAPÍTULO

Los cromosomas tienen machos genes.
A menos que se separen por entrecruzamiento, los alelos presentes en los machos loci que hay en cada homólogo segregan como una unidad en la formación de los gametos.
Los gametos recombinantes que aparecen por entrecruzamiento aumentan la variabilidad genética en una especie y son la base para construir mapas de cromosomas.

Ya en 1903, Walter Sutton, que junto con Theodor Boveri unió los campos de la citología y de la genética, señaló la posibilidad de que en los organismos hay muchos más «factores» que cromosomas. 
Muy pronto, las investigaciones genéticas en muchos organismos revelaron que ciertos genes no se transmitían de acuerdo con la ley de la transmisión independiente. 
Cuando en los cruces se estudian juntos estos genes, parecían segregar como si estuvieran de alguna manera ligados o unidos.
Nuevas investigaciones demostraron que tales genes formaban parte del mismo cromosoma v que se transmitían realmente como unidades sencillas.

Sabemos ahora que la mayor parte de los cromosomas constan de un número muy grande de genes y, de hecho, tienen suficiente DNA como para codificar miles de estas unidades.
Se dice que los genes que forman parte del mismo cromosoma están ligados y demuestran ligamiento en cruces genéticos.

Debido a que el cromosoma es la unidad de transmisión en la meiosis, y no el gen, los genes ligados no son libres para transmitirse independientemente.
Por el contrario, los alelos de todos los loci de un cromosoma se transmitirán, en teoría, como una unidad en la formación de los gametos.
Sin embargo, en mucho casos no ocurre así. 
En la primera profase meiótica, cuando los homólogos se aparean (o sufren sinapsis), puede tener lugar un intercambio recíproco de fragmentos de cromosomas.
Este fenómeno, que se llama entrecruzamiento , da lugar a una mezcla o recombinación de los alelos entre homólogos.

El grado de entrecruzamiento entre dos loci cualesquiera de un cromosoma es proporcional a la distancia que los separa. 
Así, el porcentaje de gametos recombinantes varía dependiendo de qué loci están siendo considerados. 
Esta correlación sirve de base para la construcción de mapas cromosómicos , que proporcionan la situación relativa de los genes en los cromosomas. 

El entrecruzamiento se contempla normalmente como un proceso de rotura física real y reunión que ocurre en la meiosis.
Este intercambio de segmentos cromosómicos proporciona un enorme potencial de variación genética en los gametos formados por cada individuo.
Este tipo de variación, en combinación con la que resulta de la transmisión independiente, asegura que todos los descendientes tengan una mezcla diversa de alelos paternos y maternos.

En este capítulo discutiremos el ligamiento, el entrecruzamiento y la construcción de mapas cromosómicos. 
Consideraremos también una serie de temas que tienen que ver con el intercambio de la información genética. 
Concluiremos el capítulo con la entretenida e intrigante cuestión de por qué Mendel, que estudió siete genes, no encontró ligamiento,
¿o lo hizo?

Ligamiento frente a transmisión independiente

Para proporcionar una visión simplificada del tema principal de este capítulo, en la Figura 5.1 se ilustran y se contrastan las consecuencias meióticas de la transmisión independiente, del ligamiento sin entrecruzamiento y del ligamiento con entrecruzamiento.

En el caso de transmisión independiente se consideran dos pares de cromosomas homólogos y en los dos casos de ligamiento un par de cromosomas homólogos.

En la Figura 5.1(a) se ilustran los resultados de la transmisión independiente de dos pares de cromosomas no homólogos, teniendo cada uno de ellos un par de genes en heterozigosis. 
No hay ligamiento entre estos dos genes.
Cuando se observa un gran número de meiosis, se forman cuatro gametos diferentes genéticamente, en proporciones iguales. 

Podemos comparar estos resultados con los que ocurren si estos mismos genes se encuentran ligados en el mismo cromosoma. 
Si no hay entrecruzamiento entre los dos genes [Figura 5.1(b)], sólo se formarán dos gametos distintos genéticamente.
Cada gameto recibe los alelos presentes en uno o en el otro homólogo, que se han transmitido intactos como consecuencia de la segregación.

Este caso ilustra el ligamiento completo , que da lugar sólo a la producción de gametos paternos o no entrecruzados . 
Los dos gametos paternos se forman en igual proporción. 

Aunque el ligamiento completo entre dos genes ocurre raramente, la consideración de las consecuencias teóricas de esta situación es útil cuando se estudia el entrecruzamiento.

La Figura 5.1(c) ilustra el resultado cuando entre los dos genes ligados hay entrecruzamiento.
Como se advertirá, el entrecruzamiento implica sólo a dos cromátidas no hermanas de las cuatro cromátidas presentes en la tétrada. 
Esté intercambio da lugar a dos nuevas combina clones alélicas, llamadas gametos recombinantes o entrecruzados .
Las dos cromátidas no implicadas en el intercambio dan lugar a los gametos no recombinantes, como los de la Figura 5.l(b).

En general, la frecuencia con la que se da el entrecruzamiento entre dos genes ligados es proporcional a la distancia a la que se encuentran los respectivos loci en el cromosoma.
En teoría, dos genes seleccionados al azar pueden estar tan próximos que no sean fácilmente detectables los raros entrecruzamientos entre ellos.
Esta circunstancia, de ligamiento completo, da lugar a la producción de gametos sólo paternos, como se muestra en la Figura 5.1 (b).
Por otro lado, si los dos genes están separados por una corta distancia, se formarán pocos gametos recombinantes y muchos paternos.
A medida que aumenta la distancia entre dos genes, aumenta la proporción de gametos recombinantes y disminuye la de gametos paternos.
Por ello, como en la Figura 5.1(c), la proporción de gametos recombinantes varía dependiendo de la distancia entre los genes estudiados. 

Como más tarde exploraremos de nuevo en este capítulo, cuando se consideran dos genes ligados, cuyos loci se encuentran muy alejados, el número de gametos recombinantes se aproxima, pero nunca excede, al 50 por ciento.
Si hay un 50 por ciento de recombinantes, se producirán cuatro tipos en las proporciones 1:1:1:1 (dos gametos paternos y dos recombinantes). 
En tal caso, la transmisión de dos genes ligados no se podría distinguir de la de dos genes no ligados, que se transmiten independientemente.
Es decir, la proporción de los cuatro posibles genotipos será idéntica, como se muestra en la Figura 5.1(a, c).

La proporción de ligamiento 

Si hay ligamiento completo entre dos genes, debido a su proximidad, y se cruzan organismos heterozigóticos para ambos loci, aparece una proporción fenotípica única en F" que designaremos como la proporción de ligamiento .
Para ilustrar esta proporción, consideremos un cruce entre dos genes mutantes recesivos íntimamente ligados, ojos brown (bow) (marrones) y venas alares heavy ( hv ) (gruesas) en Drosophila melanogaster (Figura 5.2).
Los alelos normales de tipo silvestre, bw+ y hv+ son ambos dominantes y dan lugar a ojos rojos y venas alares finas, respectivamente. 

En este caso, se cruzan moscas con ojos mutantes marrones y con venas alares finas con moscas de ojos normales y venas alares gruesas.
Podemos nombrar a estas moscas de una forma más concisa refiriéndonos a ellas por sus fenotipos mutantes y decir que se cruzan moscas de ojos marrones con moscas de venas alares gruesas.
Si ampliamos el sistema para usar los símbolos genéticos, establecidos en el Capítulo 4, los genes ligados se pueden representar situando el símbolo de sus alelos por encima y por debajo de una línea, doble o sencilla, horizontal.
Aquéllos situados por encima de la línea son loci localizados en uno de los homólogos, y los situados debajo loci localizados en el otro homólogo.
Así, podemos representar la generación P' como sigue:
marrones, finas rojos, gruesas.

Debido a que los genes están situados en un autosoma, y no en el cromosoma X, no es necesario distinguir a machos de hembras.

En la generación F1, cada mosca recibe un cromosoma de cada pareja de cada padre; todas las moscas son heterozigóticas para ambos pares génicos y presentan los caracteres dominantes de ojos rojos y venas finas:

Como se muestra en la Figura 5.2, debido al ligamiento completo, cuando la generación F1 se cruza entre sí, cada individuo de F1 forma sólo gametos paternos.
Después de la fecundación, la generación F2 dará lugar a una proporción genotípica y fenotípica 1:2:1.
La cuarta parte de las moscas de esta generación presentarán ojos marrones y venas finas, la mitad presentará ambos caracteres normales, es decir, ojos rojos y venas finas y una cuarta parte presentará ojos rojos y venas gruesas.
En resumen, la proporción es 1 marrones:2 silvestres:1 gruesas.
Tal proporción es característica del ligamiento completo, que se observa sólo cuando los dos genes están muy íntimamente ligados y el número de descendientes es relativamente pequeño.

En la Figura 5.2 también se muestran los resultados de un cruzamiento prueba con moscas F1.
Tal cruzamiento da lugar a una proporción 1:1 de moscas marrones, delgadas y rojas, gruesas.
Si estos genes que controlan estos caracteres tuvieran ligamiento incompleto o estuvieran localizados en autosomas distintos, se producirían cuatro fenotipos en lugar de dos.

Cuando se investiga un gran número de genes mutantes presentes en cualquier especie dada, los genes localizados en el mismo cromosoma presentarán evidencias de ligamiento entre sí.
Por ello, se pueden establecer grupos de ligamiento, uno para cada cromosoma.
En teoría, el número de grupos de ligamiento tiene que corresponder con el número haploide de cromosomas. 
En organismos en donde se dispone de un gran número de cenes mutantes para su estudio, esta correlación se cumple siempre.

Ligamiento incompleto, entrecruzamiento y mapas cromosómicos

Si se seleccionan al azar dos genes ligados en el mismo cromosoma, es muy improbable que se encuentren tan íntimamente ligados uno del otro que muestren ligamiento completo.
En los cruces entre dos genes seleccionados al azar que se encuentran ligados, casi siempre se producirá un porcentaje de descendientes que provengan de gametos recombinados.
Este porcentaje es variable y depende de la distancia que hay entre los dos genes. 
Este fenómeno fue explicado por primera vez por dos genéticos de Drosophila, Thomas H. Morgan y su estudiante Alfred H. Sturtevant.

Morgan y el entrecruzamiento

Como recordará de nuestra anterior discusión en el Capítulo 4, Morgan fue el primero en descubrir el fenómeno del ligamiento al X.
En sus estudios investigó numerosas mutaciones de Drosophilalocalizadas en el cromosoma X.
Cuando analizó cruces para un solo carácter, pudo deducir el modo de herencia ligada al X.
Sin embargo, cuando hizo cruces que implicaban simultáneamente a dos genes ligados al X, sus resultados fueron al principio desconcertantes. 
Por ejemplo, como se muestra en el Cruce A de la Figura 5.3, cruzó hembras mutantes de cuerpo yellow (y) (amarillo) y de ojos white (w) (blancos) con machos de tipo silvestre (cuerpo gris y ojos rojos).
Las hembras F1 eran de tipo silvestre, pero los machos expresaban ambos caracteres mutantes.
En F2, el 98,7 por ciento de los descendientes presentaban los fenotipos paternos, es decir, moscas de cuerpo amarillo y ojos blancos y moscas de tipo silvestre (cuerpo gris y ojos rojos).
El restante 1,3 por ciento de las moscas eran o bien de cuerpo amarillo y ojos rojos o de cuerpo gris y ojos blancos.
Parecía como si los genes se hubieran separado de alguna manera en la formación de los gametos en las moscas F1.

Cuando Morgan hizo cruces con otros genes ligados al X, los resultados fueron todavía más sorprendentes (Cruce B de la Figura 5.3).
Se observó el mismo patrón básico, pero las proporciones de los fenotipos F2 diferían; por ejemplo, en un cruce entre mutantes de ojos white y alas miniature (miniatura), sólo el 62,8 por ciento de la F2 presentaba los fenotipos paternos, mientras que el 37,2 por ciento de los descendientes aparecía como si los genes mutantes se hubieran separado en la formación de los gametos.

En 1911 Morgan se enfrentó a dos cuestiones: (1) 
¿Cuál era la causa de la separación de los genes?
y (2)
¿por qué la frecuencia de la separación aparente variaba dependiendo de los genes estudiados?
La respuesta propuesta a la primera cuestión se basó en su conocimiento de anteriores observaciones citológicas realizadas por F. Janssens y otros.

Janssens había observado que los cromosomas homólogos en sinapsis en la meiosis se entrelazaban entre sí, originando quiasmas en donde eran evidentes puntos de solapamiento.
Morgan propuso que estos quiasmas podrían representar puntos de intercambio.

En los cruces que se presentan en la Figura 5.3, Morgan postuló que si ocurría un intercambio entre los genes mutantes situados en los dos cromosomas de las hembras F', se obtendrían los resultados observados.
Sugirió que tal intercambio daría lugar a un 1,3 por ciento de gametos recombinantes en el cruce yellow-white y a un 37,2 por ciento en el cruce white-miniature .
De acuerdo con esto y con otros experimentos, Morgan concluyó que existen genes ligados en un orden lineal a lo largo del cromosoma y que entre cualesquiera dos genes hay un porcentaje variable de intercambio.

Como respuesta a la segunda cuestión, Morgan propuso que era menos probable que entre dos genes localizados relativamente cerca uno de otro se diera un quiasma que si los dos genes se encontraban alejados en el cromosoma.
Por consiguiente, cuanto más próximos estén dos genes, menos probable es que ocurra entre ellos un intercambio.
Morgan propuso el término entrecruzamiento para describir el intercambio físico que da lugar a la recombinación.

Sturtevant y la obtención de mapas

Alfred H. Sturtevant, estudiante de Morgan, fue el primero en darse cuenta de que la propuesta de su maestro podía también utilizarse para cartografiar la secuencia de genes ligados.
De acuerdo con Sturtevant, «En una conversación con Morgan ... me di cuenta súbitamente de que la variación en la fuerza del ligamiento, atribuida ya por Morgan a diferencias en la separación espacial de los genes, ofrecía la posibilidad de determinar secuencias a lo largo de un cromosoma. 
Fui a casa y empleé gran parte de la noche (descuidando mis deberes de clase) en obtener el primer mapa cromosómico...». 
Por ejemplo, Sturtevant compiló datos sobre la recombinación entre los genes mutantes yellow , white y miniature , estudiados inicialmente por Morgan.
Las frecuencias de entrecruzamiento entre cada par de estos tres genes era:

(1) yellow, white 0,5% 
(2) white , miniatura 34,5%
(3) yellow , miniatura 35,4%

Debido a que la suma de (1) y (2) es aproximadamente igual a (3), Sturtevant argumentó que las frecuencias de recombinación entre genes ligados son aditivas.
De acuerdo con esto, predijo que el orden de los genes en el cromosoma X era yellow-white-miniature .
Al llegar a esta conclusión razonó lo siguiente: 
los ganes yellow y white se encuentran aparentemente próximos porque su frecuencia de recombinación es baja.
Sin embargo, estos dos genes se encuentran muy alejados de miniature debido a que las combinaciones white, miniature y yellow, miniature muestran frecuencias de recombinación altas.
Debido a que miniature muestra más frecuencia de recombinación con yellow que con white (35,4 respecto de 34,5), se deduce que white se encuentra entre los otros dos genes, no fuera de ellos.

Sturtevant sabía por el trabajo de Morgan que la frecuencia de intercambio podía tomarse como una estima de la distancia relativa entre dos genes o loci a lo largo del cromosoma.
Construyó un mapa para los tres genes del cromosoma X, siendo la unidad de mapa igual al 1 por ciento de recombinación entre dos genes [1] .
En el ejemplo anterior, la distancia entre yellow , y white sería de 0,5 unidades de mapa y entre yellow , y miniatura de 3 5,4 unidades de mapa.

Se deduce que la distancia entre yellow , y miniature debería ser (35,4- 0,5) o 34,9.
Esta estima es muy próxima a la frecuencia real de recombinación entre white y miniatura (34,5).
El mapa para estos tres genes se muestra en la Figura 5.4.

Además de estos tres genes, Sturtevant consideró otros dos genes del cromosoma X y obtuvo un mapa más extenso que incluía a los cinco genes.
Él y su colega, Calvin Bridges, comenzaron pronto a investigar el ligamiento autosómico en Drosophila.
En 1923 habían demostrado claramente que el ligamiento y el entrecruzamiento no estaban restringidos a los genes ligados al X.
Habían descubierto genes ligados en los autosomas entre los que ocurría entrecruzamiento.

En este trabajo realizaron otras observaciones interesantes. 
Se demostró que el entrecruzamiento en Drosophila ocurría sólo en hembras.
El hecho de que no se diera entrecruzamiento en los machos hizo que el análisis de los mapas fuera mucho menos complejo en Drosophila.
Sin embargo, en muchos otros organismos hay entrecruzamiento en ambos sexos.

Figura 5.1

Comparación de los resultados de formación de gametos cuando dos genes en heterozigosis se encuentra en (a) dos parejas diferentes de cromosomas, (b) en el mismo par de homólogos, pero no dándose intercambio entre ellos, y (c) en el mismo par de homólogos, con intercambio entre dos cromátidas no hermanas

Figura 5.2

Resultado de un cruce entre dos genes localizados en el mismo cromosoma, que demuestran ligamiento completo

En la parte (a) se generan los resultados de F2 del cruce.
En la parte (b) se generan los resultados de un cruzamiento prueba con F1.

Figura 5.3

Resultados de F1 y F2 de los cruces entre las mutaciones de cuerpo yellow y ojos white y las mutaciones ojos white y alas miniatura

En F2 del cruce A, el 1,3 por ciento de las moscas presentaba fenotipos recombinantes, que manifestaban white y yellow .
En F2 del cruce B, el 37,2 por ciento de las moscas presentaba fenotipos recombinantes, que manifiestan miniature o white .

Figura 5.4

Un sencillo mapa de los genes situados en el cromosoma X yellow (y), white (w) y miniatura (m) de Drasophila melanogaster

Cada número representa el porcentaje de descendientes recombinantes producidos en los tres cruces, implicando cada uno a dos genes diferentes.

[01] En honor al trabajo de Morgan, una unidad de mapa se denomina a menudo un centimorgan (CM).

Recombinación y cartografía en bacterias y bacteriófagos

CONCEPTOS DEL CAPÍTULO

Las bacterias y los bacteriófagos (virus que parasitan bacterias) han estado sujetos a un profundo análisis genético.
Presentan varios mecanismos que dan lagar a recombinación, proceso que puede utilizarse para realizar mapas genéticos.
A menudo las bacterias tienen DNA extracromosómico en forma de plásmidos.
Tanto el DNA de los plásmidos como el de los bacteriófagos puede recombinar e integrase en el cromosoma bacteriano.
Un amplio análisis de la estructura del gen en el bacteriófago T4 estableció las bases de la complementación génica y de la prueba de complementación, un método que sigue siendo importante en la investigación de hoy día. 

En este capítulo pasamos del estudio de la transmisión de la información genética en eucariotas a la discusión de varios fenómenos genéticos en bacterias (procariotas) y bacteriófagos , que son los virus que parasitan a las bacterias.
El estudio de las bacterias y de los bacteriófagos ha sido esencial en el progreso del conocimiento en muchas áreas de estudio genético.
Por ejemplo, gran parte de lo que se sabe en genética molecular sobre el fenómeno de la recombinación y de la estructura del gen proviene inicialmente del trabajo experimental llevado a cabo con estos organismos.
Además, como veremos en los Capítulos 15 y 16, el amplio conocimiento sobre las bacterias y sus plásmidos ha servido de base para su uso generalizado en el clonado del DNA en estudios de DNA recombinante.

Su éxito en la investigación genética se debe a muchos factores.
Tanto las bacterias como sus virus tienen ciclos reproductivos extremadamente cortos.

Literalmente, se pueden producir en períodos cortos de tiempo cientos de generaciones, dando lugar a billones de bacterias o fagos genéticamente idénticos.
Además, se pueden estudiar en cultivos puros.
Es decir, se pueden aislar e investigar independientemente de otros organismos similares una sola especie o una cepa mutante bacteriana o un tipo de virus.

En este capítulo, nuestro mayor énfasis se centrará en la recombinación genética que se da tanto en bacterias como en bacteriófagos.
En las poblaciones de estos microorganismos y de sus virus han evolucionado procesos específicos que facilitan la recombinación. 
Como veremos, estos procesos sirven de base para la realización de análisis de mapas cromosómicos.
Tales estudios establecieron los fundamentos del conocimiento acerca de la identidad genética de bacterias y bacteriófagos, lo que sirvió de piedra angular para posteriores investigaciones en genética molecular.

Mutaciones y crecimiento bacterianos 

1.os estudios genéticos en bacterias dependen de nuestra habilidad para aislar mutaciones en estos organismos. 
Aunque mucho antes de 1943 se sabía que cultivos puros de bacterias podían dar lugar a un número pequeño de células que presentaban variación heredable, particularmente en relación con la supervivencia en diferentes condiciones ambientales, la causa de dicha variación era debatida calurosamente.
La mayoría de los bacteriólogos creían que los factores ambientales inducían cambios en ciertas bacterias, lo que les permitía sobrevivir o adaptarse a nuevas situaciones.
Por ejemplo, se sabía que cepas de E. coli eran sensibles a la infección por el bacteriófago T1.
La infección por el bacteriófago daba lugar a la reproducción de los virus a expensas de las bacterias, que se lisaban o destruían.
Si una placa con E. coli se infecta de manera homogénea con T1, casi todas las células se lisan.
Sin embargo, algunas pocas células de E. coli sobreviven a la infección v no se lisan.
Si se aísla a estas células y se establece un cultivo puro, todos los descendientes son resistentes a la infección por T1.
Se supuso que las mutaciones responsables de la resistencia a T1 se «inducían» por la presencia de los virus T1 y que, en ausencia de los virus T1, la mutación no hubiera ocurrido.
Sin embargo, como exploraremos en un capítulo posterior, tales células resistentes a T1 aparecen por mutación espontánea , lo que fue elegantemente probado por Salvador Luria y Max Delbruck en 1943 (véase el Capítulo 14).

Las células bacterianas que llevan mutaciones espontáneas, como las de resistencia a T1, se pueden aislar y establecer en una cepa independiente a partir de la cepa paterna, utilizando diversas técnicas de selección.
En consecuencia, en la actualidad se pueden inducir y aislar limitaciones para casi cualquier característica que se desee.
Debido a que las bacterias y los virus que las infectan son haploides, todas las mutaciones se expresarán directamente en los descendientes de las células mutantes, facilitando el estudio de estos microorganismos.

Las bacterias crecen en un medio de cultivo líquido o en una placa de Petri, sobre una superficie de agar semi-sólida. 
Si los c nutritivos del medio de crecimiento son muy simples y están formados sólo por una fuente de carbono orgánico (como glucosa o lactosa) y una serie de iones, como Na+ , K+ , Mg++ , Ca++ y NH4+ presentes como sales inorgánicas, se denomina medio mínimo .
Para crecer en tal medio, una bacteria debe ser capaz de sintetizar todos los c orgánicos esenciales (por ejemplo, aminoácidos, purinas, pirimidinas, azúcares, vitaminas, ácidos grasos).
Una bacteria que puede llevar a cabo esta notable hazaña biosintética -algo que nosotros mismos no podemos copiar- se denomina protótrofa .
Se dice que es de tipo silvestre en cuanto a los requerimientos nutritivos.
Por otro lado, si una bacteria pierde, por mutación, la capacidad para sintetizar uno 0 más c orgánicos, se dice que es auxótrofa .
Por ejemplo, si pierde la capacidad para fabricar histidina, entonces se tiene que añadir dicho aminoácido como suplemento al medio mínimo para que pueda haber crecimiento.
Las bacterias resultantes se identifican como auxótrofa his-, en oposición a la protótrofa his+.
Adviértase que un medio que ha sido ampliamente suplementado se denomina medio completo .

Para estudiar bacterias mutantes de un modo cuantitativo, se sitúa un inóculo bacteriano en un medio de cultivo líquido.
Se presentará un patrón de crecimiento característico, tal como se ilustra en la Figura 6.1.
Inicialmente, durante la fase lag , el crecimiento es lento.
Luego sigue un periodo de crecimiento rápido llamado la fase log .
En esta fase la célula se divide muchas veces, con un intervalo de tiempo fijo entre divisiones celulares, dando lugar a un crecimiento logarítmico.
Cuando la densidad celular alcanza, aproximadamente, unas 109 células por mililitro, los nutrientes y el oxígeno llegan a ser limitantes y las células entran en una fase estacionaria .

Como el tiempo en el que se dobla la población en la fase log puede ser de unos 20 minutos, con un inóculo inicial de unas pocas miles de células se puede alcanzar fácilmente la densidad celular máxima de un cultivo en una noche.

Las células que crecen en un medio líquido pueden cuantificarse transfiriéndolas a un medio semisólido en una placa de Petri.
Después de la incubación y de muchas divisiones, cada célula da lugar a una colonia visible sobre la superficie del medio.
El número de colonias permite calcular el número de bacterias presentes en el cultivo original.
Si el número de colonias es demasiado grande como para poder contarlas, entonces se pueden hacer diluciones seriadas del cultivo líquido original y sembrarlas hasta que el número de colonias se reduzca a un punto en donde puedan contarse.
Tales cálculos son útiles para diversos estudios.

Por ejemplo, supongamos que se ha hecho un muestreo de un cultivo líquido de bacterias en tres placas.
Inicialmente se ha separado un mililitro ( ml ), y deseamos determinar cuántas células hay por mililitro.
Si se somete el mililitro inicial a una serie de diluciones seriadas, esto se puede conseguir fácilmente.

Supongamos que las tres placas de Petri representan diluciones de 10, 10 y 10-5 , respectivamente.
Necesitamos sólo la placa en donde el número de colonias se pueda contar con precisión.
Debido a que cada colonia surge probablemente de una sola bacteria, el factor de dilución representa el número de bacterias en el mililitro original. 
En nuestro caso la placa más a la derecha tiene 15 colonias.
Como es una dilución de 10 - 5 , el número inicial de bacterias se estima en 15 x 10 por mililitro.

Recombinación genética en bacterias: conjugación

El desarrollo de técnicas que permitieron la identificación y el estudio de las mutaciones bacterianas condujo a investigaciones detalladas sobre la disposición de los genes en el cromosoma bacteriano.
Joshua Lederberg y Edward Tatum iniciaron estos estudios en 1946.
Demostraron que las bacterias sufren conjugación , un proceso parasexual mediante el cual la información genética de una bacteria se transfiere y recombina con la de otra bacteria.
Como en el entrecruzamiento meiótico en eucariotas, la recombinación en bacterias proporciona las bases para el desarrollo de metodologías para la cartografía cromosómica.
Adviértase que el término recombinación , cuando se aplica a bacterias y bacteriófagos, se refiere a la sustitución de uno o más genes presentes en una cepa por los de otra cepa genéticamente distinta. 
Aunque esto es algo diferente respecto del concepto de recombinación en eucariotas, en donde el término se refiere al entrecruzamiento que da lugar a un intercambio reciproco , el efecto global es el mismo.
La información genética se transfiere de un organismo a otro, dando lugar a un genotipo alterado.
Otros dos fenómenos, la transformación y la transducción , también dan lugar a la transferencia de información genética de una bacteria a otra y también se han utilizado para determinar la disposición de los genes en el cromosoma bacteriano.
Volveremos a discutir estos procesos en las secciones siguientes de este capítulo.

Los experimentos iniciales de Lederberg y Tatum se realizaron con dos cepas auxótrofas múltiples (mutantes nutricionales) de E. coli K12 .
La cepa A requería metionina (met) y biotina (bio) para poder crecer, mientras que la cepa B requería treonina ( thr ), leucina ( lea ) y tiamina ( thi ), como se muestra en la Figura 6.2.
Ninguna de las dos cepas podía crecer en un medio mínimo.
Las dos cepas se sembraron primero separadamente en un medio suplementado, y luego se mezclaron células de ambas cepas y se hicieron crecer juntas durante varias generaciones.
Luego se sembraron en un medio mínimo.

Cualquier célula bacteriana que creciera en el medio mínimo sería protótrofa (bacteria de tipo silvestre).
Era altamente improbable que cualquiera de las células con dos o tres genes mutantes sufriera mutaciones espontáneas simultáneamente en los dos o tres loci. 
Por consiguiente, cualquier protótrofo recuperado debía haber surgido como consecuencia de alguna forma de intercambio genético y recombinación.

En este experimento, se recuperaron protótrofos con una frecuencia de 1/107 (10- 7) respecto de las células sembradas.
Los controles de este experimento consistieron en el sembrado separado de células de las cepas A y B en medios mínimos.
No se recuperaron protótrofos. 
Basándose en estas observaciones,
¡ Lederberg y Tatum propusieron que había ocurrido recombinación! 

Bacterias F+ y F- 

Muy pronto siguieron numerosos experimentos, diseñados para aclarar las bases genéticas de la conjugación. 
En seguida quedó claro que cepas diferentes de bacterias estaban implicadas en una transferencia unidireccional de material genético.
Las células de la cepa que podían servir de donantes de parte de su cromosoma se denominaron células F+ (F por «fertilidad»). 

Las bacterias receptoras, que sufren recombinación al recibir el material cromosómico donante, que ahora se sabe es el DNA, y que por recombinación lo integran como parte de su propio cromosoma, se denominan células F- .

Posteriormente se constató que el contacto entre las celarlas era esencial para la transferencia cromosómica. 
Bernard Davis proporcionó apoyo a esta idea, al diseñar un tubo en U en el que crecían células F+ y F- (Figura 6.3).
En la base del tubo había un filtro de porcelana con un tamaño de pero que permitía el paso del medio líquido, pero que era demasiado pequeño como para permitir el paso de las bacterias.
Las células F+ se situaron a un lado del filtro y las F- al otro.

El medio se desplazó a un lado y otro del filtro, por lo que las células bacterianas compartían esencialmente un medio común durante su incubación.
Luego se sembraron independientemente muestras de ambos lados del tubo en un medio mínimo, pero no se recuperaron protótrofos. 
Davis concluyó que el contacto físico era esencial para la recombinación .
Esta interacción física es la fase inicial del proceso de conjugación y se realiza mediante una estructura denominada el pilus sexual o F.
Las bacterias tienen a menudo muchos pili, que son extensiones microscópicas de la célula. 
Diferentes tipos de pili realizan diferentes funciones celulares, pero todos los pili están implicados de alguna manera en la adhesión.
Una vez se ha establecido el contacto entre las bacterias vía los pili F+, comienza la transferencia de DNA.

Posteriormente se obtuvieron pruebas de que las células F+ tenían un factor de fertilidad (llamado factor F ), que les confiere capacidad para donar parte de su cromosoma en la conjugación.
En experimentos realizados por Joshua y Esther Lederberg y por William Hayes y Luca Cavalli-Sforza, se demostró que en ciertas condiciones el factor F podía eliminarse de células anteriormente fértiles.
Sin embargo, si estas células «no fértiles» se hacían crecer con células donantes fértiles, recuperaban el factor F.

Esto hizo pensar que el factor F era un elemento móvil, lo que se confirmó posteriormente por la observación de que, después de la conjugación y la recombinación, las células receptoras siempre se convertían en F+.

Así, además del caso raro de transferencia de genes del cromosoma bacteriano, el factor F pasaba a todas las células receptoras.
De acuerdo con esto, los cruces iniciales de Lederberg y Tatum (Figura 6.2) se pueden denominar: CEPA A DONANTE, CEPA B RECEPTORA

La confirmación de estas conclusiones se basa en el aislamiento del factor F.
Como el cromosoma bacteriano, pero distinto a él, se ha demostrado que el factor F es una molécula de DNA de doble cadena, circular.
Tiene una cantidad de DNA equivalente al 2 por ciento del que constituye el cromosoma bacteriano (alrededor de 100.000 pares de bases). 
En el factor F se encuentran, entre otros, 19 genes, cuyos productos están implicados en la transferencia de la información genética (genes tra ), incluyendo aquellos que son esenciales para la formación de los pili. 

Como veremos pronto, el factor F es en realidad una unidad genética autónoma, que se denomina plásmido .
Sin embargo, en nuestra explicación histórica de su descubrimiento, continuaremos refiriéndonos a él como «factor».

Se cree que la transferencia del factor F en la conjugación implica la separación de las dos cadenas del DNA que constituye el factor F y el desplazamiento de una de las cadenas hacia la célula receptora.
La otra cadena permanece en la célula donante.
Ambas cadenas paternas sirven de molde para la replicación del DNA, dando lugar a dos factores F completos, uno en cada una de las dos células F+.
Este proceso se esquematiza en la Figura 6.4.

En resumen, las células de E. coli pueden contener o no el factor F.
Cuando está presente, la célula es capaz de formar un pilas sexual y actuar potencialmente como dadora de información genética.
En la conjugación casi siempre se transfiere una copia del factor F de la célula F+ a la célula receptora F- , convirtiéndola en F+.
Queda la pregunta de cuál es el porcentaje exacto, muy bajo, de células F- que sufren recombinación. 
Como discutiremos, la respuesta requiere nuevos experimentos. 

Bacterias Hfr y cartografía cromosómica

Descubrimientos posteriores no sólo clarificaron cómo ocurre la recombinación sino que también determinaron un mecanismo mediante el que podía cartografiarse el cromosoma de E. coli .
Nos referiremos primero a la cartografía cromosómica.

En 1950, Cavalli-Sforza trató a una cepa F+ de E. coli con gas mostaza, una sustancia química que se sabe induce mutaciones.
De estas células tratadas recuperó una cepa bacteriana donante que sufría una tasa de recombinación de 1/104 (10 4), 1.000 veces más frecuente que las cepas F+ originales.
En 1953, William Hayes aisló otra cepa que presentaba una frecuencia elevada similar.
Ambas cepas se designaron Hfr, o con alta frecuencia de recombinación.
Debido a que las células Hfr se comportan como donantes de cromosomas, es una clase especial de células F+.

Se advirtió otra diferencia importante entre la cepas Hfr y las cepas F+ originales.
Si la cepa donante es Hfr, las células receptoras, aunque presenten recombinación, nunca se convierten en Hfr; es decir, permanecen como F-.
Comparando ambos tipos' tendremos F+ X F- ( F+ (baja tasa de recombinación), Hfr X F- ( F- (alta tasa de recombinación).

Quizá la característica más significativa de las cepas Hfr es la naturaleza de la recombinación. 
En cualquier cepa dada, ciertos genes recombinan más frecuentemente que otros, y algunos no lo hacen en absoluto. 
Se demostró que este patrón de transferencia de genes no aleatorio variaba de una cepa Hfr a otra .
Aunque estos resultados eran enigmáticos, Hayes los interpretó diciendo que había ocurrido alguna alteración fisiológica del factor F, dando lugar a la producción de cepas Hfr de E. coli .

Hacia mediados de los años 50, experimentos de Ellie Wollman y François Jacob explicaron las diferencias entre las células Hfr y las F+ y demostraron cómo las células Hfr permitían la cartografía genética de los cromosomas de E. coli .
En sus experimentos mezclaron cepas Hfr y F-, con los genes marcadores adecuados, y comprobaron la recombinación de genes concretos en momentos diferentes.
Para llevar a cabo esto, se incubó primero un cultivo con una mezcla de una cepa Hfr y otra F y se extrajeron muestras en diversos momentos y se situaron en un agitador.
La agitación separaba a las bacterias en conjugación de tal manera que se interrumpía de manera electiva la transferencia del cromosoma.
Luego se comprobaron las células en cuanto a la recombinación. 
Para facilitar la recuperación de sólo los recombinantes, la cepa Hfr era sensible a un antibiótico mientras que la cepa receptora era resistente.
Después del tratamiento con el agitador se hicieron crecer las células en un medio que tenía antibióticos para asegurar la recuperación de sólo las células receptoras.

Este proceso, llamado la técnica de conjugación interrumpida , demostró que genes concretos de una cepa dada Hfr se transferían y recombinaban antes que otros.
En la Figura 6.5 se ilustra este punto. 
En los primeros 8 minutos, después de que las cepas se hubieran mezclado, no se detectó recombinación. 
Alrededor de los 10 minutos se detectó la recombinación del gen aziR, pero no se detectó la transferencia de los genes tonS, lac+ y gal+.
A los 1S minutos, el 70 por ciento de los recombinantes eran aziR , el 30 por ciento tonS, pero ninguno era lac+o gal+.
A los 20 minutos se encontró el gen lac+entre los recombinantes, y a los 30 minutos también se había transferido gal+.
Por consiguiente, Wollman y Jacob habían demostrado una transferencia secuencial de genes que estaba correlacionada con el tiempo en el que se había permitido que la conjugación se diera.

Parecía que el cromosoma de la cepa Hfr se transfería linealmente y que podía predecirse de tales experimentos el orden de los genes y la distancia entre ellos, medida en minutos (Figura 6.ó).
Esta información sirvió de base para el primer mapa genético del cromosoma de E. coli .
Los «minutos» en el mapa bacteriano son equivalentes a las «unidades de mapa» en eucariotas.

Después, Wollman y Jacob repitieron el mismo experimento con otras cepas Hfr, obteniendo resultados similares pero con una diferencia importante.
Aunque los genes siempre se transferían linealmente con el tiempo, como en su primer experimento, los genes que entraban primero y seguían después parecían variar de una cepa Hfr J otra [Figura 6.7(a)].
Cuando reexaminaron el orden de entrada de los genes, es decir, los diferentes mapas genéticos de cada cepa, surgió un patrón definido.
Las diferencias más importantes entre todas las cepas se debían simplemente al punto de origen y a la dirección en la que procedía la entrada a partir de dicho punto [Figura 6.7(b)].

Para explicar estos resultados Wollman y Jacob propusieron que el cromosoma de E. coli es circular.
Si el punto de origen (O) variaba de una cepa a otra, se transferiría en cada caso una secuencia distinta de genes.
Pero,
¿qué determina O?
Propusieron que en varias cepas Hfr el factor F se integra en el cromosoma en puntos distintos. 
Su posición determina el lugar 0.
Un caso de tal integración se muestra en la Figura 6.8.
En conjugaciones posteriores entre esta célula Hfr y una F- , la posición del factor F determina el punto inicial de transferencia.
Aquellos genes próximos a O se transferirán primero.
El mismo factor F es lo último que se transfiere.
Aparentemente, la conjugación raramente dura lo suficiente como para permitir que pase todo el cromosoma por el tubo de conjugación. 
Esta propuesta explica por qué las células receptoras, cuando se cruzan con células Hfr, permanecen F-.

En la Figura 6.8 también se representa el modo en el que las dos cadenas de la molécula de DNA se desespiralizan durante la transferencia, permitiendo la entrada de una de las cadenas del DNA en la receptora.
Después de la replicación del DNA que ha entrado, tiene ahora la potencialidad de recombinar con las regiones homólogas del cromosoma huésped.
La cadena de DNA que permanece en la donante también sufre replicación.

La utilización de la técnica de la conjugación interrumpida con diferentes cepas Hfr ha servido de base para cartografiar el cromosoma completo de E. coli .
El tiempo en minutos del mapa es de 100 minutos para la cepa K12 (o E. coli K12 ).
En este mapa se han situado unos 900 genes.
En la mayor parte de los casos hay una sola copia de cada gen.

Figura 6.1

Curva de crecimiento típica de una población bacteriana, en la que se observa la fase inicial lag, la fase posterior log, durante la que se da el crecimiento exponencial, y la fase estacionaria, que se presenta cuando los nutrientes se han consumido

Figura 6.2

Recombinación entre dos cepas auxotróficas que producen protótrofos 

Ninguno de los auxótrofos crecerá en un medio mínimo, pero si que lo harán los protótrofos, permitiendo su recuperación.

Figura 6.3

Cuando se hacen crecer cepas auxotróficas A y B en un medio común, pero separadas por un filtro, no hay recombinación y no se producen protótrofos.

Este aparato se denomina tubo en U de Davis.

Figura 6.4

Cruce F+ x F- que muestra cómo la célula receptora F- convierte en F+

Durante la conjugación se replica el DNA del factor F y una copia nueva entra en la célula receptora, convirtiéndola en F+.
Se ha añadido una barrita negra a los factores F para seguir su rotación. 

Figura 6.5

Transferencia progresiva en la conjugación de varios genes desde una cepa específica Hfr de E. coli a una cepa F-

Ciertos genes ( azi y ton) se transfieren antes que otros y recombinan más frecuentemente.
Otros ( loc y gai)tardan más tiempo en transferirse y recombinan con menor frecuencia.
Otros ( thr y leu) se transfieren siempre y se utilizan como prueba inicial de recombinantes.

Figura 6.6

Un mapa de tiempos de los genes estudiados en el experimento representado en la Figura 6.5

Figura 6.7

El orden de la transferencia de los genes en cuatro cepas Hfr sugiere que el cromosoma de E. coli es circular

En cada cepa se identifica el punto a partir del cual se inicia la transferencia (Origen).
Adviértase que la transferencia puede ir en cualquier dirección, dependiendo de la cepa.
El origen viene determinado por el punto de integración en el cromosoma del factor F, y la dirección de transferencia por la orientación del factor F cuando se integra.

Ampliaciones del análisis genético 

CONCEPTOS DEL CAPÍTULO

Este capítulo se refiere a una serie de temas que representan ampliaciones del tema general de la genética de la transmisión, que se introdujo en capítulos anteriores.
Estos temas incluyen una consideración sobre la variación en la expresión fenotípica, el análisis cuantitativo de caracteres poligénicos, la heredabilidad y el análisis de mapas realizados en eucariotas haploides.
La expresión del fenotipo no es siempre reflejo directo del genotipo, sino que puede estar modificado por machas situaciones internas y externas.
El análisis cuantitativo de caracteres poligénicos requiere el uso de una metodología estadística y biométrica.
La cartografía de cromosomas en organismos haploides requiere técnicas diferentes de las utilizados en la cartografía de organismos diploides, pero utiliza los mismos principios generales.

Hasta ahora hemos introducido en el texto un conjunto de informaciones que constituye el cuerpo general del conocimiento denominado genética de la transmisión .
En este capítulo volveremos a varios temas avanzados, que son más accesibles ahora que ya se han obtenido los fundamentos que se proporcionaron en los primeros seis capítulos.
Estos temas constituyen ampliaciones relevantes de nuestras discusiones anteriores y tienen una naturaleza muy analítica.

Comenzaremos tratando el tema de la expresión del fenotipo , revisando los factores que lo afectan.
Como veremos, la expresión de la información genética no es siempre tan clara como muchos de nuestros ejemplos anteriores pudieran sugerir.
Un organismo que tiene un genotipo mutante puede presentar una variación considerable en la expresión del fenotipo correspondiente.
En algunos casos, un porcentaje de individuos mutantes puede no expresar el fenotipo en absoluto. 
Como veremos, hay muchos factores que modifican la expresión del fenotipo.

A continuación, ampliaremos nuestra discusión anterior sobre la herencia poligénica del Capítulo 4 con un examen más detenido de la herencia cuantitativa .
Cuando lo hagamos, consideraremos varias técnicas estadísticas que se utilizan a menudo al analizar los caracteres cuantitativos y al expresar la varianza de los datos poblacionales.
Luego introduciremos el concepto de heredabilidad , es decir, la evaluación del grado en que los factores genéticos contribuyen a la variación fenotípica en poblaciones.
Concluiremos este capítulo volviendo a los temas del ligamiento y entrecruzamiento y ampliaremos nuestra discusión incluyendo la cartografía de cromosomas en organismos haploides .
Examinaremos este tema en el hongo eucariota Neurospora y en el alga verde Chlamydomonas .

Expresión fenotípica 

La expresión del gen se considera a menudo como si los genes actuasen en un sistema cerrado, de «caja negra», en el que la presencia o ausencia de productos funcionales determina directamente el fenotipo conjunto de un individuo.
La situación es en realidad mucho más compleja.
La mayoría de los productos génicos actúan en el medio interno de la célula; las células interactúan unas con otras de varios modos y el organismo debe sobrevivir en diversas situaciones ambientales.
La expresión del gen y el fenotipo resultante es modificada a menudo mediante la interacción entre el genotipo concreto del individuo y el ambiente interno y externo.

El grado de influencia ambiental puede variar desde inapreciablemente sutil a muy marcado.
Las interacciones débiles son mucho más difíciles de detectar y documentar y conducen al no resuelto conflicto "se nace - se hace" en el que los científicos debaten la importancia relativa de los genes frente al ambiente.
En esta sección trataremos algunas de las variables que se sabe modifican la expresión del gen.

Penetración y expresividad 

Algunos genotipos mutantes se expresan siempre con un fenotipo concreto, mientras que otros dan lugar a una proporción de individuos cuyos fenotipos no se pueden distinguir del normal (tipo silvestre).
El grado de expresión de un carácter concreto se puede estudiar cuantitativamente determinando la penetración y la expresividad del genotipo en estudio.
El porcentaje de individuos que muestran, al menos en algún grado, la expresión de un genotipo mutante define la penetración de la mutación. 
Por ejemplo, la expresión fenotípica de muchos alelos mutantes de Drosophila puede solapar con el tipo silvestre.
Moscas homozigóticas para el gen mutante recesivo eyeless (sin ojos) dan lugar a fenotipos que van desde la presencia de ojos normales, pasando por una reducción parcial en el tamaño, hasta la ausencia completa de uno o de ambos ojos (Figura 7. 1).
Si el 15 por ciento de las moscas muestran la apariencia silvestre, se dice que el gen mutante tiene una penetración del 85 por ciento.

Por otro lado, el rango de expresión define la expresividad del genotipo mutante. 
En el caso de eyeless , aunque la reducción promedio del tamaño del ojo es de un cuarto a un medio, el rango de expresividad es desde la ausencia completa de ambos ojos a ojos completamente normales (Figura 7.1).

Ejemplos parecidos al de la expresión del fenotipo en eyeless han proporcionado las bases para diseñar experimentos y determinar las causas de la variación fenotípica.
Si el ambiente del laboratorio se mantiene constante y todavía se observa una amplia variación, se puede investigar el fondo genético.
Es posible que otros genes estén influenciando o modificando el fenotipo eyeless .
Por otro lado, si el fondo genético no es la causa de la variación fenotípica, se pueden comprobar factores ambientales, como la temperatura, la humedad o la nutrición.
En el caso del fenotipo eyeless , se ha determinado experimentalmente que tanto el fondo genético como los factores ambientales influyen en su expresión.

Fondo genético: supresión y efectos de posición

Con sólo ciertas excepciones, es difícil estimar el efecto específico del fondo genético y la expresión de un gen responsable para determinar el fenotipo potencial.
Dos de los efectos mejor caracterizados del fondo genético se describen a continuación.

Primero, la expresión de otros genes del genoma puede tener efecto sobre el fenotipo producido por el gen en cuestión.
El caso de los genes supresores es un ejemplo.
Genes mutantes, como suppressor of sable (su-s), suppressor of forked</foreign >(su-f) y suppressor of Hairy-wing ( su-Hw ) restauran parcial o completamente el fenotipo normal de un organismo homozigótico (o hemizigótico) para sable, forked y Hairy-wing, respectivamente. forked .
Por ejemplo, moscas hemizigóticas para forked (una mutación de las quetas) y su-f tienen quetas normales.

El fenómeno de la supresión se da en una gran variedad de organismos.
En los microorganismos, en donde los estudios moleculares son más fáciles de realizar, algunos genes supresores codifican moléculas que actúan durante el proceso de traducción.
Por ejemplo, hay mutaciones en los RNA transferentes ( tRNA ), que son genes que suprimen la expresión de mutaciones presentes en otros genes.
En la traducción, el tRNA alterado lee mal la información mutante presente en el mRNA , restaurando la función silvestre del producto génico.
La consecuencia es la supresión del fenotipo mutante.

También se ha propuesto que el producto de un gen supresor podría proporcionar o activar una ruta metabólica alternativa, evitando un bloqueo en una ruta biosintética causada por una mutación primaria.
Tal situación también se ha documentado.
Los genes supresores son ejemplos excelentes de fondo genético que modifica efectos génicos primarios.

Segundo, la localización física de un gen en relación con otro material genético puede influir en su expresión.
Tal situación se denomina efecto de posición .
Por ejemplo, si un gen queda incluido en una translocación o inversión (mediante las que un fragmento de un cromosoma se recoloca o se reordena), la expresión del gen puede quedar afectada.
Esto es especialmente cierto si el gen se sitúa en o cerca de ciertas áreas de los cromosomas que son genéticamente inertes, que reciben el nombre de heterocromatina .

Un ejemplo de tal efecto de posición se da en hembras de Drosophila , heterozigóticas para el mutante de color de ojos white (w), recesivo y ligado al X.
Antes de la translocación, el genotipo w+/w da lugar a un color de los ojos rojo ladrillo de tipo silvestre. 
Sin embargo, si la región del cromosoma X que tiene el alelo silvestre w+ se transloca, de tal manera que el alelo quede cerca de una región heterocromática, se modifica la expresión del alelo w+ (Figura 7.2).
Por consiguiente, después de la translocación, el alelo dominante del alelo normal w+ se reduce.
En lugar de tener un color rojo, los ojos son variados, o marcados con manchas blancas y rojas.
Se produce un efecto de posición similar si una región heterocromática se recoloca junto al locus white en el cromosoma X.
Aparentemente, las regiones heterocromáticas inhiben la expresión de los genes adyacentes.
Loci de muchos otros organismos también presentan efecto de posición, proporcionando pruebas que la alteración de la disposición normal de la información genética puede modificar su expresión. 

Efectos de la temperatura

Debido a que la actividad química depende de la energía cinética de las sustancias que reaccionan, que a su vez depende de la temperatura circundante, podemos esperar que la temperatura influya en el fenotipo.
Por ejemplo, la hierba del asno produce flores rojas cuando se cultiva a 23°C y flores blancas a 18°C.
Los gatos siameses y los conejos Himalaya presentan pelaje oscuro en la nariz, orejas y patas debido a que la temperatura de esas extremidades del cuerpo es ligeramente mas fría que el resto.
En estos casos, parece que la enzima responsable para la formación de pigmento es funcional a las temperaturas más bajas que hay en las extremidades, pero pierde su función catalítica a las temperaturas ligeramente más elevadas que se encuentran en el resto del cuerpo.

Se dice que las mutaciones que están afectadas por la temperatura son condicionales y se denominan sensibles a la temperatura .
Se conocen ejemplos en una serie de organismos, como bacterias, virus, hongos y Drosophila .
En casos extremos, un organismo portador de un alelo mutante puede expresar un fenotipo mutante cuando se cultiva a una temperatura, pero expresar el fenotipo silvestre cuando se cultiva a otra temperatura.
Este tipo de efecto de la temperatura es útil para estudiar mutaciones que interrumpen procesos esenciales en la reproducción y el desarrollo, y por ello son, normalmente, detrimentales para el organismo.
Por ejemplo, virus bacterianos que llevan ciertas mutaciones sensibles a la temperatura pueden infectar a una bacteria cultivada a 42°C; la infección progresa hasta que el producto génico esencial es necesario y entonces se detiene; esta temperatura recibe el nombre de condición restrictiva .
Si se cultiva en una condición permisiva a 25°C, el producto génico es funcional, la infección progresa normalmente y se producen nuevos virus.
El uso de mutaciones sensibles a la temperatura, que pueden inducirse y aislarse, ha sido muy útil en el estudio de la genética de los virus. 

De igual manera, en Drosophila se han descubierto muchas mutaciones sensibles a la temperatura que afectan al desarrollo, a la morfología y al comportamiento. 
La mayoría son letales recesivos y han sido importantes en la disección genética de estos procesos.

Efectos de la nutrición

Otro ejemplo de mutaciones condicionales se refiere a la nutrición.
Como vimos en el Capítulo 6, en los microorganismos, las mutaciones que impiden la síntesis de moléculas nutritivas son muy corrientes.
Estos mutantes nutricionales surgen cuando una enzima esencial de una vía biosintética se inactiva.
En los micro-organismos, una bacteria que lleve tales mutaciones se denomina auxótrofa .
Si el producto final de una ruta bioquímica no se puede sintetizar, y si dicha molécula es esencial para el crecimiento y el desarrollo normales, la mutación impide el crecimiento y puede ser letal.
Por ejemplo, si el moho del pan Neurospora no puede sintetizar el aminoácido leucina, no se pueden sintetizar proteínas, a menos que se añada leucina al medio de cultivo.
Si se añade leucina, el efecto detrimental se supera.
Los mutantes nutricionales han sido cruciales en el estudio de la genética molecular y sirvieron de base para que George Beadle y Edward Tatum propusieran a principios de los años 40 que la función de un gen es producir una enzima (véase el Capítulo 13). 

En la especie humana se conoce una serie de circunstancias ligeramente diferentes.
La presencia o ausencia de ciertas sustancias en la dieta, que los individuos normales pueden consumir sin perjuicios, puede afectar gravemente a individuos con constituciones genéticas anormales.
A menudo, una mutación puede evitar que un individuo metabolice una sustancia que normalmente se encuentra en la dieta.
Por ejemplo, los afectados por la enfermedad genética fenilcetonuria no pueden metabolizar el aminoácido fenilalanina.
Aquellos con galactosemia no pueden metabolizar la galactosa.
Otros individuos no toleran el azúcar lactosa de la leche y se dice que presentan intolerancia a la lactosa .
Otros, con diabetes , no pueden metabolizar adecuadamente la glucosa.
En este caso se acumulan cantidades excesivas de la molécula en el cuerpo, lo que es tóxico, y da lugar a un fenotipo característico.
Sin embargo, si se reduce drásticamente, o se elimina, la ingestión de la molécula correspondiente en la dieta, se puede invertir el fenotipo asociado.

El caso de la intolerancia a la lactosa es un ejemplo de los principios generales implicados.
La lactosa es un disacárido que consta de una molécula de glucosa y otra de galactosa.
Está presente en un 7 por ciento en la leche humana y en un 4 por ciento en la leche de las vacas.

Para metabolizar la lactosa la especie humana necesita la enzima lactasa, que divide al disacárido.
En los primeros años de vida se producen cantidades adecuadas de lactasa .
Sin embargo, en mucho grupos étnicos y raciales, el nivel de esta enzima cae pronto drásticamente y los adultos se convierten en intolerantes a la leche.
El efecto fenotípico principal implica diarreas intestinales graves, flatulencia y calambres abdominales. 
Esta situación es particularmente prevalente en esquimales, africanos, asiáticos y nativos americanos, aunque no está limitada sólo a éstos.
En estas culturas, la leche normalmente se convierte en otros alimentos, como el queso, la mantequilla y el yogur.
De esta forma, la cantidad de lactosa se reduce significativamente y casi se pueden eliminar los efectos adversos.
En los Estados Unidos se comercializa leche bala en lactosa, y para ayudar la digestión de otros alimentos que contienen lactosa, la lactasa es ahora un producto comercial que se puede ingerir. 

Inicio de la expresión génica 

No todos los caracteres se expresan al mismo tiempo durante la vida de un organismo.
En muchos casos, la edad a la que el gen se expresa se corresponde con la secuencia normal del crecimiento y desarrollo.
En la especie humana, las fases prenatal, infantil, juvenil y adulta requieren tipos diferentes de información genética.
De manera similar, puede esperarse que muchas anomalías genéticas se manifiesten en estadios diferentes de la vida.
Los genes letales explican muchos de los frecuentes abortos espontáneos que se dan en las poblaciones humanas.
Se cree que estas mutaciones alteran productos génicos esenciales para el desarrollo prenatal.

En la especie humana, muchas enfermedades hereditarias graves normalmente no se manifiestan hasta después del nacimiento.
Por ejemplo, la enfermedad de Tay-Sachs , que se hereda como autosómica recesiva, es una enfermedad letal del metabolismo lipídico, que implica una enzima anormal, la hexosaminidasa A .
Sin embargo los recién nacidos parecen normales durante los primeros cinco meses.
Luego se da un progresivo deterioro y los niños afectados mueren antes de la edad de cuatro años.

El síndrome de Lesch-Nyhan , que se hereda como un carácter recesivo ligado al X, da lugar a un metabolismo anormal de los ácidos nucleicos (recuperación bioquímica de las bases púricas nitrogenadas), dando lugar a la acumulación de ácido úrico en sangre y tejidos, retraso mental, parálisis y automutilación de labios y dedos.
La enfermedad se debe a la mutación del gen que codifica a la hipoxantín guanosín fosforribosil transferasa (HPRT) .
Los recién nacidos son normales hasta los seis u ocho meses de edad, antes de la aparición de los primeros síntomas.
Otro ejemplo es el de la distrofia muscular de Duchenne (DMD) , una enfermedad recesiva ligada al X asociada con una degradación muscular progresiva.

Normalmente se diagnostica entre los tres y los cinco años de edad.
Incluso con intervención médica, la enfermedad es a menudo mortal a la edad de veinte años. 

Quizá la más variable de todas las enfermedades hereditarias humanas respecto de la edad de inicio es la trágica enfermedad de Huntington .
Se hereda como autosómica dominante y afecta a los lóbulos frontales del córtex cerebral, en donde durante más de una década se da una muerte celular progresiva.
El deterioro del cerebro viene acompañado por movimientos espasmódicos incontrolados, deterioro emocional e intelectual y, finalmente, la muerte.
Aunque su inicio se ha señalado en cualquier edad, se da más frecuentemente entre los 30 y los 50 años, con una aparición media a los 38 años.
Muy a menudo, la aparición temprana está relacionada con un padre varón afectado, aunque tal relación no es general entre los descendientes de varones con la anomalía.

Observaciones parecidas apoyan el concepto de que la expresión crítica de los genes normales varía a lo largo del ciclo biológico de los organismos, incluida la especie humana.
Los productos génicos pueden jugar papeles más esenciales en determinados momentos.
Además, parece que el ambiente fisiológico interno de un organismo cambia con la edad.

Anticipación genética 

En relación con el inicio de la expresión génica, se han obtenido muchas ideas estudiando casos en donde se había observado un patrón de expresión inicial muy definido.
El fenómeno de la anticipación genética se refiere a la aparición de una anomalía hereditaria con una edad de inicio progresivamente más temprana en generaciones sucesivas.
Como veremos, el inicio temprano está correlacionado con un aumento de la gravedad de la anomalía.

Al menos tres enfermedades humanas demuestran claramente anticipación genética: la distrofia miotónica (DM) , el retraso mental del X frágil y la atrofia muscular espinal y bulbar (enfermedad de Kennedy) .
Nos centraremos en la información derivada del estudio de la DM para discutir la anticipación genética.

La distrofia miotónica, el tipo más corriente de distrofia muscular en los adultos, es una enfermedad autosómica dominante, de gravedad y de inicio con la edad variables.
Los individuos afectados débilmente desarrollan cataratas cuando son adultos, con poca o ninguna debilidad muscular.
Los individuos gravemente afectados demuestran una miopatía más grave y pueden tener retraso mental.
En su forma más extrema, la enfermedad es mortal justo después del nacimiento.
Durante el pasado medio siglo, cuidadosas investigaciones han revelado que el aumento en la gravedad está correlacionado con su inicio más temprano, relacionando la enfermedad con la anticipación genética.

Estudios recientes sobre la distrofia miotónica han generado una gran conmoción entre los genéticos. 
En 1989, C. J. Howeler y colaboradores publicaron su estudio sobre 61 grupos de padres e hijos.
En 60 de los 61 casos la edad de inicio era más temprana en los hijos. 
En 1992 varios grupos de investigación habían descubierto una posible base molecular para este caso de anticipación genética.
Localizada en el cromosoma 19 ( 19ql.3 contiene el locus DM) hay una secuencia de DNA inestable, caracterizada por un trinucleótido (CTG) que se repite un número variable de veces.
El descubrimiento notable es la correlación entre el tamaño de la repetición del trinucleótido y tanto la gravedad como la edad de inicio de la enfermedad.
En las generaciones siguientes el tamaño del segmento repetido aumenta.
Los individuos normales tienen como promedio 5 copias; los individuos mínimamente afectados tienen unas 50 copias, mientras que los individuos gravemente afectados presentan
¡hasta 1.000 copias! 

Aunque no está claro de qué manera la expansión del tamaño de esta región asociada con el gen afecta al inicio y a la expresión fenotípica, la correlación es muy elevada.
Es muy interesante que tanto el síndrome del X frágil como la enfermedad de Kennedy presenten también asociación entre la amplificación del DNA asociada al gen y la gravedad de la enfermedad.
Recientemente se ha encontrado un hecho similar en el gen que codifica la enfermedad de Huntington.

Volveremos a este tema en el Capítulo 14.
Por ahora, sólo podemos esperar ansiosamente más información que relacione el defecto molecular con el fenómeno de la anticipación genética.

Impronta genómica (paterna) 

Una de las principales excepciones a las suposiciones subyacentes a las leyes de la herencia de Mendel se refiere al caso en donde la expresión genética varía dependiendo del origen paterno o materno del cromosoma que lleva un gen concreto. 
El fenómeno se denomina impronta genómica (o paterna) [1] .
En algunas especies, ciertas regiones de los cromosomas y de los genes que se encuentran dentro de éstas, parecen tener memoria, o una «marca,» de su origen paterno, que influye en su expresión genética.
En consecuencia, genes concretos se expresan o permanecen silenciosos genéticamente, es decir, no se expresan debido a que están marcados. 

Por ejemplo, en la enfermedad de Huntington, como se señaló anteriormente, el inicio temprano de la enfermedad ocurre más a menudo cuando el gen mutante se hereda del padre.
En la distrofia miotónica ocurre exactamente lo contrario. 
En esta enfermedad, cuando se observa un inicio temprano, el hijo afectado normalmente hereda el gen de su madre.

Se piensa que la impronta ocurre antes o durante la formación de los gametos, dando lugar a genes (o regiones cromosómicas) marcados diferencialmente en el tejido que forma el esperma respecto del que forma los óvulos.
El proceso es claramente diferente de la mutación, debido a que en el caso citado anteriormente, el marcado puede invertirse en generaciones siguientes, cuando estos genes autosómicos pasan de la madre al hijo, a la nieta, y así sucesivamente.

Otro ejemplo de impronta es la inactivación de uno de los cromosomas X en las hembras de los mamíferos.
Como discutiremos en el Capítulo 9, hay un mecanismo de compensación de dosis mediante el cual ocurre una inactivación aleatoria del cromosoma X paterno o materno durante el desarrollo embrionario.
Sin embargo, en los ratones, antes del desarrollo del propio embrión, se produce la impronta en los tejidos en los que el cromosoma X de origen paterno es inactivado genéticamente en todas las células, mientras que los genes del cromosoma X de origen materno permanecen activos.
Cuando posteriormente se inicia el desarrollo embrionario, la marca es «eliminada» y puede darse una inactivación aleatoria del cromosoma X paterno o materno.

En 1991 se dispuso de información más concreta, cuando se encontró que tres genes específicos del ratón sufren impronta.
Uno de ellos es el gen que codifica el factor II (Igf2) del crecimiento, parecido a la insulina. 
Un ratón que lleva dos alelos normales de este gen tiene tamaño normal, mientras que un ratón que lleva los dos alelos mutantes carece del factor de crecimiento y es enano.
El tamaño de un ratón heterozigótico (un alelo normal y otro mutante, véase Figura 7.3) depende del origen paterno del alelo normal.
El ratón tiene tamaño normal si el alelo normal vino del padre, pero es enano si el alelo normal vino de la madre.
De esto se puede deducir que el alelo normal del gen Igf2 está marcado para funcionar deficientemente durante la producción de los óvulos en las hembras pero funciona normalmente cuando pasa a través del tejido productor de esperma de los machos.

La continuación de la impronta depende de si el gen pasa a la siguiente generación a través de los tejidos formadores de esperma o de los tejidos formadores de óvulos.
Por ejemplo, un macho heterozigoto de tamaño normal (caso anterior) pasará un alelo de tipo silvestre con «funcionamiento normal» a la mitad de sus descendientes que contrarrestará al alelo mutante recibido de la madre.

En la especie humana hay dos enfermedades genéticas que se cree están ocasionadas por impronta diferencial de la misma región del cromosoma 15 ( lSql ).

En ambos casos, las enfermedades parece que se deben a una delación idéntica de esta región en uno de los miembros del par cromosómico 15.
La primera enfermedad, el síndrome de Prader-Willi (SPW) aparece sólo cuando permanece sin delecionar un cromosoma materno.
Si permanece sólo un cromosoma paterno sin delecionar aparece una enfermedad totalmente diferente, el síndrome de Angelman (SA) .

El síndrome de Prader-Willi es claramente diferente del SA.
En el primero se advierte retraso mental además de una anomalía grave en la alimentación caracterizada por un apetito incontrolado, obesidad y diabetes.
Por otro lado, el SA da lugar a unas manifestaciones clínicas diferentes que implican tanto al comportamiento como al retraso mental.
Se puede concluir que la región 15ql queda marcada de manera diferente en los gametos masculinos que en los femeninos y que es necesario para un desarrollo normal tanto la región materna como la paterna.

La impronta genómica ha recibido recientemente un gran impulso investigador; muchas cuestiones permanecen sin respuesta. 
No se sabe cuántos genes están sujetos a impronta ni su papel en el desarrollo.
Aunque parece que se marcan regiones de cromosomas en lugar de genes concretos, el mecanismo molecular de la impronta está todavía sujeto a discusión.
Se ha supuesto que pueda estar implicada la metilación del DNA .
En los vertebrados se pueden añadir grupos metilo al carbono en posición 5 de la citosina (véase el Capítulo 10) como consecuencia de la actividad de la enzima DNA metil transferasa .
Se añaden grupos metilo cuando en la cadena de DNA se encuentra el dinucleótido CpG o grupos de CpG (llamadas islas CpG).
La metilación del DNA es un mecanismo razonable para establecer una marca molecular, ya que hay algunos datos de que un elevado nivel de metilación puede inhibir la actividad génica y que los genes activos (o sus secuencias reguladoras) están a menudo sin metilar. 
Además, en el ratón se ha advertido variación en la metilación de los genes que sufren impronta.
Cualquiera que sea la causa de este fenómeno, es un tema fascinante, que recibirá considerable atención en futuros estudios.

Variación continua y poligenes 

En el Capítulo 4 consideramos de qué manera los patrones clásicos de herencia mendeliana (por ejemplo, las proporciones de F2 3:1 y 9:3:3 :1 ) se modifican debido a la interacción génica o a la acción de genes múltiples sobre un carácter.
En aquel contexto discutimos ejemplos de caracteres poligénicos , como la herencia del color del grano en el trigo, un carácter controlado por tres pares de genes.
Estos caracteres se discutieron para ilustrar que, incluso en patrones complejos de herencia, los principios fundamentales de la segregación y de la transmisión independientes descubiertos por Mendel son operativos.

Habiendo considerado los patrones de herencia característicos de los caracteres poligénicos en discusiones anteriores, en este capítulo describiremos los métodos utilizados por los genéticos para estudiar los caracteres controlados por varios genes.
Estos métodos son a menudo estadísticos e implican el análisis de caracteres utilizando herramientas matemáticas, además de las propias de la biología molecular o de la bioquímica.
Los caracteres poligénicos son el centro de varias disciplinas genéticas, como la mejora vegetal y animal y la gestión de especies silvestres. 
Los caracteres poligénicos son también parte importante de la genética humana; se cree que caracteres como la inteligencia, el color de la piel, la obesidad y la predisposición a ciertas enfermedades se encuentran bajo el control de poligenes. 
En la siguiente discusión, recuerde que en la herencia poligénica, como en la herencia monogénica, salvo accidentes, el genotipo se fija en el momento de la fecundación, mientras que el fenotipo es más flexible y cambia durante la vida del organismo a medida que el genotipo interactúa con el ambiente.

Figura 7.1

Gradación en el fenotipo, desde el tipo silvestre a la ausencia de ojos, asociada con la mutación eyeless de Drosophila

Figura 7.2

Fenotipo del ojo en dos hembras de Drosophila, heterozigóticas pan el gen white

(a) Fenotipo dominante normal, con los olas de color rolo ladrillo.
(b) Color variado del ojo motivado por la reordenación del gen white en otra localización del genoma.

Figura 7.3

Efecto de la impronta en el gen Igf2 del ratón, que da lugar en homozigosis a ratones enanos

Los descendientes heterozigotos, que reciben el alelo normal de su padre, tienen tamaño normal.
Los heterozigotos que reciben el alelo normal de su madre, que ha sido «marcado» (i), son enanos.

[5] En Inglés, imprinting. 
A veces se ha traducido por huella.
La definición que da el Diccionario de la R.A E. del término impronta, en su acepción biológica, es casi la traducción literal de la definición que da The Random House Dictionary of the English Language para la palabra imprintig .
El término proviene de la Etología y es la traducción del término alemán utilizado por vez primera por Konrad Lorenz.

Variación cromosómica y determinación del sexo

CONCEPTOS DEL CAPÍTULO

La información genética de un organismo diploide se encuentra en delicado equilibrio, tanto en su contenido como en su localización en el genoma. 
Como consecuencia del estudio de la variación de los cromosomas sexuales, se han obtenido importantes ideas sobre el modo de determinación del sexo en muchos organismos. 
Otros estudios han revelado que un cambio en el número de cromosomas o en la ordenación de regiones de un cromosoma da lagar a menudo a variación fenotípica o a interrupción del desarrollo del organismo.
Ya que el cromosoma es la unidad de transmisión en la meiosis, tales variaciones pasan a los descendientes de un modo predecible, dando lugar a machas situaciones genéticamente informativas.

Hasta este momento hemos subrayado de qué manera las mutaciones y los alelos resultantes afectan al fenotipo de un organismo, y cómo pasan los caracteres de padres a hijos de acuerdo con los principios mendelianos.
En este capítulo estudiaremos la variación fenotípica que se produce como consecuencia de cambios en el material genético que son más importantes que la alteración de genes concretos.
Estos implican modificaciones al nivel de los cromosomas.

Aunque la mayoría de los miembros de las especies diploides tienen normalmente dos dotaciones haploides exactas de cromosomas, se conocen algunos casos con variaciones de este patrón.
Las modificaciones incluyen variación en el número de cromosomas concretos así como reordenaciones del material genético dentro o entre cromosomas.
Tales cambios se denominan, en conjunto, mutaciones cromosómicas o aberraciones cromosómicas , para distinguir tales alteraciones genéticas de las mutaciones génicas.
Ya que el cromosoma es la unidad de transmisión de acuerdo con las leyes de Mendel, las aberraciones cromosómicas se transmiten a los descendientes de una manera predecible, dando lugar a muchos ejemplos interesantes de variación fenotípica heredable.

En este capítulo consideraremos muchos tipos de aberraciones cromosómicas, sus consecuencias para el organismo y lo que podemos aprender de estas situaciones.

También discutiremos su papel en la evolución. 
Veremos que los c genéticos de un organismo diploide se encuentran en un delicado equilibrio en cuanto a su contenido y localización en el genoma.
Incluso pequeñas alteraciones del contenido o localización pueden dar lugar a algún tipo de variación fenotípica, mientras que cambios más sustanciales suelen ser letales, especialmente en animales.

Comenzaremos este capítulo con el análisis de la variación de los cromosomas sexuales en la especie humana y en Drosophila.
Estos estudios han conducido a una comprensión de cómo está determinado el sexo en estos organismos.
También estudiaremos otros temas relacionados con la función genética de los cromosomas sexuales.

Variación en el número de cromosomas: generalidades

Antes de embarcarnos en la discusión de las variaciones que implican al número de cromosomas en los organismos y en la determinación del sexo, es útil establecer la terminología para describir tales cambios.
La variación en el número de cromosomas va desde la adición o pérdida de uno o más cromosomas a la adición de una o más dotaciones haploides de cromosomas.
Cuando un organismo gana o pierde uno o más cromosomas, pero no una dotación completa, se origina la aneuploidía .
La pérdida de un único cromosoma da lugar a monosomía .
La ganancia de un cromosoma en un genoma diploide resulta en una trisomía .
Estas anomalías contrastan con la situación de euploidía , en donde están presentes dotaciones haploides completas de cromosomas.
Si hay tres o más dotaciones, se aplica el término más general de poliploidía . 
Aquellos que tiene tres dotaciones son específicamente triploides , con cuatro dotaciones tetraploides , y así sucesivamente.
En la Tabla 9.1 se proporciona un útil compendio de la terminología para poder seguirla cuando discutamos cada una de estas categorías y subgrupos.

Tabla 9.1

Terminología de la variación en el número de cromosomas

Término.
Explicación.


Aneuploidía.
2n más o menos cromosomas .


Monosomía.
2n1 .


Trisomía.
2n+ 1 .


Tetrasomía, pentaploidía, etc. .
2n+2,2n+3 , etc. .


Euploidía.
Múltiplos de n.


Diploidía.
2n.


Poliploidía .
3n, 4n, 5n,...


Triploidía .
3n.


Tetraploidía, pentaploidía, etc. .
4n, 5n, etc. .


Autopoliploidía .
Múltiplos del mismo genoma.


Alopoliploidía (anfidiploidía).
Múltiplos de diferentes genomas.


El número diploide de cromosomas en la especie humana 

Cuando se observaron por primera vez células en división de la especie humana, los genéticos intentaron determinar el número exacto de cromosomas de nuestra propia especie.
El primer intento serio se llevó a cabo en 1912, cuando H. Von Winiwarter contó 47 cromosomas en una preparación metafásica de espermatogonia. 

En aquel momento, los genéticos creían que el mecanismo de la determinación del sexo en la especie humana se basaba en la presencia de un cromosoma extra en las mujeres, es decir, se creía que las mujeres tenían 48 cromosomas.
En los años 20, Theophilus Painter observó entre 45 y 48 cromosomas en células de tejido testicular y también descubrió al pequeño cromosoma Y, que se sabe se encuentra sólo en los varones. 
En su trabajo, Painter se inclinó por 46 como el número diploide en la especie humana, pero más tarde concluyó, incorrectamente, que había 48 cromosomas tanto en varones como en mujeres.

Este número se aceptó durante 30 años. 
Más tarde, en 1956, Joe Hin Tjio y Albert Levan introdujeron mejoras en las técnicas de preparación de cromosomas.
Estas mejoras técnicas condujeron a una preparación muy clara de estados metafásicos que demostraba que el número diploide de cromosomas en la especie humana es realmente 46.
Más tarde, ese mismo año, C. E. Ford y John L. Hamerton, trabajando también con tejido testicular, confirmaron este hallazgo.

Dentro de las 23 parejas normales de cromosomas humanos, se vio que una pareja variaba en su aspecto entre varones y mujeres. 
Estos dos cromosomas se designaron como los cromosomas sexuales X e Y.
La mujer tiene dos cromosomas X y el varón un cromosoma X y otro Y.
Sin embargo, como veremos, estas observaciones son insuficientes para concluir que el cromosoma Y determina la masculinidad.

Cromosomas, diferenciación del sexo y determinación del sexo en la especie humana 

En nuestra discusión sobre el ligamiento al X en el Capítulo 4, señalábamos que, como parte de la dotación diploide de cromosomas, tanto en la especie humana como en Drosophila, las hembras poseen dos cromosomas X mientras que los machos tienen un cromosoma X y un cromosoma Y.
Esta observación nos podría llevar a concluir que el cromosoma Y es responsable de la masculinidad en ambas especies; sin embargo, esto no es necesariamente así. 
Quizá la ausencia de un segundo cromosoma X de alguna manera da lugar a la masculinidad, mientras que el Y no jugaría un papel en la determinación del sexo. 
Quizá la presencia de dos cromosomas X dé lugar a la feminidad mientras que él Y no juega ningún papel.

Las pruebas que clarifican cuál de las explicaciones es la correcta tuvieron que esperar al estudio de las variaciones en la composición de cromosomas sexuales, tanto en la especie humana como en las moscas.
Como veremos, la primera explicación de que el Y determina la masculinidad, es cierta en la especie humana pero no lo es en Drosophila .

Síndromes de Klinefelter y de Turner

Hacia 1940 se observó que dos anormalidades en la especie humana, los síndromes de Klinefelter y de Turner , se caracterizan por un desarrollo sexual anormal.
Los individuos con el síndrome de Klinefelter tienen genitales y conductos internos que son masculinos, pero sus testículos están poco desarrollados y no producen esperma.
Aunque hay un desarrollo masculino, el desarrollo sexual femenino no está totalmente suprimido. 
Por ejemplo, es corriente un ligero crecimiento de las glándulas mamarias.
Este desarrollo sexual ambiguo se denomina intersexual , y puede dar lugar a un desarrollo social anormal.

En el síndrome de Turner, los individuos afectados tienen genitales externos y conductos internos femeninos, pero los ovarios son rudimentarios.
Otras anormalidades características incluyen estatura baja (normalmente por debajo del metro y medio), cuello ancho y tórax en escodo.

En 1959 se determinaron, independientemente, los cariotipos de individuos con estos síndromes y se vio que eran anormales en relación con los cromosomas sexuales.
Los individuos con el síndrome de Klinefelter eran muy a menudo trisómicos y tenían, además de los 44 autosomas, un complemento XXY [Figura 9.1(a)].
Las personas con este cariotipo se designan 47,XXY .

Los individuos con el síndrome de Turner son, muy a menudo, monosómicos y tienen sólo 45 cromosomas, incluido un único cromosoma X.
Se designan 45,X [Figura 9.1(b)].
Advierta la convención utilizada para designar las constituciones cromosómicas anteriores.
El número indica cuántos cromosomas hay y la información después de la coma indica la desviación respecto del contenido diploide normal. 
Ambas situaciones son consecuencia de una no disyunción de los cromosomas X en la meiosis (véase la Figura 2.12). 

Estos cariotipos y sus correspondientes fenotipos sexuales nos permiten concluir que el cromosoma Y determina la masculinidad en la especie humana.
En ausencia del Y, el sexo del individuo es femenino, incluso si sólo hay presente un cromosoma X.
La presencia del cromosoma Y en el individuo con el síndrome de Klinefelter es suficiente para determinar su masculinidad, aun cuando su expresión no sea completa.
Igualmente, en ausencia de un cromosoma Y, como en el caso de los individuos con el síndrome de Turner, no se da la masculinización. 

El síndrome de Klinefelter ocurre en unos 2 nacimientos masculinos cada 1.000.

Los cariotipos 48,XXXY , 48,XXYY , 49,XXXXY y 49,XXXYY son similares fenotípicamente al 47,XXY , pero las anomalías son a menudo más graves en individuos con un mayor número de cromosomas X.
Cariotipos distintos al 45,X también dan lugar al síndrome de Turner.
Incluyen individuos que presentan en sus tejidos dos líneas celulares genéticamente diferentes, cada una con un cariotipo distinto.
Tales individuos se denominan mosaicos .
Las dos líneas celulares son el resultado de errores en la mitosis en las primeras etapas del desarrollo, siendo las combinaciones cromosómicas más corrientes 45,X/46,XY y 45,X/46,XX .
Así, un embrión que comienza su desarrollo con un cariotipo normal puede dar lugar a un individuo cuyas células presenten una mezcla de cariotipos y expresen este síndrome.

El síndrome de Turner se observa sólo en 1 de cada 3.000 nacimientos femeninos, una frecuencia mucho más baja que la del síndrome de Klinefelter.
Una explicación de esta diferencia es el hecho de que una proporción importante de fetos 45,X muere en el útero y se abortan espontáneamente.

Por ello, podría darse una frecuencia similar de los dos síndromes en el momento de la fecundación. 

Síndrome 47,XXX

La presencia de tres cromosomas X junto con la dotación normal de autosomas (47,XXX) da lugar a diferenciación femenina.
Este síndrome, que se estima ocurre en 1 de cada 1.200 nacimientos femeninos, es muy variable en su expresión.
Frecuentemente, las mujeres 47,XXX son perfectamente normales.
En otros cases pueden darse un menor desarrollo de las características sexuales secundarias, esterilidad y retraso mental.
En casos raros se han encontrado cariotipos 48,XXXX y 49,XXXXX .
Los síndromes asociados con estos cariotipos son similares al del 47,XXX , pero más pronunciados.
Así, en muchos casos, la presencia de cromosomas X extras parece romper el delicado equilibrio de la información genética, esencial para un desarrollo femenino normal.

Caso 47,XYY

Se ha descubierto otra trisomía que implica a los cromosomas sexuales en la especie humana, la 47,XYY , que ha sido intensamente investigada.
Los estudios de este caso, en donde la única desviación de la diploidía es la presencia de un cromosoma Y adicional en un cariotipo masculino, por otra parte normal, ha dado lugar a una interesante controversia.

En 1965, Patricia Jacobs descubrió en una prisión de máxima seguridad escocesa a 9 varones, de 315, que tenían el cariotipo 47,XYY.
Estos varones tenían una estatura significativamente por encima de la media y habían estado implicados en actos criminales de graves consecuencias sociales.
De los nueve varones estudiados, siete tenían una inteligencia por debajo de lo normal y todos ellos sufrían anomalías de la personalidad.
En otros estudios se obtuvieron resultados similares.

Debido a estas investigaciones, se ha examinado más ampliamente el fenotipo y la frecuencia de los casos 47,XYY en población criminal y no criminal (véase la Tabla 9.2).
Se ha confirmado su estatura por encima de la media (normalmente por encima de los 180 cm ) y su menor inteligencia, y la frecuencia de los varones que manifiestan este cariotipo es realmente superior en instituciones penales y mentales comparado con la de varones no encarcelados.

La probable correlación entre esta situación cromosómica y un comportamiento antisocial y criminal ha sido de un interés considerable.
Una cuestión particularmente importante radica en las características manifestadas por los varones XYY que no han sido encarcelados. 
La única asociación casi constante es que tales individuos tienen una estatura por encima de los 180 cm . 

Se inició un estudio sobre este tema para identificar individuos 47,XYY en el momento del nacimiento y seguir sus patrones de comportamiento durante su desarrollo juvenil y adulto.
En 1974, los dos principales investigadores, Stanley Walzer y Park Gerald, habían identificado unos 20 recién nacidos XYY entre 15.000 nacimientos en el Hospital Maternal de Boston.
Sin embargo, muy pronto recibieron grandes presiones para que abandonaran sus investigaciones.

Los opuestos al estudio argüían que la investigación no podía justificarse y podría causar graves perjuicios a aquellos individuos que presentaran dicho cariotipo.
Argüían que (1) no se había establecido anteriormente asociación entre un cromosoma Y adicional y un comportamiento anormal en poblaciones en general y (2) «marcando» a estos individuos se podría crear una profecía autorrealizante.
Es decir, como resultado de la participación en el estudio, los padres, parientes y amigos podrían tratar a aquellos identificados como 47,XYY de manera diferente, conduciéndoles quizá al tipo de comportamiento antisocial esperado en ellos.
A pesar del apoyo de fondos de una agencia gubernamental y de la facultad de la Escuela de Medicina de Harvard, Walzer y Gerald abandonaron sus investigaciones en 1975.

Debido a que está claro ahora que muchos varones XYY no presentan ninguna forma de comportamiento antisocial y llevan vidas normales, debemos concluir que hay una correlación alta, pero no constante, entre el cromosoma Y extra y el comportamiento.

Tabla 9.2

Frecuencia de individuos XYY en varias muestras

Muestra.
Restricción.
Número estudiado.
Número XYY.
Frecuencia XYY.


Población control.
Recién nacidos.
28.366.
29.
0,10%.


Mental-penal.
Sin restricción de altura.
4.239.
82.
1,93.


Penal.
Sin restricción de altura.
5.805.
26.
0,44.


Mental.
Sin restricción de altura.
2.562.
8.
0,31.


Mental-penal.
con restricción de altura.
1.048.
48.
4,61.


Penal.
Con restricción de altura.
1.683.
31.
1,84.


Mental.
Con restricción de altura.
649.
9.
1,38.


Diferenciación sexual en la especie humana

Una vez que los investigadores establecieron que en la especie humana es el cromosoma Y el que alberga la información genética necesaria para la masculinidad, se realizaron esfuerzos para identificar el gen 0 genes específicos capaces de proporcionar la «señal» responsable de la determinación del sexo .
Antes de proseguir con la discusión de este tema, es importante introducir el concepto de diferenciación sexual a fin de comprender mejor cómo aparecen los adultos masculinos y femeninos en la especie humana.
En las primeras etapas del desarrollo, cada embrión pasa por un periodo en el que es potencialmente hermafrodita o bisexual. 
Los primordios gonadales surgen como un par de crestas asociadas con cada riñón embrionario.
Las células gonadales primordiales migran a estas crestas en donde se forma un córtex externo y una médula interna.
El córtex es capaz de desarrollarse en ovario y la médula interna se puede desarrollar en testículo.
Además, en cada embrión hay dos series de conductos indiferenciados masculinos (wolffianos) y femeninos (mullerianos).

La cresta genital aparece después de las cinco semanas de gestación, y si estas células tienen una constitución XY, se puede detectar a la séptima semana un desarrollo masculino de la región medular.
Sin embargo, en ausencia del cromosoma Y, no se inicia el desarrollo masculino y la cresta genital se destina posteriormente para formar tejido ovárico.
Luego se da el desarrollo paralelo de los sistemas de conductos apropiados masculinos o femeninos, y el otro sistema de conductos degenera.
Una enorme cantidad de pruebas indican que en los varones, una vez se ha iniciado la diferenciación de los testículos, el tejido testicular embrionario segrega una hormona que es necesaria para continuar con la diferenciación sexual masculina.

En ausencia de desarrollo masculino, se da la diferenciación de los ovarios y del sistema de conductos femeninos.
Cuando se aproxima la duodécima semana de desarrollo fetal, las oogonias de los ovarios comienzan la meiosis y se pueden detectar oocitos primarios.
Hacia la vigésimo quinta semana de gestación, la meiosis de todos los oocitos se detiene, permaneciendo inactivos hasta la pubertad, que se alcanza entre 10 y 15 años más tarde.
Por otro lado, los espermatocitos primarios no se forman en los varones hasta que se alcanza la pubertad.

Podemos concluir que la información genética que se encuentra en los cromosomas sexuales es responsable de los fenómenos primarios de la determinación sexual. 

En condiciones normales, una vez que ha comenzado el desarrollo de los testículos o de los ovarios, hay una posterior diferenciación temprana bajo la influencia de las hormonas sexuales masculinas o femeninas.
Estas hormonas apoyan, si es que no determinan, la expresión de otras características sexuales secundarias durante el desarrollo.

El cromosoma Y y el desarrollo masculino 

Antes de volver a otros tipos de variación cromosómica y a sus efectos en la especie humana y en otros organismos, es conveniente revisar la información disponible acerca de cómo el cromosoma Y da lugar al desarrollo de varones en la especie humana.
Hemos aludido anteriormente al hecho de que este cromosoma, a diferencia del X, está casi vacío de genes.
Aunque comparte sólo una homología limitada con loci del cromosoma X, lleva información genética que controla el desarrollo sexual.

Por consiguiente, hay alguna región en el cromosoma Y, probablemente un gen, que es responsable de la codificación de un producto llamado el factor de la determinación testicular (TDF) , un producto génico que, de alguna manera, induce a que el tejido gonadal indiferenciado del embrión forme testículos.
En su ausencia se da desarrollo femenino.
La investigación se ha centrado exactamente en lo que constituye la región y el producto.

Está claro en la actualidad que en una pequeña parte del cromosoma Y se encuentra el gen llamado SRY (región del Y que determina el sexo) que codifica a TDF.
Las pruebas de que SRY es el gen responsable se apoyan en la habilidad de los genéticos moleculares en identificar la presencia o ausencia de secuencias de DNA en los escasos individuos cuya composición de cromosomas sexuales no se corresponde con su fenotipo sexual.
Por ejemplo hay varones que tienen dos cromosomas X y no tienen cromosoma Y.
Tienen unido a uno de sus cromosomas X la región del cromosoma Y que tiene el gen SRY.
Hay también mujeres que tiene un cromosoma X y un Y.
El Y ha perdido la región SRY.
Estas observaciones son argumentos poderosos a favor del papel del SRY en el desarrollo masculino.

La prueba final de que este gen es el responsable de la masculinidad proviene de un experimento utilizando ratones transgénicos .
Tales animales se producen a partir de huevos fecundados que tienen DNA foráneo que se les ha inyectado, el cual se ha incorporado a la constitución genética del embrión en desarrollo.
En los ratones normales se ha identificado una región cromosómica denominada Sry que es comparable a la SRY de la especie humana.
Cuando se inyecta DNA que tiene sólo el Sry de ratón en óvulos de ratón normales XX, la mayoría de los ratones se desarrollan como machos. 

Estos estudios sugieren que se ha identificado el gen pertinente. 
Se encuentra en todos los mamíferos examinados hasta el momento, habiéndose conservado a lo largo de la evolución.
De qué manera el producto de este gen dispara en el tejido gonadal embrionario el desarrollo de testículos en lugar de ovarios es en la actualidad una pregunta razonable para contestar y susceptible de investigación. 

Proporción de sexos en la especie humana

La presencia de cromosomas sexuales heteromórficos en un sexo de una especie pero no en el otro proporciona un mecanismo potencial para producir descendientes con igual proporción de machos que de hembras.
La proporción real de machos y hembras en los descendientes permite el cálculo de lo que se llama la proporción de sexos .
Se puede estimar de dos maneras.
La proporción de sexos primaria refleja la proporción de machos y hembras de una población en el momento de la concepción .
La proporción de sexos secundaria refleja la proporción entre los nacidos .
La proporción de sexos secundaria es mucho más fácil de determinar, pero tiene la desventaja de no contabilizar la mortalidad no proporcional embrionaria o fetal, si se da.

Cuando se determina la proporción sexual secundaria en las poblaciones humanas teniendo en cuenta los flatos censales mundiales, no sale igual a 1.
Por ejemplo, en la población blanca de los Estados Unidos la proporción secundaria es 1,06, indicando que nacen 106 varones por cada 100 mujeres. 
En la población negra de los Estados Unidos la proporción es 1,025.
En otros países el exceso de nacimientos masculinos es todavía mayor que el que reflejan estos valores.
Por ejemplo, en Corea, la proporción sexual secundaria es 1,15.

Para explicar la diferencia entre las proporciones sexuales primaria y secundaria, una posibilidad es que la mortalidad femenina prenatal sea mayor que la masculina.
Si fuera así, sería posible que la proporción sexual primaria fuera 1 y que se alterase antes del nacimiento.
Sin embargo, se ha demostrado que esta hipótesis es falsa.
De hecho, ocurre exactamente lo contrario.
En un estudio del Carnegie Institute, publicado en 1948, se determinó el sexo de unos 6.000 embriones o fetos recuperados de abortos. 
De acuerdo con los datos de este estudio, se estimó la proporción sexual primaria en 1,079.
Datos más recientes consideran que este número es incluso superior, entre 1,2 y 1,ó.

Por consiguiente, en las poblaciones humanas se conciben más varones que mujeres.

No está claro el porqué de tan importante desviación respecto de la proporción sexual primaria de 1.
Una posible explicación sólo puede derivarse examinando las suposiciones sobre las que se basa la proporción teórica:

1. Debido a la segregación, se produce igual número de esperma masculino con X que con Y.
2. Cada tipo de esperma tiene una viabilidad y movilidad equivalentes en el tracto reproductivo femenino.
3. La superficie del óvulo es igualmente receptiva para el esperma con X o con Y.

No hay pruebas experimentales claras que sugieran que alguna de estas suposiciones no sea válida.
Sin embargo, se ha especulado que, debido a que el cromosoma Y humano es más pequeño que el cromosoma X, el esperma que lleva Y tiene menos masa y por consiguiente es más móvil.
Si esto es cierto, entonces aumenta la probabilidad de que una fecundación dé lugar a un zigoto masculino, proporcionando una posible explicación de la proporción primaria observada.

Compensación de dosis en la especie humana

La presencia de dos cromosomas X en las mujeres normales y de un único cromosoma X en los varones normales es un caso singular cuando se compara con el número igual de autosomas presentes en las células de ambos sexos.
Sólo en teoría, es posible especular que esta disparidad daría lugar a un problema de «dosis génica» entre varones y mujeres para todos los genes ligados al X.
Las mujeres tienen dos copias y los varones sólo una.
Por ello, potencialmente, se podría producir el doble del producto génico en todos los genes ligados al X.
Los cromosomas X adicionales, tanto en varones como en mujeres, que se presentan en varios síndromes discutidos anteriormente en este capítulo, servirían para plantear este problema de dosis.
En esta sección describiremos ciertas investigaciones relativas a la expresión de genes ligados al X que demuestran la existencia real de un mecanismo genético subyacente de compensación de dosis .

Figura 9.1

Cariotipos y aspecto fenotípico de (a) el síndrome de Klinefelar (47,XXY) y (b) el síndrome de Turner (45X)

Se indica la composición cromosómica sexual que varía del cariotipo humano normal.

Estructura y análisis del DNA y del RNA

CONCEPTOS DEL CAPÍTULO

Salvo algunas excepciones, el DNA es el material genético de todos los seres vivos.
La estructura del DNA proporciona las bases químicas para almacenar y expresar la información genética en las células, as, como para transmitirla a las generaciones futuras.
La molécula tiene forma de hélice de doble cadena, estando ambas cadenas unidos por puentes de hidrógeno entre bases complementarias.
El RNA comparte muchas semejanzas con el DNA en cuanto a su estructura, pero está formado por una cadena sencilla.
En general, el RNA se utiliza en la expresión de la información genética, y sirve de material genético en algunos virus.

La 1a Parte del libro trata de la presencia en los cromosomas de genes que controlan las características fenotípicas así como la manera en que los cromosomas se transmiten a los descendientes a través de los gametos.
Por lógica, los genes deben contener algún tipo de información que, una vez pasada a una nueva generación, influye en la forma y en las características de los descendientes; esta información recibe el nombre de información genética .
También podríamos concluir que esta misma información debe de dirigir de alguna manera los complejos procesos que conducen a la forma adulta.

Hasta 1944, no quedó claro qué componente químico de los cromosomas formaba los genes y constituía el material genético.
Puesto que se sabía que los cromosomas estaban formados por ácidos nucleicos y proteínas, ambos c se consideraron candidatos. 
Sin embargo, en 1944 surgieron pruebas experimentales directas de que los ácidos nucleicos, en concreto el DNA, eran los que servían de base a la información en el proceso de la herencia.

Una vez reconocida la importancia del DNA en los procesos genéticos, se intensificó la investigación para conocer tanto la base estructural de esta molécula como la relación entre su estructura y su función. 
Entre 1944y 1953, muchos científicos buscaron información que pudiera responder a una de las preguntas más intrigantes de la historia de la biología: 
¿cómo puede servir el DNA de base genética para los procesos de la vida?
Dadas las complejas pero ordenadas funciones adscritas a la molécula de DNA, se creía que la respuesta dependería de su estructura química.

Estos esfuerzos se vieron recompensados en 1953 cuando James Watson y Francis Crick expusieron sus hipótesis sobre la naturaleza de doble hélice del DNA.
La suposición de que las funciones de la molécula se clarificarían más fácilmente tras determinar su estructura general fue correcta.
Este capitulo repasa inicialmente las pruebas de que el DNA es el material genético y luego se discute la interpretación de su estructura. 

Características del material genético

El material genético presenta varias características: replicación, almacenaje de la información, expresión de esta información, y variación por mutación .
La «replicación» del material genético es una de las facetas del ciclo celular, una propiedad fundamental de todos los organismos vivos. 
Una vez el material genético de las células se ha replicado, se debe repartir equitativamente en las células hijas.
En la formación de los gametos, el material genético se replica y se reparte de manera que cada célula recibe sólo la mitad de la cantidad original de material genético.
Este proceso recibe el nombre de meiosis .
Aunque los productos de la mitosis y de la meiosis son diferentes, ambos procesos forman parte del fenómeno general de la reproducción celular.

La característica de «almacenaje» debe verse como una información genética en la que están depositadas todas las características hereditarias de un organismo.

Sin embargo, esta información puede o no expresarse. 
Está claro que, aunque la mayoría de las células contienen un juego completo de DNA, en cualquier momento dado expresan sólo una parte de su potencial genético. 
Por ejemplo, las bacterias «activan» muchos genes solamente en respuesta a condiciones ambientales especificas, y los «desactivan» únicamente cuando esas condiciones cambian.
En los vertebrados, las células epiteliales pueden tener los genes de la melanina activos pero nunca activan los de la hemoglobina; las células digestivas activan muchos genes específicos para su función, pero no activan los de la melanina.

La necesidad de que el material genético tenga que codificar la casi infinita variedad de productos génicos que se encuentran en las innumerables formas de vida presentes en nuestro planeta es inherente al concepto de almacenaje.
El lenguaje químico del material genético debe ser capaz de realizar esta tarea ya que éste almacena información que se transmite a las células y organismos descendientes.

La «expresión» de la información genética almacenada es un proceso complejo y la base para el concepto de flujo de información en la célula.

La Figura 10.1 muestra un esquema simplificado de este concepto.
El suceso inicial es la transcripción del DNA, cuyo resultado es la síntesis de tres tipos de moléculas de RNA: RNA mensajero ( mRNA ), RNA transferente ( tRNA ) y RNA ribosómico ( rRNA ) .
De ellos, los mRNA se traducen en proteínas.
Cada tipo de mRNA es el producto de un gen especifico y dirige la síntesis de una proteína diferente.
La traducción se realiza en unión con los ribosomas, que contienen rRNA .
En ella participa también el tRNA , que actúa como adaptador para convertir la información química del mRNA en los aminoácidos que forman las proteínas.
En conjunto, estos procesos sirven de base para el dogma central de la genética molecular :
«el DNA fabrica el RNA, y éste a su vez produce las proteínas».

El material genético es también el origen de nueva «variabilidad» para los organismos por el proceso de mutación.
Si ocurre una mutación -un cambio en la composición química del DNA-, la alteración se reflejará en la transcripción y en la traducción, afectando a menudo a la proteína especificada.
Si una mutación está presente en los gametos, ésta pasará a las generaciones futuras y, con el tiempo, puede extenderse a la población. 
La variación genética, que a menudo incluye reordenaciones dentro y entre cromosomas (véase el Capítulo 9), proporciona la materia prima para el proceso de la evolución. 

El material genético: 1900-1944 

La idea de que el material genético se transmite de manera física de los progenitores a sus descendientes fue aceptada desde los inicios del concepto de herencia.
La investigación de la estructura de las biomoléculas, que se inició a finales del siglo XIX, progresó considerablemente, sentando las bases para la descripción del material genético en términos químicos. 
Tanto las proteínas como los ácidos nucleicos se consideraban como los mejores candidatos para desempeñar la función de material genético.
Hasta la década de los 40 muchos genéticos se inclinaban por las proteínas.
Tres factores contribuyeron J esta creencia.

Primero, las proteínas son abundantes en las células. 
Aunque el contenido en proteínas puede variar considerablemente, más del 50 por ciento del peso celular seco pertenece a estas moléculas.
Puesto que las células tienen tal cantidad y variedad de proteínas, no es de extrañar que los primeros genéticos creyeran que algunas de estas proteínas pudiesen tener la función de material genético.

El segundo factor fue la propuesta aceptada a principios y mediados del siglo XX sobre la estructura química de los ácidos nucleicos.
El químico suizo Friedrick Miescher fue el primero en estudiar el DNA, en 1868.
Tras separar los núcleos del citoplasma celular, aisló una substancia ácida de los núcleos a la que llamó nucleína .
Miescher demostró que la nucleína contenía grandes cantidades de fósforo pero no de azufre, característica que la diferenciaba de las proteínas.

Cuando las técnicas analíticas mejoraron se observó que los ácidos nucleicos, incluyendo el DNA, estaban formados por cuatro bloques moleculares similares llamados nucleótidos.
Hacia 1910, Phoebus A. Levene propuso la hipótesis del tetranucleótido para explicar la disposición química de los nucleótidos en los ácidos nucleicos.
Propuso que una unidad simple de cuatro nucleótidos, como la mostrada en la Figura 10.2, se repetía a lo largo del DNA.

Levene basó su propuesta en estudios de la composición de los cuatro tipos de nucleótidos.
Aunque sus datos reales mostraban que las proporciones de los cuatro nucleótidos variaban considerablemente, él supuso una proporción de 1:1:1:1.
Esta discrepancia fue atribuida a técnicas analíticas inadecuadas.

Como una estructura de cuatro nucleótidos unidos covalentemente es relativamente simple, los genéticos creyeron que los ácidos nucleicos no suministraban la suficiente cantidad de variación química que se esperaba para el material genético.
En cambio, las proteínas contienen 20 aminoácidos diferentes, proporcionando así la base para una variación substancial. 
El resultado de todo esto fue que se desvió la atención de los ácidos nucleicos, reforzando las especulaciones de que las proteínas eran el material genético. 

El tercer factor que contribuyó a estas creencias fue simplemente un asunto de las áreas de investigación más activas en genética.
Antes de 1940, la mayoría de los genéticos estaban ocupados en el estudio de la genética de la transmisión y de la mutación.
El entusiasmo generado en estas áreas diluyó sin duda la preocupación por encontrar la molécula precisa que servía de material genético. 

Así, las proteínas eran los candidatos más prometedores y fueron aceptados de manera más bien pasiva. 

Entre 1910 y 1930 se hicieron otras propuestas sobre la estructura de los ácidos nucleicos, pero en general fueron abandonadas en beneficio de la hipótesis del tetranucleótido. 
En la década de los 40, el trabajo de Erwin Chargaff hizo comprender que la hipótesis de Levene era incorrecta. 
Chargaff mostró que, en la mayoría de organismos, la relación de 1:1:1:1 era inexacta, refutando así la hipótesis de Levene.

Pruebas a favor del DNA como material genético en bacterias y en bacteriófagos

El suceso inicial que condujo a la aceptación del DNA como material genético fue la publicación en 1944 de Oswald Avery, Colín MacLeod y Maclyn McCarty sobre la naturaleza química de un "principio transformante". 
Este trabajo, junto con posteriores descubrimientos por parte de otros equipos de investigación, constituyó la prueba experimental directa de que, en los organismos estudiados, el DNA, y no las proteínas, es la biomolécula responsable de la herencia.
Este periodo marcó el inicio de una era de descubrimientos en biología que ha revolucionado el conocimiento de la vida sobre la Tierra.
El impacto de estos descubrimientos corre paralelo al trabajo que siguió a la publicación de la teoría de la evolución de Darwin y al redescubrimiento de los postulados de Mendel sobre genética de la transmisión, constituyendo todas ellas tres grandes revoluciones en biología.

La prueba inicial que señaló al DNA como material genético proviene de estudios en bacterias procariotas y de los virus que las infectan.
Las razones para usar bacterias y virus bacterianos se harán patentes a medida que se discutan los experimentos.
La razón principal es que las bacterias y los virus son capaces de mantener un rápido crecimiento, completando su ciclo biológico en horas. 
El hecho de que se puedan manipular experimentalmente así como el que puedan inducirse y seleccionarse mutaciones con facilidad las hace ideales para experimentos de este tipo. 

Experimentos de transformación 

La investigación que proporcionó la base para el trabajo de Avery, MacLeod y McCarty la inició Frederick Griffitb, oficial médico del Ministerio de Sanidad Británico, en 1927.
Realizó experimentos con varias cepas diferentes de la bacteria Diplococcus pneumoniae [0] .
Algunas cepas eran virulentas , es decir, causan neumonía en algunos vertebrados (principalmente en la especie humana y en el ratón), mientras que otras no lo eran ( avirulentas ), es decir, no causan la enfermedad.

La diferencia de carácter virulento viene dada por la cápsula de polisacáridos de la bacteria.
Las cepas virulentas están encapsuladas, mientras que las no virulentas no lo están.
Las bacterias sin cápsula son fagocitadas y destruidas fácilmente por las células fagocitarias del sistema circulatorio de los animales.
Las bacterias virulentas, que poseen la cubierta de polisacáridos, son difíciles de fagocitar; por lo tanto, pueden multiplicarse y causar la neumonía. 

La base para otra diferencia característica entre las cepas virulentas y las no virulentas radica en la presencia o ausencia de cápsula.
Las bacterias encapsuladas forman colonias lisas (llamadas S, del inglés smooth ) al cultivarlas en placas de agar; las cepas no encapsuladas producen colonias rugosas (R) (Figura 10.3).

Esta característica permite distinguir fácilmente las cepas virulentas de las no virulentas mediante técnicas microbiológicas normales de cultivo.

Cada cepa de Diplococcus puede estar formada por una docena de tipos diferentes denominados serotipos .
La especificidad del serotipo viene dada por la estructura química fina del polisacárido que forma la gruesa y viscosa cápsula.
Los serotipos se identifican mediante técnicas inmunológicas y se nombran generalmente con números romanos.
En los Estados Unidos, los tipos I y II son la causa más común de neumonía. 
En los experimentos esenciales que condujeron a los nuevos conceptos sobre el material genético, Griffith usó los tipos II y III.
La Tabla 10.1 resume las características de las dos cepas de Griffith.

A partir del trabajo de otros investigadores, Griffith sabía que sólo las células virulentas vivas producían neumonía en ratones.
Si se inyectan en el ratón bacterias virulentas muertas por calor, no provocan neumonía, del mismo modo que las bacterias no virulentas vivas tampoco la producen.
En su experimento decisivo (Figura 10.3), Griffith inyectó a ratones células vivas IIR (no virulentas) junto con células BIS (virulentas) muertas por calor.
Como ninguno de los dos tipos de células causaban la muerte de los ratones cuando se inyectaban por separado, Griffith esperaba que la doble inyección tampoco los mataría. 
Pero, después de cinco días, todos los ratones que habían recibido la doble inyección habían muerto.
¡El análisis de la sangre de los ratones muertos mostró grandes cantidades de bacterias vivas del tipo BIS (virulentas)!

Hasta donde se pudo determinar, el tipo de polisacárido de la cápsula de estas bacterias ms era idéntico al de la cepa BIS a partir de la que se había hecho el preparado celular muerto por calor.
Los ratones control, a los que para este experimento se les había inyectado bacterias no virulentas BR, no desarrollaron neumonía, manteniéndose sanos.
Este hecho anulaba la posibilidad de que las celarlas IIR no virulentas sencillamente hubiesen cambiado (o mutado) a células BIS virulentas en ausencia de la fracción BIS muerta por calor.
Por el contrario, se requería algún tipo de interacción entre las células IIR vivas y las BIS muertas por calor.

La conclusión de Griffith fue que las bacterias BIS muertas por calor eran responsables de alguna manera de convertir las células IIR no virulentas en BIS virulentas.
Llamó a este fenómeno transformación , y sugirió que el principio transformante podría ser alguna parte de la cápsula de polisacáridos o algún compuesto necesario para la síntesis de la cápsula, aunque la cápsula no causase neumonía por sí sola.
Usando las palabras de Griffith, el principio transformante de las células BIS servía como «alimento» para las células ILR.

Tabla 10.1

Cepas de Diplococcus pneumoniae utilizadas por Frederick Griffith en sus experimentos de transformación 

Serotipo.
Morfología de la colonia .
Cápsula.
Virulencia.


IIR .
Rugosa.
Ausente.
No virulenta .


IIIS .
Lisa.
Presente .
Virulenta.


El trabajo de Griffth llevó a otros médicos y bacteriólogos a investigar el fenómeno de la transformación. 
En 1931, Henry Dawson, del Rockefeller Institute, confirmó las observaciones de Griffith y amplió su trabajo.

Dawson y sus colaboradores demostraron que la transformación puede darse in vitro (dentro de un tubo de ensayo).
Si se incubaban células BIS muertas por calor junto con células IIR vivas, se recuperaban células BIS vivas.
Por tanto, no es necesario inyectar ratones para que se dé la transformación.
En 1933, Lionel J. Alloway, refinó los experimentos in vitro al usar extractos crudos de células S y células R vivas.

¡El filtrado soluble obtenido de las células S muertas por calor era tan efectivo para inducir transformación como las células intactas!
Alloway y otros no asociaron la transformación con un proceso genético, sino más bien con algún tipo de modificación fisiológica.
No obstante, las pruebas experimentales de que una substancia química era la responsable de la transformación eran convincentes.

Posteriormente, en 1944, y tras 10 años de trabajo, Avery, MacLeod y McCarty publicaron los resultados que están considerados actualmente como un artículo clásico en genética molecular.

En este artículo informaron que habían obtenido el principio transformante en estado puro y que, más allá de cualquier duda razonable, el DNA era la molécula responsable de la transformación.

La Figura 10.4 presenta, en esquema, los detalles del trabajo denominado a menudo como el experimento de Avery, MacLeod y McCarty. 
Estos investigadores empezaron el proceso de aislamiento con grandes cantidades de cultivos líquidos (de 50 a 75 litros) de células IIIS virulentas.
Se centrifugaron las células, se recogieron y se mataron por calor.
Tras homogeneizar y extraer varias veces con un detergente llamado desoxicolato (DOC), obtuvieron un filtrado soluble que, una vez probado, contenía el principio transformante.
Se eliminaron las proteínas del filtrado activo mediante varias extracciones en cloroformo, y los polisacáridos fueron digeridos enzimáticamente y también eliminados. 
Finalmente, una precipitación con etanol produjo una masa fibrosa que mantenía la capacidad de inducir transformación a células del tipo IIR no virolentas. 
De los 75 litros originales de la muestra, este procedimiento produjo entre 10 y 2S mg del «factor activo».

Nuevas pruebas establecieron, más allá de cualquier duda razonable, que el DNA era el principio transformante. 
Inicialmente se analizó la proporción nitrógeno/ fósforo de la masa fibrosa, coincidiendo ésta con la proporción del «desoxirribonucleato sódico», nombre químico que se daba por aquel entonces al DNA. 
Para dar más solidez a sus descubrimientos, trataron de eliminar, tanto como fuese posible, todos los contaminantes que pudiera tener su producto final.

Para ello, lo trataron con las enzimas proteolíticas tripsina y quimiotripsina, y posteriormente con una enzima que digiere el RNA llamada ribonucleasa.
Estos tratamientos destruyeron cualquier resto de proteínas y de RNA.
Sin embargo, la actividad transformante permanecía.
Pruebas químicas del producto final dieron fuertes reacciones positivas para el DNA.
La confirmación final vino de experimentos en los que se utilizaron muestras de desoxirribonucleasa , una enzima que digiere el DNA, y que fue aislada a partir de sueros de perro v de conejo.
La digestión con esta enzima destruyó la actividad transformante.
¡Pocas dudas podía haber de que el principio activo transformante fuese el DNA!

La gran cantidad de trabajo, las comprobaciones de las conclusiones, y la lógica inequívoca del diseño experimental de la investigación de estos tres científicos son realmente impresionantes.
Sin embargo, las conclusiones de la publicación de 1944 fueron expuestas de una manera muy simple:
«La prueba presentada apoya el hecho de que un ácido nucleico de tipo desoxirribosa es la unidad fundamental del principio transformante del Pneumococcus Tipo III».

Avery y sus colaboradores reconocieron las implicaciones genéticas y bioquímicas de su trabajo al observar que «los ácidos nucleicos de este tipo no sólo deben ser considerados importantes estructuralmente sino también funcionalmente, puesto que determinan las actividades bioquímicas y las características específicas de las células dé neumococos».
Esto sugería que el principio transformante interaccionaba con las células IIR dando lugar a una serie de reacciones enzimáticas coordinadas que culminaban en la síntesis de la cápsula de polisacáridos del tipo BIS.
Estos científicos subrayaron que, una vez ocurre la transformación, las sucesivas generaciones también producen la cápsula de polisacáridos.
Por lo tanto, la transformación es hereditaria, y el proceso afecta al material genético. 

Tras la publicación de este artículo, diversos investigadores dirigieron o intensificaron sus estudios de transformación para aclarar la función del DNA en los mecanismos genéticos. 
En particular, el trabajo de Rollin Hotchkiss contribuyó a confirmar que el DNA es el factor esencial de la transformación, y no las proteínas.
En 1949, en un estudio distinto, Harriet Taylor aisló una cepa mutante extremadamente rugosa (ER) a partir de una cepa rugosa (R).
Esta cepa ER producía colonias que eran más irregulares que las de la cepa R.
El DNA de la cepa R efectuaba la transformación de la cepa ER en la cepa R.
De esta manera, la cepa R, que era la receptora en los experimentos de Avery, demostró que también era capaz de ser dadora de DNA en transformaciones. 

La transformación se ha demostrado en Hemophilus influenzae , Bacillus subtilis , Shigella paradysenteriae en Escherichia coli , entre muchos otros micro-organismos.
También se ha demostrado la transformación de numerosas características genéticas distintas de la morfología de las colonias, como la resistencia a antibióticos y la capacidad de metabolizar diversos nutrientes.

Estas observaciones fortalecieron la creencia de que, más que un simple cambio fisiológico, la transformación es básicamente un fenómeno genético.
Esta idea se tratará de nuevo al final de este capítulo en la sección "Ideas y Soluciones".

Figura 10.1

Esquema simplificado del flujo de información en el interior de las celarlas en el que participan DNA, RNA y proteínas

Figura 10.2

Representación esquemática del tetranucleótido propuesto por Levene

Cada uno de ellos contiene una molécula de cada una de las cuatro bases nitrogenadas:
adenina (A), citosina (C), guanina (G) y timina (T).
Cada cuadrado representa un nucleótido.
También son posibles otras secuencias diferentes de los cuatro nucleótidos. 

Figura 10.3

Resumen de los experimentos de transformación de Griffith

Figura 10.4

Resumen del experimento de Avery, MacLeod y McCarthy que demuestra que el DNA es el principio transformante

[0] Actualmente, este organismo recibe el nombre de Streptoroccus pneumoniae .

Replicación y recombinación del DNA

CONCEPTOS DEL CAPÍTULO

La continuidad genética entre las células progenituras y sus descendientes es posible gracias a la replicación semiconservativa del DNA, como predice el modelo de Watson-Crick. 
Cada cadena de la hélice progenitura sirve de molde para la producción de su complementaria.
La síntesis de DNA es un proceso complejo y ordenado, orquestado por una miríada de encimas y de otras moléculas. 

Todas ellas funcionan conjuntamente con gran fidelidad para polimerizar nucleótidos en cadenas polinucleotídicas. 
La recombinación genética es un proceso. 

Tras la propuesta de Watson y Crick de la estructura del DNA, los científicos centraron su atención en cómo se replica esta molécula.
Este proceso es una función esencial del material genético y debe ser ejecutado con precisión si, tras la división celular, se ha de mantener la continuidad genética entre células. 
Ésta es una tarea enorme y compleja.
Consideremos por un momento que hay del orden de 109 pares de bases (aproximadamente unos tres mil millones) en los 23 cromosomas del genoma humano. 
Duplicar fielmente el DNA de uno solo de estos cromosomas requiere un mecanismo de extremada precisión.
Incluso una tasa de error de tan solo 10-6 (uno en un millón), crearía 3.000 errores en cada ciclo de replicación del genoma, lo que obviamente es un número excesivo.
Aunque no está exento de error, en todos los organismos ha evolucionado un sistema de replicación del DNA mucho más preciso.

Como Watson y Crick sugirieron en su artículo de 1953, el modelo de la doble hélice les proporcionó la pista inicial de cómo puede producirse la replicación. 

Este modo de replicación, denominado replicación semiconservativa , ha recibido desde entonces sólidas confirmaciones experimentales en estudios de virus, procariotas y eucariotas.

Una vez clarificado el modelo general de la replicación, se intensificó la investigación para determinar los detalles precisos de la síntesis de DNA.
Lo que se ha descubierto desde entonces es que se necesitan numerosos enzimas v otras proteínas para copiar una hélice de DNA.
Debido a la complejidad de los fenómenos químicos que acaecen durante la síntesis, este tema es todavía una área de investigación extremadamente activa.

Este capítulo examinará el modelo general de replicación así como los detalles específicos de la síntesis de DNA.
La investigación que conduce a estos conocimientos es otro eslabón en nuestra comprensión de los procesos de la vida en el ámbito molecular.

Modos de replicación del DNA

Para Watson y Crick, estaba claro que, debido a la ordenación y naturaleza de las bases nitrogenadas, cada cadena de DNA de la doble hélice podía servir de molde para la síntesis de su complementaria (Figura 11.1).
Propusieron que si la hélice estuviese desenrollada, cada nucleótido a lo largo de las dos cadenas parentales tendría afinidad por su nucleótido complementario.
Como aprendimos en el Capítulo 10, la complementariedad se debe a los puentes de hidrógeno que pueden formarse.
Si el ácido timidílico (T) estuviese presente, «atraería» al ácido adenílico (A); si el ácido guanidílico (G) estuviese presente, «atraería» al ácido citidílico (C); y así sucesivamente.
Si a lo largo de ambos moldes estos nucleótidos se uniesen covalentemente en cadenas polinucleotídicas, el resultado sería la producción de dos dobles cadenas de DNA idénticas.
Cada molécula de DNA replicada consistiría en una cadena «vieja» y una cadena «nueva»; de aquí el nombre de «replicación semiconservativa».

Hay otras dos posibles formas de replicación (Figura 11.2) que también se basan en las cadenas parentales como moldes.
En la replicación conservativa , la síntesis de las cadenas polinucleotídicas complementarias se produciría como se ha descrito anteriormente. 
Después de la síntesis, sin embargo, las dos cadenas recién creadas se juntarían, y las cadenas parentales se reasociarían.
Así se «conservaría» la hélice original. 

En la segunda forma alternativa, denominada replicación dispersiva , las cadenas parentales se dispersarían entre las dos nuevas dobles hélices después de la replicación.
De este modo, cada cadena estaría formada por DNA viejo y nuevo.
Este modelo implicaría el corte de las cadenas parentales durante la replicación.
Es la más compleja de las tres posibilidades y, por lo tanto, la menos probable. 
Sin embargo, no puede descartarse como modelo experimental. 
En la Figura 11.2 se comparan los resultados teóricos de dos generaciones de replicación según las formas conservativa y dispersiva con los de la forma semiconservativa. 

El experimento de Meselson-Stahl 

En 1958, Matthew Meselson y Franklin Stahl publicaron los resultados de un experimento que proporcionaba sólidas pruebas de que la replicación semiconservativa es la forma que utilizan las células para producir nuevas moléculas de DNA.
Se hizo crecer células de E. coli durante muchas generaciones en un medio en el que el 15NH4Cl (cloruro de amonio) era la única fuente de nitrógeno 15N es un isótopo «pesado» del nitrógeno que tiene un neutrón más que el isótopo natural 14N .
A diferencia de los isótopos «radioactivas», el 15N es estable y no radiactivo.
Después de muchas generaciones, todas las moléculas que contienen nitrógeno en la células de E. coli, incluidas las bases nitrogenadas del DNA, contenían el isótopo pesado. 
El DNA que contiene 15N puede distinguirse del que contiene 14N mediante la técnica de centrifugación por equilibrio de sedimentación , en la que, por centrifugación, se «obliga» a las muestras a emigrar por un gradiente de densidad de una sal de un metal pesado como el cloruro de cesio (véase el Capítulo 10).
El DNA- 15N , que es más denso, alcanzará el equilibrio en el gradiente en un punto más cercano a la base (donde la densidad es mayor) que el DNA- 14N .

En este experimento, se hicieron crecer células de E. coli durante varias generaciones en presencia del isótopo pesado, obteniéndose células marcadas uniformemente con '5N .
Después, estas células fueron transferidas a un medio que contenía sólo 14N H4C1 .
De este modo, las síntesis posteriores de DNA acaecidas durante la replicación contenían el isótopo «ligero» del nitrógeno. 
El momento de la transferencia se tomó como tiempo O (t = 0).
Se permitió que las células de E. coli se replicasen durante varias generaciones, obteniéndose muestras de las células a distintos tiempos.

Se aisló el DNA de cada muestra y se centrifugó por equilibrio de sedimentación.

Los resultados se muestran en la Figura 11.3.

Después de una generación, el DNA aislado se presentaba en una única banda de densidad intermedia, resultado que cabía esperar para la replicación semiconservativa.
Cada molécula replicada estaba compuesta de una cadena nueva con 14N y una cadena vieja con 15N , como se ve en la Figura 11.4.
Este resultado no es congruente con el modelo de replicación conservativa, en el que deberían visualizarse dos bandas distintas. 

Después de dos divisiones celulares, las muestras de DNA mostraron dos bandas de distinta densidad.
Una era intermedia y la otra era ligera, correspondiendo a la posición del 14N en el gradiente.
Resultados parecidos se obtuvieron después de una tercera generación, excepto que la proporción de la banda de 14N se incrementó.
De nuevo, esto era congruente con la interpretación de que la replicación es semiconservativa. 

Dos observaciones sirvieron de base para descartar el modelo de replicación dispersiva.
Una implicaba el análisis del DNA después de la primera replicación en el medio que contenía 14N .
La observación de una molécula que mostraba densidad intermedia también es congruente con la replicación dispersiva.
Sin embargo, Meselson y Stahl aislaron esta molécula híbrida y la analizaron tras desnaturalizarla por calor.
Recuérdese del Capítulo 10 que en la desnaturalización por calor el dúplex se separa en cadenas sencillas.
Cuando se determinaron las densidades de las cadenas sencillas del híbrido, éstas mostraron un perfil de 15N o un perfil de 14N , pero no una densidad intermedia. 
Esta observación es congruente con el modelo semiconservativo y es además incongruente con el modelo dispersivo.

Además, si la replicación fuese dispersiva, todas las generaciones después de t = 0 mostrarían DNA de densidad intermedia.
Después de la primera generación, y en cada una de las siguientes generaciones, la proporción de 15N14N decrecería y la banda híbrida sería cada vez más ligera, acercándose con el tiempo a la banda de 14N .
No se observó este resultado.
El experimento de Meselson Stahl proporcionó en bacterias un resultado muy concluyente de la replicación semiconservativa y contribuyó a descartar los modelos conservativo y dispersivo.

Replicación semiconservativa en eucariotas

En 1957, un año antes de que se publicase el trabajo de Meselson y sus colaboradores, J. Herbert Taylor, Philip Woods y Walter Hughes presentaron pruebas de que la replicación semiconservativa también se daba en organismos eucariotas. 
Estos autores experimentaron con puntas de raíz [1] de haba ( Vicia faba ).
Las puntas de raíz son una fuente excelente de células en división.
Estos investigadores examinaron los cromosomas de estas células tras la replicación del DNA.
Pudieron seguir el proceso de replicación marcando el DNA con timidina-3H (timidina tritiada), un precursor radioactivo del DNA, y realizando autorradiografía .

En este experimento, se encontró timidina marcada asociada sólo a cromosomas que contenían DNA recién sintetizado.

La técnica de autorradiografía, tratada en detalle en el Apéndice A, es un procedimiento citológico que permite localizar al isótopo dentro de la célula. 

En este procedimiento, se pone emulsión fotográfica sobre una sección de material celular (meristemos radiculares en este experimento), y se guarda la preparación protegida de la luz.
Se revela el portaobjetos de manera semejante a como se procesa una película fotográfica.
Tras el revelado, y puesto que el radioisótoipo emite energía, la emulsión se vuelve negra en el punto de emisión.

El resultado final es la presencia de manchas oscuras o «granos» en la superficie de la sección, identificando así al DNA recién sintetizado dentro de la célula. 

Se hizo crecer a los meristemos radiculares en presencia del radioisótopo durante aproximadamente una generación, y seguidamente se transfirieron a un medio sin marcar, donde la división celular continuó.
Al final de cada generación, los cultivos se detuvieron en metafase añadiendo colchicina (un derivado químico del azafrán que «envenena» la fibras del huso nuclear), y los cromosomas se examinaron por autorradiografía.
La Figura 11.5 muestra la replicación de un único cromosoma en dos ciclos de división así como la distribución de granos.

Los resultados son compatibles con el modelo de replicación semiconservativa.

Tras el primer ciclo de replicación, en presencia del isótopo, se detecta radioactividad en las dos cromátidas hermanas.
Este es el resultado esperado ya que cada cromátida contendrá una cadena «nueva» de DNA radioactiva y una cadena «vieja» de DNA no marcada.
Después del segundo ciclo de replicación, que se realiza en un medio no marcado, únicamente una de las dos nuevas cromátidas hermanas debería ser radioactivo ya que la mitad de las cadenas parentales no está marcada.
Este es el resultado que se observa, con unas pocas excepciones debidas al intercambio entre cromátidas hermanas (véase el Capítulo 5).

Conjuntamente, los experimentos de Meselson-Stahl y de Taylor, Woods y Hughes condujeron a la aceptación general del modelo de replicación semiconservativa.

Se ha llegado a las mismas conclusiones en estudios realizados con otros organismos.
Estos experimentos también apoyaban la propuesta de Watson y Crick sobre el modelo de doble hélice del DNA.

Orígenes, horquillas y unidades de replicación

El modo de replicación que acabamos de establecer representa el patrón general por el que se duplica el DNA.
Antes de analizar los detalles bioquímicos de cómo se sintetiza realmente el DNA, examinaremos brevemente otras cuestiones relevantes para la descripción completa de la replicación semiconservativa.
La primera cuestión hace referencia al origen de replicación. 
¿En qué lugar del cromosoma se inicia la replicación?
¿Hay un solo origen, o hay más de un punto en el que empieza la síntesis del DNA?
Sea cual sea el punto de origen,
¿es éste aleatorio o está localizado en una región específica del cromosoma?

La segunda cuestión aborda el sentido de la replicación. 
Una vez empieza la replicación,
¿continúa desde el origen en un solo sentido o en ambos?
Esta consideración distingue entre replicación unidireccional y bidireccional respectivamente.

Para abordar estas preguntas, necesitamos introducir dos conceptos implicados tanto en la discusión como en la ejecución de la replicación del DNA.
El primero hace referencia al punto real del cromosoma en el que se está realizando la replicación.
En este punto, las cadenas de la hélice deben estar desenrolladas, formando lo que se denomina horquilla de replicación. 
Esta «horquilla» aparecerá inicialmente en el punto de origen de la síntesis.
Entonces, a medida que avance la replicación, la horquilla de replicación se moverá a lo largo del DNA dúplex.

Si la replicación es bidireccional, habrá dos horquillas migrando en direcciones opuestas a partir del origen.

El segundo concepto hace referencia a la longitud del fragmento de DNA que se replica en cada suceso de replicación dado en un solo origen.
Este fragmento de DNA se considera como una unidad denominada replicón.

Las pruebas concernientes a las cuestiones plantea das inicialmente son razonablemente claras.
En las bacterias y en la mayoría de los virus bacterianos, que tienen un único cromosoma circular, hay sólo una región en la que se inicia la replicación.
En el cromosoma de E. coli se ha cartografiado esta región específica, que recibe el nombre de oriC .

Esta consiste en 245 pares de bases, pero sólo un pequeño número de ellas es realmente imprescindible para el inicio de la síntesis de DNA.
Ya que en bacteriófagos y en bacterias hay un único punto de origen de replicación, el cromosoma íntegro constituye un replicón.

¿Se realiza la replicación en un único sentido, o en ambos?
En E. coli , la evidencia es clara.
La replicación es bidireccional a partir de oriC , continuando en ambos sentidos.
De esta manera se producen dos horquillas de replicación, como se esquematiza en la Figura 11.6 Cuando se completa la replicación semiconservativa de todo el cromosoma estas horquillas convergen en una región terminadora denominada ter. 

En eucariotas existe una diferencia importante en cuanto a la replicación si se compara con la descrita para bacterias. 
Aunque la replicación es bidireccional, formando dos horquillas de replicación y una burbuja de replicación en cada ciclo de replicación, hay múltiples orígenes en el cromosoma.
En consecuencia, durante la fase S o interfase, se producen numerosos sucesos de replicación a lo largo de cada cromosoma.
Finalmente, las numerosas horquillas de replicación se fusionan, completando la replicación de todo el cromosoma.
La presencia de numerosos replicones está indudablemente relacionada con la mayor longitud y complejidad de un cromosoma eucariota comparado con un cromosoma bacteriano.
Por lo tanto, la replicación puede completarse en un periodo de tiempo más razonable.
En este capitulo volveremos a considerar la síntesis de DNA en eucariotas.

Síntesis de DNA en microorganismos 

La determinación de que la replicación es semiconservativa y bidireccional indica únicamente el patrón de duplicación del DNA y la asociación entre las cadenas terminadas una vez se ha completado la síntesis. 
Una cuestión mucho más compleja es cómo se produce la síntesis real de una larga cadena polinucleotídica complementaria sobre un molde de DNA.
Como en la mayoría de estudios de biología molecular, esta pregunta se abordó inicialmente usando microorganismos.
La investigación empezó al mismo tiempo que el trabajo de Meselson-Stahl, y todavía hoy es un área activa de investigación. 
Lo que es más evidente de esta investigación es la enorme complejidad química de la síntesis biológica del DNA.

La DNA polimerasa I

La primera publicación sobre la enzimología de la replicación del DNA la hicieron Arthur Kornberg y sus colaboradores en 1957.
Estos investigadores aislaron una enzima de E. coli que podía dirigir la síntesis del DNA en un sistema exento de células ( in vitro ).

Actualmente, esta enzima recibe el nombre de DNA polimerasa I, ya que fue la primera en aislarse.
Kornberg determinó que hay dos requisitos importantes para la síntesis de DNA in vitro dirigida por esta enzima:
1. Todos los desoxirribonucleósidos trifosfato ( dATP, dCTP, dGTP, dTTP = dNTP [0] )
2. DNA molde

Si se omite de la reacción cualquiera de los cuatro desoxirribonucleósidos trifosfato no se detecta síntesis.
Si se usan derivados de estas moléculas precursoras diferentes de los nucleósidos trifosfato (nucleótidos o nucleósidos difosfato), no se produce síntesis. 
Si no se añade DNA molde, la síntesis de DNA se ve enormemente reducida.
La mayoría de la síntesis dirigida por la enzima de Kornberg parecía ser exactamente como la requerida según la replicación semiconservativa.
La reacción se resume en la Figura 11.7, en la que se representa la adición de un nucleótido. 
Se ha demostrado que la enzima consiste en una única cadena polipeptídica de 928 aminoácidos.

La manera en que cada nucleótido se añade a la cadena en crecimiento es función de la especificidad de la DNA polimerasa I. Como se esquematiza en la Figura 11.8, el dNTP precursor contiene los tres grupos fosfato unidos al carbono 5' de la desoxirribosa.
Como durante la síntesis se cortan los dos fosfatos terminales, el fosfato restante unido al carbono 5' se une covalentemente al grupo 3'-OH de la desoxirribosa a la que se añade el nucleótido. 
Cada paso alarga en un nucleótido la cadena que está creciendo.
De acuerdo con la descripción anterior y la Figura 11.8, el alargamiento ("elongación") de la cadena se produce en dirección 5'-3' por la adición de nucleótidos al extremo 3' en crecimiento.
Cada paso proporciona un nuevo grupo 3'-OH expuesto que puede participar en la siguiente adición de un nucleótido a medida que prosigue la síntesis de DNA.

Figura 11.1

Modelo general de la replicación semiconservativa del DNA

Figura 11.2

Resultados de dos ciclos consecutivos de replicación de DNA para cada uno de los tres posibles tipos de replicación

Cada ciclo de síntesis se muestra de un color diferente.

Figura 11.3

Experimento de Meselson-Stahl 

Figura 11.4

Resultados esperados en dos generaciones de replicación semiconservativa en el experimento de Meselson-Stahl

Figura 11.5

Esquema del experimento de Taylor, Woods y Hughes, que demuestra el modelo de replicación semiconservativo del DNA en meristemos radiculares de Vicia faba 

(a) una cromátida no marcada realiza el ciclo celular en presencia de timidina- 3H.
Al entrar en mitosis, ambas cromátidas hermanas del cromosoma están marcadas, como muestra la autorradiografía.
Después de una segunda generación de replicación, esta vez en ausencia de timidina-3H, se espera que sólo una cromátida de cada cromosoma está rodeada de granos (b).
Excepto donde se han producido intercambios recíprocos entre cromátidas hermanas (c), se cumple lo previsto.
Las micrografías pertenecen a los autorradiogramas originales obtenidos en el experimento.

Figura 11.6

Replicación bidireccional del cromosoma de E. coli 

Las flechas señalan el avance de las horquillas de replicación.
La micrografía muestra un cromosoma bacteriano en proceso de replicación.

[6] Estos científicos utilizaron puntas de raíz puesto que es donde se encuentran los meristemos radiculares, zonas con una elevada tasa de proliferación mitótica implicadas en el crecimiento en longitud de estos órganos.

[03] Se denomina dNTP a las formas de desoxirribosa de los cuatro nucleósidos trifosfato; del mismo modo, dNMP hace referencia a las formas monofosfato.

Almacenaje y expresión de la información genética

CONCEPTOS DEL CAPÍTULO

La información genética, almacenada en el DNA y transferida al RNA durante el proceso de transcripción, se presenta como un código de tres letras.
Usando cuatro letras diferentes, correspondientes a los cuatro ribonucleótidos del RNA, los 64 codones de tripletes resultantes no sólo especifican a los 20 aminoácidos diferentes encontrados en las proteínas sino que también proporcionan señales de iniciación y terminación de la síntesis proteica.
Este idioma especial es la base de la vida tal como la conocemos.

En capítulos anteriores establecimos que el DNA es el material genético y examinamos su estructura y su replicación.
En este capítulo examinaremos dos aspectos más de la genética molecular.
Primero, trataremos cómo el DNA codifica la información genética.
Segundo, examinaremos cómo se descodifica esta información durante la expresión del material genético.
Como veremos, el elaborado sistema que proporciona las bases física y química para el almacenaje y expresión de la información genética tiene el potencial de producir una variedad casi ilimitada de moléculas proteicas.
Para llevar a cabo dicha síntesis, primero se transcribe el DNA de un gen activo en su complementario de RNA, que luego se traduce a proteína.
La información almacenada en una secuencia de desoxirribonucleótidos (DNA) se transfiere primero a una secuencia complementaria de ribonucleótidos (RNA), que después se descodifica ya que el RNA especifica el orden de inserción de los aminoácidos durante la síntesis proteica.
Las proteínas, como productos finales de diversas secuencias génicas, son responsables de los fenotipos normales y mutantes de los organismos. 

Por consiguiente, una pregunta fundamental es cómo una molécula de RNA, que consiste en sólo cuatro grupos diferentes de ribonucleótidos (A, U, C y G), puede especificar 20 aminoácidos diferentes.
Esta pregunta plantea un fascinante problema teórico.
Cuando a este problema se aplicó una ingeniosa investigación analítica, se descifró el código genético. 
Se estableció que el código es de naturaleza ternaria.
Las palabras del código, o codones , que están formados por tres ribonucleótidos, dirigen la inserción de los aminoácidos durante la síntesis de una cadena polipeptídica.

El proceso por el que se sintetizan las moléculas de RNA sobre un molde de DNA se denomina transcripción .
La secuencia ribonucleotídica del RNA puede entonces dirigir el proceso de traducción .
Durante la traducción, se sintetizan las cadenas polipeptídicas, que maduran en proteínas. 
La síntesis proteica depende de una serie de moléculas de RNA transferente ( tRNA ) que sirven de adaptadores entre los codones del mRNA (RNA mensajero) y los aminoácidos que éste especifica.

Además, el proceso de traducción se realiza únicamente en conjunción con un complicado componente celular, el ribosoma .

Los procesos de transcripción y traducción son fenómenos moleculares complejos.

Igual que la replicación del DNA, ambos se basan principalmente en las afinidades de emparejamiento de bases entre nucleótidos complementarios.
La transferencia inicial de la información almacenada en el DNA produce una molécula, generalmente mRNA , complementaria a la secuencia nucleotídica de una de las dos cadenas de la doble hélice.
Después, cada codón de mRNA , complementario a la región anticodón del tRNA que lleva el correspondiente aminoácido, dirige la inserción secuencial de aminoácidos en la cadena polipeptídica.

En este capítulo describiremos detalladamente cómo se descifró el código genético y cómo se realiza la expresión génica.
Los trabajos que condujeron a estos descubrimientos se realizaron básicamente al final de la década de los 50 y al principio de la de los 60, uno de los períodos más excitantes en la historia de la genética molecular.
Estas investigaciones revelaron las complejidades del particular lenguaje químico que sirve de base a todas las formas vivas de la Tierra.

Generalidades del código genético

Antes de considerar los diversos enfoques analíticos usados para alcanzar nuestra actual comprensión del código genético, debemos proporcionar un resumen de los rasgos generales que lo caracterizan:

1. El código genético está escrito de manera lineal utilizando como letras las bases ribonucleotídicas que componen las moléculas de mRNA.
Por supuesto, la secuencia ribonucleotídica proviene de su complementaria del DNA. 

2. Cada palabra del mRNA contiene tres letras.
Así, el código está formado por tripletes. 
Denominados codones, cada grupo de tres ribonucleótidos especifica un aminoácido.

3. El código no contiene ambigüedades, en el sentido de que cada triplete especifica sólo un único aminoácido.

4. El código es degenerado, en el sentido en que más de un triplete específica un aminoácido dado.
Es así para 18 de los 20 aminoácidos.

5. El código es ordenado.
Múltiples codones que especifican un aminoácido dado pueden clasificarse juntos, variando muy a menudo sólo en la tercera base. 

6. El código contiene los signos de puntuación de «inicio» y «fin».
Son necesarios determinados tripletes para iniciar y terminar la transcripción.

7. El código no analiza comas (o puntuaciones internas). 
Así, se dice que el código no tiene comas. 
Una vez empieza la traducción del mRNA , los tripletes se leen por orden, uno detrás de euro. 

8. El código no es solapado. 
Una vez empieza la transcripción, cada ribonucleótido, en una posición específica en el mRNA , forma parte de un único triplete.

9. El código es casi universal. 
Salvo pequeñas excepciones, casi todos los virus, procariotas y eucariotas usan un solo diccionario.


Primeras ideas sobre el código 

Antes de que quedase claro que el mRNA sirve de intermediario transfiriendo información genética del DNA a las proteínas, se pensaba que el DNA podía codificar por sí mismo a las proteínas durante su síntesis.
Una vez se descubrió el mRNA , quedó claro que, aunque la información genética se almacena en el DNA, el código que se transcribe a proteínas reside en el RNA.
La pregunta central era cómo únicamente cuatro letras (los cuatro nucleótidos) podían especificar 20 palabras (los aminoácidos).

Al principio de la década de los 60 Sidney Brermer razonó, por el tamaño del código y sobre fundamentos teóricos, que el código debía estar formado por tripletes ya que palabras de tres letras representaban la combinación mínima de cuatro letras para especificar 20 aminoácidos.
Por ejemplo, cuatro nucleótidos tomados de dos en dos proporcionan sólo 16 palabras codificadas diferentes (42).

Aunque un código de tripletes proporciona 64 palabras (43) -evidentemente más de las 20 necesarias- es mucho más simple que un código de cuatro letras, que especificaría 256 palabras (44).

Brenner también razonó que el código no era solapado.
Suponiendo un código de tripletes, Brenner consideró las restricciones que debería tener si fuese solapado.
Por ejemplo, consideró secuencias teóricas de nucleótidos que codificasen una proteína de tres aminoácidos.
En la secuencia nucleotídica GTACA, la parte del triplete central, TAC, está compartida por los tripletes exteriores, GTA y ACA, como se muestra a continuación.
Brenner razonó que si fuese así, sólo se podrían encontrar determinados aminoácidos adyacentes al codificado por el triplete central.

Para cada aminoácido central dado, sólo son posibles 16 combinaciones (24) de secuencias de «tres aminoácidos» (un tripéptido).

Por lo tanto, Brenner concluyó que si el código fuese solapado, las secuencias de tripéptidos en las proteínas deberían ser limitadas.
Buscando en las secuencias aminoacídicas disponibles que habían sido estudiadas, no encontró dichas restricciones en las secuencias de tripéptidos.
Para cada aminoácido central, encontró muchos más de 16 tripéptidos diferentes. 

Un segundo argumento en contra del código solapado se refería al efecto de un solo cambio nucleotídico característico de una mutación puntual.
En un código solapado, se verían afectados los dos aminoácidos adyacentes.
Sin embargo, mutaciones en los genes que codifican la proteína de la cubierta del virus del mosaico del tabaco (TMV), la hemoglobina humana y la enzima bacteriana triptófano sintetasa mostraban invariablemente cambios en un solo aminoácido.

El tercer argumento en contra del código solapado lo presentó Francis Crick en 1957, cuando predijo que el DNA no sirve de molde directo para la formación de proteínas.
Crick razonó que cualquier afinidad entre nucleótidos y aminoácidos requeriría puentes de hidrógeno.

Químicamente, sin embargo, tales afinidades específicas parecían improbables.
En cambio, Crick propuso que debería existir una molécula adaptadora que pudiese unirse covalentemente al aminoácido y que pudiese también formar puentes de hidrógeno con la secuencia nucleotídica.
Puesto que, durante la traducción, diversos adaptadores tendrían que solaparse unos con otros de alguna manera sobre los nucleótidos, Crick razonó que las restricciones físicas harían el proceso demasiado complejo y, quizás, incluso ineficiente durante la traducción.
Como veremos posteriormente en este capítulo, la predicción de Crick era correcta; el RNTA transferente ( tRNA ) sirve de adaptador en la síntesis proteica.
Y el ribosoma aloja, durante la traducción, dos moléculas de tRNA cada vez.

En conjunto, los argumentos de Crick y Brenner sugerían firmemente que durante la traducción, el código genético no es solapado .
Sin excepción, este concepto se ha confirmado.

El código: nuevos progresos 

Entre 1958 y 1960 se continuó acumulando información sobre el código genético.

Además de la propuesta del adaptador, Crick hipotetizó, basándose en pruebas genéticas, que el código no tiene comas ; es decir, creía que no había signos de puntuación internos en la fase de lectura [1] .
Crick también especuló que sólo 20 de los 64 tripletes posibles especificaban un aminoácido y que los 44 restantes no tenían ningún código asignado.

En esa época, sin embargo, todavía no había pruebas experimentales de que el código fuera ciertamente de tripletes; y no se había establecido el concepto de RNA mensajero, como intermediario entre el DNA y las proteínas. 
Puesto que ya se habían identificado los ribosomas, se creía que la información del DNA se transfería en el núcleo al RNA de los ribosomas, que a su vez servia de molde para la síntesis proteica en el citoplasma.

Pronto este concepto se hizo insostenible ya que se acumulaban pruebas que demostraban que el molde intermediario era inestable. 
Se encontró, en cambio, que el RNA de los ribosomas era extremadamente estable.
Como consecuencia, en 1961 François Jacob y Jacques Monod postularon la existencia del RNA mensajero ( mRNA ) .
De este modo, la escena estaba a punto para demostrar la característica de triplete del código genético, contenido en el mRNA intermediario, y para descifrar las asignaciones de cada codón específico.

También quedan otras preguntas aparte de la asignación de los tripletes.
¿Son las señales de inicio-fin signos de puntuación ?
¿Es ambiguo el código, con un triplete especificando más de un aminoácido?
¿Estaba equivocado Crick respecto al código con 44 tripletes «en blanco»?
Es decir,
¿es degenerado , con más de un triplete asignado a muchos, sino todos, los aminoácidos?
¿Es universal ?
Como veremos, durante la siguiente década se respondieron éstas y otras preguntas.

El estudio de las mutaciones de cambio de fase

Antes de tratar los experimentos en que se descifraron las asignaciones de cada codón específico, consideraremos el ingenioso trabajo experimental de Crick, Leslie Barnett, Brenner y R. J. Watts-Tobin.
Su trabajo representa la primera prueba sólida de la característica de triplete del código.

En su trabajo, estos investigadores indujeron mutaciones por inserción y delación en el cistrón B del locas rII del fago T4.
El cistrón B es una de las dos zonas funcionales de este locas que, en los mutantes, causa una lisis rápida y calvas identificables. 
Los mutantes del locus rII infectan con éxito a la cepa B de E. coli , pero no pueden reproducirse en otra cepa distinta denominada K12.
Criek y sus colaboradores usaron el colorante de acridina proflavina para inducir mutaciones (véase la Figura 14.12).
Este agente mutagénico se intercala entre la doble hélice de DNA, causando a menudo durante la replicación la inserción o deleción de uno o más nucleótidos.
Como se muestra en la Figura 12.1(a), una inserción de un único nucleótido provoca un cambio en la fase de lectura, cambiando la secuencia específica de todos los tripletes posteriores de la derecha de la inserción. 
Con respecto a la traducción, se alterará la secuencia de aminoácidos de la proteína codificada. 
Estas mutaciones se denominan de cambio de fase (o de cambio de pauta de lectura ).
Cuando se presentan en el locas rII , T4 no se reproduce en E. coli K12 .

Criek y sus colaboradores razonaron que si un fugo con estas mutaciones inducidas se trataba otra vez con proflavina, se produciría otra inserción o delación. 
Un segundo cambio podría dar por resultado un fugo revertiente, que se comportaría como silvestre e infectaría con éxito a E. coli K12 . 

Por ejemplo, si el mutante original contenía una inserción (+), un segundo suceso que causase una delación (-) cercana a la inserción restauraría la fase de lectura original.
Así mismo, un suceso que produjera una inserción (+) podría corregir una delación (-) original.

Al estudiar muchas mutaciones de este tipo, estos investigadores pudieron comparar diversas combinaciones de mutantes en la misma secuencia de DNA.

Encontraron que diferentes combinaciones de una (+) y una (-) causaban ciertamente la reversión al comportamiento silvestre.
Otras observaciones aclararon el número de nucleótidos que forman el código genético. 
Cuando dos (+) o dos (-) estaban juntas, no se restablecía la pauta correcta de lectura.
Esto era un argumento en contra del código de dos letras (doblete).
Sin embargo, cuando tres (+) [Figura 12.1 (b)] o tres (-) se presentaban juntas, se restablecía la pauta original.
Estas observaciones apoyaban firmemente la característica de triplete del código.

Estos datos sugirieron además que el código es degenerado, contrariamente a la propuesta inicial de Criek. 
Un código degenerado es aquél en el que más de un codón especifica el mismo aminoácido. 
El razonamiento que conduce a esta conclusión es el siguiente.
En los casos en que se restablece la función silvestre, por ejemplo (+) y (-), (+++) y (- - -), se recupera la fase de lectura original.
Sin embargo, entre las diversas adiciones y deleciones, debería haber numerosos tripletes que estarían fuera de la pauta.
Si 44 de los 64 posibles tripletes estuviesen en blanco y no especificasen ningún aminoácido, probablemente se habría producido uno de estos tripletes (denominados tripletes sin sentido ) en el trecho de nucleótidos todavía fuera de la pauta.
Se razonó que si se encontrase un codón sin sentido durante la síntesis proteica, el proceso se pararía o se terminaría en ese punto.
Si fuese así, no se produciría el producto del locus rII-B y no ocurriría el restablecimiento de la fase de lectura.
Ya que las diversas combinaciones mutantes podían reproducirse en E. coli K12 , Crick y sus colaboradores concluyeron que, con toda probabilidad, la mayoría si no todos los restantes 44 codones no estaban en blanco.
Por lo tanto el código genético es degenerado .
Como veremos, este razonamiento demostró ser correcto.

Descodificación: investigaciones iniciales

En 1961, Marshall Nirenberg y J. Heinrich Matthaei publicaron resultados en que caracterizaban las primeras secuencias específicas codificantes.
Estos resultados sirvieron de piedra angular para el análisis completo del código genético. 
Su éxito estribó en la utilización de dos herramientas experimentales:
un sistema de síntesis proteica in vitro (exento de células) y una enzima, la polinucleótido fosforilasa , que permite la producción de mRNA sintéticos.
Estos mRNA sirvieron de molde para la síntesis de polipéptidos por el sistema exento de células.

En el sistema exento de células, los aminoácidos pueden incorporarse en cadenas polipeptídicas.
Esta mezcla utilizada in vitro , como podría esperarse, debe contener los factores necesarios para la síntesis proteica en la célula:
ribosomas, tRNA , aminoácidos y otras moléculas esenciales para la traducción.
Para seguir (o rastrear) la síntesis proteica, uno o más aminoácidos debe ser radioactivo.
Finalmente, debe añadirse un mRNA , que sirva de molde a traducir.

En 1961 no se podía todavía aislar el mRNA .
Sin embargo, la utilización de la enzima polinucleótido fosforilasa permitía la síntesis artificial de moldes de RNA, que podían añadirse al sistema exento de células.
Esta enzima, aislada de bacterias, cataliza la reacción que se muestra en la Figura 12.2.

Descubierta en 1955 por Marianne Grunberg-Manago y Severo Ochoa, la función metabólica de la enzima en células bacterianas es degradar RNA.
Sin embargo, in vitro , con altas concentraciones de ribonucleósidos difosfato, la reacción puede «forzarse» en la dirección opuesta para sintetizar RNA, como se muestra. 

Al contrario que la RNA polimerasa, la polinucleótido fosforilasa no precisa un molde de DNA.
El resultado es que cada adición de un ribonucleótido es al azar, basado en la concentración relativa de los cuatro ribonucleósidos difosfato añadidos a la mezcla de reacción.
La probabilidad de inserción de un ribonucleótido específico es proporcional a la disponibilidad de esa molécula, relativa a los otros ribonucleótidos disponibles.
Este punto es realmente importante para entender el trabajo de Niremberg y de otros científicos en la siguiente discusión.

En conjunto, el sistema exento de células para la síntesis proteica y la disponibilidad de mRNA sintético proporcionó un medio para descifrar la composición ribonucleotídica de diversos tripletes que codifican aminoácidos específicos.

Los códigos de los homopolímeros de Nirenberg y Matthaei

En sus experimentos iniciales, Niremberg y Matthaei sintetizaron homopolímeros de RNA formados por un único tipo de ribonucleótidos.
Por lo tanto, el mRNA que se añadía al sistema in vitro era UUUUUU... , AAAAAA... CCCCCC... , o GGGGGG... 
Al probar cada mRNA , podían determinar qué aminoácido, si es que lo había, se había incorporado en las proteínas recién sintetizadas.
Los investigadores lo determinaban marcando uno de los 20 aminoácidos añadidos al sistema in vitro y realizando una batería de experimentos, cada uno con un aminoácido radioactivo diferente.

Figura 12.1

Esquema de los efectos de las mutaciones de cambio de fase en la secuencia de un DNA formado por el triplete repetido GAG

En la inserción de un nucleótido ha cambiado todos los tripletes posteriores.
En la inserción de tres nucleótidos cambia sólo dos tripletes, mientras que todos los demás conservan la secuencia original. 

Figura 12.2

Reacción catalizada por la enzima polinucleótido fosforilasa

Obsérvese que el equilibrio de la reacción favorece la degradación del RNA, pero puede «forzarse» para que favorezca la síntesis.

[7] Para referirse al trecho de tripletes que son traducidos a proteína, se utilizan las expresiones fase de lectura, pauta de lectura, o marco de lectura indistintamente, siendo todas ellas sinónimas 

Las proteínas: el producto final de la expresión genética

CONCEPTOS DEL CAPÍTULO

Los productos finales de la expresión de la mayoría de los genes son las cadenas polipeptídicas. 
Estas adquieren una conformación tridimensional basada en la secuencia primaria de aminoácidos, y a menudo interaccionan con otras cadenas polipeptídicas para formar moléculas proteicas funcionales.
La función de las proteínas está muy relacionada con su estructura, que puede alterarse por mutación produciendo efectos fenotípicos distintivos.

En el Capítulo 12 establecimos que hay un código genético que almacena información en forma de tripletes de nucleótidos en el DNA, y que esta información puede expresarse mediante procesos ordenados de trascripción y traducción.
El producto final de la expresión génica es, casi siempre, una cadena polipeptídica formada por una serie lineal de aminoácidos cuya secuencia ha dictado el código genético.
En este capítulo revisaremos los experimentos que confirmaron que las proteínas son los productos finales de los genes, y trataremos brevemente los diversos niveles estructurales de las proteínas, su diversidad y su función.
Esta información proporciona una base importante para comprender cómo la mutación, que se origina en el DNA, puede producir los diversos efectos fenotípicos observados en los organismos. 

Garrod y Bateson: errores congénitos del metabolismo

La primera idea de la función de las proteínas en los procesos genéticos provino de las observaciones hechas por Sir Archibald Garrod y William Bateson a principios de siglo.
Garrod pertenecía a una familia inglesa de científicos.
Su padre era un médico muy interesado en las bases químicas de la artritis reumatoide, y su hermano mayor era un eminente zoólogo londinense. 
No es sorprendente, pues, que como médico activo, Garrod se interesara por diversas enfermedades humanas que parecían hereditarias.
Aunque también estudió el albinismo y la cistinuria, describiremos sus investigaciones de la enfermedad de la alcaptonuria .
Los individuos afectados no pueden metabolizar el alcaptón ácido 2,5-dihidroxifenilacético, también conocido como ácido homogentisico . 
En consecuencia, se obstruye una importante ruta metabólica (Figura 13.1).
El ácido homogentísico se acumula en las células y tejidos y se excreta por la orina. 
El producto de la oxidación de la molécula es negro, y es fácilmente detectable en los pañales de los recién nacidos.
Los productos tienden a acumularse en las zonas cartilaginosas, provocando el oscurecimiento de las orejas y de la nariz.
Estas deposiciones conducen a una condición de artritis benigna en las articulaciones. 
Esta enfermedad poco frecuente no es grave, pero persiste durante toda la vida del individuo.

Garrod estudió la alcaptonuria incrementando la cantidad de proteínas en la dieta o añadiendo a ésta los aminoácidos fenilalanina o tirosina, ambos relacionados químicamente al ácido homogentísico. 
Bajo estas condiciones, el nivel de ácido homogentísico se incrementa en la orina de los alcaptonúricos (individuos afectados), pero no se incrementa en la orina de los individuos no afectados.
Garrod concluyó que los individuos normales pueden romper, o catabolizar, este alcaptón, mientras que los afectados no pueden.
Estudiando el patrón de herencia de esta enfermedad, Garrod concluyó, además, que la alcaptonuria se hereda como un carácter recesivo simple.

Basándose en estas conclusiones, Garrod hipotetizó que la información hereditaria controla las reacciones químicas del organismo y que las enfermedades hereditarias que estudió son el resultado de formas alternativas del metabolismo.
Como los términos gen y enzima no eran comunes en la época de Garrod, utilizó los conceptos correspondientes de factores y fermentos .

Garrod publicó sus observaciones iniciales en 1902. 

Sólo unos pocos genéticos, entre los que estaba Bateson, estaban familiarizados con los experimentos de Garrod.
Las ideas de Garrod encajaban bien con la creencia de Bateson de que las condiciones hereditarias estaban causadas por la falta de alguna sustancia esencial.
En 1909, Bateson publicó Mendel's Principies of Heredity ( Los Principios de la Herencia de Mendel ), en el que relacionaba a los fermentos con la herencia. 

Sin embargo, durante casi 30 años, la mayoría de los genéticos no vieron las relaciones entre genes y enzimas.
Garrod y Bateson, como Mendel, se adelantaron a su tiempo.

Fenilcetonuria

La fenilcetonuria (PKU) , descrita inicialmente en 1934, puede provocar retraso mental y se hereda como una enfermedad autosómica recesiva.
Los individuos afectados tienen obstruido un paso diferente de la citada ruta metabólica (Figura 13.1).
Estas moléculas se diferencian por un sólo grupo hidroxilo (-OH), presente en la tirosina, y ausente en la fenilalanina.
La reacción está catalizada por la enzima fenilalanina hidroxilasa , inactiva en los individuos afectados y activa en los heterocigotos en un 30 por ciento.
La función de esta enzima se localiza en el hígado.
Mientras que el nivel normal de fenilalanina en sangre es de 1 mg / 100 ml , los fenilcetonúricos presentan niveles de hasta 50 mg / 100 ml .

Al acumularse la fenilalanina, ésta puede convertirse en ácido fenilpirúvico y, posteriormente, en otros derivados.
Estos son reabsorbidos en el riñón con mucha menos eficiencia y tienden a verterse en la orina más lentamente que la fenilalanina.
Tanto la fenilalanina como sus derivados entran en el fluido cerebroespinal, lo que provoca niveles elevados en el cerebro.
Se cree que la presencia de estas sustancia durante el desarrollo inicial provoca retraso mental.

El retraso mental puede prevenirse si se detecta tempranamente la presencia de niveles altos de PKU en los recién nacidos. 
Si se detecta esta condición en el análisis de sangre de un niño, se le establece un régimen dietético estricto.

Una dieta baja en fenilalanina puede reducir los productos derivados como el ácido fenilpirúvico, disminuyendo así las anormalidades que caracterizan a la enfermedad. 
En casi todos los estados de los Estados Unidos se realiza el análisis rutinario de los recién nacidos [1] .
La fenilcetonuria se da en 1 de cada 11.000 nacimientos.

El conocimiento de enfermedades metabólicas hereditarias como la alcaptonuria y la fenilcetonuria ha provocado una revolución en el pensamiento y en la práctica médicos.
Las enfermedades humanas ya no pueden atribuirse sólo a la acción de microorganismos, virus o parásitos invasores.
Actualmente sabemos que miles de anormalidades fisiológicas están causadas por errores en el metabolismo como resultado de genes mutados.
El número conocido de estas enfermedades bioquímicas humanas se está incrementando e incluye a todos los tipos de biomoléculas orgánicas.

La hipótesis de un gen: una enzima

En dos investigaciones diferentes iniciadas en 1933, George Beadle proporcionó la primera prueba experimental convincente de que los genes son los responsables directos de la síntesis de las enzimas.
La primera investigación, dirigida con la colaboración de Boris Ephrussi, implicaba a los pigmentos oculares de Drosophila.
Animado por estos descubrimientos, Beadle colaboró entonces con Edward Tatum para investigar mutaciones nutricionales del moho rosa del pan Neurospora crassa. 
Esta investigación condujo a la hipótesis de un gen:una enzima.

Beadle y Ephrussi: los pigmentos oculares de Drosophila

Los estudios de Beadle y Ephrussi se refirieron a los discos imagínales de Drosophila .
Los discos imagínales son grupos de células embrionarias que se encuentran en las larvas de los insectos [2] .
Durante la metamorfosis, estas células se diferencian en diferentes estructuras adultas como los ojos, las antenas y los genitales.

Beadle y Ephrussi encontraron que si se transplanta un disco imaginal de una larva al abdomen de otra, durante la metamorfosis el disco se diferenciará y se podrá recuperar la correspondiente estructura adulta del abdomen de la mosca adulta. 
Por ejemplo, si se transplanta un disco de ojo, se desarrollará una estructura ocular que se podrá recuperar y analizar. 

Estos investigadores querían saber si un disco de ojo mutante transplantado a una larva silvestre se alteraría como consecuencia del nuevo ambiente.
¿Estaría pigmentado como un ojo normal, o desarrollaría el color mutante característico?
El primer mutante que se utilizó fue vermillion , una mutación ligada al cromosoma X que produce ojos de color rojo brillante. 
¡Se revirtió el desarrollo mutante!
El disco de ojo vermillion (v) se alteró, produciendo ojos silvestres.

El color normal del ojo es rojo ladrillo y se produce por la combinación de pigmentos marrón y rojo brillante. 
El mutante vermillion produce el pigmento rojo brillante, pero no tiene el pigmento marrón, lo que indica que su síntesis está inhibida.
Sin embargo, en el abdomen silvestre, el disco mutante también sintetiza pigmento marrón con normalidad, produciéndose el color de ojo silvestre.

Beadle y Ephrussi realizaron entonces experimentos similares con otros 25 mutantes.
Sólo uno, cinnabar ( cn ), otra mutación que da color de ojo rojo brillante por falta de pigmento marrón, se comporta como vermillion cuando se transplanta; desarrolla el color normal del ojo.
Los otros 24 se comportan «autónomamente», mostrando su respectivo color mutante cuando se han diferenciado. 

Estos investigadores concluyeron que las moscas con el fenotipo vermillion o cinnabar tienen ojos mutantes debido a la ausencia, en sus discos, de alguna substancia difusible durante el desarrollo.

Ya que esta substancia es difusible, estaba presente en el abdomen silvestre y era así accesible a los discos v o cn transplantados.
Estas condiciones permitieron que los transplantes mutantes produjeran todos los pigmentos.
Sin embargo, en las 24 mutaciones que se desarrollaron autónomamente, la substancia que podría haber permitido la producción normal de pigmento no era difusible sino que estaba confinada a las células del huésped que forman el ojo.
Por lo tanto, no se observaba «reparación». 

Beadle y Ephrussi se preguntaron entonces si a los mutantes vermillion y cinnabar les faltaba la misma substancia.

Para responder a esta pregunta, transplantaron discos de ojo vermillion a larvas cinnabar y viceversa. 
Lo que encontraron fue enigmático.
Los discos v se «curaban» o se convertían en silvestres en los abdómenes cn , pero los discos cinnabar se desarrollaban autónomamente en los huéspedes vermillion , conservando su fenotipo mutante. 
Estos experimentos se esquematizan en la Figura 13.2. 

Para explicar estos resultados, los investigadores propusieron que en la síntesis normal del pigmento marrón están implicadas dos reacciones químicas secuenciales.
Una es inactiva en las moscas con la mutación vermillion y la otra es inactiva en las moscas con la mutación cinnabar .
Cada reacción produce un producto que es difusible en los tejidos de la larva silvestre. 
Como se muestra en la Figura 13.2, se produce primero la substancia del alelo silvestre del locus vermillion (substancia Y) Entonces se convierte en una segunda substancia (Z) de la ruta.
El alelo silvestre del locus cinnabar controla esta conversión.

Conocer el orden de esta ruta nos permite entender los resultados de los transplantes recíprocos.
Considere primero el disco vermillion (v) en el huésped cinnabar ( cn ).
El disco vermillion no tiene la enzima v+ pero sí la enzima cn+ .
El tejido cinnabar del huésped puede fabricar la substancia Y ya que tiene el alelo silvestre del gen vermillion (v+).
La substancia Y difunde y entra en el disco vermillion , donde la enzima cn+ la convierte en la substancia Z.
Entonces, la substancia Z se convierte en el pigmento marrón, recuperándose el color silvestre del ojo.

Considere ahora el disco cinnabar en el huésped vermillion .
El tejido huésped, al que le falta el gen v+, no puede sintetizar la enzima v+, y por lo tanto no puede producir la substancia Y.
Aunque la enzima cn+ esté presente, no puede producir la substancia Z ya que no hay substancia Y disponible.
En consecuencia el disco cn transplantado todavía está bloqueado.
Él puede producir substancia Y (tiene el alelo v+) pero no puede completar la transición por Z hacia el pigmento marrón (tiene el alelo cn mutado). 
De esta manera,
¡el ojo permanece de color rojo brillante!

Al cabo de pocos años, otros investigadores habían estudiado la ruta bioquímica que conduce al pigmento marrón (xantomatina) en Drosophila.
Como se muestra en la Figura 13.3, el pigmento marrón es un derivado del aminoácido triptófano.
Se ha identificado la enzima especifica que controla las reacciones que están alteradas en los mutantes vermillion y cinnabar .
También se han estudiado otras mutaciones autosómicas recesivas que afectan a enzimas de esta ruta, como scarlet y cardinal .
Lo más importante para nuestra exposición fue la confirmación, en 1940, de que genes mutantes que controlan diferentes fenotipos morfológicos están ligados directamente a «errores» bioquímicos, probablemente debido a la falta de función enzimática. 

Beadle y Tatum: mutantes de Neurospora 

A principios de la década de los 40, Beadle y Tatum eligieron al hongo Neurospora crassa para sus experimentos debido a lo mucho que se sabía de su bioquímica y a que se podían inducir y aislar mutaciones de manera relativamente simple.
Induciendo mutaciones, obtuvieron cepas que tenían bloqueos genéticos en reacciones esenciales para el crecimiento del organismo.

Beadle y Tatum sabían que Neurospora podía fabricar casi todo lo necesario para su desarrollo normal.
Por ejemplo, utilizando fuentes básicas de carbono y nitrógeno, este organismo puede sintetizar 9 vitaminas solubles en agua, 20 aminoácidos, diversos pigmentos caroténicos, purinas y pirimidinas.
Beadle y Tatum irradiaron conidios asexuales (esporas) con rayos X para incrementar la frecuencia de mutación, y permitieron que crecieran en un medio "completo" que contenía todos los factores necesarios para su crecimiento (vitaminas, aminoácidos, etc. ).
En estas condiciones, una cepa mutante que no podría crecer en un medio mínimo podía crecer gracias a los suplementos presentes.
Luego se transfirieron todos los cultivos a medio mínimo.
Si en el medio mínimo se producía crecimiento, es que los organismos podían sintetizar todos los factores necesarios para su crecimiento, y se concluía que no tenían ninguna mutación. 
Si no crecían, es que contenían una mutación nutricional, y lo único que faltaba era determinar de qué tipo.
En la Figura 1 3.4(a) se esquematizan ambos casos.

Mediante este procedimiento, se aislaron muchos miles de esporas a las que se hizo crecer en medio completo.
En los ensayos posteriores realizados en medio mínimo muchos cultivos no crecieron, lo que indicaba que se había producido una mutación nutricional.
Para identificar el tipo de mutación, se mantenía la cepa mutante en una serie de medios mínimos diferentes [Figura 13.4(b) y (c)], conteniendo cada uno diferentes grupos de suplementos, y posteriormente en medios que contenían sólo una de las vitaminas, aminoácidos, purinas o pirimidinas, hasta que se encontraba qué suplemento específico permitía el crecimiento. 
Beadle y Tatum razonaron que el suplemento que restablecía el crecimiento era la molécula que la cepa mutante no podía sintetizar.

La primera cepa mutante aislada precisaba vitamina B-6 en el medio (piridoxina) para crecer, y la segunda precisaba vitamina B-1 (tiamina).
Utilizando el mismo procedimiento, Beadle y Tatum aislaron y estudiaron cientos de mutantes deficientes en la síntesis de otras vitaminas, de aminoácidos y de otras substancias.

Lo que encontraron tras probar más de 80.000 esporas convenció a Beadle y Tatum de que genética y bioquímica tienen mucho en común.
Parecía probable que cada mutación nutricional causase la pérdida de una actividad enzimática que facilita una reacción esencial en los organismos silvestres.
También parecía que se podía encontrar una mutación para casi cada reacción controlada enzimáticamente. 

Beadle y Tatum habían proporcionado así pruebas experimentales convincentes para la hipótesis de que un gen especifica una enzima , una idea a la que Garrod y Bateson habían aludido 30 años antes.
Con algunas modificaciones, este concepto se convirtió en un importante principio de la genética.

Genes y enzimas: análisis de rutas metabólicas

El concepto de un gen:una enzima y los métodos que lo acompañan se han utilizado para resolver muchos detalles del metabolismo de Neurospora, de Escherichia coli y de otros diversos microorganismos.
Una de las primeras rutas metabólicas que se investigó detalladamente fue la que conduce a la síntesis del aminoácido arginina en Neurospora.
Estudiando siete cepas mutantes, precisando todas arginina para crecer (arg.), Adrian Srb y Norman Horowitz pudieron determinar la ruta bioquímica parcial que conduce a la síntesis de esta molécula.
La lógica seguida en sus experimentos ilustra cómo puede utilizarse el análisis genético para conseguir información bioquímica.

Srb y Horowitz probaron la capacidad de cada cepa mutante para restablecer el crecimiento si se utilizaba citrulina u ornitina , dos compuestos químicamente muy similares a la arginina, como suplemento para el medio mínimo.
Razonaron que, si cualquiera de ellos podía sustituir a la arginina, el mutante debía estar implicado en la ruta biosintética de esta molécula [3] .
Encontraron que ambas moléculas podían sustituirla en más de una cepa.

De las siete cepas mutantes, cuatro (arg. 4-7) crecían si se les suministraba citrulina, ornitina o arginina.
Dos de ellas (arg. 2 y 3) crecían si se les suministraba citrulina o arginina.
Una cepa (arg. 1) crecía sólo si se le suministraba arginina; ni la citrulina ni la ornitina podían sustituirla.
De estas observaciones experimentales, dedujeron la siguiente ruta metabólica y los consiguientes bloqueos metabólicos.

El razonamiento que apoya estas conclusiones se basa en la siguiente lógica.
Si los mutantes arg. 4 - 7 pueden crecer si se suministra cualquiera de estas tres moléculas como suplemento al medio mínimo, las mutaciones que no permiten el crecimiento deben provocar un bloqueo metabólico anterior a la participación de la ornitina, de la citrulina o de la arginina en la ruta metabólica. 
Cuando se añade cualquiera de estas tres moléculas, su presencia supera el bloqueo.
En consecuencia, se puede concluir que tanto la citrulina como la ornitina están implicadas en la biosíntesis de la arginina.
Sin embargo, con estos datos, no puede determinarse su secuencia de participación en la ruta.

Por otra parte, los mutantes arg. 2 y 3 crecen si se les suministra citrulina pero no si se les suministra ornitina.
Por lo tanto, la ornitina debe participar en la ruta antes del bloqueo.
Su presencia no superará el bloqueo metabólico.
La citrulina, sin embargo, supera el bloqueo, por lo que debe estar implicada más allá del punto de bloqueo.
Por lo tanto, la conversión de ornitina en citrulina representa la secuencia correcta de la ruta.

Finalmente, se puede concluir que arg. 1 representa la mutación que evita la conversión de citrulina en arginina.
Ni la ornitina ni la citrulina pueden superar el bloqueo metabólico ya que ambas participan en puntos anteriores de la ruta.

En conjunto, estos razonamientos apoyan la secuencia biosintética mostrada.
Con posterioridad a los experimentos de Srb y Horowitz, realizados en 1944, se ha descrito la ruta metabólica detallada y se han caracterizado las enzimas que controlan cada paso.
En la Figura 13.5 se muestra la ruta metabólica. 

No todos los genéticos aceptaron inmediatamente el concepto de un gen:una enzima desarrollado a principios de la década de los 40.
Esto no es sorprendente, ya que todavía no estaba claro cómo las enzimas mutantes podían causar la variación de tantos caracteres fenotípicos.
Por ejemplo, los mutantes de Drosophila mostraban alteraciones del tamaño de los ojos, de la ferina de las alas, del patrón de las nerviaciones de las alas, etc. 
Las plantas mostraban variedades mutantes de la textura de las semillas, de la altura y del tamaño de los frutos. 
Para muchos genéticos resultaba confuso cómo una enzima mutante inactiva podía provocar tales fenotipos. 
Otra razón de su resistencia a aceptar este concepto fue la exigüidad de información disponible de genética molecular.

No fue hasta 1944 que Avery, MacLeod y McCarthy mostraron que el DNA era el factor transformante, y no fue hasta principios de la década de los 50 que la mayoría de los genéticos creyeron que el DNA servía de material genético (véase el Capítulo 10).
Sin embargo, en esa época, las pruebas que apoyaban el concepto de las enzimas como los productos génicos eran aplastantes, y se aceptaban como válidas.
No obstante, la pregunta de cómo el DNA especifica la estructura de las enzimas aún no tenía respuesta.

Figura 13.1

Ruta metabólica en la que participan la fenilalanina y la tirosina

Diferentes bloqueos metabólicos como consecuencia de mutaciones provocan la fenilcetonuria, la alcaptonuria, el albinismo y la tirosinemia.

Figura 13.2

Experimentos de transplantes de discos imagínales de ojo de Drosophila realizados por Beadie y Ephrussi

Un disco mutante transplantado al abdomen de una larva silvestre puede desarrollarse autónomamente, generando un ojo de color mutante (a), o bien puede "curarse" la mutación, generando un ojo de color silvestre (b).
Cuando se transplanta un disco vermillion a un huésped cinnabar (c), el disco transplantado desarrolla un ojo de color silvestre.

En el experimento recíproco (d), el disco transplantado se desarrolla autónomamente, generando un ojo de color mutante.
Con estos resultados, los investigadores concluyeron que el paso biosintético controlado por el producto del gen v+ se produce con anterioridad al controlado por el producto del gen cn+ , pero dentro de la misma ruta.

Figura 13.3

Ruta biosintética que conduce a la conversión del triptófano en xantomatina, el pigmento marrón de las olas de Drosophila

Se muestran tres bloqueos producidos por mutaciones que interrumpen la síntesis en diferentes lugares de la ruta.
En ausencia de xantomatina, las tres mutaciones presentan el mismo fenotipo si están en homocigosis ojos de color rojo brillante.

Figura 13.4

Inducción, aislamiento y caracterización de una mutación nutricional auxotrófica en Neurospora

En (a), la mayoría de conidios no están afectados, excepto un conidio (en color rojo) que contiene dicha mutación.
En (b) y en (c) se determinan las características precisas de la mutación, que está implicada en la biosíntesis de la tirosina.

Figura 13.5

Ruta resumida de la síntesis de arginina en Neurospora

[8] Lo mismo sucede en nuestro país.

[9] El Capitulo 19 trata más extensamente de los discos imagínales de Drosophila y de su función durante el desarrollo.

[10] Ya se sabía que los compuestos relacionados químicamente se convertían unos en otros en la células mediante enzimas.

Mutación génica, reparación del DNA y elementos transponibles

CONCEPTOS DEL CAPÍTULO

Aparte de las mutaciones cromosómicas, la base más importante de la diversidad entre los organismos, incluso los que están estrechamente relacionados, es la variación genética.
El origen de dicha variación es la mutación génica, por la que se alteran las secuencias codificantes como consecuencia de la sustitución adición o delación de una o más bases.
La inserción de elementos genéticos móviles en el genoma también altera la expresión normal de los genes creando mutaciones.
Afortunadamente han evolucionado mecanismos muy elaborados para reparar diversos errores y lesiones del DNA.
Las mutaciones génicas son las herramientas de trabajo de los genéticos y son la materia prima de la evolución.

En la exposición previa del DNA como material genético (Capitulo 10) definimos las cuatro características o funciones adscritas a la información genética: la replicación, el almacenaje, la expresión y la variación por mutación.
De hecho, la mutación es un error en el almacenaje de la información genética.
Si se produce un cambio en la información almacenada, éste puede quedar reflejado en la expresión de esta información y puede propagarse por replicación. 

Históricamente, la palabra mutación incluía tanto los cambios cromosómicos como los cambios en un solo gen.
En el Capítulo 9 tratamos las alteraciones cromosómicas, a las que nos referimos colectivamente como mutaciones o aberraciones cromosómicas.
En este capítulo trataremos las denominadas mutaciones génicas .
Como veremos, el cambio puede ser la sustitución de un solo nucleótido, o puede implicar la adición o delación de uno o más nucleótidos en la secuencia normal del DNA.

Hugo DeVries acuñó el término mutación en 1901 para explicar la variación que observó en los cruzamientos de la hierba del asno, Oelothera lamarckiana . 
La mayoría de esta variación se debe en realidad a translocaciones múltiples, pero se ha demostrado que dos de los casos se deben a mutación génica. 
Al avanzar las investigaciones de mutación, pronto se hizo evidente que las mutaciones génicas son el origen de la mayoría de los alelos, y por lo tanto de mucha de la variación existente entre poblaciones.
Los nuevos alelos que surgen constituyen la materia prima para el proceso evolutivo de la selección natural, que determinará si éstos son perjudiciales, neutros o beneficiosos.

Las mutaciones proporcionan la base para las investigaciones en genética.
La variabilidad fenotípica resultante permite investigar los genes que controlan las características que se han modificado.
En este sentido, las mutaciones sirven de "marcadores" para identificar los genes, de manera que puedan seguirse durante la transmisión de padres a hijos.
Sin la variabilidad fenotípica proporcionada por la mutación, sería imposible realizar análisis genéticos.
Por ejemplo, si todas las plantas de guisantes tuviesen el mismo fenotipo, Mendel no habría tenido ninguna base para sus experimentos.
Debido a la importancia de las mutaciones, se ha prestado una gran atención a su origen, inducción y clasificación.

Determinados organismos se prestan a la inducción de mutaciones, que pueden detectarse y estudiarse fácilmente en ciclos biológicos relativamente cortos.

Los virus, las bacterias, los hongos, las moscas de la fruta y los ratones cumplen estos criterios en diversos grados. 
Así, a menudo se han utilizado estos organismos para estudiar la mutación y la mutagénesis, y también han contribuido, en otros experimentos, a aspectos más generales del conocimiento de la genética.

Una vez hayamos completado la exposición de la mutación, dirigiremos la atención hacia dos temas relacionados: 
la reparación del DNA y los elementos genéticos transponibles.
Estos temas son extensiones lógicas de la mutación génica.
Los procesos de reparación sirven para contrarrestar las mutaciones.
Los elementos transponibles alteran a menudo la estructura normal de los genes, creando por lo tanto mutaciones.

Mutaciones aleatorias versus mutaciones adaptativas

Aunque antes de 1943 se sabía que en cultivos puros de bacterias podían aparecer algunas células que mostraban variación hereditaria, especialmente en lo referente a la supervivencia en diferentes condiciones ambientales, la fuente de esta variación se debatía apasionadamente. 
La mayoría de bacteriólogos creían que los factores ambientales incluían cambios en determinadas bacterias, permitiéndoles sobrevivir o adaptarse a las nuevas condiciones.
Por ejemplo, se sabía que algunas cepas de Escherichia eran sensibles a la infección del bacteriófago T1.
La infección del bacteriófago conduce a la reproducción del virus a expensas de la célula bacteriana, que se lisa o se destruye (véase la Figura 10.5).
Si se distribuye homogéneamente T1 en una placa con E. coli , casi todas las células se lisan.
Sin embargo, algunas (pocas) células de E. coli sobreviven a la infección y no se lisan. 
Si estas células se aíslan y se establecen cultivos puros de ellas, todos sus descendientes son resistentes a la infección de T1.
La hipótesis de la adaptación , propuesta para explicar este tipo de observaciones, implica que la interacción entre el fugo y la bacteria es esencial para la adquisición de la inmunidad.
En otras palabras, el fugo ha «inducido» de algún modo resistencia en la bacteria.

Las mutaciones aleatorias (o espontáneas) proporcionan un modelo alternativo para explicar el origen de la resistencia a la infección de T1 en E. coli .
En 1943, Salvador Luria y Max Delbruck presentaron la primera prueba directa de que la mutación en bacterias se produce aleatoriamente en el cromosoma bacteriano. 
Es decir, no se puede predecir cuándo o dónde se producirá la mutación en el cromosoma.
Este experimento marcó el inicio de la experimentación genética moderna en bacterias.

La prueba de la fluctuación de Luria-Delbruck

En un bonito ejemplo de experimentación analítica y teórica, Luria y Delbruck realizaron experimentos para diferenciar entre las hipótesis de mutación adaptativa y de mutación aleatoria.
En este experimento utilizaron el sistema de E. coli / T1 que acabamos de describir.

Cultivaron muchos cultivos líquidos de E coli sensible al fugo T1.
Después añadieron alícuotas de cada cultivo a placas de Petri que contenían medio de agar al que previamente se habían añadido bacteriófagos T1.
Después de incubarlas, contaron el número de colonias de bacterias resistentes en cada placa.
Fue fácil determinar el número de colonias resistentes, ya que las células mutantes eran las únicas que no se habían lisado, y por lo tanto las únicas que habían sobrevivido.
Para obtener datos cuantitativos, también determinaron el número exacto de bacterias que habían añadido a cada placa antes de la incubación. 

La lógica experimental que permita distinguir entre estas dos hipótesis era la siguiente:

1. Adaptación : Cada bacteria tiene una probabilidad pequeña, pero constante, de adquirir resistencia como consecuencia del contacto con los fagos de la placa de Petri.
Por lo tanto, el número de células resistentes dependerá sólo del número de bacterias y fagos que se han añadido a cada placa.
Los resultados finales deberían ser independientes de cualquier otra condición experimental. 
Por lo tanto, la hipótesis de la adaptación predice que si se utiliza un número constante de bacterias y de fagos en cada cultivo y si el tiempo de incubación es constante, se deberían observar fluctuaciones leves en el número de células resistentes entre las diferentes placas, y entre diferentes experimentos.

2. Mutación Aleatoria : por otro lado, si la resistencia se adquiere como consecuencia de mutaciones aleatorias, ésta se adquirirá durante la incubación en el medio líquido antes de sembrar , es decir, antes de que se produzca cualquier contacto con los fagos.
Si las mutaciones se producen pronto durante la incubación en el medio líquido, la reproducción posterior de la bacteria mutante producirá, en términos relativos, grandes cantidades de células resistentes.
Cuando la mutación se produce relativamente tarde durante esta incubación, habrá muchas menos células resistentes.


Por lo tanto, la hipótesis de la mutación aleatoria predice que se observará una fluctuación importante en el número de células resistentes al comparar los distintos experimentos.
Esta variación es un reflejo de los distintos tiempos en los que se ha producido la mayoría de mutaciones espontáneas durante el cultivo líquido.

Tabla 14.1

Experimento de Luria-Delbruck, que demuestra que las mutaciones espontáneas son la fuente de la resistencia a fagos en bacterias

N° de nuestras.
Número de bacterias resistentes a T1.
Mismo cultivo (control).
Diferentes cultivos.


1.
14.
6.


2.
15.
5.


3.
13.
10.


4.
21.
8.


5.
15.
24.


6.
14.
13.


7.
26.
165.


8.
16.
15.


9.
20.
6.


10.
13.
10.


Media.
16.7.
26.2.


Varianza.
15.0.
2178.0.


La Tabla 14.1 muestra un grupo de valores representativos de los experimentos de Luria-Delbruck.
La columna central muestra el número de mutantes recuperados de una serie de alícuotas que provienen de un único cultivo líquido.
En un cultivo grande, como el anterior, las diferencias experimentales se igualan, ya que el cultivo está mezclándose constantemente.
Por lo tanto, estos valores sirven de control ya que el número de mutantes en cada alícuota debería ser casi idéntico. 
Como se ha predicho, se observa poca fluctuación. 
Sin embargo, la columna de la derecha muestra el número de mutantes resistentes recuperados de una única alícuota obtenido de cada uno de los 10 cultivos líquidos independientes. 
La amplitud de la fluctuación de estos valores apoyará a una sola de estas hipótesis alternativas. 
Por este motivo, este experimento se denomina prueba de la fluctuación .
Sí se observa una gran fluctuación entre cultivos, apoyando la naturaleza aleatoria de las mutaciones.

La fluctuación se mide mediante la varianza, que es un cálculo estadístico.

La conclusión obtenido en el experimento de Luria-Delbruck de que las mutaciones aleatorias justifican la variación hereditaria en bacterias, también recibió otras confirmaciones experimentales.
Sin embargo, los partidarios más fieles de la teoría de la adaptación no se convencieron hasta la década de los 50.

Mutación adaptativa en bacterias 

Aunque ya no se ha dudado más del concepto de mutación aleatoria en virus, en bacterias e incluso en organismos superiores, la posibilidad de que organismos como las bacterias puedan también «seleccionar» un conjunto específico de mutaciones que se produzcan como resultado de presiones ambientales ha continuado intrigando a los genéticos.
Dos investigaciones independientes, publicadas en 1988 por John Cairns y por Barry Hall y sus colaboradores han proporcionado pruebas preliminares de que esto también podría suceder.
Aunque los descubrimientos que se presentan a continuación todavía son controvertidos, actualmente este tema es objeto de una investigación de gran importancia potencial.

Cairns diseñó un protocolo experimental que mejora el utilizado en la prueba de la fluctuación de Luria y Delbruck, comentado anteriormente.
El protocolo se diseñó para detectar mutaciones no aleatorias que surgiesen en respuesta a factores del ambiente en el que se cultivaban las bacterias.
Los resultados de este experimento sugieren que algunas bacterias podrían seleccionar mutaciones que las «adaptasen» al ambiente. 

En vez de utilizar la característica de resistencia al fugo T1, en la que las únicas células que sobreviven son las que han mutado, Cairns utilizó en su experimento una cepa de bacterias que contenían una mutación que no les permitía utilizar la lactosa como fuente de carbono ( lac- ) (Véase la exposición del operón lac en el Capítulo 18).
Primero, las células se hicieron crecer en un medio líquido completo que les proporcionaba una fuente de carbono distinta a la lactosa.
De esta manera tanto las células lac- como cualquier mutante lac+ espontáneo podían crecer y reproducirse adecuadamente.
Posteriormente, se sembraron alícuotas de las células en placas de Petri que contenían medio mínimo al que se había añadido lactosa como fuente de carbono.
Las células lac- , que son la inmensa mayoría, sobrevivirán en las placas, pero debido a que no hay ninguna fuente de carbono que puedan metabolizar, no podrán proliferar ni formar colonias.
Por otra parte, las células que han mutado a lac+ , tanto si la mutación se ha producido durante la fase de cultivo líquido como si ha sido en la placa, formarán colonias y se detectarán.
Aquellas células que no hayan mutado a lac+ en el cultivo líquido, tendrán la oportunidad de hacerlo después de sembradas, y también podrán detectarse, ya que formarán colonias.
Esta es la diferencia experimental más importante entre este enfoque y el que utilizaron Luria y Delbruck 45 años antes.

Utilizando un análisis matemático más sofisticado que el que utilizaron Luria y Delbruck, Cairns intentó identificar el mismo tipo de «fluctuación», o su ausencia, como indicación de mutación aleatoria (espontánea) o adaptativa, respectivamente.
¡ Cairns obtuvo ambos tipos de datos!
Se calculó la distribución compuesta para el caso en que se produjesen mutaciones espontáneos en el medio líquido en momentos aleatorios (creando «fluctuaciones») y para el caso en que surgiesen mutaciones dirigidas como respuesta a la presencia de lactosa en las placas de Petri.
De hecho, se observó dicha distribución compuesta.

Pero,
¿cómo se puede probar que, de hecho, la elevada frecuencia de mutación producida en las placas fuese en respuesta a la lactosa?
Para responder a esta pregunta, Cairns y sus colaboradores se preguntaron si otras mutaciones, no relacionadas con el metabolismo de la lactosa, también se habían producido en las placas.
Eligieron detectar mutaciones de ValR , que confieren resistencia a altas concentraciones del aminoácido valina.
Este ensayo se realizó fácilmente cubriendo las placas con agar que contenía valina y glucosa. 
En estas condiciones, sólo los mutantes ValR crecerán.
Cairns y sus colaboradores razonaron que si las mutaciones lac+ no se hubiesen producido necesariamente en respuesta a la lactosa, las mutaciones ValR deberían aparecer con una distribución similar a la observada previamente para las mutaciones lac+ .
El resultado fue que las placas que acumulaban mutantes lac+ no acumulaban mutantes ValR al mismo tiempo.

Estos investigadores concluyeron que el incremento en la frecuencia de mutación que afectaba al metabolismo de la lactosa era consecuencia de la presencia de lactosa en el medio.
Por supuesto, ésta es una inferencia indirecta. 

El experimento de Cairns sugiere que la maquinaria genética de la célula bacteriana puede responder al ambiente produciendo mutaciones adaptativas.
Esta conclusión es contraria a la creencia actual de que las mutaciones son sucesos completamente espontáneos, de los cuales la mayoría se producen como errores aleatorios durante la replicación del DNA. 
Como Cairns subrayó, estos descubrimientos parecen apoyar, a primera vista, la idea romántica de que la vida es misteriosa y de que determinadas observaciones no pueden explicarse en términos simples y directos, basados en las leyes de la física.
Sea como sea, las observaciones de Cairns no son casos aislados.

El experimento de Barry Hall demostró una respuesta adaptativa parecida, al hacer crecer E. coli en salicina.
En este caso, es interesante el hecho de que se precisan dos cambios genéticos.
El primero se produjo en respuesta a la salicina, aunque la mutación no ofrecía ninguna ventola selectiva sin una segunda alteración genética, la eliminación, o delación, de un segmento de nucleótidos denominado secuencia de inserción (IS) .
Además, los cambios deben producirse secuencialmente. 
Éstos se producen en frecuencias mucho más altas que las predichas, siempre y cuando haya salicina en el medio.
La frecuencia observada es varios órdenes de magnitud superior.

¿Qué explicación puede justificar observaciones como éstas?
Como Barry Hall subrayó, el no poder explicar adecuadamente estos fenómenos podría atribuirse a que ignoramos los mecanismos fundamentales y a las tasas de mutación en las células que no están creciendo .
Esta información y este conocimiento son extremadamente importantes ya que esta condición fisiológica es mucho más parecida a un ambiente natural que la que se encuentra en un medio completo en condiciones de laboratorio.
Una propuesta es que en condiciones de presión nutricional (ayuno), las bacterias podrían activar mecanismos que crean un estado hipermutable en genes que incrementan la supervivencia.

La idea importante de esta exposición, en un libro de texto como éste, es que a menudo el conocimiento puede provenir de observaciones que, al principio, no pueden explicarse. 
Estas observaciones aparentemente inexplicables, los debates resultantes y las controversias que generan, son los que mantienen constantemente la intriga y el interés en la ciencia.

Clasificación de las mutaciones 

Ahora vamos a ver cómo se clasifican las mutaciones. 
Las mutaciones pueden organizarse de distintas maneras, que no son excluyentes, sino que dependen de los aspectos de la mutación que se están investigando o exponiendo. 
En esta sección describiremos tres tipos de características diferentes de las mutaciones.

Mutaciones espontáneas versus mutaciones inducidas

Independientemente de la característica aleatoria de las mutaciones, toda mutación se describe como espontánea o inducida .
Aunque estas dos categorías tienen un cierto solapamiento, las mutaciones espontáneas son las que se producen en la naturaleza.
No hay ningún agente específico asociado a ellas y se supone, en general, que son debidas a cambios aleatorios de la secuencia nucleotídica de los genes.
La mayoría de estas mutaciones están relacionadas a procesos químicos normales que alteran la estructura de las bases nitrogenadas que forman parte de genes ya existentes.
Se cree que la mayoría de las mutaciones espontáneas se producen durante el proceso enzimático de replicación del DNA, idea que trataremos posteriormente en este capítulo.
Una vez se produce un error en el código genético, éste puede reflejarse en la composición de aminoácidos de la proteína que codifica.
Si el aminoácido alterado se encuentra en una parte esencial de la molécula para su estructura o para su actividad bioquímica, puede producirse una alteración de su función.

En contraste a estos sucesos espontáneos, se consideran mutaciones inducidas aquellas que surgen como resultado de la influencia de cualquier factor artificial.
En general, se está de acuerdo en que cualquier fenómeno natural que aumente la reactividad química de las células inducirá mutaciones.
Por ejemplo, la mayoría de los organismos están expuestos a fuentes de energía como son las radiaciones cósmicas, las provenientes de minerales y la radiación ultravioleta del Sol.
Como tales, estas fuentes de energía pueden ser factores que causen mutaciones espontáneas.
Sin embargo, la primera demostración de inducción artificial de mutaciones se produjo en 1927, cuando Herman J. Muller publicó que los rayos X pueden provocar mutaciones en Drosophila.
En 1928, Lewis J. Stadler comunicó el mismo descubrimiento en la cebada.
Además de los diversos tipos de radiaciones, se conoce un amplio espectro de agentes químicos que son mutagénicos, cuyas características examinaremos en este capítulo.

Mutaciones gaméticas versus mutaciones somáticas

Cuando se consideran los efectos de las mutaciones en organismos eucarióticos, es importante distinguir si el cambio se produce en células somáticas o en gametos.
Las mutaciones surgidas en células somáticas no se transmiten a las generaciones posteriores.
Además, si generan alelos autosómicos recesivos, generalmente no tienen consecuencias para el organismo.
Es probable que la expresión de la mayoría de estas mutaciones quede enmascarada por el alelo dominante.
Las mutaciones somáticas tendrán un efecto mayor si son dominantes o si están ligadas al cromosoma X, ya que es probable que estas mutaciones se expresen inmediatamente.
Del mismo modo, el efecto será mayor si la mutación somática se ha producido pronto en el desarrollo, donde células indiferenciadas originan diversos tejidos y órganos diferenciados. 
A menudo, las mutaciones producidas en tejidos adultos quedan enmascaradas por los miles de células no mutantes que realizan la función normal.

Las mutaciones en los gametos o en los tejidos que los forman implican a la línea germinal, y son de gran importancia, ya que se transmiten a la descendencia.
Pueden expresarse en todas las células de un descendiente.
Las mutaciones autosómicas dominantes se expresarán en el fenotipo de la primera generación.
Las mutaciones recesivas ligadas al cromosoma X , surgidas en los gametos de una hembra homogamética, pueden expresarse en los descendientes machos, que son hemizigotos, siempre que reciba el cromosoma X afectado.
Debido a la heterozigosidad, la aparición de una mutación autosómica recesiva en los gametos tanto de machos como de hembras (incluso los alelos letales) puede pasar desapercibida durante muchas generaciones, hasta que el alelo resultante se haya extendido por la población.

El nuevo alelo se manifestará sólo cuando un apareamiento aleatorio reúna dos copias de éste en homozigosis.

Organización genómica del DNA

CONCEPTOS DEL CAPÍTULO

El DNA que forma el genoma de los organismos está organizado de manera acorde con la complejidad de la estructura del huésped a la que está asociado.
Los virus, las bacterias, las mitocondrias y los cloroplastos contienen una molécula de DNA corta, a menudo circular, y relativamente exenta de proteínas.
Las células eucarióticas contienen mayores cantidades de DNA, que se encuentra organizado en nucleosomas y se presenta en forma de fibras de cromatina. 
Este incremento en complejidad está relacionado con la mayor cantidad de información genética presente así como con la mayor complejidad asociada a sus funciones genéticas.
En eucariotas, los genes se organizan de maneras diversas, desde copias únicas hasta familias de genes relacionados ordenados en tándem.
El genoma eucariótico contiene grandes cantidades de DNA no codificante, que en ocasiones interrumpe las partes codificantes de los genes. 

Una vez se comprendió que el DNA alberga la información genética, fue muy importante determinar cómo se organiza el DNA en genes y cómo estas unidades básicas de la función genética se organizan en cromosomas. 
Brevemente, la pregunta más importante era cómo se organiza el material genético para formar el genoma de los organismos.
Ha habido mucho interés en esta cuestión ya que conocer la organización del material genético y de las moléculas asociadas es importante para la comprensión de muchas otras áreas de la genética.
Por ejemplo, cómo se almacena la información genética, cómo se expresa y cómo se regula debe estar relacionado con la organización molecular de la molécula genética, el DNA.

Cómo varía la organización genómica en diferentes organismos - desde virus y bacterias hasta eucariotas - proporcionará, sin duda, una mejor comprensión de la evolución de los organismos en la Tierra.
En la genética de los eucariotas, un par de preguntas importantes son cómo el DNA se organiza con proteínas para formar la cromatina, y cómo las fibras de cromatina características de la interfase se condensan en las estructuras cromosómicas visibles durante la mitosis y la meiosis. 

En este capítulo proporcionaremos, primero, las generalidades de las distintas maneras en que el DNA se organiza en cromosomas, incluyendo ejemplos de virus, bacterias, y eucariotas. 
El material genético se ha estudia do utilizando numerosos enfoques, incluyendo el análisis molecular y la visualización directa por microscopía óptica y electrónica.

Después consideraremos cómo se organizan los genes en los cromosomas.
Como veremos, esta organización es menos compleja en bacteriófagos y en bacterias.
En sus genomas, un cromosoma consiste básicamente en una ordenación de genes contiguos densamente empaquetados. 
La inmensa mayoría del DNA codifica información genética o está formada por elementos de control implicados en la regulación de la expresión génica. 

En cambio, la inmensa mayoría del DNA en la mayoría de eucariotas no codifica información genética y no está implicado en la regulación genética. 
Los genes no están estrechamente empaquetados. 
Como vimos anteriormente (véase el Capítulo 12), una porción considerable de muchos genes eucarióticos está formada por secuencias de DNA no codificantes denominadas intrones y por secuencias flanqueantes que no forman parte del transcrito final de RNA.
Además, veremos que, a menudo, genes relacionados están dispuestos y regulados en grupos o familias, y que una gran porción del DNA total que forma el genoma de los eucariotas consiste en regiones no codificantes que a menudo están ocupadas por secuencias de DNA repetitivas.

Muchos de los descubrimientos relacionados con la organización del genoma realizados durante las últimas décadas, que explicaremos en este capítulo, representan algunos de los descubrimientos más excitantes e inesperados hechos en el campo de la genética.
Para todos los genéticos, pero especialmente para los formados antes de la década de los 70, estos descubrimientos han sido una fuente constante de asombro.
Quizá estaremos mejor preparados para aprender la complejidad de los genes y de los genomas, especialmente los eucarióticos, si prestamos atención a la cita del libro de Lewis Carroll que fue perspicazmente incluida por Richard B. Goldschmidt en su visión del futuro durante su alocución presidencial en el IX International Congress of Genetics, en 1954:

Cromosomas víricos y bacterianos 

Comparados con los cromosomas eucarióticos, los cromosomas de los virus y de las bacterias son mucho menos complicados. 
Normalmente consisten en una única molécula de ácido nucleico, desprovista en general de proteínas asociadas.

Dentro del único cromosoma de los virus y de las bacterias hay mucha menos información que en los múltiples cromosomas que forman el genoma de los eucariotas.
Estas características han simplificado enormemente el análisis, proporcionando una idea bastante clara de la estructura de los cromosomas víricos bacterianos.

El cromosoma de los virus está formado por una molécula de ácido nucleico -DNA o RNA- que puede ser de cadena sencilla o de doble cadena.
Pueden ser estructuras circulares (lazos cerrados), o pueden ser moléculas lineares.
El DNA de cadena sencilla del bacteriófago X174 y el DNA de doble cadena del virus del polioma son moléculas circulares alojadas dentro de la cubierta proteica del virus maduro.
En cambio, el bacteriófago lambda (l) posee, antes de la infección, una molécula de DNA de doble cadena lineal, que se cierra formando un círculo tras infectar la célula huésped. 
Otros virus, como los bacteriófagos T-par , tienen cromosomas de DNA lineales de doble cadena, que no forman círculos dentro de la bacteria huésped. 
Así, el ser circulares no es un requisito imprescindible para la replicación de algunos virus.

Las moléculas de ácido nucleico de los virus se han visualizado con el microscopio electrónico.
Una característica constante compartida por los virus, las bacterias y las celarlas eucarióticas es la capacidad de empaquetar una molécula de DNA extremadamente larga en un volumen relativamente pequeño.
En &bsol;, el DNA tiene 17 &mgr;m de longitud y debe encajar en la cápside del fugo, cuyos lados tienen menos de 0.1 &mgr;m.

La Tabla 17.1 compara la longitud de los cromosomas de varios virus con el tamaño de su cápside.
En todos ellos debe llevarse a cabo una proeza de empaquetamiento. 
Rara vez el espacio disponible en la cápside del virus es el doble que el volumen del cromosoma.
En muchos casos se llena casi todo el espacio, lo que indica que el empaquetamiento es casi perfecto.
Una vez empaquetado dentro de la cápside, el material genético es funcionalmente inerte hasta que se libera en una célula huésped.

Tabla 17.1

Material genético de alearnos virus y bacterias representativos

Ácido nucleico.
Organismo.
Tipo.
CS o DC.
Longitud (&mgr;m).
Tamaño global de la cúspide vírica o de la bacteria (&mgr;m).


X174 .
DNA.
CS .
2,0.
0.025 x 0.025 .


Virus.
Virus del mosaico del tabaco.
RNA.
CS .
3,3.
0.30 x 0 02.


Fago lambda.
DNA .
DC.
17,0.
0,07 x 0,07.


Fago T2.
DNA.
DC.
52,0.
0.07 x 0.10.


Bacterias.
Hemophilus influenzae .
DNA.
DC.
832,0.
1.00 x 0.30.


Escherichia .
DNA.
DC.
1200,0.
2.00 x 0.50 .


CS = cadena sencilla; DC = doble cadena .


Los cromosomas bacterianos también son relativamente simples.
Siempre están formados por una molécula de DNA de doble cadena, compactada en una estructura que se denomina nucleoide .
Eschenchia coli , la bacteria más extensivamente estudiada, tiene un cromosoma circular grande que mide aproximadamente 1.200 mm (1,2 mm ) de longitud. 
Cuando la célula se lisa suavemente y el cromosoma se libera, éste puede visualizarse con el microscopio electrónico.

Su DNA está asociado a varios tipos de proteínas de unión al DNA , como las denominadas HU y H .

Son proteínas pequeñas y abundantes en la célula, y contienen un alto porcentaje de aminoácidos cargados positivamente que pueden unirse iónicamente a las cargas negativas de los grupos fosfato del DNA [1] .
Como veremos pronto, estas proteínas se parecen estructuralmente a las moléculas denominadas histonas que se encuentran asociadas al DNA eucariótico. 
A diferencia del cromosoma vírico empaquetado, el cromosoma bacteriano no es funcionalmente inerte.
A pesar de la condición de compactación del cromosoma bacteriano, la replicación y la transcripción se producen con facilidad.

DNA superenrollado y circular

Una idea importante de como se organiza y se empaqueta el DNA proviene del descubrimiento del DNA superenrollado , característico de las moléculas circulares cerradas covalentemente y de los lazos cromosómicos.
El DNA superenrollado se propuso inicialmente como resultado de una investigación de moléculas de DNA de doble cadena provenientes del virus del polioma, que causa tumores en ratón.
En 1963, se observó que cuando este DNA se someta a ultracentrifugación, se separaba en tres c diferentes, cada uno con una diferente densidad y compactación.
El menos compacto, y por lo tanto menos denso, tema una velocidad de sedimentación menor; las otras dos fracciones mostraban velocidades mayores debido a su mayor compactación y densidad.
El peso molecular de las tres era idéntico.

En 1965, Jerome Vinograd propuso una explicación para las observaciones anteriores.
Postuló que las dos fracciones de mayor velocidad de sedimentación eran moléculas de DNA de polioma circulares, mientras que la fracción de menor velocidad de sedimentación contenía moléculas de DNA de polioma lineares. 
Las moléculas circulares cerradas son más compactas, y sedimentan más rápidamente que las moléculas lineales de la misma longitud y peso molecular. 

Vinograd propuso además que la fracción más densa de las dos moléculas circulares consistía en hélices de DNA cerradas covalentemente cuya doble hélice estaba ligeramente menos enrollada comparada con las moléculas circulares menos densas.
Las fuerzas energéticas que estabilizan la doble hélice se resisten a este menor enrollamiento, lo que provoca un superenrollamiento para mantener el emparejamiento de bases normal.
Vinograd propuso que la forma superenrollada es la que provoca un empaquetamiento más compacto, lo que incrementa la velocidad de sedimentación.

Las transiciones descritas anteriormente pueden visualizarse en la Figura 17.1.

Considere una molécula lineal de doble cadena que sea una hélice dextrógira normal de Watson-Crick [Figura 17.1 (a)].
Esta hélice contiene 20 giros completos, que definen el número de unión de esta molécula (L = 20).
Si se sellan los extremos de esta molécula se forma un círculo cerrado que está energéticamente relajado [Figura 17.1(b)].
Suponga, sin embargo, que se corta el circulo, se desenrollan unas cuantas vueltas completas de la hélice, y se vuelven a sellar sus extremas [Figura 17.1(c)].

Esta estructura, en la que L es igual a 18, está energéticamente «forzada», y en consecuencia sólo existirá temporalmente en esta forma.

Para asumir una conformación energéticamente más estable, la molécula puede formar superenrollamientos en dirección opuesta a la de la hélice desenrollada. 

En nuestro ejemplo [Figura 17.1(d)] se introducen espontáneamente dos superenrollamientos negativos, lo que restablece el número total de «vueltas» de la hélice.
El término «negativo» hace referencia al hecho de que, por definición, los superenrollamientos son levógiros. 
El resultado final es la formación de una estructura más compacta con una mayor estabilidad física. 

En la mayoría de moléculas de DNA circular de las bacterias y de sus fagos, el DNA está ligeramente deseo rallado [como en la Figura 17.1 (c)].
Por ejemplo, el virus SV40 contiene 5.200 pares de bases, y cada vuelta completa de la hélice está ocupada por 10,4 pares de bases. 
El número de unión puede calcularse como L = 5.200/10,4 = 500 Cuando se analiza el DNA circular de SV40, éste está desenrollado 25 vueltas, por lo que L es igual a 475.
Como puede predecirse, se observa la presencia de 25 superenrollamientos negativos.
En E. coli se observa incluso un número mayor de superenrollamientos, lo que facilita enormemente la condensación del cromosoma en la región del nucleoide.

Cuando dos moléculas idénticas tienen un número de unión diferente, se dice que son topoisómeros. 
La única manera en que dos topoisómeros cuyos extremos están fijados (no están libres para girar) puedan interconvertirse es cortando uno o ambos extremos de las cadenas y enrollando o desenrollando la hélice antes de volver a sellar los extremos.
Biológicamente, hay un grupo de enzimas, denominadas topoisomerasas , que pueden realizar estos procesos.
Descubiertas inicialmente por Martin Gellert y James Wang, estas moléculas catalíticas pueden ser de tipo I o de tipo II, en función de si cortan una o ambas cadenas de la hélice, respectivamente. 
En E. coli la topoisomerasa I sirve para reducir el número de superenrollamientos negativos de una molécula de DNA circular.
La topoisomerasa II introduce superenrollamientos negativos en el DNA.

Se cree que esta enzima se une al DNA, lo enrosca, corta las dos cadenas, y luego las pasa por el «lazo» que ha creado.
Cuando se restablecen los enlaces fosfodiéster, el número de unión ha disminuido, por lo que se forman espontáneamente uno o más superenrollamientos. 

En eucariotas también se encuentra DNA superenrollado y topoisomerasas.
Aunque en general los cromosomas de estos organismos no son circulares, pueden hacerse superenrollamientos cuando zonas del DNA están incrustadas en una trama de proteínas asociadas a las fibras de cromatina.
Esta asociación forma extremos «fijados» que proporcionan la estabilidad para el mantenimiento de los superenrollamientos una vez las topoisomerasas los han introducido. 

La transcripción del DNA eucariótico en RNA afronta varios obstáculos que los procariotas no encuentran. 
El más importante es la manera en la que el DNA está empaquetado en las estructuras denominadas nucleosomas .
En la mayoría de las investigaciones realizadas hasta la fecha se ha implicado a la formación de superenrollamientos inducidos por topoisomerasas como uno de los factores más importantes para facilitar la transcripción. 
Estas enzimas podrían también desempeñar otras funciones genéticas implicadas en cambios conformacionales del DNA.

Tabla 17.2

Tamaño del mtDNA de diversos organismos

Organismo.
Tamaño en kilobases.


Especie humana.
16,6.


Ratón.
16,2.


Xenopus (rana).
18,4.


Drosophila (mosca de la fruta).
18,4.


Saccharomyces (levadura).
84,0.


Pisum sativum (guisante).
110,0.


DNA mitocondrial y cloroplástico

Numerosas observaciones han demostrado que tanto las mitocondrias como los cloroplastos contienen su propia información genética.
El descubrimiento de mutaciones en levadura, en otros hongos, y en plantas que alteran la función de estos orgánulos sugirió este hecho (véase el Capítulo 8).
Además, la transmisión de estas mutaciones no suele mostrar los patrones de herencia biparental característicos de los genes nucleares.
Lo que se observa es un modo de herencia uniparental , en el que las características pasan de la madre [2] a los descendientes. 

Puesto que la herencia de las mitocondrias y de los cloroplastos es también uniparental, estas observaciones sugirieron que estos orgánulos podían albergar su propio DNA que influencia sus funciones.

Por lo tanto, los genéticos han buscado pruebas más directas de la presencia de DNA en estos orgánulos. 
Utilizando el microscopio electrónico no sólo se ha documentado la presencia de DNA en estos dos orgánulos, sino que también se ha visto que este DNA tiene una forma diferente al DNA nuclear de las células eucariotas que albergan a estos orgánulos.
¡Este DNA es muy parecido al observado en virus y en bacterias!
Como veremos, esta semejanza, junto con otras observaciones, condujo a la idea de que las mitocondrias y los cloroplastos provienen de organismos primitivos de vida libre muy parecidos a las bacterias.

Organización molecular y función del DNA mitocondrial

Actualmente se dispone de mucha información de los aspectos moleculares del DNA mitocondrial ( mtDNA ) y de la función de sus genes.
En la mayoría de eucariotas, el mtDNA es un dúplex circular que se replica de manera semiconservativa y que no está asociado con las proteínas cromosómicas características del DNA eucariótico.
Esto puede verse al examinar la información presentada en la Tabla 17.2.
En diferentes animales, el mtDNA está formado por 16.000 a 18.000 pares de bases (16-18 kb ).
En vertebrados, hay entre S y 10 moléculas de DNA por orgánulo. 
En las mitocondrias de las plantas hay cantidades considerablemente mayores de mtDNA , donde 100 kb no son anormales.

Ahora pueden hacerse varias afirmaciones generales acerca del mtDNA .

Parece que hay pocos o ningún gen repetido, y que la replicación depende de enzimas codificadas por el DNA nuclear.
Se ha identificado que los genes mitocondriales codifican los RNA ribosómicos, para más de 20 tRNA , y para numerosos productos necesarios para las funciones de respiración celular de estos orgánulos.

El aparato de síntesis proteica y los c moleculares para la respiración celular provienen conjuntamente del DNA nuclear y del mitocondrial.

Los ribosomas encontrados en este orgánulo son diferentes de los citoplásmicos.

El coeficiente de sedimentación de estas partículas varía entre especies.
Los datos de la Tabla 17.3 muestran que los coeficientes de sedimentación de los ribosomas mitocondriales varían considerablemente (55S a 60S en vertebrados, 70S en algunas algas y en algunos hongos, y 80S en determinados protozoos y hongos).

Tabla 17.3

Coeficientes de sedimentación de los ribosomas mitocondriales

Reino.
Ejemplos.
Coeficiente de sedimentación.


Animales.
Vertebrados.
55-60.
Insectos.
60-71.


Protistas.
Euglena.
71.
Tetrahymena.
80.


Hongos.
Neurospora.
73-80.
Saccharomyces.
72-80.


Vegetales.
Maíz.
77.


Figura 17.1

Esquema de las transformaciones que conducen al superenrollamiento del DNA circular, como se describe en el texto

L es el número de unión.

[11] Se ha propuesto un modelo por el que estas proteínas organizarían el cromosoma en dominios.
Cada dominio podría consistir en un lazo de unas 40 kb de DNA negativamente superenrollado (véase a continuación la explicación del DNA superenrollado).

[12] De hecho, pasan de un progenitor a sus descendientes, ya que, por ejemplo, no se puede hablar propiamente de «madre» en la herencia del carácter petite en Saccharomyces o en la del carácter Poky en Neurospora (véase el Capítulo 8).
Además, también se conocen casos de herencia uniparental en los que es el «padre» (el progenitor masculino) quien contribuye a este tipo de herencia, como la herencia paterna de plástidos en coníferas (gimnospermas).

Regulación de la expresión génica en bacterias y en bacteriófagos 

CONCEPTOS DEL CAPÍTULO 

La expresión eficiente de la información genética depende de mecanismos reguladores que activan o reprimen la actividad génica.
En bacterias, hay mecanismos que regulan los genes para satisfacer las necesidades metabólicas de la célula.
La regulación génica en bacteriófagos determina el tipo de existencia del virus dentro del huésped bacteriano.
En bacterias y en sus fugas, estos mecanismos proporcionan respuestas a las condiciones celulares y extracelulares más comunes.

En capítulos anteriores se estableció cómo el DNA se organiza en genes, cómo los genes almacenan la información genética, y cómo se expresa esta información.
Ahora consideraremos una de las preguntas más importantes de la genética molecular:
¿cómo se regula la expresión genética?
La prueba que apoya la idea de que los genes pueden activarse y desactivarse es muy convincente.
El análisis detallado de las proteínas de Escherichia coli ha demostrado que las concentraciones de las aproximadamente 4.000 cadenas polipeptídicas codificadas por su genoma varían enormemente.
De algunas proteínas sólo hay de 5 a 10 moléculas por célula, mientras que de otras, como las proteínas ribosómicas y muchas de las proteínas implicadas en la ruta glucolítica, hay hasta 100.000 copias por célula.
Aunque en procariotas hay continuamente un nivel basal (unas pocas copias) de la mayoría de productos génicos, este nivel puede incrementar dramáticamente para posteriormente decrecer.
Esto indica claramente que deben existir mecanismos reguladores fundamentales que controlen la expresión de la información genética.

En este capítulo exploraremos lo que se conoce de la regulación de la expresión genética en bacterias y en bacteriófagos.
En muchos casos se dispone de información muy detallada.
En el próximo capítulo nos centraremos en la regulación de la expresión génica en eucariotas.

Regulación genética en procariotas:

generalidades 

La regulación de la expresión génica se ha investigado extensamente en procariotas, especialmente en E. coli .
Se ha encontrado que se han desarrollado eficientes mecanismos genéticos que activan y desactivan los genes en función de las necesidades metabólicas de la célula de los respectivos productos génicos.
La actividad de las enzimas también puede regularse una vez éstas se han sintetizado, pero nos centraremos básicamente en lo que se sabe de la regulación a nivel de la transcripción génica.
Es interesante recordar que, para que las funciones celulares sean eficientes en diferentes condiciones ambientales, lo importante son los niveles de las proteínas presentes o ausentes en la célula.

Que los microorganismos regulen la síntesis de productos génicos no es un concepto especialmente nuevo.
En 1900 ya se demostró que cuando hay lactosa (un disacárido que contiene galactosa y glucosa) en el medio de cultivo de levaduras, se producen enzimas específicas del metabolismo de la lactosa.
Cuando no hay lactosa, las enzimas no se producen.
Pronto se demostró que las bacterias se «adaptan» al ambiente químico en el que se encuentran produciendo determinadas enzimas sólo cuando hay substratos específicos.
Estas enzimas se denominaron adaptativas .
En cambio, las enzimas que se producen continuamente, sea cual sea la composición química del ambiente, se denominaron constitutivas .
Desde entonces, la palabra adaptativo se ha remplazado por otra más exacta en enzimología descriptiva.
Ahora se denominan enzimas inducibles , lo que refleja la función del substrato, que sirve de inductor de su producción.

Investigaciones más recientes han revelado otros casos en los que la presencia de una molécula específica causa la inhibición de la expresión genética.

Generalmente, esto sucede en las moléculas que son productos finales de rutas biosintéticas.
Por ejemplo, las células bacterianas pueden sintetizar el aminoácido triptófano.
Si se suministra este aminoácido exógenamente al ambiente o al medio de cultivo, no es energéticamente rentable para la célula sintetizar las enzimas necesarias para la producción de triptófano; así pues, se ha desarrollado un mecanismo mediante el que el triptófano desempeña una función en la represión de la transcripción del RNA esencial para la producción de las enzimas biosintéticas apropiadas.
Al revés que el sistema inducible que controla el metabolismo de la lactosa, el que dirige la síntesis de triptófano se denomina reprimible .

Como veremos pronto, los ejemplos de represión, tanto si son inducibles como reprimibles, pueden estar bajo control positivo o negativo .
En el control negativo, la expresión genética se produce hasta que es desconectada por algún tipo de regulador.
En cambio, en el control positivo, la transcripción no se produce hasta que la molécula reguladora estimula directamente la producción de RNA.
En teoría, los dos tipos de control pueden dirigir sistemas inducibles y reprimibles.
Los ejemplos que se exponen en las siguientes secciones de este capítulo ayudarán a distinguir entre estos posibles mecanismos.

Metabolismo de la lactosa en E. coli :

un sistema génico inducible 

En procariotas, los genes estructurales tienden a estar organizados en grupos controlados por un sólo sitio de regulación.
En conjunto, este grupo de genes constituye lo que se denomina un operón .
El sitio regulador está unido al grupo génico que controla y se denomina elemento de regulación en cis .
Generalmente, los sitios de actuación en cis están localizados corriente arriba del grupo génico que regulan (en el extremo 5').

También generalmente, los genes del grupo producen productos con funciones relacionadas, como las reacciones implicadas en una ruta metabólica.
Las interacciones en los sitios de regulación conllevan la unión de moléculas que controlan la transcripción del grupo génico.
Estas moléculas se denominan elementos de regulación en trans , debido a que no están físicamente unidas a los genes que regulan.
Las interacciones en los sitios de regulación determinan si los genes se expresan o no.
En esta sección expondremos cómo se regula coordinadamente uno de estos grupos génicos bacterianos.

El grupo génico implicado en el metabolismo de la lactosa en E. coli se ha investigado en profundidad, y hoy en día sirve de paradigma para el estudio de la regulación genética.
Los primeros experimentos, realizados por Jacques Monod en 1946, y las importantes contribuciones de Joshua Lederberg, François Jacob, y Andre L'woff durante la siguiente década, acumularon pruebas genéticas y bioquímicas de la regulación de este grupo génico.
Esta investigación proporciona ideas de cómo se reprime la actividad génica responsable del metabolismo de la lactosa cuando no hay lactosa en el medio, y de cómo se induce cuando se dispone de lactosa.
En presencia de lactosa, la concentración de la enzima responsable de su metabolismo incrementa rápidamente de S a 10 moléculas por célula a miles de ellas.
Esta enzima se describe como inducible , y la lactosa sirve de inductor .

El descubrimiento de las unidades reguladoras que forman parte del grupo génico fue más importante que la comprensión de cómo se controla la expresión génica en este sistema.
Este grupo génico está formado por un gen regulador y por un sitio de regulación, ninguno de los cuales codifica las enzimas necesarias para el metabolismo de la lactosa.
Hay otros tres genes responsables de la producción de las enzimas implicadas en el metabolismo de la lactosa.
En la Figura 18.1 se esquematiza el grupo completo.
Como se muestra, los tres genes estructurales y los sitios reguladores adyacentes constituyen el operón.
En conjunto, el grupo completo funciona de manera integrada para proporcionar una respuesta rápida a la presencia o ausencia de lactosa.

Genes estructurales 

Los genes que codifican la estructura primaria de las enzimas se denominan genes estructurales .
El gen lacZ especifica la secuencia de aminoácidos de la enzima B - galactosidasa , que convierte la lactosa en glucosa y galactosa (Figura 18.2).
Esta conversión es esencial para que la lactosa pueda servir de fuente de energía primaria en la glucolisis.
El segundo gen, lacY , especifica la estructura primaria de la B- galactosidasa permeasa , una enzima que facilita la entrada de lactosa en la célula bacteriana.
El tercer gen, lacA , codifica la enzima transacetilasa , cuya función fisiológica no está del todo clara.

El examen de los genes que codifican estas tres enzimas se basa en el aislamiento de muchas mutaciones que eliminan la función de una u otra de estas enzimas.
Joshna Lederberg fue el primero en aislar y examinar estos mutantes lac- .
Las células mutantes que no pueden producir B - galactosidasa ( lacZ- ) o permeasa ( lacY ) activas son incapaces de utilizar la lactosa como fuente de energía.
También se encontraron mutaciones en el gen de la transacetilasa.
Los experimentos de cartografía realizados por Lederberg establecieron que los tres genes están estrechamente unidos (son contiguos), siguiendo el orden Z-Y-A (véase la Figura 18.1).

Hay otras dos observaciones importantes sobre lo que se descubrió de estos genes estructurales.
Primero, esta estrecha unión condujo a descubrir que los tres genes se transcriben como una sola unidad, lo que resulta en lo que se denomina un mRNA policistrónico (Figura 18.3).
En consecuencia, los tres genes se regulan coordinadamente ya que un solo mensajero sirve de base para la traducción de los tres productos génicos.
Además, se ha demostrado que tras la inducción por lactosa, la rápida aparición de las enzimas proviene de síntesis de novo de este mRNA.
Aunque este descubrimiento pueda parecer obvio o esperable, es contrario a la hipótesis de que la inducción de la actividad enzimática puede ser consecuencia de la activación de formas existentes pero inactivas de las enzimas.

El descubrimiento de las mutaciones de regulación 

¿Cómo activa la lactosa los genes estructurales, y cómo induce la síntesis de las enzimas relacionadas?
El descubrimiento y el análisis de los inductores gratuitos excluyó una de las posibilidades que podrían considerarse.
Estas moléculas, que son análogos químicos de la lactosa, se comportan como inductores pero no sirven de substrato para las enzimas cuya síntesis inducen.
Uno de estos inductores gratuitos es un análogo que contiene azufre, el isopropiltiogalactósido (IPTG) , que se muestra en la Figura 18.4.
El descubrimiento de que estas moléculas existen es una prueba sólida de que la inducción inicial no depende de la interacción entre el inductor y la enzima. 
¿Cuál es, pues, la función de la lactosa en la inducción?

La respuesta a esta pregunta requiere el examen de un tipo de mutaciones denominadas mutaciones constitutivas .
En este tipo de mutantes, las enzimas se producen sin tener en cuenta la presencia o la ausencia de lactosa.
Estas mutaciones se utilizaron para investigar el mecanismo regulador del metabolismo de la lactosa.

Los mapas de uno de los tipos de mutaciones constitutivas, denominadas lacI- , muestran que estas mutaciones están localizadas en un sitio del DNA cercano al de los genes estructurales, pero distinto a él.
Como veremos pronto, el gen lacI es un gen represor.
Hay un segundo conjunto de mutaciones que producen los mismos efectos y que está situado en una región adyacente a los genes estructurales.
Este tipo de mutaciones se denominan lacOc e identifican la región operadora del operón.
Puesto que en estos dos tipos de mutaciones se ha eliminado la capacidad de inducir la producción de estas enzimas (ya que las enzimas se producen continuamente), obviamente representan cambios que han alterado la regulación de estos genes estructurales.

El modelo del operón:

control negativo 

En 1961, Jacob y Monod propusieron un diseño de control negativo de la regulación denominado modelo del operón , en el que un grupo de genes se repula y se expresa como una unidad.
En este modelo específico (Figura 18.5), el operón está formado por los genes estructurales Z, Y y A, y por las secuencias adyacentes de DNA conocidas como región operadora.
Estos científicos argumentaron que el gen lacI regula la transcripción de los genes estructurales produciendo una molécula represora .
Hipotetizaron que el represor era una molécula alostérica ; esto significa que es una molécula que puede interaccionar con otra de manera reversible, lo que provoca un cambio conformacional de su forma tridimensional v un cambio en su actividad química.

Jacob y Monod sugirieron que el represor interacciona normalmente con la secuencia de la región operadora del DNA.
Cuando está interaccionando, inhibe la acción de la RNA polimerasa, reprimiendo la transcripción de los genes estructurales [Figura 18.5(b)].
Sin embargo, cuando hay lactosa presente, este disacárido se une al represor, provocando el cambio alostérico conformacional.

Este cambio altera el sitio de unión del represor, haciendo que no pueda interaccionar con el operador [Figura 18.5(c)].
En ausencia de la interacción represor-operador, la RNA polimerasa transcribe los genes estructurales, y se producen las enzimas necesarias para el metabolismo de la lactosa.
Puesto que la transcripción se produce sólo cuando el represor no se une a la región operadora, se dice que está ejerciendo un control negativo .

El modelo del operón utiliza estas posibles interacciones moleculares para explicar la regulación de los genes estructurales.
En ausencia de lactosa no se necesitan las enzimas codificadas por estos genes, y por eso se reprimen.
Cuando hay lactosa presente, ésta induce indirectamente la activación de los genes uniéndose al represor [1] .
Si se metaboliza toda la lactosa, no queda lactosa para unirse al represor, el cual puede unirse otra vez al DNA operador para reprimir la transcripción.

Las mutaciones constitutivas I- y Oc interfieren con estas interacciones moleculares, lo que permite la transcripción continua de los genes estructurales.
En los mutantes I-, el producto represor está alterado y no puede unirse a la región operadora, por lo que los genes estructurales están siempre activados.
En los mutantes O', la secuencia nucleotídica del DNA operador está alterada y no puede unir la molécula represora normal.
El resultado es el mismo:
los genes estructurales se transcriben continuamente.
En la Figura 18.5 (d) y (e) se esquematizan ambos tipos de mutaciones constitutivas.

Pruebas genéticas del modelo del operón 

El modelo del operón es especialmente bueno puesto que hace predicciones que pueden comprobarse para determinar su validez.
Las predicciones más importantes que deben examinarse son: 
(1) el gen I produce un producto celular difusible;
(2) la región O está implicada en la regulación pero no produce ningún producto; y
(3) la región O debe ser adyacente a los genes estructurales para poder regular su transcripción.


La construcción de bacterias parcialmente diploides (véase el Capítulo 6) permite evaluar estas suposiciones, especialmente las que predicen la presencia de elementos reguladores en trans .
Por ejemplo, determinados diseños de apareamiento posibilitan la construcción de genotipos en los que se introduce un gen I+ en un huésped I-, o en los que se añade una región O+ a un huésped Oc.
En estos casos, aparte del cromosoma completo del huésped aparecen uno o pocos genes extras escogidos.
Estos genes están insertados en un plásmido denominado factor F, designado como F' (véase el Capítulo 6).
El modelo del operón de Jacob-Monod predice que si se añade un gen I+ a una célula I-, la célula debería recuperar la capacidad de inducirse, ya que se produciría un represor normal.
En cambio, si se añade una región O+ a una célula o no debería tener ningún efecto en la producción constitutiva de las enzimas, ya que la regulación depende de una región O+ inmediatamente adyacente a los genes estructurales.

En la Tabla 18.1 se muestran los resultados de estos experimentos (Z representa los genes estructurales).
Los genes insertados se enumeran después de la denominación F'.
Los resultados de los dos experimentos que se acaban de describir son congruentes con el modelo de Jacob-Monod (parte B de la Tabla 18.1).
La parte C muestra los experimentos recíprocos, en los que se añade un gen I- o un gen Oc a células con fenotipo inducible silvestre.
El modelo predice que estos diploides parciales deben seguir siendo inducibles, y esto es lo que se observa.

Otra predicción del modelo del operón es que determinadas mutaciones del gen I deberían tener el efecto opuesto a I-.
Es decir, que este otro tipo de moléculas represoras mutantes, en vez de ser constitutivas por falta de interacción con el operador, no deberían poder interaccionar con el inductor (la lactosa).
En consecuencia, el represor siempre estaría unido a la secuencia operadora, y los genes estructurales estarían permanentemente reprimidos.
Si fuese así, la presencia de un gen I+ adicional no tendría ningún efecto (o tendría un efecto muy pequeño) en la represión.

De hecho, como se muestra en la parte D de la Tabla 18.1, se descubrió una mutación como la que se acaba de describir (Is), en la que el operón está siempre reprimido.
Un gen I+ adicional no repara la represión de la actividad génica.
Estas observaciones también apoyan el modelo de regulación génica del operón.

Figura 18.1

Organización del grupo génico y de las unidades reguladoras implicadas en el control del metabolismo de la lactosa 

Figura 18.2

Conversión catabólica del disacárido lactosa en los monosacáridos que la forman, la galactosa y la glucosa 

Figura 18.3

Genes estructurales del operón lac de E coli 

Los tres genes se transcriben en un solo mRNA policistrónico, que se traduce secuencialmente en las tres enzimas codificadas por el operón.

[13] Técnicamente, el inductor es la alolactosa, un isómero de la lactosa.
En el proceso inicial del metabolismo de la lactosa, la B - galactosidasa produce alolactosa (Nota del traductor:
Se puede producir alolactosa a pesar de que aun no se haya inducido la síntesis de las enzimas necesarias puesto que siempre hay un nivel basal de expresión (véase el inicio de este capítulo).

Regulación de la expresión génica en eucariotas

CONCEPTOS DEL CAPÍTULO

La regulación genética en eucariotas puede producirse a diferentes niveles, siendo el control transcripcional el mecanismo más importante que controla la expresión génica.
La transcripción se modula mediante la interacción entre moléculas reguladoras y secuencias cortos de DNA localizados generalmente corriente arriba de los genes que afectan.
Los mecanismos postranscripcionales implican la selección de productos alternativos a partir de un solo transcrito y el control de la estabilidad del mRNA .

El descubrimiento de los operones en Escherichia coli , realizado por Jacob y Monod en 1960, fue el primer paso para descifrar los mecanismos que regulan la expresión génica. 
Aunque algunos de los principios de regulación encontrados en el operón lac también se encuentran en eucariotas, es obvio que los eucariotas han desarrollado un sistema más complejo de regulación génica.

En eucariotas pluricelulares, las células del páncreas no producen pigmentos de la retina, ni las células de la retina producen insulina.
En estos organismos, la regulación diferencial de la expresión génica es la base de la diferenciación y de la función celular.
La pregunta es, pues, ¿cómo un organismo expresa un conjunto de genes en un tipo celular, y un conjunto diferente de genes en otro tipo celular?
A nivel celular, esta regulación no se realiza eliminando la información genética que no se utiliza; por el contrario, se han desarrollado mecanismos que activan porciones específicas del genoma y que reprimen la expresión de otros genes.
La activación y la represión de loci seleccionados representa un delicado equilibrio para un organismo; la expresión de un gen en un momento equivocado, en un tipo celular erróneo o en cantidades anormales puede conducir a un fenotipo deletéreo, aunque el gen sea normal.

En este capítulo expondremos las características generales de la regulación en eucariotas, resumiremos los c necesarios para el control de la transcripción, y trataremos las interacciones de estos c.
También consideraremos la función de los mecanismos postranscripcionales en la regulación de la expresión génica en eucariotas.

Regulación génica en eucariotas: generalidades

Las células eucarióticas contienen mucha más información genética que las células procarióticas, y su DNA está acomplejado a histonas y a otras proteínas para formar la cromatina.
Los eucariotas contienen la información genética repartida en muchos cromosomas (en lugar de en uno solo), y estos cromosomas están rodeados por la membrana nuclear.
En eucariotas, la transcripción está separada de la traducción tanto espacial como temporalmente.
Los transcritos de los genes eucarióticos se procesan, se cortan y se unen antes de transportarse al citoplasma.
A menudo, las células altamente diferenciadas de los eucariotas sintetizan grandes cantidades de un número restringido de productos génicos a pesar de que contienen una dotación de información genética completa.

La regulación de la expresión génica en eucariotas puede producirse a muchos niveles (Figura 19.1). 
(1) control transcripcional;
(2) procesamiento del pre- mRNA ;
(3) transporte al citoplasma;
(4) estabilidad del mRNA ;
(5) selección del mRNA que se transcribe; y
(6) modificación postranscripcional del producto proteico.


La mayoría de genes eucarióticos están regulados, en parte, a nivel transcripcional.
En las siguientes secciones se pondrá énfasis en el control transcripcional, aunque también se expondrán los otros niveles de control.
Hay dos c principales en el control transcripcional de la expresión génica:
las secuencias cortas de DNA que sirven de sitio de reconocimiento, y las proteínas reguladoras que se unen a estos sitios.

Elementos reguladores y genes eucarióticos

En los capítulos 12 y 17 se expuso la estructura interna de los genes eucarióticos.
Además, también hay secuencias reguladoras adyacentes a los genes que controlan la transcripción.
Estas secuencias son de dos tipos:
los promotores y los intensificadores (Figura 19.2).
Estos elementos reguladores pueden estar en cualquier lado del gen y a cierta distancia de éste, y se denominan reguladores en cis puesto que se encuentran adyacentes a los genes estructurales que regulan, en oposición a los reguladores en trans (por ejemplo, las proteínas de unión), que no son adyacentes a los genes que regulan.

Los promotores

Los promotores están formados por secuencias nucleotídicas que sirven de punto de reconocimiento para la unión de la RNA polimerasa.
Estas regiones se denominan promotores , como en los procariotas.
Los promotores representan la región necesaria para iniciar la transcripción y están localizadas a una distancia fija del sitio en que se inicia la transcripción. 
Los promotores están localizados inmediatamente adyacentes a los genes que regulan, y se considera que son parte del gen.

Generalmente, las regiones promotoras tienen varios cientos de nucleótidos de longitud.

Los promotores eucarióticos requieren la unión de varios factores proteicos para iniciar la transcripción. 
Los promotores reconocidos por la RNA polimerasa II consisten en secuencias de DNA cortas y modulares, localizadas, generalmente, dentro de un intervalo de 100 pb corriente arriba del gen (en dirección S').
La región promotora de la mayoría de genes contiene varios elementos.
El primero es una secuencia denominada caja TATA (Figura 19.3). 
Esta secuencia se localiza a unas 25-30 bases corriente arriba del punto inicial de transcripción (se designan de -25 a -30), y está formada por una secuencia consenso de 8 pares de bases (conservada en la mayoría o en todos los genes analizados) compuesta sólo por pares A=T, a menudo flanqueadas a ambos lados por regiones ricas en G=C.
Las mutaciones en la caja TATA reducen fuertemente la transcripción y, a menudo, las delaciones alteran el punto de inicio de la transcripción (Figura 19.4).

Muchos promotores contienen otros c:
uno de ellos se denomina caja CAAT .
Su secuencia consenso es CAAT o CCMT [1] , se localiza frecuentemente en la región de -70 a -80 pb del sitio de inicio de la transcripción, y puede funcionar orientada tanto 5' - 3' como 3' - 5'.
El análisis mutacional sugiere que la caja CAAT es esencial para la capacidad del promotor de facilitar la transcripción (Figura 19.4). 
Mutaciones a ambos lados de este elemento no tienen ningún efecto en la transcripción, mientras que mutaciones dentro de la secuencia CAAT disminuyen dramáticamente la tasa de transcripción.
Un tercer elemento modular de algunos promotores se denomina caja GC , tiene la secuencia consenso GGGCGG y a menudo se encuentra, aproximadamente, en la posición -110.
Este módulo se encuentra en cualquier orientación y, a menudo, en copias múltiples.

Hay variación en la localización y en la organización de las secuencias que comprenden los elementos reguladores corriente arriba de los genes eucarióticos (Figura 19.5).
Obsérvese que en genes diferentes hay diferencias significativas en el número y en la orientación de los elementos promotores, así como en la distancia entre ellos.
Además, los genes eucarióticos utilizan más de una forma de RNA polimerasa para la transcripción. 
Éstas se clasifican como de tipo I (RNA ribosómico), de tipo II ( mRNA y snRNA ) y de tipo III ( tRNA , 5S rRNA y otros diversos RANA celulares pequeños). 
Cada tipo de polimerasa utiliza diferentes secuencias para reconocer los factores de transcripción que se unen a los promotores de tipo I, de tipo II y de tipo III.

Los intensificadores

Además de las regiones promotoras, la transcripción de la mayoría, si no de todos los genes eucarióticos, está regulada por secuencias de DNA adicionales denominadas intensificadores .
Estas regiones interaccionan con proteínas reguladoras y pueden incrementar la eficiencia de la iniciación de la transcripción o pueden activar el promotor.

Por lo tanto, hay algunas analogías entre los intensificadores v las regiones operadoras de los procariotas.
Sin embargo, los intensificadores parecen ser mucho más complicados tanto estructural como funcionalmente.
Los intensificadores estimulan enormemente la actividad transcripcional del promotor y tienen diversas características que los distinguen de los promotores:
1. La posición del intensificador no necesita ser fija; puede estar localizado corriente arriba, corriente abajo o dentro del gen que regula.
2. Su orientación puede invertirse sin que se produzcan efectos significativos en su acción.
3. Si un intensificador se mueve a otra posición en el genoma, o si un gen no relacionado se coloca cerca del intensificador, se intensifica la transcripción del gen adyacente.

La mayoría de genes eucarióticos están controlados por intensificadores.
Los genes de las cadenas pesadas de las inmunoglobulinas tienen un intensificador localizado en un intrón entre dos regiones codificantes.
En este caso, el intensificador está localizado dentro del gen que regula.

Este intensificador es activo en las células en las que se expresan los genes de las inmunoglobulinas, lo que indica que la expresión génica específica de tejido puede modularse mediante los intensificadores.

Se han descubierto intensificadores internos en otros genes eucarióticos, como los del gen de la cadena ligera de las inmunoglobulinas.
El gen de la beta- globina humana y el de la timidina quinasa de pollo tienen intensificadores localizados corriente abajo.
En el pollo hay un intensificador situado entre los genes de la globina beta y de la globina épsilon que, aparentemente, actúa en una dirección para controlar el gen épsilon durante el desarrollo embrionario y en la dirección opuesta para regular el gen beta durante la vida adulta.

Levadura tiene secuencias reguladoras parecidas a los intensificadores, denominadas secuencias activadores corriente arriba UAS , (del inglés upstream activator sequences ), que pueden funcionar corriente arriba a distancias variables y en cualquier orientación. 
Se diferencian de los intensificadores en que no pueden actuar corriente abajo del sitio de inicio de la transcripción. 

Como los promotores, los intensificadores contienen módulos formados por secuencias cortas de DNA.
El intensificador del virus SV40 tiene una estructura compleja.
Está formado por dos secuencias adyacentes de 72 pb localizadas a unos 200 pb corriente arriba del sitio de inicio de la transcripción (Figura 19.6).

Cada una de las regiones de 72 pb contiene cinco secuencias que contribuyen a alcanzar tasas máximas de transcripción. 
Si se deleciona cualquiera de estas regiones, no se produce ningún efecto en la transcripción, pero si se delecionan ambas, la transcripción in vivo se reduce enormemente.
Estos y otros intensificadores contienen una copia de la secuencia (ATGCAAATNA) que se encuentra tanto en promotores como en intensificadores, pero hasta la fecha ha resultado difícil determinar la unidad básica de la estructura de los intensificadores.

Las secuencias promotoras son esenciales para la transcripción, mientras que los intensificadores no lo son.
Los intensificadores pueden estimular los niveles de transcripción a cierta distancia, mientras que esto no sucede con los promotores.
Además, los intensificadores son los responsables de la expresión génica específica de tejido. 

Como grupo, los promotores contienen módulos (la caja TATA ) que deben localizarse en sitios fijos corriente arriba del sitio de inicio de la transcripción; los intensificadores no contienen estos módulos.
Aunque los intensificadores y los promotores comparten secuencias parecidas, a menudo las secuencias de los intensificadores son contiguas; en los promotores, estas secuencias están separadas. 
Por otra parte, los intensificadores y los promotores tienen algunas semejanzas sorprendentes:
ambos tienen una organización modular, con algunas secuencias modulares comunes, y ambos se unen a factores de transcripción. 

La pregunta más intrigante de los intensificadores es cómo pueden ejercer el control de la transcripción a gran distancia tanto de los promotores como del sitio de inicio de la transcripción.
Como trataremos en una sección posterior, los factores de transcripción se unen a los intensificadores y alteran la configuración de la cromatina doblando o haciendo lazos en el DNA para que los intensificadores distantes y los promotores establezcan un contacto directo, con el fin de formar complejos con los factores de transcripción y las polimerasas.

En esta nueva configuración, la transcripción se estimula hasta el nivel más alto, incrementando la tasa global de síntesis de RNA.

Unión de los factores de transcripción a promotores y a intensificadores

Como en procariotas, el control transcripcional en eucariotas implica la interacción entre secuencias de DNA adyacentes a los genes y proteínas de unión a DNA.
Como se resumió en el Capítulo 17, se han identificado las secuencias reguladoras adyacentes de una amplia gama de genes, se han cartografiado y se han secuenciado.
El desarrollo de las pruebas de protección de DNA y de geles de retardo ha permitido detectar proteínas de unión al DNA en extractos celulares, y se han utilizado nuevas técnicas de cromatografía de afinidad para aislar factores de transcripción presentes en concentraciones muy bajas.
El resultado ha sido una virtual explosión de información de los factores de transcripción eucarióticos.
Esta sección expondrá algunas de estas nuevas informaciones, como la organización corriente arriba de los genes eucarióticos, las características de los factores de transcripción, sus interacciones con las secuencias de DNA y con otros factores de regulación y la iniciación de la transcripción por la RNA polimerasa. 

Las proteínas que no forman parte de la molécula de RNA polimerasa, pero que son necesarias para la iniciación de la transcripción, se denominan factores de transcripción .
Estas proteínas controlan dónde, cuándo y cómo se expresan los genes. 
Los factores de transcripción son estructuras modulares con al menos dos dominios funcionales:
uno que se une a secuencias de DNA de los promotores y de los intensificadores ( dominio de unión a DNA ), y otra que activa la transcripción mediante interacciones proteína- proteína ( dominio de activación en trans).
Los dominios proteicas son agrupaciones de aminoácidos que desempeñan una función específica.
Esta región se une a la RNA polimerasa o a otros factores de transcripción. 
La presencia de dominios separados en los factores de transcripción se demostró, inicialmente, por análisis genético de mutantes de levadura.

Análisis genético de los factores de transcripción

En levadura, el análisis genético ha identificado loci que codifican enzimas necesarias para el metabolismo de la galactosa.
La expresión de los genes que codifican estas enzimas se regula mediante la presencia y la ausencia de galactosa.
En ausencia de galactosa, estos genes no se transcriben.
Si se añade galactosa al medio de cultivo, empieza la transcripción inmediata de los tres genes, y la concentración de mRNA de estos transcritos se incrementa mil veces.
Una mutación en el gen GAL4 evita la activación de estos genes en presencia de galactosa, lo que indica que la transcripción está regulada genéticamente.

La transcripción de los genes estructurales del metabolismo de la galactosa está controlado por UASG (secuencia activadora corriente arriba).
Recuerde que las UAS son funcionalmente parecidas a los promotores y a los intensificadores de los eucariotas superiores.
Utilizando técnicas de DNA recombinante, se encontró que la proteína Gal4 se une a la región UASG y que activa la transcripción de los genes del metabolismo de la galactosa, lo que establece que el gen GAL4 codifica un factor de transcripción.

El producto del gen GAL4 es una proteína de 881 aminoácidos que incluye un dominio de unión a DNA que reconoce y se une a secuencias de las UASG, y un dominio de activación en trans que activa la transcripción. 
Estos dominios funcionales se identificaron mediante la clonación y la expresión de genes GAL4 truncados, analizando la capacidad de unión al DNA y la capacidad de activar la transcripción de los correspondientes productos génicos (Figura 19.7).
Puesto que la proteína contiene dos dominios funcionales, es posible delecionar una de estas regiones dejando la otra intacta y funcional.

Se encontró que una región del extremo N-terminal de la proteína es el dominio de unión a DNA (Figura 19.7).
Las proteínas con delaciones que abarcan desde el aminoácido 98 al extremo C-terminal (aminoácido 881) mantienen la capacidad de unirse a la región UASG. 
El dominio de unión a DNA es responsable del reconocimiento y de la unión a la secuencia nucleotídica de UASG. 
Experimentos parecidos han identificado dos regiones de la proteína implicadas en la activación de la transcripción:
la región I, formada por los aminoácidos 148-196; y la región II, formada por los aminoácidos 768-881 (Figura 19.7).
Las construcciones que contienen el dominio de unión a DNA y la región I (aminoácidos 1-96 y 149-196) o el dominio de unión a DNA y la región II sola (aminoácidos 1-96 y 768-881) tienen una actividad transcripcional reducida. 

Figura 19.1

Niveles posibles de regulación durante la expresión del material genético

Figura 19.2

Organización de un gen eucariótico típico y de las regiones de control de la transcripción

La región promotora está formada por secuencias de DNA modulares, generalmente localizados en un intervalo de 100 a 110 pb del sitio de inicio de transcripción. 
Los elementos promotores acostumbran a estar formados por secuencias modulares.

Figura 19.3

El promotor de los genes eucarióticos está formado por varios elementos modulares como la caja TATA (de -25 a -30), la cala CAAT de -70 a -80) y la caja GC (aproximadamente a - 110)

Figura 19.4

Resumen de los efectos de las mutaciones puntuales en la transcripción del gen de la B-globina

Cada línea representa el nivel de transcripción (respecto al silvestre) producido por la mutación de un solo nucleótido en la región promotora en experimentos separados.
Los puntos representan nucleótidos de los que no se obtuvieron mutaciones.
Obsérvese que las mutaciones dentro de los elementos del promotor son las que tienen el mayor efecto sobre el nivel de la transcripción. 

Figura 19.5

Organización corriente arriba de varios genes eucarióticos, que muestran la variabilidad de las características, del numero y de la ordenación de los elementos controladores
- Región de control de SV40, con los genes tempranos a la derecha y los genes tardíos a la izquierda.
- Timidina quinasa
- Gen de la insulina.

Figura 19.6

Secuencia de DNA del intensificador de SV40

Las secuencias rastreadas son las necesarias para que el efecto intensificador sea máximo.
Los corchetes que hay debajo de la secuencia indican los diferentes motivos de la secuencia en esta región.
También se indican los dos dominios del intensificador (A y B).

Figura 19.7

La proteína GAL4 tiene tres dominios que participan en la activación de los loci GAL 1 y GAL 10

Los aminoácidos 1-98 o I -147 se unen a los sitios de reconocimiento del DNA en UASG.
Para la activación de la transcripción se precisan los aminoácidos 148-196 y 768-881.
La actividad de unión de GAL80 reside en la región aminoacídica 851- 881.

Figura 19.8

Hélice-giro-hélice o dominio homeótico en el que se establecen los tres planos diferentes de las hélices a de la proteína

Estos dominios se unen a los surcos de la molécula de DNA.

[14] Según la mayoría de autores, la caja CAAT tiene una secuencia consenso algo más larga: 5'-GG (C o T)CAATCT-3' (Figura 19.3).


Comunicación intracelular

Las células de nuestro organismo contienen unas redes de comunicación interna sorprendentes.
De la comprensión de la organización de estos circuitos depende la creación de nuevas terapias para machas enfermedades graves.

Cualquiera que conozca el juego del «teléfono» sabe perfectamente que al pasar el mensaje de una persona a otra las palabras se degradan poco a poco y acaban siendo irreconocibles. 
Sorprende, pues, que las moléculas del interior de nuestras células, que realizan sin cesar su propia versión del teléfono, lo hagan sin que se pierda un ápice del mensaje.

No podríamos vivir sin esa precisa señalización de las células.
El organismo opera en buen orden porque existe una fluida comunicación entre sus células. 
Las del páncreas, por ejemplo, segregan insulina y, con ello, ordenan a las células musculares que capten el azúcar de la sangre y produzcan energía.
Las células del sistema inmunitario envían instrucciones a sus parientes cercanos para que ataquen al invasor; las del sistema nervioso emiten mensajes de una zona a otra del cerebro. 
Esos mensajes reciben la respuesta idónea por la exclusiva razón de que se transmiten correctamente al interior de una célula receptora y las moléculas adecuadas pueden cumplir con fidelidad las órdenes dictadas. 

¿Cómo consiguen los circuitos del interior de las células esta transmisión de alta fidelidad? 
Durante mucho tiempo los biólogos sólo podían dar explicaciones rudimentarias.
Pero a lo largo de los últimos 15 años se han realizado avances importantes en el desciframiento del código intracelular. 
Los nuevos descubrimientos sugieren estrategias radicalmente nuevas para combatir el cáncer, la diabetes y alteraciones del sistema inmunitario, enfermedades que se producen o se exacerban por una comunicación defectuosa en las células. 

Figura 1

La transmisión intracelular de señales comienza cuando las moléculas mensajeras, hormonas por ejemplo, se aferran a moléculas receptoras de la superficie de la célula

Afinando la cuestión

Las primeras noticias acerca de la transferencia de información entre las células se remontan a finales de los años cincuenta, cuando Edwin G. Krebs y Edmond H. Fischer, de la Universidad de Washington, y Earl W. Sutherland, Jr., de la Universidad de Vanderbilt, identificaron las primeras moléculas conocidas de transmisión de señales en el citoplasma (sustrato que existe entre el núcleo y la membrana externa de la célula).
Los tres recibieron el premio Nobel por sus descubrimientos.

A comienzos de los años ochenta se había avanzado bastante en el conocimiento de los mecanismos de transmisión de señales.
Expresado con la imagen de la llave y su cerradura, se sabía ya, por ejemplo, que suele iniciarse con el acoplamiento provisional entre un mensajero responsable de transmitir información de una célula a otra (a menudo una hormona), y un receptor de la célula destinataria. 
Estos receptores, cuya función evoca la de una antena, transmiten la orden del mensajero a la célula porque se hallan físicamente conectados con el citoplasma. 
El receptor típico es una proteína, una cadena de aminoácidos plegada.
Comprende al menos tres dominios: la región de acoplamiento para una hormona u otro mensajero, el componente que trasciende la membrana externa de la célula y una "cola" que se adentra en el citoplasma. 
Cuando un mensajero se une a la porción exterior, el enganche de marras induce un cambio en la forma de la cola citoplásmica, que facilita la interacción entre ésta y una o varias moléculas transmisoras de información en el citoplasma.
Ese juego de interacciones, a su vez, desencadena cascadas de señales intracelulares ulteriores. 

Nadie sabía por qué los comunicados llegaban a su destinatario sin degradarse en el camino.
En esos años, representábamos las células a la manera de bolsas hinchadas y rellenas de un caldo citoplásmico que contenía proteínas y orgánulos flotantes (compartimientos ceñidos por membranas, como el núcleo o las mitocondrias).
Resultaba difícil atisbar, en semejante medio sin estructurar, de qué modo podría encontrar sistemática y prestamente una molécula mensajera interna su destino preciso y transmitir la orden a los encargados de cumplirla, alojados en las profundidades de la célula.

Señalización modular 

Las moléculas que integran los circuitos celulares de señalización son, a menudo, modulares.
Sus elementos componentes realizan distintas tareas.
A ese descubrimiento se llegó desde el estudio de las tirosinaquinasas receptoras (con forma de zanco en el primer panel ).
Cuando en la superficie celular una hormona se acopla a estas moléculas ( segundo panel ), los receptores se emparejan y agregan fosfatos al aminoácido tirosina de las colas citoplásmicas. 
Luego, en las tirosinas modificadas se prenden módulos SH2 de ciertas proteínas ( último panel ).
A través de esa unión, los módulos enzimáticos «portavoces» recogen la orden del mensajero y la transmiten.

Acoplamiento por bloques

Cuanto ahora conocemos de la cuestión se debe en buena medida a los esfuerzos empeñados en identificar las primeras proteínas citoplásmicas que entraban en contacto con los receptores activados (ligados a mensajeros) en una familia numerosa e importante: las tirosinaquinasas receptoras. 
En cumplimiento de su misión vital, estos receptores transmiten las órdenes de muchas hormonas que regulan la replicación celular, la especialización o el metabolismo.
En cuanto enzimas quinasas, añaden grupos fosfato («fosforilan») a determinados aminoácidos de una cadena proteica.
Y, como ha demostrado Tony R. Hunter, del Instituto Salk de Estudios Biológicos de La Jolla, colocan específicamente fosfatos en el aminoácido tirosina.

En los años ochenta, Joseph Schlessinger, de la Universidad de Nueva York, y otros pusieron de manifiesto que la unión de las hormonas a las tirosinaquinasas receptoras, en la superficie celular, determinaba que las moléculas receptoras se asociaran en parejas y agregaran fosfatos a las tirosinas de las respectivas colas citoplásmicas.
¿Qué sucedía entonces?
El grupo de Pawson, coautor de este artículo, se propuso averiguarlo. 
Halló que los receptores modificados interaccionaban directamente con proteínas dotadas de un dominio SH2 .
Por «dominio» o «módulo» se entiende una secuencia bastante corta, de unos 100 aminoácidos, que adopta una estructura tridimensional definida dentro de una proteína.

Creíase por entonces que los mensajes se transmitían en el interior celular a través de reacciones enzimáticas, sobre todo.
En tales reacciones, una molécula modifica a una segunda sin establecer un enlace firme con ella ni sufrir en su estructura alteración alguna.
Mas, para sorpresa de los investigadores, los receptores fosforilados no alteraban obligadamente la química de las proteínas portadoras de dominios SH2 .
Antes bien, muchas se limitaban a inducir que los dominios SH2 se encadenaran a las tirosinas con fosfato; diríase que los dominios SH2 y las tirosinas eran una suerte de broches, macho y hembra, que encajaban entre sí.

Llegamos a mediados de los noventa.
Los grupos de Pawson, Hidesaburo Hanafusa, de la Universidad de Rockefeller, y otros revelaron que muchas de las proteínas implicadas en las comunicaciones internas constaban de hileras de módulos, algunos de los cuales desempeñaban la función primaria de conectar proteínas entre sí.
A veces, proteínas enteras de las vías de señalización sólo contenían módulos de conexión. 

Pero,
¿de qué forma participaban tales módulos no enzimáticos en la comunicación celular, tan rápida y certera?
Puede responderse: 
ayudan a los dominios enzimáticos en la transmisión eficiente de la información.

Cuando una proteína dotada de una unidad de conexión porta, además, un módulo enzimático, el engarce de esa región conectora en otra proteína se aprovecha para alojar el módulo enzimático allí donde más se necesita.
En el proceso de enlace de una enzima, se acerca una región de ésta a los factores que la tornan activa y en contacto inmediato con la diana buscada por dicha enzima.
En el caso de ciertas proteínas portadoras de SH2 , el módulo de conexión podría hallarse, en un principio, plegado alrededor del dominio enzimático hasta el punto de sofocar la función enzimática.
En cuanto el dominio SH2 se suelta para trabar un receptor activado, la enzima queda liberada, presta para actuar sobre su objetivo. 

Aun cuando una proteína esté formada exclusivamente por módulos de conexión con proteínas puede funcionar como un adaptador indispensable.
Para representarlo nos sirve la imagen del cajetín-alargo con varios enchufes, unido por un macho a un solo enchufe.
En el adaptador se conecta un módulo con un complejo capaz de incluir señales; en tanto que los otros módulos permiten que se unan a la red más proteínas.

Tales adaptadores moleculares logran que las células echen mano de enzimas que quedarían al margen de determinado circuito de señalización; lo que no es pequeña ventaja.

Tampoco se agota ahí el papel de los módulos no enzimáticos en su labor de apoyo a la comunicación. 
Ciertas moléculas de una vía de señalización presentan un módulo de unión a proteínas y un módulo de unión a ADN, que reconoce un segmento específico de la secuencia de nucleótidos del ADN de un gen.
(Los nucleótidos son los componentes de los genes, que determinan la secuencia de aminoácidos de las proteínas).
James Darnell, Jr., de la Universidad Rockefeller, demostró que, cuando una de estas proteínas se une, por medio de su módulo de conexión, a una quinasa receptora activada, la interacción provoca el desprendimiento de la proteína, su tránsito al núcleo y su asociación con un gen determinado, con la inducción consiguiente de la síntesis de una proteína.
En este ejemplo, la enzima, única, de la cadena de señalización es el propio receptor. 
Cuanto sucede después de la activación del receptor corre a cargo del reconocimiento de otras proteínas o del ADN .

Conforme se iba desgranando este rosario de descubrimientos, la investigación celular demostraba que el citoplasma distaba de ser una entidad amorfa.
Se hallaba repleto de orgánulos y proteínas, en apretado empaquetamiento. 
Quedaba, por todo ello, manifiesto que la señalización precisa y fiel del interior celular dependía estrictamente de este tipo de broches a través de los cuales se acoplan ciertas proteínas.
Los complejos formados aseguran que las enzimas o los módulos que se unen al ADN y sus objetivos se presenten juntos y en la secuencia correcta, a partir de la activación del receptor instalado en la superficie de la célula.

Figura 2

Dominio SH2 (estructura globular) de una molécula de señalización 

Se halla unido a un segmento de un receptor (modelo de varillas).
Los dos encajan; ello se debe, en parte, a la presencia de un bolsillo dotado de carga positiva en SH2 que se atrae al fosfato, cargado negativamente y que se ha añadido al aminoácido tirosina en el receptor.

También los aminoácidos del receptor cercanos encajan en un surco hidrofóbico con SH2 .
Todos los dominios SH2 pueden unirse a tirosinas fosforiladas, pero difieren en el socio al que se enlazan, porque varían en su capacidad para acoplarse a los aminoácidos situados junto a la tirosina en la cadena polipeptídica.

Ventajas de los adaptadores

Las moléculas adaptadoras, constituidas en su integridad por módulos de conexión ( SH2 y SH3 ), intervienen de una forma destacada en numerosas vías de señalización.
Gracias a ellas, las células pueden servirse de proteínas que, por si mismas, serían incapaces de participar en un circuito de comunicación.

Ilustramos, por mor de ejemplo, la proteína adaptadora Grb2 (roja); ésta arrastra la proteína enzimática -Sos- hacia una vía encabezada por un receptor que, por si solo, carece de la posibilidad de acoplarse a Sos.

Seguridad y especificidad

Los estudios sobre las tirosinaquinasas receptoras y los dominios SH2 han contribuido también a esclarecer por qué las células garantizan que sólo se combinen las proteínas correctas cuando de establecer una vía de señalización se trata.
Poco después de que se identificaran los dominios SH2 , se observó que tales módulos aparecían en un centenar largo de proteínas. 

¿Qué era lo que evitaba que diferentes receptores activados atrajeran las mismas proteínas portadoras de grupos SH2 y produjeran, por tanto, efectos idénticos en las células?
(Para el buen funcionamiento del organismo importa que receptores y hormonas diferentes ejercen efectos distintos en las células.
Semejante especificidad exige que los receptores se hallen vinculados con vías de comunicación algo diferentes).

La respuesta es bastante sencilla.
Cada dominio SH2 incluye una región que encaja suavemente sobre una tirosina portadora de un fosfato, lo que se llama una fosfotirosina. 
Pero cada dominio abarca también una segunda región, que difiere de un SH2 a otro.
Esa segunda región, como Lewis C. Cantley de la Universidad de Harvard ha demostrado, reconoce una particular secuencia de unos tres aminoácidos próximos a la fosfotirosina.
Por tanto, todos los dominios SH2 pueden unirse a la tirosina fosforilada, pero estos módulos varían en su preferencia por los aminoácidos adyacentes en un receptor.
Los aminoácidos alrededor de la tirosina sirven, pues, de clave para especificar qué versión del dominio SH2 puede unirse a un determinado receptor que porte una fosfotirosina.
Puesto que cada dominio SH2 se une a un dominio enzimático diferente o módulo de conexión, esta clave dicta también qué vías se activarán con ese receptor.
Otros tipos de módulos de conexión operan de manera similar.

Para ilustrar los principios descritos, vengamos a la vía que se activa por el factor de crecimiento derivado de las plaquetas. 
Trátase de una proteína que se libera cuando se daña un vaso sanguíneo.
Su unión a una tirosinaquinasa receptora, en una célula del músculo liso de las paredes de los vasos, hace que esos receptores se asocien y se fosforilen en presencia de tirosina.
Este cambio atrae al receptor la proteína Grb2 , constituida por un dominio SH2 específico y flanqueado en ambos lados por otro dominio de conexión SH3 .
Un adaptador clásico, Grb2 carece de poder enzimático; pero sus dominios SH3 (que se unen al aminoácido prolina) enlazan al receptor una proteína Sos dotada de actividad enzimática. 
A su vez, Sos activa a la proteína Ras, asociada a la membrana.
Ras pone en marcha una serie de procesos enzimáticos que terminan por estimular proteínas del núcleo que activan genes promotores de la división celular, lo que redunda en la cicatrización de la herida. 

Según parece, las redes de señalización dirigidas por tirosinaquinasas receptoras fían en proteínas adaptadoras de escasa magnitud.
De los análisis de circuitos de comunicación de neuronas cerebrales se desprende que ciertas proteínas de las vías nerviosas poseen un número ingente de dominios de conexión. 
Tales proteínas, denominadas a menudo moléculas de andamiaje, mantienen en su sitio y de forma permanente grupos de proteínas de señalización.

La propia existencia de dichos andamios significa que hay redes de señalización tendidas en el interior celular; semejante cableado refuerza la velocidad y exactitud de la transferencia de información.

Líneas de comunicación v patologías 

Son múltiples las enfermedades humanas que tienen que ver con una señalización celular aberrante.
El cáncer, caracterizado por una proliferación celular descontrolada y una migración sin freno, constituye un ejemplo paradigmático.
En su raíz el cáncer es el resultado de mutaciones genéticas.
Algunas ejercen su efecto perverso convirtiendo en hiperactivas las proteínas de las vías de transmisión de señales, en particular de aquellas vías que, en condiciones normales, inducen la división celular en respuesta a órdenes externas.
Las proteínas afectadas provocan que las células se comporten como si recibieran permanentemente la orden de dividirse, aun cuando tal imperio no exista.

En el tratamiento del cáncer de mama se emplean ya agentes de bloqueo de señales. 
Y se investiga en otros tumores más.
A este respecto, determinadas pruebas clínicas recientes sugieren que un fármaco capaz de detener el «habla» exaltada de tirosinaquinasa de Abelson, una enzima, podría coadyuvar en el tratamiento de ciertas formas de leucemia.

No menos nefasto es ese exceso de celo señalizador en la enfermedad linfoproliferativa ligada al cromosoma X(XLP) , un síndrome hereditario. 
En los pacientes de XLP, el virus de Epstein-Barr , habitualmente benigno, insta una respuesta letal de las células T asesinas, que pertenecen al sistema inmunitario.

La razón de esa reacción mortal se descubrió hace un par de años.
Los enfermos de XLP carecen de la proteína SAP, constituida por un dominio SH2 (emparentado con el dominio SH2 mencionado en el artículo).
Cuando las células T asesinas detectan que hay otras células infectadas por el virus de Epstein-Barr , desencadenan una cascada señalizadora que les capacita para atacar a las infectadas.
La SAP suele mantener bajo control el ataque, ocupando los sitios interactivos de algunos componentes de señalización y cortando así la cadena de señales. 
Pero sin la SAP, los pacientes de XLP carecen de un inhibidor importante de la hiperactividad celular.

Los procesos patológicos pueden producirse también en condiciones opuestas; es decir, cuando los sistemas de señalización intracelular que deberían estar activos caen en una quietud indolente. 
Sucede en los trastornos provocados por falta de respuesta contundente del sistema inmunitario. 
Se da,asimismo, una señalización insuficiente en la diabetes de tipo 2, que se presenta en la madurez. 
Los miocitos y los adipocitos del organismo captan azúcar de la sangre tras recibir la orden de la insulina, segregada por el páncreas. 
Si los receptores de insulina de esas células dejan de enviar el mensaje de la insulina a las moléculas intermedias del interior, puede producirse la diabetes (desmesurado nivel de azúcar en la sangre). 
La medicación oral indicada para aumentar la actividad de los receptores de insulina o de los agentes posteriores en la cadena de señalización podría reemplazar las inyecciones de insulina en algunos diabéticos. 
Uno de esos compuestos, que estimula el receptor de insulina, se ha probado con éxito en el ratón.

Las bacterias y los virus son expertos en aprovechar los sistemas de señalización de las células humanas para su multiplicación y propagación. 
Esta capacidad se revela con toda claridad en Yersinia pestis , la bacteria responsable de la epidemia de peste en el siglo XIV, y en algunas cepas patógenas de Escherichia coli . 
Los microorganismos inyectan sus propias proteínas en las células humanas. 
Algunas de estas proteínas alteran las vías de señalización en el sentido de promover la asociación de las bacterias con las células del huésped y desarmar las defensas antibacterianas de las células.

Los virus, por su parte, penetran en las células humanas engarzados en los receptores que dirigen circuitos de señalización. 
Allí instalados, pueden luego modificar las redes de comunicación interna de una célula para potenciar su propia replicación y difusión. 
El virus de la inmunodeficiencia humana (VIH), causante del sida, es uno de los múltiples que hacen uso de estas vías nefastas.

A medida que se vayan descifrando los nexos entre fallos en la señalización y enfermedad, aparecerán terapias que compensen o reparen tales disfunciones.

LA PESTE NEGRA, epidemia que causó una gran mortandad en el siglo XIV, fue provocada por Yersinia pestis .
Esta bacteria se aprovecha de las vías de señalización de las células del huésped para promover su propia difusión.

Los andamios aceleran la transmisión de señales

Las proteínas de andamiaje, que descansan sobre otras proteínas, aseguran la operación conjunta de múltiples moléculas de señalización. 
La InaD ( diagrama ), una de ellas, actúa en las células del ojo de la mosca del vinagre -estructura compuesta de muchos ocelos ( fotografía )- y participa en el envío de mensajes visuales al cerebro.
Tres de los cinco dominios de conexión «PDZ» ; del andamiaje atrapan, por separado, un canal de iones, una enzima que abre el canal cuando la luz incide sobre un receptor luminoso cercano (rodopsina) y una enzima que cierra el canal inmediatamente después. 
Los dos dominios PDZ restantes contribuyen a la transmisión de la información reteniendo en su sitio otras moléculas de señalización. 

Redes y andamios

La proteína que responde al nombre de PSD-95 , bien estudiada, opera fundamentalmente en las neuronas implicadas en el aprendizaje.
En el tejido nervioso las señales pasan de una neurona a otra en las sinapsis o puntos de contacto.
La primera neurona libera un neurotransmisor químico en la hendidura que media entre las células. 
Los receptores que hay en la segunda célula atrapan el neurotransmisor e instan la apertura de los canales de iones de la membrana.
La entrada de iones activa las enzimas necesarias para la propagación del impulso eléctrico. 
Una vez generado, el impulso viaja por el axón hasta las múltiples ramitas; les induce a liberar más neurotransmisor.

Para que el impulso en cuestión se produzca, deben entrar en acción, casi simultáneamente, muchos componentes del sistema de señalización.

Entre los múltiples módulos de conexión de PSD-95 , nos interesan tres, los PDZ .
Uno de estos dominios se enlaza a la cola citoplásmica del receptor del neurotransmisor glutamato; otro se une a un canal de iones que atraviesa la membrana (controla la entrada de potasio), y el tercer dominio se prende de proteínas del citoplasma (como si se agregara un módulo adicional al andamio).
Así, el PSD-95 ayunta a varios componentes de señalización a la vez, posibilitándoles coordinar sus actividades.
El ojo de la mosca del vinagre se apoya también en una proteína de andamiaje que contiene PDZ - la InaD - para la transmisión eficiente de información del ojo al cerebro.

Se acaba de descubrir otro complejo preformado en las neuronas de mamíferos.
El elemento central es yotiao, una proteína de andamiaje.
Según demostrara el grupo de Scott, coautor del artículo, esta molécula cumple una doble función:
en cuanto proteína que abarca todo el espesor de la membrana, es receptor del glutamato y canal de iones.
Se une a una quinasa que añade fosfato al canal de iones y, por tanto, lo abre cuando el glutamato activa al receptor.
Y se ancla, por fin, en una fosfatasa, una enzima que elimina de las proteínas el fosfato.
Esta fosfatasa engarzada cierra el canal de iones siempre que el glutamato falte del receptor.
Con semejante disposición, tan elegante, se asegura que los iones fluyan sólo por el canal cuando el glutamato está presente en el receptor. 

Quinasas y fosfatasas controlan buena parte de la actividad celular.
Si una quinasa estimula una proteína, una fosfatasa se encargará de inactivarla, o viceversa; las células humanas fabrican centenares de quinasas y fosfatasas diferentes.
Así las cosas, las proteínas de andamiaje constituyen una solución para evitar que una fosfatasa o una quinasa inadecuada opere sobre un objetivo impropio; facilitan las reacciones indicadas manteniendo las quinasas y fosfatasas seleccionadas en la proximidad de las proteínas pertinentes; es decir, junto a las que se supone que han de regular. 

Figura 3

DOS PROTEÍNAS DE ANDAMIAJE resaltan en la neurona mayor de esta micrografía

Una, yotiao (verde), lleva enzimas transmisoras de mensajes junto a un canal de iones implicado en la transmisión de señales.
La otra, PSD-95 (rojo), congrega a un receptor y diferentes canales de iones en sinapsis elegidas, los puntos de contacto entre las neuronas. 
El azul de ambas neuronas define la ubicación de una enzima señalizadora específica.

Figura 4

FLUJO DE UN MENSAJE en una célula de la piel

Para evidenciarlo se han coloreado dos componentes de una vía de señalización: una proteína de andamiaje ( verde ) y una de las dos enzimas columpiadas en el andamio ( azul ).
La actina, un elemento estructurado de las células, aparece en rojo.
La célula superior se halla en reposo. 
Poco después de que un mensajero externo activara una vía de señalización en la célula inferior, la proteína de andamiaje movió sus enzimas ancladas hacia objetivos adentrados en la célula.
Ese movimiento se manifiesta en el tinte amarillo, que se produce por la superposición de componentes señalizadores coloreados y actina. (A la actina están unidos los objetivos de la enzima).
La masa azul del centro refleja la existencia de copias adicionales de la enzima coloreada.

Múltiples compromisos

Desde la perspectiva de la evolución celular, la adquisición de un sistema modular de señalización resultaría de innegable interés.
Con la combinación y acoplamiento de los módulos existentes, la célula genera muchas moléculas y agregados de moléculas, para así construir redes de vías interconectadas; no necesita, pues, inventar un gigantesco repertorio de bloques de construcción.
Y lo que reviste mayor interés:
cuando surge un nuevo módulo, su combinación con los disponibles multiplica la versatilidad, a la manera en que la ampliación del código de una ciudad incrementa las posibilidades de números de teléfono para nuevos abonados.

El biólogo se siente recompensado si logra abrir pequeños resquicios del misterio encerrado en la función celular, múltiple y variopinta.
Pero el significado de los nuevos hallazgos trasciende el puro afán intelectual. 

El Proyecto Genoma Humano nos ha revelado la secuencia de nucleótidos.
Antes de traducir esa información en algo útil para el tratamiento de las enfermedades, habrá que acotar la función biológica de los genes que se vayan identificando.
Dicho de otro modo, deberá averiguarse qué misión desempeñan las proteínas correspondientes y qué sucede cuando se producen en número desproporcionado, por exceso o por defecto, o se sintetizan de manera incorrecta.

Conocemos ya las secuencias aminoacídicas y las funciones de muchos módulos de proteínas de señalización. 
Disponemos, pues, de una suerte de clave para determinar si la secuencia nucleotídica de un nuevo gen descubierto cifra una proteína de señalización y, si es así, con qué moléculas se relacionará ésta.
Cuando alcancemos a desentrañar un número suficiente de tales interacciones, tendremos en nuestras manos el mapa de redes, el «diagrama de cables» de cada tipo celular del organismo.
Bastará incluso un diagrama parcial, quizá, para que podamos reparar disfunciones en la red:
cortar señales aberrantes o encaminarlas hacia objetivos de nuestra elección; por ejemplo, trocar las órdenes de proliferación de las células cancerosas en instrucciones de autodestrucción.
Si desciframos el lenguaje que las células usan para comunicarse entre sí, podremos entender sus conversaciones y hallar vías para intervenir cuando las comunicaciones salgan de su cauce.


Genética y cognición 

La investigación sobre capacidades cognitivas especificas ayuda a esclarecer la influencia de los genes en la conformación de componentes del intelecto.

En punto a inteligencia y sus manifestaciones, puede decirse que no hay dos individuos iguales.
Las diferencias se hacen patentes no sólo en clase, del parvulario a la universidad, sino también en el quehacer cotidiano:
en el vocabulario que la gente usa y entiende, en la soltura para leer un mapa o seguir una ruta, en la facilidad de recordar números de teléfono o calcular el cambio de la compra.
La variación observada en esas capacidades específicas es tan palmaria, que nadie se detiene en ella.
Pero,
¿a qué se debe tamaña disparidad?

Parece razonable atribuir al entorno el origen de las diferencias en habilidad cognitiva.
De acuerdo con esa tesis, somos lo que aprendemos.
Nadie nace con un vocabulario desarrollado; todos hemos de aprender palabras.
En el aprendizaje reside, pues, el mecanismo en cuya virtud surgen las diferencias en dominio del vocabulario entre individuos.
Y las diferencias en la experiencia vivida - incidencia de los padres en el uso rico del vocabulario y en su estimulo para adquirirlo, así como el nivel de la docencia del lenguaje en las aulas- tienen que ser las responsables de las diferencias de aprendizaje manifestadas. 

A comienzos de siglo dominaba en psicología la interpretación ambientalista de la diversidad en habilidades cognitivas.
Pero hace ya algún tiempo que los psicólogos optan en su mayor parte por una explicación más equilibrada: 
naturaleza y crianza se imbrican en el desarrollo cognitivo. 
A lo largo de los últimos 25 años, la investigación genética ha ido forzando la atribución a la herencia de un papel sustancial en la configuración de los componentes del intelecto.
Se buscan ahora los genes implicados en la función cognitiva.
No se trata de negar la participación de factores ambientales en la modulación del proceso de aprendizaje.
Antes bien, se sugiere que las diferencias génicas entre individuos condicionan la facilidad del aprendizaje.

¿En qué medida genes y ambiente influyen sobre habilidades cognitivas específicas, sobre dominio del vocabulario, pongamos por caso?
De eso vamos a ocuparnos. 
Nuestro herramienta de trabajo ser la genética cuantitativa, disciplina estadística que estudia las causas de la variación de los caracteres entre individuos.
La comparación de resultados obtenidos por hermanos gemelos y niños adoptados en ciertas pruebas sobre habilidades cognitivas, por ejemplo, permite apreciar la contribución respectiva de naturaleza y educación (o crianza).

Al revisar los resultados de pruebas realizadas por otros investigadores y nosotros mismos, hemos empezado a ver la luz sobre las relaciones entre determinadas características del intelecto (razonamiento verbal y espacial), así como sobre las relaciones entre la función cognitiva normal y ciertas discapacidades (la dislexia, por ejemplo).
El recurso a la genética molecular nos llevar a identificar los genes que afectan a estas capacidades y discapacidades.
Del conocimiento de los genes interesados, cabe esperar, se llegará a los mecanismos bioquímicos implicados en la inteligencia humana.
Sonar entonces la hora de idear medios de intervención que moderen o prevengan los efectos de los trastornos cognitivos.

Ante la idea de un condicionamiento genético de la inteligencia, algunos se ponen en guardia alarmados o al menos denuncian su perfil confuso.
Conviene, pues, dejar claro a qué nos referimos al hablar de influencia genética. 
El término "heredabilidad" es una medida estadística de la contribución genética a las diferencias entre individuos.

La heredabilidad nos dice en qué proporción las diferencias individuales en el seno de una población -la varianza- han de adscribirse a los genes.
Al afirmar que un carácter presenta una heredabilidad del 50 por ciento indicamos que la mitad de la varianza en dicho rasgo esta ligada a la herencia.
La heredabilidad es, pues, una forma de explicar lo que hace a unos individuos diferentes de otros; no es lo que constituye la inteligencia de un individuo.
En general, si la heredabilidad de un rasgo es elevada, la influencia de los genes sobre dicho rasgo ser también muy intensa. 

Los primeros pasos para acotar la heredabilidad de las capacidades cognitivas se dieron con el estudio de familias. 
El análisis de semejanzas entre padres e hijos y entre hermanos ha puesto de manifiesto que las habilidades cognitivas tienen que ver con el parentesco.
Los resultados del estudio m s amplio realizado con familias sobre las capacidades cognitivas, acometido en Hawai en los años setenta, ayudaron a cuantificar dicha similaridad.

En el estudio en cuestión trabajaron en equipo la Universidad de Colorado en Boulder y la de Hawai.
Participaron más de 1000 familias y pares de hermanos.
Se determinaron las correlaciones (un estadístico de la semejanza) entre parientes en pruebas de competencia verbal y espacial.
Una correlación de 1,0 indicaría que las puntuaciones de los miembros de una familia eran idénticas; una correlación de cero indicaría que los resultados alcanzados no guardaban una similaridad mayor que los de cualquier otro par de personas elegidas al azar.

Puesto que los niños comparten, en promedio, la mitad de los genes con cada progenitor y con sus hermanos, la correlación más alta en el resultado de las pruebas que cabría esperar, ciñéndonos a la genética, sería de 0,5.

El estudio de Hawai demostró que los miembros de una misma familia evidencian, en punto a mediciones de capacidades cognitivas específicas, un parecido mayor que los individuos no emparentados.
En estas pruebas de competencia verbal o espacial, se observaron correlaciones de 0,25.
Por sí solas, tales correlaciones no nos aclaran si su asociación por familias obedece a la genética o al entorno.

Para resolverlo, se recurre a dos "experimentos":
la gemelaridad (un experimento de la naturaleza) y la adopción (un experimento social).

La investigación con gemelos constituye el caballo de tiro de la genética del comportamiento.
Se compara el parecido entre gemelos idénticos, dotados de la misma composición genética, con su parecido de otros hermanos, que comparten sólo la mitad de los genes.
Si las capacidades cognitivas vienen condicionadas por los genes, los gemelos idénticos ofrecerán, en los resultados de los tests de capacidad, un parecido mayor que los demás hermanos.
A partir de las correlaciones observadas, se calcula en qué medida los genes explican la varianza en la población en general.
Puede hacerse un cálculo somero de la heredabilidad doblando la diferencia entre correlaciones de gemelos idénticos y de gemelos no idénticos. 

Pasemos al experimento social.
La adopción nos enseña el camino m s directo para averiguar cuánto de naturaleza y cuánto de educación o ambiente hay en las semejanzas familiares, toda vez que permite comparar parejas de individuos con parentesco genético si bien no comparten el mismo entorno familiar.
Partiendo de las correlaciones entre parejas de estos individuos se calcula la parte que corresponde a la genética en la semejanza familiar. 
Asimismo, la adopción produce también parejas de individuos sin parentesco genético que comparten un mismo ambiente familiar; guiados por sus correlaciones se pondera la incidencia de un ambiente compartido en la similaridad.

Los estudios con gemelos sobre competencias cognitivas específicas, realizados a lo largo de tres decenios y en cuatro países, han dado resultados harto coherentes. 
Las correlaciones entre gemelos idénticos superan con creces las de los gemelos no idénticos en lo que se refiere a competencia verbal y espacial en niños, adolescentes y adultos.
Los resultados del primer estudio llevado a cabo con personas mayores -publicado el año pasado por el equipo de Gerald E. Mc- Clearn y por el de Stig Berg- ponen de manifiesto que las semejanzas entre gemelos idénticos y entre gemelos no idénticos persisten entrada la edad avanzada.
Por más que los gerontólogos se empeñen en que las diferencias genéticas se van borrando con la acumulación de nuevas experiencias a lo largo de la vida, la investigación demuestra lo contrario. 
De acuerdo con los cálculos basados en hallazgos combinados de estos estudios, en la población general deberíase a la genética el 60 por ciento de la varianza en competencia verbal y alrededor de un 50 por ciento de la varianza en competencia espacial.

Las investigaciones con adoptados han llegado a conclusiones similares.
Dos estudios recientes sobre gemelos educados por separado -uno de Thomas J. Bouchard, Jr., Matthew McGue y colaboradores, el otro de un proyecto internacional dirigido por Nancy L. Pedersen- atribuyen a la heredabilidad un 50 por ciento de ambos tipos de competencia: verbal y espacial.

En nuestro propio Proyecto sobre Adoptados en Colorado, iniciado en 1975, nos basamos en las posibilidades de la adopción para investigar el papel de los genes y del ambiente, para cribar tendencias de desarrollo en las capacidades cognitivas y para averiguar en qué medida se desarrolla la mutua interrelación entre capacidades cognitivas.
El proyecto, que sigue vivo, compara las correlaciones entre m s de 200 niños adoptados y sus padres, biológicos y de adopción, con las correlaciones de un grupo control de niños que han crecido junto a sus progenitores.

De los datos obtenidos se desprenden algunas ideas sorprendentes. 
Hasta la mitad de la infancia, por ejemplo, las madres biológicas y sus hijos adoptados por otros guardan un parecido similar al que registran los padres control y sus hijos, en cuanto a competencia verbal y espacial.
La puntuación de los niños adoptados, sin embargo, no se asemejan en absoluto a la anotada por sus padres adoptivos.
Estos resultados confieren un sólido respaldo a la idea de que un ambiente familiar común no fuerza la igualación entre los miembros de una familia.
Antes bien, la semejanza basada en esos criterios diríase controlada por la genética; los factores ambientales contribuyen, a menudo, a hacer diferentes los miembros de la familia, no a igualarlos.

Los datos del proyecto de Colorado revelan también una interesante tendencia del desarrollo.
La influencia genética aumenta durante la infancia; en la adolescencia, la heredabilidad alcanza un nivel equiparable al observado en adultos.
En las correlaciones de competencia verbal, por ejemplo, la semejanza entre padres biológicos y sus hijos adoptados por otros aumenta aproximadamente de 0,1 a los tres años a 0,3 a los 16.
Un patrón similar se manifiesta en las pruebas de competencia espacial.
Según parece, en el comienzo de la escolaridad, a los siete años, se produce una transformación, genética- mente instada, de la función cognitiva.
Cuando se alcanza la edad de 16 años, la genética explica el 50 por ciento de la varianza en competencia verbal y el 40 por ciento en competencia espacial; valores ambos que no se apartan mucho de los que se obtienen a propósito de capacidades cognitivas específicas en los estudios con gemelos.

Con otros, el proyecto de Colorado ha contribuido también a arrojar luz sobre diferencias y semejanzas entre habilidades cognitivas.
La neurología cognitiva admite la existencia de un modelo modular de la inteligencia; los diferentes procesos cognitivos se hallarían anatómicamente aislados en módulos discretos del cerebro.
Dicha modularidad exige que las capacidades cognitivas presenten un fundamento genético distinto. vale decir, que el condicionamiento genético de la competencia verbal no debe solaparse con el condicionamiento genético de la competencia espacial. 

Los psicólogos, sin embargo, admiten, desde hace tiempo, que la mayoría de las habilidades cognitivas especializadas, incluidas la competencia verbal y la espacial, guardan una moderada correlación mutua.
Es decir, la gente que realiza con buenos resultados un tipo de prueba, también tiende a hacerlo bien con otros tipos.
Así, es habitual que las correlaciones entre competencia verbal y espacial tengan un valor alrededor de 0,5.
Unas correlaciones de este orden implican una potencial vinculación genética. 

Tampoco la investigación sobre genética de las capacidades cognitivas específicas respalda el modelo modular.
Todo indica, por contra, que los responsables del solapamiento entre las distintas capacidades cognitivas son los genes.
El análisis de los datos del proyecto de Colorado, por ejemplo, señala que la genética gobierna el 70 por ciento de la correlación entre competencia verbal y la espacial.
A resultados semejantes se ha llegado con gemelos en márgenes de edades que van de la infancia a la madurez.
Parece, pues harto probable que, cuando se consiga identificar qué genes están asociados con una determinada capacidad cognitiva, sean los mismos que condicionen otras capacidades cognitivas.

La investigación sobre resultados escolares da a entender que los genes asociados con las capacidades cognitivas podrían incidir en el rendimiento escolar.
En los años setenta John C. Loehlin y Robert C. Nichols realizaron diversos estudios sobre más de 2000 pares de gemelos de secundaria.
Observaron que los resultados alcanzados por gemelos idénticos guardaban un parecido mayor que los de gemelos no idénticos en cuatro reas de conocimiento: lengua, matemáticas, ciencias sociales y ciencias naturales. 
A tenor de esa línea de trabajo, la genética respondería del 40 por ciento de la variación en las pruebas.

La influencia genética sobre el rendimiento escolar queda corroborada en estudios sobre gemelos de primaria y en nuestro trabajo sobre niños adoptados.

Los genes, tal parece, podrían influir en el rendimiento, escolar lo mismo que en las habilidades cognitivas.
Resultados que no dejan de sorprender ante la doctrina pedagógica imperante desde hace tiempo según la cual el rendimiento escolar era un producto del esfuerzo en mayor grado que de la capacidad.
Y lo que reviste mayor interés:
los efectos genéticos se solapan entre diferentes categorías de rendimiento y estos genes solapados son, verosímilmente, los factores genéticos que influyen en las destrezas cognitivas. 

Estos datos prestan sólido apoyo a la tesis según la cual la inteligencia sería una cualidad difusa o global de la mente, es decir, no modular.
Las observaciones recogidas no sólo destacan la importancia de las capacidades cognitivas en la vida real, sino que, además, sugieren que los genes relacionados con las capacidades cognitivas son los vinculados con el rendimiento escolar, y a la inversa.

Así las cosas, podría pensarse que las discapacidades cognitivas y un pobre rendimiento académico reflejaran a su vez un condicionamiento genético.
Sin embargo, aun cuando haya genes implicados en trastornos cognitivos, no tienen por qué ser los mismos que influyen en las funciones cognitivas normales.

Lo vemos ilustrado en el retraso mental.
En grado leve, es un fenómeno familiar; no, si se trata de un retraso grave.
El retraso mental grave depende de factores genéticos y ambientales -mutaciones nuevas, complicaciones en el momento del parto y lesiones cerebrales- que no intervienen en la gama normal de la inteligencia.

Hay que sopesar los nexos genticos entre lo normal y lo anómalo, entre los rasgos que son parte de un continuo y los trastornos reales de funciones cognitivas.
No basta con darlos por supuestos.

En su mayoría los trabajos se han centrado en las dificultades de lectura.

Afectan al 80 por ciento de los niños diagnosticados de algún tras torno del aprendizaje.
Los niños con discapacidad para lectura, los disléxicos, leen con lentitud, su comprensión es limitada y tienen problemas para leer en voz alta.

Uno de los autores (DeFries) ha demostrado que la dislexia constituye un fenómeno familiar en el que los factores genéticos explican el parecido.
El riesgo de que la dislexia aparezca en los dos hermanos gemelos idénticos es del 68 por ciento si se ha diagnosticado en uno, mientras que cae al 38 por ciento si los gemelos no son idénticos. 

¿Guarda este efecto genético alguna relación con los genes asocia dos con la variación normal de la competencia para la lectura?
La pregunta plantea ciertos retos metodológicos.
La propia idea de trastorno cognitivo es ya problemática, en el sentido de que se aborda la discapacidad desde una óptica cualitativa - se tiene o no se tiene - y no con una vara cuantitativa que mida el grado de discapacidad.
Este enfoque abre un hiato analítico entre trastornos y caracteres que son dimensionales (varían a lo largo de un continuo) y, por definición, cuantitativos. 

A lo largo de los últimos diez años se ha desarrollado una nueva técnica genética que cubre ese intervalo entre dimensiones y trastornos al recoger información cuantitativa de familiares de individuos cuya discapacidad se diagnosticó de un modo cualitativo.
Nos referimos al análisis DF de extremos, método así llamado en homenaje a sus creadores, DeFries y David W. Fulker.

En el caso de una discapacidad para la lectura, se procede a examinar gemelos idénticos y no idénticos sometiéndolos a pruebas cuantitativas de lectura.
No basta con el diagnóstico de dislexia.
Si la discapacidad para la lectura viene pro movida por genes que afectan también a la variación normal en esa competencia, entonces los valores obtenidos en gemelos idénticos disléxicos deberían acercarse a los del grupo con igual trastorno más que a los valores de los gemelos no idénticos. (Un mismo gen puede ejercer efectos diferentes si presenta m s de una forma en la población, de manera que dos personas pueden heredar dos versiones un tanto distintas.
Ejemplos de éste tipo de genes variables son los que determinan el color de los ojos y la altura).

En cuanto grupo, los gemelos idénticos con discapacidad para la lectura se asemejan a otros sujetos disléxicos en este tipo de pruebas cuantitativas, mientras que los gemelos no idénticos alcanzan mejores resultados que los del grupo de discapacitados (aunque peores que el resto de la población). 
Los genes implicados en la discapacidad para la lectura podrían ser, pues, los mismos que los involucrados en la dimensión cuantitativa de la capacidad para la lectura medida en este estudio.
Además, de acuerdo con el análisis DF de extremos podría atribuirse a la genética alrededor de la mitad de la diferencia de la puntuación en la prueba de lectura entre disléxicos y población general.

En la discapacidad para la lectura podría entonces haber un nexo genético entre lo normal y lo anormal, sin que debamos generalizar ese vínculo a otras discapacidades. 
Cabe la posibilidad de que la discapacidad en cuestión represente el extremo último de un continuo de competencia para la lectura, no un trastorno definido; es decir, la dislexia podría representar un grado cuantitativo, más que una desviación cualitativa, a lo largo de la gama de esta capacidad.
Si se descubriera, pues, un gen relacionado con la discapacidad para la lectura, no tendría nada de extraño que se hallara asociado con la gama normal de variabilidad de la competencia para la lectura.
La prueba definitiva llegar cuando se identifique un gen específico que está asociado con la competencia para la lectura o con la discapacidad correspondiente.

Hasta ahora nos hemos ceñido a la genética cuantitativa, una disciplina que mide la heredabilidad de rasgos, sin prestar mayor atención al tipo o número de genes implicados.
Para esto último debemos recurrir a la genética molecular.
Si logramos identificar los genes implicados en el comportamiento y caracterizar las proteínas que codifican, se abrirán nuevas posibilidades de intervención en las discapacidades.

En la investigación en ratones y mosca de la fruta se han identificado genes relacionados con el aprendizaje y la percepción espacial.
El estudio de la variabilidad natural en poblaciones humanas ha conducido al hallazgo de mutaciones génicas que provocan retraso mental.
Piénsese en las mutaciones de los genes relacionados con la fenilcetonuria y el síndrome del cromosoma X frágil, ambas causantes de retraso mental.
Las deficiencias en genes individuales que están asocia dos con la distrofia muscular de Duchenne, el síndrome de Lesch-Nyhan, la neurofibromatosis de tipo I y el síndrome de Williams podrían hallarse también vinculados a las discapacidades cognitivas que se manifiestan. 

Se conoce más de un centenar de mutaciones unigénicas que dañan el desarrollo cognitivo.
El funcionamiento cognitivo normal se halla orquestado, casi con toda certeza, por muchos genes que actúan de una forma sutilmente coordinada, no por genes únicos que libran la guerra por su cuenta. 
Se supone que estos genes solidarios influyen en la cognición de una manera probabilística; se les denomina loci de caracteres cuantitativos, o QTL (" quantitative trait loci "). 
El nombre, que se aplica a los genes implicados en una dimensión compleja -la cognición, por ejemplo-, subraya la naturaleza cuantitativa de determinados caracteres físicos y conductuales.
Se han identificado ya QTL en la diabetes, la obesidad y la hipertensión, así como en problemas de comporta miento involucrados en la sensibilidad y adicción a las drogas.

Ahora bien, encontrar los QTL es bastante m s difícil que identificar mutaciones en un gen responsables de trastornos cognitivos.
Fulker abordó el problema con un nuevo método, suyo y semejante al análisis DF de extremos; en él, las variaciones conocidas de ADN se correlacionan con diferencias en rasgos cuantitativos entre hermanos.
La de terminación genética se muestra con mayor nitidez en los extremos de una dimensión; por eso, el método funciona mejor cuando al menos uno de los dos hermanos representa el extremo del carácter en cuestión. 
En el centro de investigaciones de discapacidades del aprendizaje de la Universidad de Colorado se empleó esa técnica de ligamiento de QTL para acotar el sitio de los loci relacionados con la discapacidad para la lectura, objetivo que se consiguió.
Los expertos de Boulder, Universidad de Denver y el Hospital Pediátrico Municipal de Omaha dieron a conocer este descubrimiento en 1994.

A imagen de otras técnicas de genética molecular, el ligamiento QTL se basa en la identificación diferencial de marcadores del ADN:
segmentos de ADN de los que se sabe que ocupan un sitio determinado en los cromosomas y varían ligeramente de un individuo a otro.
Las versiones de un marcador, igual que las de un gen, se llaman ale los.
Puesto que las personas por tan dos copias de cada cromosoma (salvo de los cromosomas X e Y que determinan el sexo en el varón), presentarán dos alelos de cada marcador de ADN.
Ello significa que los hermanos compartirán uno, dos o ningún alelo de un marcador.
En otras palabras, en lo concerniente al marcador, los hermanos serán gemelos idénticos (si comparten ambos alelos), gemelos no idénticos (si sólo la mitad de sus alelos) o hermanos adoptivos (si no comparten ningún alelo).

Los descubridores de los QTL relacionados con la discapacidad para la lectura identificaron un gemelo afectado; luego, averiguaron qué resultados obtenía su hermano gemelo en las pruebas de lectura.
Si la puntuación alcanzada por el segundo era peor cuando compartían alelos de un marcador dado, el marcador debería caer cerca de un QTL relacionado con la discapacidad para la lectura, en la misma región cromosómica.
Hallaron ese marcador en el brazo corto del cromosoma 6 de dos muestras independientes, una en hermanos mellizos y otra en hermanos de distintas edades. 
Se han confirmado los resultados.

Si bien esa línea de investigación ha propiciado que se delimite la localización de un gen (o genes) implicado en la discapacidad para la lectura, el gen (o genes) no se ha podido caracterizar todavía.
Distinción que nos permite expresar la situación en que se encuentra la genética de la cognición: en el umbral de un des cubrimiento importante.
La identificación de genes que condicionan capacidades cognitivas específicas revolucionar la investigación de los procesos biológicos relacionados con la mente.
La genética molecular tendrá, sin duda, profundas consecuencias en el estudio del comportamiento humano.
Muy pronto se podrán investigar las conexiones genéticas entre diferentes caracteres y entre comportamientos y mecanismos biológicos.
Y con ello seguir mejor la trayectoria del desarrollo de la influencia genética y establecer con mayor precisión las interacciones entre genes y entorno.

El descubrimiento de genes vinculados con enfermedades y discapacidades facilitar la labor terapéutica y diagnóstica de los individuos en riesgo, antes de que se manifiesten los síntomas.
Que las cosas avanzan en esa dirección lo vemos en el alelo ApoE4, asociado con demencia senil y declive de las capacidades cognitivas en la vejez.
Los nuevos conocimientos traerán también nuevos problemas de índole ética.
Las investigaciones genéticas siempre levantan temores de que los marca dores de ADN se utilicen para seleccionar "niños a la carta".

Capacidades cognitivas e inteligencia 

Desde los albores de la psicología nunca hubo acuerdo entre los expertos en torno la naturaleza de la inteligencia. 
Para unos se trataría de una facultad heredada, mientras que otros insisten en la importancia de la educación y la crianza en su conformación.
Unos la consideran una cualidad global que penetra todos los aspectos de la cognición; otros opinan que el intelecto consta de capacidades especializadas discretas -el talento artístico o la facilidad para las matemáticas- sin que compartan un principio común. 

De un tiempo a esta parte, con la incorporación de la genética, los psicólogos se inclinan por atribuir a la herencia un peso importante en la inteligencia. 

Hasta la mitad de la variabilidad individual manifestada cabria asignarla a factores genéticos.

Los psicólogos se adhieren en su mayoría a una descripción globalizante de la inteligencia.
Esa competencia cognitiva general, o "g", así la denominan, se refleja en el solapamiento de unas habilidades cognitivas sobre otras.
Como Robert Plomin y John C. DeFries señalan las personas que no alcanzan una buena puntuación en las pruebas de una habilidad cognitiva suelen dar también valores bajos en pruebas que exploran otras.
En esa correlación se apoyan las pruebas del cociente intelectual, que producen un valor único a partir de las valoraciones combinadas de distintas habilidades cognitivas.

Por guardar dicha relación la capacidad cognitiva general y las específicas, a nadie sorprender que los hallazgos referentes a las capacidades especificas sean en numerosas ocasiones un eco de lo que ya se conoce acerca de la capacidad general.
La heredabilidad observada en habilidades especificas es similar a la heredabilidad que se ha determinado para "g". 
Las tendencias manifestadas durante el desarrollo -en que la influencia genética sobre capacidades cognitivas especificas aumenta a lo largo de la infancia y alcanza el nivel propio del adulto en la adolescencia- se repiten a propósito de la capacidad cognitiva general.

Puesto que las medidas de "g" derivan de intercorrelaciones entre la competencia verbal y la espacial, un gen que está ligado a estos dos caracteres desempeñar alguna función en la capacidad cognitiva general; y viceversa.
En un número reciente de Psychological Science , Plomin y sus colaboradores hacen público el descubrimiento del primer gen asociado con la capacidad cognitiva general.

Aunque este hallazgo promueve el avance de nuestro conocimiento sobre la naturaleza de la cognición, es probable que reencienda el debate.

¿Qué significa la heredabilidad? 

El alcance de los registros de heredabilidad se interpreta a menudo de forma incorrecta.
La heredabilidad es una medida estadística, que se expresa en términos de tanto por ciento, y des cribe el grado en el que los factores genéticos contribuyen a la variabilidad de un carácter de terminado en el seno de una población.

El que los genes influyan en un carácter no significa, sin embargo, que "la biología sea determinista".
La genética ha contribuido a confirmar la importancia de los factores ambientales, que suelen explicar la variabilidad del comportamiento humano en una cuantía pareja a la atribuida a la asignada a los genes.
Si un 50 % de la inteligencia se hereda, otro tanto corresponder a los factores ambientales en la explicación de las diferencias entre individuos. 
Aun cuando la genética tenga un peso notable, como en ciertos tipos de retraso mental, la implicación del entorno podría a menudo superar los "determinantes" genéticos.
Sin ir m s lejos, los efectos devastadores de la fenilcetonuria, una enfermedad congénita que arrastra un retraso mental, pueden anularse con una dieta adecuada.

Por último, el grado de heredabilidad de un carácter no es algo esculpido en roca.
La influencia respectiva de genes y ambiente puede cambiar.
Si los factores ambientales fueran casi idénticos para todos los miembros de una población hipotética, cualquier diferencia en la capacidad cognitiva en esa población tendría que atribuirse entonces a la genética, y la heredabilidad tendría valores cercanos al 100 por cien en lugar del 50 por cien.
La heredabilidad describe lo que es, no lo que puede (o debería) ser.

Figura 1

LOS GEMELOS CONSTITUYEN SUJETOS IDEALES para investigar las capacidades cognitivas

Las parejas de gemelos idénticos ( página precedente ) y no idénticos (abajo) que aparecen en las fotografías participan en nuestros estudios.
Realizan una prueba de competencia espacial en la que deben reconstruir un bloque modular de pises con piezas de juguete.
En este tipo de pruebas, que se ofrece a cada niño por separado, los resultados que obtienen los gemelos idénticos (con todos sus genes iguales) guardan un parecido mayor que los resultados de gemelos no idénticos (que comparten sólo la mitad de los genes), señal de que la herencia influye en la competencia espacial.

Figura 2

ENTRE LAS PRUEBAS de capacidades específicas a desarrollar por adolescentes y adultos se incluyen tareas similares a éstas

Las pruebas calibran cada capacidad cognitiva de varias maneras; las pruebas múltiples se combinan para obtener una medida fiable de cada capacidad.
(Las respuestas aparecen al final del articulo).

Figura 3

EN LOS ESTUDIOS CON GEMELOS IDENTICOS y no idénticos se examinan las correlaciones de competencia verbal y espacial

Cuando los resultados de estudios distintos se comparan entre sí, se aprecia una notable influencia genética sobre las capacidades cognitivas de la infancia a la vejez; en todos estos grupos, los resultados obtenidos por gemelos idénticos guardan un mayor parecido mutuo que los de los gemelos no idénticos. 

Figura 4

SEGÚN EL PROYECTO DE ADOPCION de Colorado en competencia verbal ( arriba ) y espacial ( abajo ) los niños adoptados se asemejan a sus padres biológicos ( barras blancas )

Lo mismo que los niños educados por sus padres biológicos( barras grises ). 
Por contra, los niños adaptados no acaban pareciéndose a sus padres adoptivos (barras negras).
A tenor de la investigación, el grueso del parecido familiar en las capacidades cognitivas deriva de actores génicos.

Figura 5

PUNTUACION en pruebas de lectura realizadas por gemelos

Revela un nexo posible entre habilidades normales y disminuidas para la lectura.
En un grupo de miembros de parejas de gemelos ( arriba ) elegidos al azar, un número restringido presentaba discapacidad para la lectura ( azul ).
Los resultados obtenidos por los gemelos idénticos ( en medio ) y no idénticos ( abajo ) de niños con discapacidad para la lectura fueron inferiores a los del grupo seleccionado al azar, y los de los gemelos idénticos fueron peores que los de los gemelos no idénticos.
Los factores génicos se hallan, pues, implicados en la discapacidad para la lectura.
Los mismos genes que influyen en la discapacidad podrían esconderse tras las diferencias en la capacidad normal para la lectura.

Figura 6

DOS MODELOS que ilustran la responsabilidad de la genética en la discapacidad para la lectura

Según la hipótesis clásica (arriba), basta una variante (arriba), o alelo, de un gen para causar el trastorno; quien porte dicho alelo estar discapacitado para la lectura (gráfico).
Pero los datos abonan una explicación distinta (abajo):
un alelo solo es inca paz de producir la discapacidad.
Pero variantes de genes múltiples si pueden actuar de forma conjunta para mermar el rendimiento y aumentar el riesgo de discapacidad. 


Desarme de los virus de la gripe

Pronto contaremos con medicinas que tratarán la gripe mediante la detención de la replicación vírica en los tejidos, fármacos que podrían desempeñar funciones preventivas.

No es raro que una cepa insólita de virus de la gripe comience a pasar, de repente, de un sujeto a otro.
Por su propio carácter insólito, son contadas, si alguna, las personas que han desarrollado inmunidad adquirida en exposiciones anteriores; hasta los vacunados carecen de defensas efectivas.
Las vacunas contra la gripe protegen de variantes del virus que los expertos suponen que serán activas en una estación dada, pero no de otras variantes inesperadas. 
Al no encontrar nada que le ponga freno, la nueva cepa se disemina intacta, sembrando enfermedad y muerte por todo el planeta.

La peor pandemia (epidemia mundial) que se recuerda es la de 1918.
Mató a más de veinte millones de personas, en algunos casos pocas horas después de manifestarse los primeros síntomas.
Al desastre, desencadenado por el virus de la "gripe española", siguieron en 1957 una epidemia de "gripe asiática", la gripe de Hong Kong en 1968 y la "gripe rusa" en 1977. (Los nombres reflejan la impresión popular sobre los lugares donde se inició la pandemia, aunque ahora se sospeche que los cuatro episodios y, posiblemente, la mayoría de los demás se originaron en China). 

Los expertos en salud pública advierten de que en cualquier momento puede aparecer otra pandemia y que ésta podría ser tan peligrosa como el episodio de 1918. En 1997 se detectaron en Hong Kong 18 casos de una variante letal de virus de la gripe que terminó con la vida de seis de ellos, en lo que se temió fuera el principio de la siguiente ola.
Las autoridades regionales consiguieron controlar el problema con rapidez.
Detectaron la fuente de contagio -pollos, patos y gansos infectados- y sacrificaron toda la población aviar de Hong Kong.

La próxima vez puede que la humanidad no tenga tanta suerte.
Si una cepa tan letal como la de Hong Kong se diseminara por nuestras comunidades densamente pobladas, un 30% de la humanidad podría morir -por acción directa del virus o por infecciones bacterianas secundarias- antes de que diera tiempo a desarrollar una vacuna con la que proteger a los que inicialmente se hubieran salvado.
Se necesitan unos seis meses para desarrollar una vacuna contra una variante cualquiera de virus de la gripe, probar su seguridad y distribuirla, demasiado tiempo para una pandemia que se extiende con tanta rapidez.

Si la temida pandemia tarda aún otro año o incluso más en aparecer, puede que ya se disponga de nuevos métodos para evitar tanta enfermedad y muerte. 
A finales de este año y después de superar extensos ensayos clínicos, se podría aprobar la comercialización de dos fármacos como nuevas armas contra la gripe.
Los agentes zanamivir (Relenza) y GS 4104 son prometedores en la prevención de la gripe y en la reducción de la duración y severidad de los síntomas en personas que empiezan el tratamiento cuando ya están enfermas.

A diferencia de las vacunas, que estimulan el sistema inmunitario para que impida que los virus ganen pie en el organismo, y a diferencia de los remedios caseros habituales, que suavizan los síntomas pero que no tienen efecto sobre la infección, estos fármacos han sido diseñados para atacar directamente al virus.
Mediante el bloqueo de la acción de la neuraminidasa, una enzima vírica decisiva, reducen notablemente la proliferación del virus en el organismo.
Otros inhibidores de neuraminidasa se encuentran también en estudio, aunque no se ha llegado a la fase de ensayo clínico. 

Ya existen en el mercado dos fármacos, amantadina y rimantadina, con acción contra los virus de la gripe. 
Pero estos dos agentes, cuyo modo de acción se sustenta en mecanismos distintos, presentan graves inconvenientes. 
Pueden causar confusión y otros efectos secundarios neurológicos y no son eficaces contra una de las dos clases principales de virus, el tipo B, que afectan a los humanos.
Aún más, los virus de la gripe parecen desarrollar sin mayor dificultad resistencia contra estos fármacos. 
Así, los individuos que en las primeras fases de una epidemia han recibido este tratamiento pueden transmitir versiones del virus resistentes a la medicación.
Este problema es especialmente acuciante en las comunidades cerradas, como las casas de convalecencia y residencias de ancianos. 

La historia de cómo se desarrollaron estos nuevos fármacos es una maravillosa combinación de suerte y lógica.
El desciframiento en 1983 de la estructura tridimensional de la neuraminidasa es el punto de partida más inmediato.
Pero fue una serie anterior de descubrimientos la que permitió que los científicos identificaran una parte específica de la molécula de neuraminidasa como un talón de Aquiles en todas las variantes de virus de la gripe, una debilidad que fármacos bien construidos podrían explotar.

Una de esas líneas de investigación puso al descubierto propiedades esenciales de los virus de la gripe y de sus estrategias de pervivencia.
Saben los biólogos que los virus son, a grande rasgos, genes envueltos en proteínas que protegen esos mismos genes o que participan en la replicación vírica en el organismo.
A veces, como en el caso del virus de la gripe, estos constituyentes están también ceñidos por una membrana lipídica.
Las enfermedades causadas por virus suelen comenzar con la invasión de tipos celulares seleccionados, en cuyo interior el virus se replica antes de salir al exterior para infectar otras células. 
Los síntomas se deben a la alteración de la función de las células colonizadas, secundaria a la proliferación vírica, y a los esfuerzos del sistema inmunitario por contener la infección, que producen inflamación, fiebre y dolores sistémicos.

Las cepas que colonizan humanos revelan especial afinidad por las células epiteliales que tapizan el tracto respiratorio. 
Si la infección tiene éxito, en uno o en un par de días aparecerán los síntomas clásicos de nariz tapada, mucosidad abundante, tos seca, estornudos, fiebre, dolores, cansancio agudo y pérdida del apetito.
Las descripciones históricas basadas en síntomas indican que las epidemias de gripe han azotado a la humanidad desde antes del siglo V a.C.

En 1933 se aisló la primera cepa de virus de la gripe a partir de un humano.
Desde entonces, se sabe que los virus gripales pertenecen a dos tipos básicos -A y B- que se diferencian en ciertas proteínas internas.
No parece que un tercer tipo (C) cause enfermedad grave.

Los virólogos clasifican las variantes del tipo A según dos proteínas de superficie que sobresalen en espícula, la hemaglutinina y la neuraminidasa (enzima diana de los nuevos fármacos).
En cuanto proteínas son cadenas de aminoácidos plegadas.
Todas las variantes de hemaglutinina adoptan esencialmente la misma conformación tridimensional y todas las variantes de neuraminidasa presentan una morfología característica.
Pero dentro de cada grupo, la secuencia de aminoácidos constituyentes de una proteína puede variar mucho.
Hasta ahora se han identificado en el virus de tipo A quince subtipos de hemaglutinina y nueve de neuraminidasa;
las variantes se denominan según las moléculas de hemaglutinina y de neuraminidasa que presenten: H1N1 , H1N2 , H2N2 , etc.

Los virus de tipo B, mucho más uniformes, portan una sola variante de hemaglutinina y otra de neuraminidasa, si bien la secuencia de aminoácidos también varía de una cepa B a otra.
Cada subtipo A se presenta en formas levemente diferentes.

Los tipos A y B no sólo divergen en sus características químicas.
Discrepan también en el alcance de su actividad.
El tipo B, que infecta exclusivamente a humanos, suele causar epidemias regionales, no pandemias.
Por contra, la gripe de tipo A afecta a humanos, cerdos, caballos, focas, ballenas y aves, aunque no todas las cepas infectan a todas las especies.
Sólo se han identificado cuatro subtipos que provoquen enfermedad en humanos: los responsables de las pandemias sufridas en nuestro siglo.

A pesar de las diferencias, ambos tipos de virus desarrollan el mismo ciclo biológico.
Para que una copia o partícula de virus entre en la célula, la hemaglutinina vírica debe enlazarse con el ácido siálico, molécula glicosada de la superficie celular.
Una vez introducido, el virus queda inicialmente secuestrado en una especie de burbuja.
Pronto se liberan sus genes, constituidos por cadenas de ARN, y las proteínas internas, listos para llegar al núcleo celular.

Allí, algunas proteínas víricas comienzan a replicar las cadenas de ARN vírico y a sintetizar ARN mensajero, una forma de ácido nucleico que lee y traduce a proteínas la maquinaria celular encargada de la síntesis de proteínas.
Las nuevas moléculas sintetizadas, proteínas y ácidos nucleicos, se unen y crean una partícula vírica nueva antes de salir al exterior. 

Desafortunadamente para el virus, las partículas emergentes permanecen recubiertas por ácido siálico, sustancia de la superficie celular en que se anclan los virus gripales.
Si el ácido siálico persistiera en el virus y en la superficie de la célula destinada a la replicación vírica, la hemaglutinina de las partículas recién formadas se enlazaría al ácido siálico, provocando que las partículas víricas se agolparan sobre la célula, como insectos pegados en el atrapamoscas. No podrían alcanzar nuevas células.

Pero el virus guarda un as en la manga.
La moléculas de neuraminidasa de las partículas víricas recién formadas seccionan las moléculas de ácido siálico. 
En otras palabras, las espículas de neuraminidasa disuelven el "pegamento" de ácido siálico y dejan libertad de movimientos a las partículas.
La enzima facilita también el desplazamiento del virus por el moco que hay entre las células de las vías aéreas. 

Hacia 1960 se sabía ya que el fármaco que bloqueara cualquier etapa del proceso de replicación impediría que el virus causara enfermedad o permitiría controlar una infección activa.
Pero se desconocía cómo llevar la idea a la práctica.
Conscientes de que el virus de la gripe crecía en el interior de las células cuya maquinaria empleaba para fabricar sus proteínas víricas, los biólogos temían que los agentes destructores del virus afectaran de paso a las células sanas.

Continuó la investigación sobre las razones por las que algunas cepas producían epidemias locales y otras desataban terribles pandemias.
Se llegó a la determinación de que, para ser útil contra la gripe, el fármaco debía hallarse capacitado para atacar a todas las variantes del virus, incluidas las que se ignora si provocan enfermedad en humanos.

Una cepa de virus de la gripe puede desendenar una epidemia, local o global, sólo en el caso de que las personas expuestas carezcan de inmunidad adecuada.
Cuando alguien contrae la gripe, su sistema inmunitario produce anticuerpos, moléculas que reconocen segmentos específicos de hemaglutinina y neuraminidasa de la superficie del virus.
Si ese sujeto sufre una nueva exposición a la misma cepa, los anticuerpos (sobre todo los dirigidos contra la hemaglutinina) bloquearán prestamente al virus e impedirán que siga infectando. 

Si el virus no cambiara nunca, como sucede con el virus del sarampión o el de las paperas, los anticuerpos producidos durante la infección o en respuesta a una vacuna proporcionarían inmunidad permanente.
Pero los virus de la gripe están en transformación incesante.
Por tanto, los anticuerpos producidos un año resultan de nula o escasa eficacia si en la siguiente estación de gripe se enfrentan a otras variantes del virus.
La cuantía del cambio determina, en buena medida, si una epidemia queda controlada o se propaga sin freno.

Los virus de la gripe se van modificando a través de la ruta de la "deriva" antigénica, una revisión gradual de la secuencia de aminoácidos de una proteína (antígeno) capaz de desencadenar una respuesta inmunitaria. 
Estas alteraciones aparecen por pequeñas mutaciones en el gen de la proteína.
Unas veces, la mutación comporta cambios sutiles en la estabilidad o en la actividad de una proteína.
Otras, atenta contra la proteína y dificulta la viabilidad del virus.
Pero puede también reforzar la pervivencia, mediante la reconfiguración de un sitio de la hemaglutinina que hasta entonces era reconocido por un anticuerpo.

Cuando los genes y las proteínas de hemaglutinina o neuraminidasa han acumulado alteración tras alteración, se tornan punto menos que irreconocibles para la mayoría de los anticuerpos.
Se desencadena así una nueva epidemia, cuya extensión se detiene cuando encuentra grupos humanos cuyo sistema inmunitario ya se había enfrentado a algunas de tales alteraciones.

Según parece, el tipo B no conoce otra vía de modificación que la deriva antigénica.
Evoluciona gradualmente en su huésped humano para ocultar su naturaleza al repertorio inmunitario de una población. 
Las cepas del tipo A pueden sufrir, por contra, cambios más drásticos.
Esas variaciones mayores se compendian en el "desplazamiento" antigénico, capaz por sí solo de desatar una pandemia.

Cuando se presenta un desplazamiento antigénico, las cepas desarrollan espículas de hemaglutinina nuevas; a veces, con una molécula incluida de neuraminidasa con la que la mayoría de las personas no han tenido nunca contacto.
De ese modo, el virus elude el repertorio antigénico entero de todas las poblaciones del mundo y desencadena una pandemia. 
Con la facilidad de transporte intercontinental, un viajero puede diseminar un virus peligroso e inédito, de una parte a otra de la tierra, en un solo día.

Un cambio tan drástico no puede resultar de una simple mutación genética.
El proceso mejor estudiado de desplazamiento antigénico se basa en la coincidencia de dos cepas víricas en una célula huésped. 
De esa guisa, los genes empaquetados en las nuevas partículas de virus (y sus correspondientes proteínas) provienen en parte de una cepa y en parte de otra.
Cabe semejante reordenación genética porque el genoma de los virus de la gripe consta de ocho cadenas discretas de ARN (cada una de ellas determina una o dos proteínas).
Estas cadenas se pueden mezclar y complementar cuando se forman nuevas partículas de virus A en células dualmente infectadas. 
Hay, en efecto, virus de la gripe que infectan a personas y a cerdos; si un cerdo se viera infectado por un virus humano y una cepa que infecta sólo a aves, el cerdo podría terminar por producir una cepa híbrida, igual en todo al virus humano menos en presentar una molécula de hemaglutinina propia del virus aviar.

Pero el desplazamiento antigénico acontece, asimismo, cuando un virus gripal de animales, incapaz hasta entonces de producir infección en humanos, de pronto se vuelve virulento para las personas.

Nadie sabe qué clase de desplazamiento antigénico desencadenó la pandemia de gripe española de 1918, causada por el subtipo H1N1 de virus de la gripe A.
Se ha comprobado que la reordenación genómica fue responsable de las pandemias de la gripe asiática de 1957 y de la gripe de Hong Kong de 1968, causadas respectivamente por H2N2 y H3N2. 
Algunos estudios sugieren que las aves acuáticas pudieron aportar genes poco frecuentes y que los cerdos sirvieron probablemente de banco de mezcla.
Este papel de los cerdos explicaría por qué las pandemias acostumbran brotar en China: millones de aves, cerdos y personas viven en estrecha relación.

H5N1 , el virus que mató seis personas en Hong Kong en 1997, no había emergido, sin embargo, de ninguna reordenación genómica.
Pasó directamente de las aves al hombre.
Pero H5N1 no pudo transmitirse de un humano a otro.
Si se le hubiera dado tiempo suficiente para adquirir, por mutación o reordenación, esa capacidad, quizás habría escapado en seguida a todo control.

El toque de alerta de 1997 ha convencido a los expertos en salud pública de que no basta con seguir el curso de la gripe en humanos, cosa que ya se viene haciendo.
Hay que vigilar también los casos que se dan en aves migratorias. 
Es probable que estas aves sirvan de reservorio de cepas del tipo A de año en año, que luego se diseminan a las aves domésticas y a otras especies.
La rápida identificación de cepas animales con potencial de dañar a las personas podría ayudar a evitar un desastre de salud pública.

El incidente de Hong Kong ha avivado el interés por estudiar la naturaleza de la "barrera de las especies", que impide que muchas cepas de la gripe puedan pasar de un animal a otro.
Si se conociera mejor ese obstáculo, podrían sellarse los huecos por donde se cuelan cepas animales para infectar al hombre.

Del estudio sobre biología de la gripe acometido a principios de los ochenta se desprendía que el fármaco ideal debería bloquear la actividad de alguna molécula relacionada con el ciclo reproductor del virus mediante la ocupación de un sitio "conservado" de la molécula efectora.
La medicina tendría que hacer blanco en una región cuya secuencia de aminoácidos persistiera constante en todas las cepas.
Actuaría así contra cualquier virus de la gripe que apareciera en personas, incluidos los transmitidos directamente por animales.

El trabajo estructural que permitió desarrollar los inhibidores de la neuraminidasa surgió a raíz de un descubrimiento accidental.
A finales de los setenta, uno de nosotros (Laver) se aprestaba a determinar si la espícula N2 del virus que había causado la pandemia gripal de Hong Kong de 1968 (H3N2) procedía de la cepa responsable de la pandemia de gripe asiática de 1957 (H2N2) ; se proponía, pues, comparar la secuencia de aminoácidos de las moléculas.
Aisló y concentró las cabezas, esto es, los dominios que emergen de la superficie del virus.

Cuando Laver liberó de los virus purificados las cabezas de neuraminidasa y las concentró en una centrífuga, halló que el amasijo de material resultante no era amorfo como suele suceder con las proteínas.
Observó cristales, distribuciones ordenadas de moléculas, esenciales para descifrar la estructura tridimensional de proteínas grandes.
En concreto, la inesperada producción de cristales de neuraminidasa implicaba que se podría descifrar la estructura de la neuraminidasa.

Eso mismo es lo que el grupo de Peter Colman, de la Organización de Investigación Científica e Industrial Commonwealth (CSIRO) de Australia, realizó en 1983.
El trabajo reveló que las espículas de neuraminidasa de los virus de la gripe constaban de 4 monómeros, moléculas idénticas entre sí.
El tetrámero resultante se asemejaba a cuatro globos colocados encima de un eje.
El eje, único, penetraba la membrana vírica y los globos sobresalían.
El grupo de Colman no tardó en descubrir que cada monómero de neuraminidasa mostraba una hendidura central profunda en su superficie.

Se observó que, si bien las moléculas de neuraminidasa podían diferir en la secuencia de aminoácido concreta, todas las versiones conocidas, tanto del tipo A como del B, presentaban una similitud impresionante.
Los aminoácidos que tapizaban la pared de la hendidura eran invariantes.

Cuando alguna parte de una molécula se resiste al cambio, el motivo habitual es que se trata de componentes esenciales para la función de la molécula.
En este caso, la uniformidad apuntaba a que la hendidura formaba el sitio activo de la neuraminidasa que rompía el ácido siálico y que los aminoácidos conservados de la hendidura eran imprescindibles para ejecutar la acción catalítica.
El trabajo posterior confirmó estas hipótesis.

Dado que los virus de la gripe no pueden diseminarse de una célula a otra sin la acción de la neuraminidasa, los nuevos descubrimientos implicaban que un fármaco capaz de ocupar y bloquear el sitio activo podría inhibir la neuraminidasa en todas las versiones del virus de la gripe.
Esto es, la "llave" serviría de cura universal para la gripe.

Persiguiendo idea tan sugestiva, el grupo de Colman identificó los aminoácidos del sitio activo que establecen contacto con el ácido siálico.
También determinó qué aminoácidos de la hendidura no se unían al ácido siálico, pero podían servir de anclaje al fármaco.
Observó, por ejemplo, que la hendidura tenía tres aminoácidos con carga positiva que se enlazaba con firmeza a un grupo carboxilato del ácido siálico con carga negativa.

Había, además, en el fondo de la hendidura una bolsa donde se alojaban dos aminoácidos de glutamato con carga negativa que no entraban en contacto con el ácido siálico, aunque estaban presentes en todas las neuraminidasas examinadas.
Un grupo hidroxilo (OH) del ácido siálico apuntaba hacia esa bolsa suplementaria, sin llegar a alcanzarla. 

Estas características sugerían que, si se cambiaba el grupo OH por un grupo atómico grande, con carga positiva, se podría obtener un derivado que se uniera firmemente.
El grupo positivo se acomodaría quizás en la bolsa suplementaria de la base del sitio activo; se anclaría en los grupos glutamato de la bolsa dotada de carga negativa y sin emplear hasta entonces.

Después de varios intentos, en 1993 Mark von Itzstein y su equipo, de la Universidad Monash de Melbourne, hallaron que, si se cambiaba el grupo OH del ácido siálico por un grupo guanidina, grande y dotado de carga positiva, se obtenía un inhibidor de las neuraminidasas gripales potentísimo. 
El inhibidor no ejercía apenas efecto sobre enzimas afines sintetizadas por bacterias y mamíferos, señal de que el compuesto no lesionaría las células humanas. 

Los estudios en animales y los ensayos preliminares en humanos revelaron que la sustancia, el zanamivir, prevenía los síntomas gripales en individuos que luego resultaron infectados por el virus y que remitía la gravedad de los síntomas en quienes tomaron el preparado después de infectarse.
Este no se administra en pastilla, sino inhalado por la nariz o la boca hacia el sistema respiratorio. 

El grupo guanidina, gracias al cual el zanamivir constituye un óptimo inhibidor, impide que se tome en pastilla.
Los fármacos ingeridos han de cruzar las células que tapizan el intestino y alcanzar el torrente sanguíneo, para distribuirse por otras partes del cuerpo.
Pero a las moléculas dotadas de carga les cuesta atravesar las membranas celulares, lipídicas y permeables sobre todo a sustancias sin carga.

La inhalación es la forma habitual de administración de medicinas que han de operar en el tracto respiratorio.
Glaxo Wellcome, de Stebenage, inició más pruebas en humanos con zanamivir.
Algunos pacientes, sin embargo, preferían la pastilla.
Por eso, Gilead Sciences, de Foster City, se alió con F. Hofman-La Roche, de Basilea, para fabricar un inhibidor de neuraminidasa que se administrara por vía oral.
Además, si el fármaco circula por la sangre podría combatir cualquier virus de la gripe virulento que infectara células fuera del tracto respiratorio.

Tras sintetizar y someter a ensayo varios compuestos, Gileat y F. Hoffman-La Roche se concentraron en GS4071, tan potente como zanamivir.
Del análisis estructural se desprendía que tenía también un grupo principal de unión a la neuraminidasa que se encontraba en el zanamivir:
un grupo carboxilato dotado de carga negativa que se unía a los aminoácidos con carga positiva del sitio activo de la enzima.
Además, la interacción entre GS4071 y el sitio activo inducía la rotación de un aminoácido de la hendidura y creaba una bolsa hidrofóbica nueva.
Esta bolsa repelente del agua servía de anclaje para un componente hidrofóbico similar (una cadena de átomos de carbono e hidrógeno) del fármaco. 

La sustancia funcionó bien en el tubo de ensayo. 
No dañaba las células sanas.
Pero fracasó en una prueba crucial:
el grupo carboxilato negativo le impedía atravesar el intestino de los animales y llegar a la sangre.
Mas bastó un pequeño reajuste -se recubrió el grupo negativo- para solucionar el problema.
La forma con envoltura GS4104 arribaba sin problemas a la circulación y se liberaba de su envoltura en la sangre y el hígado.
Convertido de nuevo en su forma original (GS4071) inhibía la actividad de la neuraminidasa y vedaba la diseminación del virus en el tracto respiratorio de los animales de ensayo.
Lo mismo que el zanamivir, dio muestras de funcionar bien en humanos. 

Los informes del último otoño sobre los exhaustivos ensayos clínicos controlados en torno a zanamivir y GS 4104 confirmaron y extendieron los descubrimientos iniciales.
Si se inicia el tratamiento con zanamivir o la forma ingerida GS4104 , día y medio después del inicio de los síntomas, el tiempo que la gente se encuentra enferma se reduce en un 30% (uno y medio a tres días). 
Los compuestos también reducen la gravedad de los síntomas.
Un ensayo con GS4104 , por ejemplo, indicó que los diarios de valoración de los pacientes mostraban que los síntomas eran entre un 25% y un 60% más suaves que en los pacientes que tomaron placebo.
Además, los fármacos disminuyen el riesgo de infecciones bacterianas secundarias -así, la bronquitis-, potencialmente letales, en un 50% o más.
Estas complicaciones son una causa muy importante de muertes por gripe, sobre todo entre los ancianos y las personas con enfermedades concomitantes. 

Cierto estudio restringido de zanamivir manifiesta que la forma inyectable del fármaco también es eficaz. 
Son a su vez prometedoras las investigaciones en el terreno de la prevención.
Cuando se probó el zanamivir, el 6% de los que no lo tomaron, por sólo el 2% de los que lo tomaron, contrajeron la gripe.
GS4104 produce resultados similares.
Hasta la fecha ninguno de los dos fármacos ha producido efectos secundarios graves.

A partir de estos estudios Glaxo Wellcome ha solicitado autorización para comercializar la forma inhalada de zanamivir en Australia, Europa, Canadá y los EE.UU.
Se espera que Gilead y F. Hoffman-La Roche hagan lo mismo con el GS4104 en EE.UU. y Europa a lo largo del año en curso.
Otros inhibidores de la neuraminidasa desarrollados por BioCryst Pharmaceuticals de Birmingham han funcionado en estudios con animales y se ha autorizado a Johnson & Johnson para un posterior desarrollo.

Para el observador ocasional, el recortar pocos días la duración de la gripe puede parecerle un triunfo menor. 
Pero se trata de algo más profundo de lo que sugiere a simple vista.
La fatiga y otras incomodidades que se sufren durante los últimos días de la gripe resultan, sobre todo, de la actividad residual del sistema inmunitario, tras la eliminación del organismo de la mayoría de los virus.
Los fármacos antigripales no pueden afectar esta actividad, pero al obstruir la replicación vírica los compuestos sí pueden recortar la duración y aligerar la gravedad de la parte inicial, la más desagradable de un episodio de gripe.

Cuanto antes se empiece el tratamiento, mejores resultados se cosecharán, en parte porque el organismo albergará una cantidad de virus menor y más domeñable y porque los fármacos no pueden curar las lesiones que el virus inflige a los tejidos antes de que se inicie la terapia.
De la combinación de tales observaciones con la investigación desarrollada en el campo de la prevención se infiere que la mejor forma de usar tales fármacos puede ser en conjunción con pruebas instantáneas que detecten la presencia de una infección incipiente antes de que la persona desarrolle síntomas.
No es disparatado pensar que, en un futuro cercano, las personas podrían aplicarse, cada mañana, una tirita de papel de prueba de detección del virus de la gripe en la lengua.
Si la tira cambia de color, se sabrá que nuestro organismo aloja ya al virus de la gripe y que hay que comenzar a tomar un fármaco antigripal para controlar los síntomas.
Existen protocolos de detección precoz en el consultorio médico y se está avanzando en adaptarlo a su empleo domiciliario.

Estos fármacos funcionan sólo ante virus de gripe.
Importa, pues, determinar si una persona ha contraído la gripe y no un simple resfriado u otra enfermedad, para evitar el desperdicio de emplearlas en combatir resfriados, causados por otros tipos de virus, alergias o infecciones bacterianas que producen síntomas pseudogripales.

Aunque los agentes bloqueantes de neuraminidasa han generado enorme expectación, quedan cuestiones pendientes.
Por lógica, la inhibición de la replicación vírica y la reducción de las complicaciones secundarias deberían salvar vidas, pero no sabemos todavía si el fármaco realmente evita muertes.

Aún más, las drogas que los humanos empleamos contra virus y bacterias acostumbran terminar superadas por las resistencias.
En su lucha por la supervivencia, estos microorganismos desarrollan variantes en las que se altera la diana del fármaco, para hacerla irreconocible y poder escapar a los efectos del fármaco.
La amantadina y la rimantadina son algunos ejemplos de fármacos contra los que se han desarrollado resistencias.
¿Existen razones para pensar que los nuevos fármacos antigripales no terminen igual?

Las hay.
Se ha intentado producir en el laboratorio cepas de virus de la gripe resistentes al zanamivir y al GS4104 . 
Hasta ahora sólo han tenido un éxito limitado. 
Algunas cepas ganaron resistencia, como se predecía, alterando los aminoácidos del sitio activo de la neuraminidasa, pero estos cambios hicieron tambalear la estabilidad de la enzima o su actividad normal, lo que nos adelanta las dificultades que rodearían a la pervivencia del virus en el organismo. 

Otras cepas no basaron su estrategia de resistencia en modificaciones de la neuraminidasa, sino en cambios químicos en la hemaglutinina.
Recuérdese que el virus de la gripe necesita la neuraminidasa para romper el ácido siálico de la superficie de nuevas partículas víricas al objeto de que la hemaglutinina no quede pegada al ácido siálico de las partículas vecinas y se evite así la diseminación hacia otras células.
El cambio en la hemaglutinina redujo su afinidad por el ácido siálico, obviando la necesidad de que interviniera la neuraminidasa.
Aunque apareció en células de cultivo, el segundo grupo de mutantes no mostró resistencia a los fármacos en animales.
Cabe imaginar que el enlace maltrecho de la hemaglutinina y el ácido siálico redujo la infectividad de las cepas al debilitar su capacidad de anclarse en las células.

La investigación no se detiene en los fármacos bloqueantes de la neuraminidasa.
Desde hace tiempo se sueña, sin éxito, con fármacos cuya diana fuera el sitio de unión de la hemaglutinina con el ácido siálico.
La eficacia de la amantadina y de la rimantadina se descubrió antes de conocer su mecanismo de acción. 
Ahora se sabe que interfieren la actividad de M2, proteína vírica que funciona como un canal iónico.
La inhibición de M2 explica por qué los fármacos no ejercen efecto sobre el tipo B de la gripe:
estos virus carecen de moléculas M2.
A la par que se avanza en la investigación de nuevos fármacos, se intenta mejorar las vacunas contra la gripe.

Entre una pandemia y otra menudean epidemias circunscritas importantes.
En 1994, según el Centro de Control y Prevención de Enfermedades de los EE.UU., se estimó que 90 millones de norteamericanos, alrededor del 35% de la población, contrajo la gripe.
En conjunto, estas personas pasaron 170 millones de días en cama y perdieron 69,3 millones de días de trabajo.
Sólo en ese país la gripe afecta de un 10 a un 20% de la población cada año y causa alrededor de 20.000 muertes por complicaciones relacionadas con la gripe.

Estas cifras, como las asociadas con las pandemias, es probable que disminuyan en los próximos años cuando se disponga de los nuevos fármacos antigripales y de vacunas que tengan una mayor difusión y se fabriquen con mayor rapidez.
La sociedad puede acercarse a una era en que la especie humana le gane la partida al temido virus de la gripe. 


Creación artificial de poros en las células 

Con la técnica del ADN recombinante podemos forjar poros artificiales en las membranas celulares al objeto de introducir fármacos o instalar biosensores para descubrir sustancias tóxicas.
La membrana externa de la célula no se limita a enclaustrar su contenido variopinto.
Actúa, y de manera muy eficaz, como guardián exigente de cuanto entra y sale.
Franquea el paso a los nutrientes y otros compuestos necesarios; las moléculas que no se precisan, permanecen en el exterior.
Da salida de la célula a los productos de desecho.
Pero en la lucha sin tregua entre las especies, las bacterias patógenas han adquirido la capacidad de infiltrarse en las defensas celulares y desbaratar el equilibrio de entradas y salidas.

Las armas utilizadas en esa batalla son ciertas proteínas liberadas por el patógeno para taladrar la membrana celular y provocar la entrada en tromba de materiales extraños hacia el interior de la célula y la salida de algunos de sus integrantes.
El hombre se protege de muchos de estos ataques con un artillería similar: el sistema inmunitario aprovecha las propiedades de sus proteínas formadoras de poros para destruir células foráneas.

En mi laboratorio investigamos los mecanismos fundamentales del comportamiento de estas proteínas y buscamos, además, posibles aplicaciones en biotecnología y medicina.
Vamos tras la idea de instalar porteros moleculares con unas hechuras cortadas a propósito, pensando en bombardear células cancerosas con proteínas que alteren sus membranas externas y, de ese modo, hacerlas más sensibles a la quimioterapia.
Podrían incluso crearse membranas sintéticas con poros artificiales incrustados que actuaran como biosensores o facilitasen la administración de fármacos. 

Las proteínas bacterianas formadoras de poros presentan múltiples propiedades.

La alfa-hemolisina, en la que me centraré, es una proteína segregada por la bacteria Staphylococcus aureus , causante de infecciones estafilocócicas. 
La proteína asalta sus células diana mediante la formación de poros de un diámetro aproximado de dos nanómetros, abertura suficiente para el tránsito de la sacarosa, pero demasiado angosta para moléculas mayores, proteínas incluidas.
Por contra, la estreptolisina-O de la bacteria Streptococcus forma poros de más de 30 nanómetros de diámetro.
Lo mismo que la alfa-hemolisina, la proteína estreptolisina-O produce la lesión, si no la muerte, de la célula a consecuencia de tales perforaciones.

La proteína de la capa S proporciona a la bacteria una envoltura protectora;
forma hojas planas con numerosas aberturas de un tamaño más o menos homogéneo, entre dos y seis nanómetros de diámetro, según el tipo de bacteria.
Los poros de estas hojas permiten el paso de determinados nutrientes hacia el interior de la célula. 

Diversas razones nos han movido a escoger la alfa-hemolisina. 
Los cultivos bacterianos producen la proteína en abundancia (hasta unos cuantos gramos si es necesario).
Alcanza un tamaño bastante pequeño, de sólo 293 aminoácidos presentes en su estructura, lo que facilita su manipulación y alteración mediante la técnica del ADN recombinante.
Además, comparada con otras proteínas, se trata de una molécula que es muy estable.
En 1984 Gary S. Gray, a la sazón en Biogen, y Michael Kehoe, en la Universidad de Ginebra por entonces, identificaron la estructura del gen que codifica la proteína.
Hace ahora un año, J. Eric Gouaux determinó con su equipo la estructura tridimensional del poro de la alfa-hemolisina: 
constituye un complejo fungiforme integrado por siete moléculas de alfa-hemolisina.

Las investigaciones del grupo que encabeza Sucharit Bhakdi en la Universidad de Maguncia sugieren una doble vía de asociación de la alfa-hemolisina para formar el poro. 
Determinadas células, así los hematíes del conejo, poseen en su superficie receptores especiales que traban la alfa-hemolisina;
estos receptores pueden instar la formación del poro o contribuir a la orientación correcta de la estructura de la hemolisina sobre la membrana celular.
Pero incluso en ausencia de este tipo de receptores las moléculas de alfa-hemolisina pueden realizar su función. 
Sobre membranas artificiales (constituidas por una doble capa de lípidos), las proteínas se organizan por sí solas en poros.
Las proteínas también se ensamblan espontáneamente cuando se mezclan con ciertos compuestos relacionados con los detergentes habituales.
El autoensamblaje ofreces ventajas reales a los biotecnólogos que buscan la síntesis de moléculas basándose en la estructura de la alfa-hemolisina, porque ya el proceso de producción se encargará de resolver otros problemas. 

Trabajos recientes sobre la alfa-hemolisina realizados en mi laboratorio y en los de Bhakdi y Gouaux han contribuido a esclarecer diversos aspectos del proceso de formación del poro.
Una vez que las moléculas de alfa-hemolisina se unen a la membrana celular o una membrana lipídica artificial, se congregan en grupos de siete y crean un complejo preporo.
El centro de cada monómero consta de una larga ristra de 40 aminoácidos.
Esta hilera penetra en la membrana celular y allí forma parte del revestimiento del canal.

Mi objetivo inicial se ceñía al empleo de técnicas de ingeniería de proteínas para manipular tres propiedades esenciales del poro de la alfa-hemolisina: el tamaño de la abertura, la selectividad del canal en el tráfico de moléculas y la capacidad del poro para abrirse y cerrarse.

En circunstancias normales, el poro de la alfa-hemolisina permanece abierto.
Se advierte que presenta una ligera preferencia, no excesiva, por franquear el paso de moléculas dotadas de carga negativa sobre las moléculas neutras o dotadas de carga positiva.
Para el propósito de mi trabajo, la abertura resultaba la deseada;
otro canal que fuera mayor o menor costaría mucho remodelarlo. 

Entendí, pues, que el poro de alfa-hemolisina constituía un excelente punto de partida para ensayar modificaciones mediante ingeniería de proteínas.

Cuando inicié esa línea de trabajo, no sabía bien cómo abrir y cerrar el canal.

Sin duda, la tarea más difícil de los tres objetivos propuestos.
Con el tiempo, logramos en mi laboratorio colocar gatillos e interruptores moleculares en la proteína que hacen que el poro se abra o se cierre a voluntad.
Nos hemos centrado en este aspecto de la investigación porque encierra notables implicaciones prácticas.

Con técnicas de ingeniería de proteínas sustituimos aminoácidos de una proteína por otros aminoácidos naturales o incluso no naturales sintetizados en mi laboratorio.
Gracias a ello podemos colocar gatillos e interruptores potenciales en la alfa-hemolisina y explorar cuáles actúan.

En principio, los interruptores moleculares pueden ser bioquímicos (activados por enzimas) o químicos (activados al enlazarse moléculas pequeñas a la proteína).
La activación por un estímulo físico, como el calor o la luz, es otra posibilidad.
Los tres enfoques sirven en el caso de la alfa-hemolisina.

Con la ayuda de Barbara J. Walker, de la Fundación Worcester de Biología Experimental en Shrewsbury, donde comencé mi investigación, introduje un gatillo bioquímico en la alfa-hemolisina, añadiendo una pequeña cadena polipeptídica -de 11 a 53 aminoácidos- a la cadena central de la proteína que habitualmente se abre paso a través de la membrana.
Este segmento adicional bloquea la apertura del poro;
el tratamiento ulterior de la proteína con una proteasa corta esta pieza adicional, permitiendo que prosiga la formación del poro. 

Podrían emplearse gatillos bioquímicos para preparar alfa-hemolisinas que penetraran sólo en células seleccionadas.
Por ejemplo, hemolisinas arregladas de suerte tal que se encaminaran a un tumor con la ayuda de fragmentos de anticuerpo (elegidos para reconocer células cancerosas) incorporados en la estructura de la proteína mediante ingeniería genética.
Cuando estas proteínas híbridas alcanzaran las células cancerosas, entonces las proteasas liberadas por las mismas células cancerosas podrían instar la formación de poros.
(Las células cancerosas metastásicas segregan proteasas que facilitan a las células cancerosas su huida del tumor primario y asentarse en nuevos lugares.)
Las hemolisinas transformadas comenzarían a horadar las células cancerosas, aumentando su permeabilidad y mostrarse con ello asequibles a fármacos citotóxicos. 
En esa línea de trabajo, Rekha G. Panchal consiguió, en mi laboratorio, alfa-hemolisinas mutantes que se activan por proteasas tumorales.

Con el tiempo, podría conseguirse la inserción de un interruptor bioquímico mediante el cual se abriese o cerrase la actividad formadora de poros, en lugar de que sucediera sólo lo primero.
Dan W. Urry, de la Universidad de Alabama en Birmingham, ha logrado no ya modificar la forma de una proteína sintética, sino devolverla después a la configuración original;
para ello se ha servido de enzimas y ha sacado partido de una reacción de fosforilación. 
Nosotros hemos comenzado a adaptar los métodos de Urry para crear interruptores bioquímicos que nos permitan abrir y cerrar el poro de alfa-hemolisina cuantas veces sea preciso. 

Hemos introducido un segundo tipo de gatillo en la estructura de la alfa- hemolisina.
Se activa con moléculas muy reactivas pequeñas.
Esta técnica podría adaptarse a la producción de membranas artificiales de empleo potencial en biosensores que detecten moléculas tóxicas, incluidos plaguicidas y gases neurotóxicos. 
En mi laboratorio se ha preparado una versión mutante de la alfa- hemolisina insertada en la superficie de un liposoma (una esférula microscópica constituida por una membrana formada por una bicapa lipídica) que se reorganiza para dar lugar a un poro, cuando un compuesto orgánico muy reactivo se une permanentemente a una región de la estructura obtenida por ingeniería genética.

La apertura del canal puede detectarse por la salida de un compuesto fluorescente que se ha colocado en el interior del liposoma en el momento de su preparación.

Hemos preparado, asimismo, un interruptor químico que se abre y cierra repetidamente mediante la unión de iones metálicos a la proteína.
Los interruptores de este tipo podrían servir para la toma continua de medidas con biosensores.
La sustitución de cinco aminoácidos diversos por cinco unidades del aminoácido histidina crea una zona donde el zinc y otros iones metálicos pueden trabarse a la proteína y, con ello, bloquear la formación del poro.
El ensamblaje del poro prosigue cuando se elimina el metal.
Una vez abierto el orificio, el ion puede volver a sellar el poro engarzándolo al canal central y obstruyendo el paso de otras moléculas.
El proceso de sellar y resellar puede repetirse numerosas veces.

Esta estructura H5 (conjunto de cinco histidinas), así como otras hemolisinas emparentadas, sintetizadas por Stephen Cheley en Worcester, podrían ser eficaces biosensores para el seguimiento de metales contaminantes.
Para la preparación de tales biosensores hemos contado con la colaboración de John J. Kasianowicz.
La Marina de los Estados Unidos se muestra, a su vez, muy interesada en la medición rápida y continuada de iones metálicos en el agua del mar;
le es vital disponer de sistemas capaces de detectar concentraciones muy bajas de metales vertidos por barcos enemigos.
Por eso el Departamento de Investigación Naval ha financiado este paso de nuestro trabajo.

Aprovechando los progresos experimentados por la ingeniería de proteínas y los nuevos datos sobre la estructura de la alfa-hemolisina, Orit Braha ha mejorado en mi laboratorio la molécula de H5 al crear poros en los que no todas las subunidades son idénticas.
En un ejemplo, seis de las unidades no se han alterado, pero hay una con un punto donde puede anclar un ion metálico.
Cuando se engarza el ion metálico, cambia una corriente eléctrica (enviada a través del poro por aplicación de un campo de potencial).
La fluctuación de esta corriente nos revela la concentración y la identidad del ion presente. 
Importa resaltar que la señal oscilante de un poro puede utilizarse para medir varios metales en un instante; 
con una batería de sensores se podría obtener información sobre mezclas complejas de sustancias.
Otra ventaja de estos sensores reside en su tamaño:
dado que basta la unión a un poro para obtener una lectura de corriente, estos sensores pueden alcanzar un tamaño mínimo.

Millares, y tal vez millones, de modificaciones diferentes podrían introducirse en las proteínas formadoras de poros, y generar así una batería extraordinaria de biosensores potenciales.
En la actualidad trabajamos por obtener sensores de sustancias no metálicas.
A este respecto, Kasianowicz acaba de demostrar la posibilidad de detectar hebras monocatenarias de ADN a su paso a través del poro.

Los hallazgos realizados con el complejo H5 ilustran de manera gráfica las interacciones entre biotecnología y ciencia básica.
La estructura proporcionó no sólo un componente prototípico para un sensor, sino que aportó además información sobre el funcionamiento de los poros:
el trabajo con H5 contribuyó a demostrar que la hebra central de la proteína natural tapiza el interior del poro.

Al pensar en el proyecto de alfa-hemolisinas que se activaran mediante un tercer tipo de interruptor, en que interviniera un estímulo físico, advertí la existencia de canales naturales de membranas que pueden activarse físicamente por impulsos mecánicos o voltajes aplicados a través de la membrana celular.
En la mayoría de las casos, sin embargo, la activación por la luz es una opción con un atractivo especial:
la luz no acostumbra entorpecer los procesos naturales y puede aplicarse con un control exquisito espacial y temporal.

Además, en mis tiempos de doctorando en Harvard, me entrené en la preparación de compuestos químicos fotosensibles para investigar la estructura de las proteínas de membrana.
No me sorprendió, pues, comprobar que, veinte años más tarde, la fotoquímica y las proteínas de membrana coincidían en mi investigación, aunque bajo un foco de atención distinto.

Nuestro enfoque se basaba en la estructura de compuestos de nitrobenzilo sensibles a la luz, que se utilizaron en síntesis orgánicas en los años sesenta en el laboratorio de Jack A. Barltrop, y adaptados posteriormente para pequeñas moléculas biológicas por Jack H. Kaplan.
Nosotros obtuvimos un derivado nitrobenzilado, el ácido bromonitrofenilacético (BNPA), que después de unirse a la proteína cierra el proceso de formación de poros.

Para conseguirlo, técnicos de mi laboratorio introdujeron una cisteína en un lugar clave de la proteína. 
Cuando ese aminoácido reacciona con el BNPA, se inactiva la proteína.
Para devolverle la operatividad, se la expone a luz ultravioleta, a longitudes de onda escasamente nocivas para la mayoría de las células.
Con el tiempo se podría sintetizar moléculas formadoras de poros que se activen con luz de una determinada longitud de onda y se desactiven con otra longitud de onda diferente.
Podría también construirse interruptores híbridos en los que una proteína se activara con la luz y desactivara con iones metálicos.

La técnica presente, sin embargo, tiene aplicaciones inmediatas en el laboratorio.
A menudo se requiere tornar porosas determinadas células de un tejido, pero sin alterar las demás.
Podemos entonces introducir pequeñas moléculas que, a modo de sondas, nos revelen la actividad celular, mientras las proteínas clave permanecen en el interior.
En mi laboratorio, hemos conseguido hacer permeable una neurona en el seno de un conjunto de ellas mediante la simple iluminación de dicha célula.
En la neurona seleccionada penetraron hemolisinas modificadas por métodos ingenieriles y expuestas a la luz;
las neuronas restantes se mantuvieron indemnes.
Con la ayuda de instrumentos ópticos avanzados, podremos permeabilizar zonas acotadas de una neurona iluminando el área de interés.

En el campo de la administración de fármacos se encuentra una de las aplicaciones más apasionantes de las proteínas formadoras de poros.
Hablamos de introducir medicamentos en el interior de liposomas y, con la ayuda de alguna de las técnicas mencionadas, liberarlos en el momento deseado a través de los poros artificiales implantados en la membrana.
Hay, además, un enorme interés en encapsular enzimas e incluso células enteras, protegiéndolas del ataque del sistema inmunitario. 
Una vez transportadas las enzimas hasta el punto de destino, podrían activarse y destruir sustancias tóxicas, que se acumulan en pacientes con ciertas afecciones genéticas, como la fenilcetonuria, en que el organismo no puede metabolizar adecuadamente el aminoácido fenilalanina.
En esta enfermedad terminan por resentirse las células nerviosas. 

Podrían utilizarse células encapsuladas para la administración de hormonas en sujetos menesterosos de las mismas; por ejemplo, en la diabetes dependiente de insulina. 
Fármacos, enzimas y células dentro de membranas con poros artificiales proporcionarían un nivel de control sobre dónde, cuándo y cuánto de un determinado medicamento es preciso administrar.

En el nuestro y en otros laboratorios se trabaja en más proteínas bacterianas formadoras de poros con propiedades diferentes; entre ellas, la estreptolisina-O y las proteínas de la capa.
El grupo que preside S. Uwe Sleytr, de la Universidad de Ciencias Agrícolas de Viena, ha adaptado ya láminas de poros de proteínas de la capa S para su uso en instrumentos de filtración.
Ken Douglas y Noel A. Clark, de la Universidad de Colorado en Boulder, y Kenneth J. Rothschild, de la de Boston, han utilizado estas hojas como gradilla para formar rejillas de dimensiones nanométricas por deposición de vapores metálicos.

En mi laboratorio nos proponemos construir gatillos e interruptores en poros proyectados a voluntad -polipéptidos lejanamente basados en estructuras naturales- avanzando en el surco roturado por Maurice Montal, de la Universidad de California en San Diego. 
A mayor abundamiento, las proteínas modeladas y basadas en la estructura de la alfa-hemolisina podrían convertirse en agentes antimicrobianos si se lograra disgregar de manera selectiva la membrana externa de las bacterias.

Es mucho lo que aún queda por hacer mediante la ingeniería de proteínas en la propia alfa-hemolisina. 
Nosotros mismos estamos investigando técnicas con las que mejorar la estabilidad mecánica y térmica de los poros de hemolisina usados en biosensores.
Otros indagan modificaciones químicas para reducir la inmunogenicidad de la alfa-hemolisina -su tendencia a desencadenar un ataque por parte del sistema inmunitario- para aplicaciones bioterapéuticas; 
la inmunogenicidad es por el momento el mayor obstáculo con el que se topa la bioterapéutica.

Si se superan esos retos, vendrán días de gloria para esta técnica.
Las proteínas formadoras de poros podrían utilizarse como componentes de instrumentos electrónicos moleculares.
Aunque las proteínas son grandes para tal menester de acuerdo con los patrones actuales, poseen propiedades refinadísimas, como la capacidad de reconocer otras moléculas, algo que no podría inducirse en materiales inorgánicos. 
Los cristales bidimensionales que se forman con la alfa- hemolisina y las proteínas de la capa S podrían servir de plantillas para la síntesis ordenada de conjuntos de moléculas en la nueva ciencia de la nanotécnica que ha echado a andar.
Podríamos pensar incluso en crear membranas que, al permitir el paso selectivo de determinadas moléculas, se emplearan en la elaboración de filtros que permitirían purificar fármacos, agua contaminada o sangre.
En el campo de las proteínas formadoras de poros, ahora sólo en sus comienzos, se vislumbra un futuro muy prometedor.


Vacunas genéticas

Las vacunas fabricados con productos genéticos puede que consigan prevenir el sida, el paludismo y otras terribles infecciones insensibles a las técnicas de inmunización actuales.

Cabe afirmar que las vacunas constituyen el mayor logro de la medicina moderna.
Han permitido erradicar la viruela, han colocado a la poliomielitis al borde de la extinción y han protegido a un sinnúmero de personas del tifus, el tétanos, el sarampión, la hepatitis A, la hepatitis B, el rotavirus y otras infecciones peligrosas.
Pero hay muchas enfermedades mortales o muy graves, entre las que se cuentan el paludismo, el sida, los virus herpes y la hepatitis C, para las que no existen vacunas eficaces.
Y esto se debe a que los métodos de inmunización habituales funcionan mal o comportan riesgos inaceptables cuando se dirigen contra ciertas enfermedades.

Es indudable que se necesitan otros enfoques.
Uno de los más prometedores es la creación de vacunas con material genético, ya sea ADN o ARN.
Durante los últimos diez años se ha producido un cambio de actitud respecto a ellas, pasándose de vilipendiarlas a estudiarlas con intensidad en los ámbitos académicos e industriales, sometiéndolas también a las primeras fases de los ensayos clínicos.

Cuando se comprenden las acciones de las vacunas tradicionales, se ven con más claridad los méritos de la inmunización genética.
Consisten fundamentalmente las primeras en una versión muerta o debilitada de un patógeno (agente que produce la enfermedad) o en algún fragmento (subunidad) suyo.
Al igual que en la mayoría de las vacunas genéticas en estudio, el propósito de las vacunas habituales consiste en preparar el sistema inmunitario para que rechace rápidamente los virus, las bacterias y los parásitos peligrosos antes de que logren establecerse en el organismo.
Consiguen este efecto engañando al sistema inmunitario para que se comporte como si el organismo estuviera siendo ya acosado por un microorganismo que se multiplicara sin freno y produjera grandes daños a los tejidos.

Cuando responde a una infección real, el sistema inmunitario se guía por los antígenos foráneos. 
Los antígenos son sustancias producidas en exclusiva por el agente causante y no por el huésped; normalmente son proteínas o fragmentos proteicas.
Dos ramas principales pueden entrar en juego y las dos reciben ayuda crucial de los leucocitos sanguíneos conocidos como linfocitos T auxiliares.
La rama humoral, dirigida por los linfocitos B, actúa sobre microorganismos patógenos que están fuera de las células.
Esas células B secretan moléculas de anticuerpo que se adhieren a los agentes infecciosos y con ello los neutralizan o los marcan para que sean destruidos por otras partes del sistema inmunitario.
La rama celular, encabezada por los linfocitos T citotóxicos (asesinos), erradica los patógenos que colonizan las células. 
Las células infectadas despliegan fragmentos de las proteínas de sus atacantes sobre su superficie de una manera particular.
Cuando los linfocitos T citotóxicos "ven" esas señales sobre las células, suelen destruirlas y con ellas los infiltrados de su interior.

Además de eliminar a los invasores, la activación del sistema inmunitario contra un patógeno específico induce la creación de células de memoria que pueden repeler a los mismos patógenos en el futuro.
Las vacunas confieren protección del mismo modo, induciendo respuestas inmunitarias y la consiguiente formación de células de memoria.

Pero las vacunas habituales varían en cuanto a la clase y a la duración de la seguridad que proporcionan. 
Las basadas en patógenos muertos (como las de la hepatitis A y la vacuna de la poliomielitis inyectada, o Salk) y en antígenos aislados de los agentes productores de la enfermedad (como la vacuna contra la hepatitis B, que consiste en una subunidad del virus) no pueden abrirse camino al interior de las células.
El resultado es que originan respuestas fundamentalmente humorales, pero no activan las células T asesinas.
Dichas respuestas son ineficaces contra muchos microorganismos que se infiltran en las células.
Además son transitorias:
aunque detengan la enfermedad, la protección desaparece después de un tiempo, por lo que hay que repetirlas periódicamente.

Las vacunas vivas atenuadas, que suelen ser de virus, entran en las células y fabrican antígenos que son desplegados por ellas.
Así estimulan el ataque no sólo de los anticuerpos, sino también de los linfocitos T asesinos. 
Esa doble actividad es esencial para bloquear la infección causada por muchos virus y para asegurar inmunidad cuando se duda de que la respuesta humoral sea suficiente por sí sola.
Las vacunas vivas (como las del sarampión, las paperas, la rubéola, la poliomielitis por vía oral &irsqb;Sabin&iilsqb; y la viruela) suelen conferir además inmunidad de por vida, considerándose por todo ello como el "patrón de oro" de las vacunas actuales.

Esto no quiere decir que no cuenten con sus propios problemas. 
No sólo pueden fallar en algunos casos, sino que, cuando funcionan, pueden inducir el desarrollo completo de la enfermedad en personas cuyo sistema inmunitario no esté en perfectas condiciones, como les sucede a los pacientes cancerosos sometidos a quimioterapia, a los que tienen sida y a los ancianos. 
Todas estas personas pueden contraer también la enfermedad por contagio de personas sanas que hayan sido inoculadas recientemente.
Por si fuera poco, los virus debilitados experimentan a veces mutaciones que restauran su virulencia como ha ocurrido en algunos monos a los que se administró una forma simia atenuada del VIH, el virus que causa el sida.
Hay enfermedades en las que los riesgos de inversión de la virulencia son intolerables.

Las vacunas de microorganismos completos, ya sean vivos o muertos tienen también inconvenientes.
Al estar compuestas por patógenos completos, conservan moléculas que no intervienen en la provocación de inmunidad protectora. 
También pueden incluir contaminantes que sean productos secundarios inevitables del proceso de fabricación, sustancias extrañas que a veces originan reacciones alérgicas o peligrosas.

La estructura de las vacunas genéticas es bastante diferente de la de las tradicionales.
Las más estudiadas consisten en plásmidos (pequeños anillos de ADN de doble hélice derivados originalmente de las bacterias, pero totalmente incapaces de producir una infección). 
Los plásmidos utilizados para inmunización han sido alterados con objeto de que transporten genes específicos de una o más proteínas antigénicas normalmente sintetizadas por un patógeno seleccionado, al tiempo que se excluyen los genes que permitirían que el patógeno se reconstituyera y causase la enfermedad.

Suelen administrarse mediante inyección o utilizando un dispositivo denominado pistola génica.
La inyección, que normalmente es en un músculo, introduce los genes directamente en algunas células e induce su captación por parte de las células situadas en los alrededores de la aguja insertada.
La pistola génica impulsa los plásmidos al interior de las células que están cerca de la superficie del organismo, que suelen ser las de la piel y las de las mucosas.
Una vez en el interior de las células, alguno de los plásmidos recombinantes se abre camino hasta el núcleo y da instrucciones a la célula para que sintetice las proteínas antigénicas codificadas.
Esas proteínas pueden desencadenar la inmunidad humoral (de tipo anticuerpos), cuando escapan de las células, y la inmunidad celular (células asesinas), cuando son descompuestas y desplegadas de la manera adecuada sobre la superficie celular (exactamente lo mismo que ocurre cuando las células albergan un patógeno activo). 

Estas peculiaridades suscitan la esperanza de que, una vez perfeccionadas para su uso en los seres humanos, las vacunas de ADN conserven todos los aspectos positivos de las vacunas tradicionales, a la vez que eviten sus riesgos.
Además de activar las dos ramas del sistema inmunológico, serán incapaces de causar infección, porque carecerán de los genes necesarios para la replicación de un microorganismo patógeno.
Presentan las ventajas adicionales de que son fáciles de diseñar y que pueden producirse en grandes cantidades utilizando las técnicas de ADN recombinante, ahora tan comunes, siendo igual de estables o más que las otras vacunas en lo que a almacenamiento se refiere.
Su fabricación y su amplia distribución deberán ser, pues, relativamente baratas.
Dado que pueden componerse de manera que transporten genes de varias cepas de un microorganismo patógeno, cuentan con la posibilidad de proporcionar inmunidad simultánea contra todas ellas, algo que será muy útil cuando se trate de microorganismos muy variables, como sucede con los virus de la gripe y del VIH.

Se están haciendo pruebas con vacunas compuestas de ARN, un pariente de cadena sencilla del ADN.
El ARN introducido en las células induce fácilmente la síntesis de las proteínas codificadas. pero es menos estable que el ADN, lo que puede ocasionar problemas a la hora de la fabricación y de la distribución de la vacuna.
Estas dificultades son probablemente superables. 
Pero como las vacunas de ARN se han estudiado con mucha menos amplitud que las de ADN, nos concentraremos en estas últimas. 

La idea de que los genes pudiesen 5, actuar como vacunas surgió en parte de investigaciones iniciadas hace casi medio siglo.
Experimentos que no tenían nada que ver con vacunas demostraron en los años cincuenta y sesenta que la liberación de material genético dentro las células de un animal podía desencadenar cierta síntesis de las proteínas codificadas, así como de anticuerpos dirigidos específicamente contra esas proteínas.
Los investigadores utilizaron a veces esta capacidad de fabricación de anticuerpos como una forma fácil de demostrar la generación de determinada proteína por un gen dado.

La posibilidad de que los genes insertados indujeran una respuesta inmunitaria atrajo luego la atención de otros investigadores, esta vez como consecuencia de un fenómeno decepcionante.
Quienes intentaban desarrollar la terapia génica (liberación de genes para corregir trastornos hereditarios y de otro tipo) observaron que, a veces, las proteínas sintetizadas a partir de los genes terapéuticos eran destruidas en el cuerpo de los animales que los recibían.
La razón era que se producía una reacción inmunológica a proteínas no familiares.

Algunos laboratorios habían empezado a explorar a principios de los noventa la posibilidad de utilizar esta respuesta inmunitaria no deseada para un buen fin, la vacunación. 
Los escépticos fueron muchos al principio; se dudaba, por ejemplo, que la inmunidad desencadenada fuera lo bastante fuerte como para proteger a un ser humano de la infección causada por un patógeno vivo.

Pero un conjunto de estudios animales realizados por grupos independientes demostró con rotundidad en 1992 que la idea era sólida.
Esos grupos estuvieron dirigidos por Stephen A. Johnston, por Philip Felgner y Margaret Liu, por Harriet L. Robinson y por Weiner, uno de los autores.

Todos ellos y muchos otros realizados en los años siguientes revelaron que las vacunas de ADN liberadas en las células podían estimular el sistema inmunitario de los roedores y de los primates para que generase respuestas de células B, de células T citotóxicas y de células T auxiliares contra muchos patógenos diferentes e incluso contra ciertos tipos de cáncer.
La investigación demostró también que podían desencadenarse respuestas inmunitarias y de protección contra las enfermedades utilizando diferentes vías de administración.
El efecto podía reforzarse además por diversos métodos que facilitaban la captación de ADN por las células.

Muchos otros investigadores han vuelto su atención a las vacunas de ADN desde entonces y la técnica ha avanzado hasta el primer escalón de los ensayos humanos, que se centran en la seguridad.
El primero empezó en 1995, cuando se liberaron plásmidos que contenían genes del VIH en pacientes ya infectados por ese virus.
Los ensayos de mayor envergadura se iniciaron en 1996 y marcaron un hito histórico por otra razón:
era la primera vez que los médicos introducían genes nuevos (que contenían información de las proteínas del VIH o de la gripe) en personas sanas, en vez de hacerlo en personas enfermas.

Los ensayos clínicos actuales se refieren a vacunas diseñadas para prevenir diversas infecciones (por el VIH, el herpes, la gripe, la hepatitis B y Plasmodium , el parásito responsable del paludismo), para reforzar la inmunidad deteriorada de los pacientes ya infectados por el VIH y para tratar una serie de cánceres (entre ellos los Infamas y las neoplasias malignas de la próstata y del colon).
Aunque el cáncer no sea una enfermedad infecciosa, hay muchos datos que indican que el hacer trabajar a las defensas inmunitarias del organismo puede contribuir a combatirlo.

En los ensayos de seguridad se plantean preguntas del tipo de
¿son tóxicos los plásmidos?
El ADN administrado en forma de fármaco,
¿incitará una respuesta inmunitaria contra el ADN propio del organismo? 
Hasta la fecha no se han descubierto efectos secundarios graves, lo que resulta bastante alentador.

Este tipo de pruebas no valora el grado de prevención ni la mejoría de la enfermedad; pero en muchas de ellas se controlan los efectos de las vacunas sobre el sistema inmunológico. 
Según los datos preliminares, parece que podrán conseguirse respuestas inmunológicas satisfactorias.
Cabe destacar que las vacunas del VIH han generado respuestas humorales y celulares; los plásmidos portadores de antígenos de Plasmodium han evocado respuestas inmunológicas celulares significativas; y una vacuna contra la hepatitis B ha provocado niveles de anticuerpos que deben ser lo bastante altos como para evitar la infección. 
Pero es probable que los enfoques genéticos tengan que combinarse a veces con estimuladores inmunológicos generalizados (coadyuvantes) para desencadenar las fuertes respuestas inmunitarias necesarias para proteger a los receptores frente a infecciones futuras, como sucede en el caso de las vacunas tradicionales.

A la vez que progresan los ensayos clínicos, en los laboratorios se intenta conocer más a fondo la forma exacta en que la inmunización génica estimula la inmunidad, en especial por parte de la rama celular del sistema de defensa, que suele resultar decisiva.
Su comprensión detallada ofrecerá pistas para mejorar la eficacia.

Los inmunólogos se han enfrentado durante muchos años a una paradoja.
Por un lado, era obvio que las vacunas de ADN activaban las células T asesinas.
Pero la mera colocación de ADN en las células cutáneas o musculares y el hecho de inducirlas a que expusieran en sus superficies fragmentos de los antígenos codificados no deberían haber producido ese resultado.
Antes de que el despliegue de esos antígenos pueda activar las células T citotóxicas, las células asesinas tienen que ser cebadas, 0 puestas en marcha, lo que se realiza en parte por la influencia específica de las denominadas celarlas presentadoras de antígeno "profesionales".
Las células T tienen que unirse concretamente a los mis mas fragmentos antigénicos que detectarán en las células inoculadas y no inmunizadas (como serían las células musculares) y, a la vez, a una segunda molécula coestimuladora (o "segunda señal"), que ordinariamente no se encuentra más que en las células presentadoras de antígenos.

Se pensó durante un tiempo que no había manera de que las vacunas de ADN entraran en las células presentadoras de antígeno y, por tanto, de que sintetizaran y exhibieran los antígenos codificados por dichas vacunas.
Varios descubrimientos recientes han demostrado, sin embargo, que tal opinión era equivocada.
Algunos de los plásmidos entran de hecho en las células presentadoras de antígeno profesionales, las cuales exponen a continuación los antígenos junto con las moléculas coestimuladoras cruciales y ayudan a preparar las células T para la acción ( véase el recuadro ).
De acuerdo con ello, para que induzcan una respuesta inmunitaria celular potente hay que administrar las vacunas de ADN de tal manera que permitan una buena captación por parte de las células presentadoras de antígenos y no sólo por los otros tipos celulares. 

Por diferentes vías de investigación se ha llegado a la conclusión de que el ADN plasmídico que rodea a los genes antigénicos consiste en algo más que un mero vehículo liberador de genes:
potencia la respuesta inmunitaria evocada por los antígenos.
Este efecto procede, según parece, de la elevada frecuencia de las secuencias GC de los plásmidos.
Cada hebra de la doble hélice de ADN está constituida por unidades denominadas nucleótidos que se distinguen por las bases que contienen:
adenina (A), citosina (C), guanina (G) o timina (T).
El ADN del plásmido, que procede de bacterias, tiene una mayor frecuencia de secuencias CG que el ADN de los vertebrados.
Las unidades CG de los plásmidos bacterianos no suelen estar además metiladas, mientras que las de los vertebrados están generalmente ligadas a grupos metilo.

Se ha propuesto que el organismo de los vertebrados interpreta la aparición de abundantes pares CG no metilados como una señal de peligro.
La respuesta consiste en que una parte relativamente primitiva del sistema inmunitario (la que no depende del reconocimiento de antígenos) intenta destruir al intruso o al menos aislarlo del organismo.

Además de analizar el comportamiento natural de las vacunas genéticas en el organismo, los inmunólogos miran al futuro y exploran ideas para incrementar la reactividad inmunológica general y mejorar la proporción de respuesta celular con respecto a la humoral.
Del estudio del ADN situado en torno a las secuencias CG ha surgido una propuesta para amplificar la capacidad de respuesta.
Se ha demostrado que el ADN plasmídico produce la respuesta inmunológica más potente cuando las secuencias CG están flanqueadas por dos purinas (adenina o guanina) en su lado "C" y dos pirimidinas (timina o citosina) en su lado "G".
Los plásmidos que contenían esas "secuencias inmunoestimuladoras" indujeron en los ratones a los que se aplicaron una actividad de anticuerpos y de células T citotóxicas más vigorosa que otra vacuna por lo demás idéntica.
El incremento del número de secuencias inmunoestimuladoras en los plásmidos bien podría amplificar la inmunogenicidad de los códigos antigénicos de una vacuna de ADN. 

Otra posibilidad consiste en la incorporación a los plásmidos portadores de antígenos o a otros de genes que codificaran para las moléculas de señalización denominadas citocinas.
Las células del sistema inmunitario liberan esas moléculas para regular sus propias actividades, así como la actividad de unas con respecto de otras.
A modo de ejemplo, una molécula denominada factor estimulante de las colonias de granulocitos-macrófagos estimula la proliferación de las células presentadoras de antígenos, entre otros efectos.
Se ha demostrado que la inclusión de sus genes estimula las respuestas generales a las vacunas de ADN.

Con objeto de asegurar que las vacunas genéticas desencadenen una respuesta celular cuando sea necesaria, se está experimentando específicamente con genes para citocinas de los cuales se sabe que promueven la actividad de las celarlas asesinas.
Se ha encontrado que las células T auxiliares, denominadas Th1, de los ratones secretan citocinas que favorecen las respuestas celulares a expensas de las humorales (anticuerpos), mientras que otras células auxiliares (las Th2 ) secretan citocinas que favorecen la inmunidad humoral.
En los seres humanos parecen intervenir más variedad de células T auxiliares, pero un predominio de citocinas de tipo Th1 sigue promoviendo una respuesta celular, mientras que la preponderancia de citocinas de tipo Th2 estimula la respuesta humoral.

Se ha demostrado que una vacuna que contenía genes para los antígenos del VIH y para la interleucina-12 (una citocina Th1 clásica) redujo la producción de anticuerpos anti-VIH en los ratones e intensificó notablemente la capacidad de respuesta de las células T citotóxicas a los antígenos del VIH.
Esta inclinación hacia la respuesta celular es particularmente alentadora, pues, según los descubrimientos recientes relativos al VIH, una potente respuesta de células T asesinas frente al VIH es de importancia fundamental para combatir su replicación. 

También podrían incorporarse genes para sustancias conocidas como quimiocinas.
Se trata de moléculas pequeñas que atraen a la vez a células presentadoras de antígenos y a células T hacia los tejidos lesionados o infectados.
Como las citocinas, las quimiocinas difieren en cuanto a la mezcla de células en las que actúan y a los efectos precisos que ejercen.
Dado que se entienden mejor sus acciones individuales, la combinación cautelosa de genes de quimiocinas específicas con genes de citocinas seleccionadas podría constituir un gran paso hacia la combinación a medida del tipo y el grado de las respuestas inmunológicas desencadenadas.

En teoría las vacunas de ADN podrían incluso evitar la necesidad de las clásicas células presentadoras de antígenos para cebar las células T citotóxicas. 
Si se reunieran un gen que especificara un antígeno con un gen para una molécula coestimuladora normalmente sintetizada por una célula presentadora de antígenos, las células cutáneas, las musculares o cualesquiera otras que fueran inoculadas desplegarían ellas mismas el antígeno y la "segunda señal" crucial, facilitando con ello a la vez el cebamiento y la activación de las células T citotóxicas.

Si las vacunas genéticas de primera generación funcionaran bien en los ensayos clínicos, quizá puedan combinarse inicialmente con las vacunas más tradicionales para conseguir aún mejores efectos.
Digamos, por ejemplo, que una vacuna subunitaria (que consiste en una proteína) evocara una buena respuesta de anticuerpos contra un patógeno, pero que se necesitara también una respuesta celular. 
Entretanto una nueva vacuna de ADN quizá demuestre ser capaz de inducir una respuesta celular, pero no de excitar una respuesta de anticuerpos ideal.
El método denominado cebamiento-estímulo permitiría a los médicos administrar la vacuna de ADN y luego estimular la respuesta de anticuerpos mediante la administración tardía de la vacuna subunitaria.
A medida que los fabricantes de vacunas aprendan a optimizar las respuestas a la inmunización genética (por ejemplo, utilizando las técnicas que se acaban de describir), quizá sea posible conseguir finalmente los efectos necesarios sólo con vacunas genéticas. 

Mientras se consideran las posibilidades excitantes y futuristas de la inmunización genética, quienes nos dedicamos al tema tenemos también que arremangarnos y ponernos manos a la obra para resolver una gran cantidad de detalles.
Por ejemplo, la mayoría de las vacunas de ADN deja de producir proteínas en abundancia aproximadamente después de un mes.
Si se encontrara una forma de prolongar la supervivencia del plásmido,
¿se induciría con ello una inmunidad más fuerte?
¿Saldría el tiro por la culata y se estimularían los ataques contra el tejido sano no vacunado?
¿Cuánto dura la inmunidad en los seres humanos?
¿En qué medida varían las respuestas de unas personas a otras? 
¿Qué dosis son más eficaces y qué tipos de regímenes de administración son los mejores? 
También necesitamos saber qué sustancias son más útiles para dirigir específicamente el material genético a células concretas (entre ellas, las células presentadoras de antígeno) y para intensificar la captación celular de plásmidos. 
¿Qué genes de un patógeno dado hay que seleccionar, a veces entre millares de ellos, para conseguir la potencia máxima?

Los ensayos clínicos que pretenden responder a estas preguntas y evaluar la eficacia de la primera generación de vacunas de ADN no finalizarán hasta dentro de cinco o diez años.
Lleguen o no al mercado esas versiones específicas, es probable que las técnicas desarrolladas para la inmunización genética resulten muy valiosas para investigar la biología básica de la respuesta inmunitaria y para diseñar vacunas aún mejores. 

Los investigadores que actualmente proyectan las vacunas suelen tener poca idea de qué componentes del sistema inmunitario tienen que ser activados con más intensidad contra un patógeno dado y qué antígenos y otras sustancias permitirán esa estimulación. 
Pero sí pueden mezclar fácilmente y emparejar los genes antigénicos y los de otras moléculas (como los que codifican las citocinas y las quimiocinas) en vacunas de ADN experimentales y comparar con bastante rapidez el éxito de las diferentes combinaciones en animales pequeños. 
Es así como podrán controlar a la vez las respuestas inmunitarias que son necesarias para la protección y los antígenos y demás proteínas que puedan generarlas.

Como resultado de estas pruebas se están creando "bibliotecas" de todos los genes de los microorganismos patógenos, estando cada gen inserto en su propio plásmido.
Subconjuntos de estas bibliotecas se administran luego a animales, que son expuestos también al patógeno vivo, identificándose así los subtipos que funcionan mejor.
Se subdividen los grupos y se vuelven a realizar pruebas, hasta obtener la mezcla de antígenos más útil.

La manipulabilidad inherente al ADN lo convertirá con el tiempo en el vehículo fundamental para desmenuzar las complejas respuestas inmunitarias del organismo a los diferentes agentes causantes de enfermedad.
Al disponer de esa información, quienes proyecten las vacunas podrían canalizar las respuestas inmunitarias por vías seleccionadas.
Hasta ahora no había forma de realizarlo de manera fácil y barata, pero es probable que vacunas de este tipo, "racionalmente" diseñadas, proporcionen en el futuro nuevas inmunoterapias para el cáncer, así como formas potentes de prevenir y de mitigar todas esas infecciones diabólicas que eluden los esfuerzos de la humanidad para evitarlas.

Cómo funcionan las vacunas 

Las vacunas de ADN provocan inmunidad protectora contra un agente infeccioso, o patógeno, fundamentalmente activando dos ramas del sistema inmunitario:
la rama humoral, que los ataca fuera de las células, y la rama celular, que elimina las células invadidas.
La inmunidad se consigue cuando esa actividad genera células de "memoria" duraderas, centinelas prestos a impedir la entrada del microorganismo patógeno.

Una descripción simplificada de la forma en que las vacunas inducen la inmunidad se inicia en el extremo izquierdo del diagrama, con la introducción de una vacuna de ADN en una célula elegido como blanco (por ejemplo, una célula muscular) y la producción subsiguiente de los antígenos que normalmente se encuentran en el microorganismo de que se trate.
La respuesta humoral ( secuencia recuadrada arriba ) consiste en que los leucocitos sanguíneos denominados células B captan copias de proteínas antigénicas que han sido liberadas, multiplicándose luego.
Muchas de las células de la descendencia secretan moléculas de anticuerpo que, cuando se produce la infección, se fijan al patógeno y lo marcan para su destrucción.
Otras se convierten en células de memoria que reprimen el patógeno que circule fuera de las células.

Mientras tanto, el despliegue de fragmentos proteicas, o péptidos, antigénicos sobre las células inoculadas (en el interior de los surcos de las moléculas de clase I del MHC) puede desencadenar una respuesta celular ( secuencia recuadrada de abajo ).
La unión a los complejos antigénicos induce la multiplicación de los leucocitos sanguíneos conocidos como células T citotóxicas (asesinas) y la destrucción de las células ligadas y de otras células que exhiban los mismos péptidos de la misma manera.
Algunas células activadas se convertirán también en células de memoria listas para eliminar células invadidas por el patógeno en el futuro.

Tienen que ocurrir diversas etapas preliminares antes de que esas respuestas puedan producirse realmente.
Con objeto de preparar el escenario para la activación de las células B ( recuadro superior ), las células presentadoras de antígeno 5'profesionales" (CPA) tienen que ingerir las moléculas de antígeno, cortarlas y mostrar los péptidos resultantes en las moléculas de clase 11 del complejo MHC.
Las células T auxiliares, a su vez, tienen que reconocer no sólo los complejos peptídicos, sino también las moléculas "coestimuladoras" que no se encuentran más que en las presentadoras profesionales de antígeno.
Si se producen estas etapas, las células auxiliares pueden secretar moléculas señaladoras conocidas como citocinas Th2 , que contribuyen a activar las células B ligadas a los antígenos.

En el cebamiento de las respuestas T citotóxicas participan también las CPA ( recuadro inferior ).
Antes de que las células citotóxicas puedan responder a los antígenos situados sobre las células inoculadas, las CPA tienen que captar los plásmidos de la vacuna, sintetizar los antígenos cifrados y exhibir fragmentos de estos antígenos en las moléculas MHC de clase I junto con moléculas coestimuladoras.
A continuación, las células T asesinas tienen que reconocer esas señales y ser además tocadas por las citocinas (esta vez del tipo Th1) de las células T auxiliares.
Las vacunas de ADN producen además las células T auxiliares de memoria necesarias para apoyar las actividades defensivas de otras células de memoria, lo que no queda reflejado en la ilustración.

Tabla

Ensayos humanos de las vacunas de ADN 

Objetivo de la vacuna.
Proteínas codificadas por los genes de la vacuna.
Resultados hasta la fecha.
Prevención de la hepatitis B.
Antígeno de superficie de la hepatitis B .
Respuestas humoral y celular.


Prevención del herpes simple.
Glucoproteína del herpes.
Análisis inmunológico en marcha.


Prevención del VIH.
Proteínas reguladoras y de la cubierta; proteínas y enzimas nucleares que intervienen en la replicación del VIH.
Respuestas celulares (en última instancia se probarán probablemente todos los genes en una sola vacuna).


Prevención de la gripe .
Hemaglutinina.
Análisis inmunológico en marcha (ensayo concluido).


Prevención del paludismo.
Proteína del circunsporozoito .
Respuestas celulares.


Tratamiento del VIH.
Proteínas reguladoras y de la cubierta; Proteínas tat, nef y reguladoras.
Respuestas humorales en el primer ensayo de la lista (que ha acabado); respuestas celulares en otro ensayo.


Tratamiento del VIH.
Proteínas y enzimas de la cubierta, reguladoras y nucleares que intervienen en la replicación del VIH.
La vacuna se combinó con tratamiento farmacológico agresivo (HMRT); análisis inmunológico en marcha .


Tratamiento de los adenocarcinomas de mama y de colon.
Antígeno carcinoembrionario (ACE).
Respuestas: celulares.


Tratamiento de linfomas de células B.
Inmunoglobulina .
Respuestas humorales.


Tratamiento de linfoma cutáneo de células T (LCT).
Receptor de células T.
Análisis inmunológicos en marcha (el ensayo ha acabado).


Tratamiento del cáncer de próstata.
Antígeno de membrana especifico de la próstata.
Análisis inmunológicos en marcha.


En este cuadro se enumeran muchas de las pruebas sobre seres humanos conocidas por los autores.
Todas las vacunas candidatas están en las primeras fases de ensayos clínicos para examinar la seguridad y las respuestas inmunitarias.
Hasta ahora todas han sido bien toleradas.
No se han iniciado ensayos de eficacia para la prevención o el tratamiento de las enfermedades.
La mayor norte de los estudios están todavía sin terminar.

Figura 1

DISEÑO DE UNA VACUNA GENÉTICA 

Se precisa normalmente el aislamiento de uno o más genes del agente causante de la enfermedad (patógeno) y el empalme de esos genes a plásmidos (a), anillos cerrados de ADN.
Los anillos se liberan luego en el interior de grupos pequeños de células, a menudo mediante inyección muscular (b) o proyectándolos sobre la piel con la denominada pistola génica (c).
Los genes elegidos codifican para los antígenos (sustancias capaces de desencadenar una respuesta inmunitaria) que suelen ser sintetizados por el microorganismo patógeno.

Figura 2

LOS ANTIGENOS NECESARIOS PARA EVOCAR INMUNIDAD se producen después de que las vacunas de ADN penetren en el núcleo de una célula (l) 

Una vez allí, los genes del plásmido que codifican para el antígeno se copian en hebras móviles de ARN mensajero (2), que son traducidas posteriormente a proteínas antigénicas en el citoplasma (3 y 4).
Los antígenos devienen perceptibles para el sistema inmunitario de dos formas. 
Una consiste en qué abandonen directamente la célula (5).
La segunda es que sean cortados en fragmentos (6) y encajados en los surcos de las denominadas moléculas de clase I del MCH (complejo mayor de histocompatibilidad) (7), de manera muy parecida a como las piedras preciosas se engarzan en los " anillos, tras de lo cual quedan expuestos sobre la superficie celular (8).

Figura 3

ESTAS CELULAS MUSCULARES, identificadas por el marcador azul ( arriba ), se hicieron brillar en verde ( abajo ) después de ser inyectadas con una vacuna de ADN portador de genes del virus de la inmunodeficiencia humana ( VIH)

El color verde indica que las células fabricaban la proteína del VIH especificada por los genes víricos. 
Estos micrografías constituyen algunas de las pruebas de que las vacunas de ADN pueden generar las proteínas necesarias para evocar las respuestas inmunitarias.

Figura 4

CAPTACION de vacunas de ADN por las células presentadoras de antígeno, un acontecimiento crucial en la inducción de inmunidad que se ha comprobado repetidamente

Un método consiste en añadir dos clases de marcadores a las células de un fragmento de tejido expuesto a una vacuna de ADN.
Una etiqueta (rolo, a la izquierda) marcaba las células presentadoras de antígeno; otra (verde, en el centro) señalaba todas las células que sintetizaban un antígeno especificado por la vacuna.
Cuando se superpusieron las imágenes de las células, la aparición de un color naranja (derecha) significaba la presencia de células presentadoras de antígeno que habían captado la vacuna y producido la proteína codificada por ella.


Fármacos contra virus

En la lucha contra los virus disponemos ya de un buen armamentario de medicinas eficaces y de otras en fase de desarrollo.
La investigación en genomas víricos acelera el progreso.

A mediados de los años ochenta, cuando se cobró conciencia de que un virus cruel producía una nueva enfermedad, el sida, las estanterías de las farmacias estaban repletas de medicamentos capaces de enfrentarse a las infecciones bacterianas; 
para las enfermedades víricas, sólo había unas cuantas vacunas.
Pero las cosas han dado un vuelco radical.
Contamos con numerosos tratamientos antivíricos, incluidas varias vacunas nuevas;
muchos más se encuentran en vías de desarrollo.
Si medio siglo atrás los antibióticos vivieron su edad de oro, se abre ahora una etapa de esplendor para los antivíricos. 

Esta riqueza brota de varias fuentes.
Los laboratorios farmacéuticos destacarían la adquisición de técnicas refinadas para descubrir todo tipo de medicinas. 
Al propio tiempo, los esfuerzos empeñados en la búsqueda de terapias eficaces contra el sida han sugerido vías posibles de lucha no sólo contra el agente responsable de esta enfermedad, el VIH, sino contra otros virus también.

Ha entrado con fuerza una disciplina quizá desconocida por el público, pero muy valiosa:
la genómica vírica, que descifra la secuencia de letras , o ácidos nucleicos, que componen el texto genético de un virus, donde reside la clave para fabricar sus propias proteínas.
A su vez, estas proteínas sirven de elementos estructurales y constituyentes operativos del virus y controlan su proceder.
Con la secuencia completa del genoma, o incluso parcial del mismo, nos viene dada importante información sobre la infección vírica y sobre las etapas del proceso vulnerables para el ataque médico. 
El genoma completo de cualquier virus puede secuenciarse en días, lo que permite observar con una rapidez sin precedentes sus puntos débiles.

La mayoría de los antivíricos aprobados están dirigidos contra el VIH, los virus del herpes (responsables de patologías muy dispares, desde las molestias de un resfriado a una encefalitis) y los virus de las hepatitis B y C (ambos pueden causar cáncer de hígado).
El VIH y estas formas de hepatitis seguirán siendo, sin duda, objeto de atención preferente por parte de la investigación durante algún tiempo;
tomadas en su conjunto, estas enfermedades afectan a millones de personas en todo el mundo. 
Los biólogos, sin embargo, se afanan también en la lucha contra otras enfermedades víricas.
Aunque no voy a detenerme en todos los tipos de fármacos antivíricos, espero ofrecer una idea de los avances extraordinarios que debemos a la genómica y otras técnicas refinadas. 

Búsqueda de fármacos

Los primeros fármacos antivíricos (contra el herpes, sobre todo), aparecidos en los años sesenta, surgieron de los métodos tradicionales empleados para el descubrimiento de nuevos medicamentos.
Los virus, de estructura elemental, constan de genes y quizás algunas enzimas (catalizadores biológicos) encerrados en una cápside proteica, a veces rodeada por una envoltura lipídica. 
Semejante disposición requiere que los virus se repliquen en el interior de las células.
Por eso, los investigadores infectaban células, las cultivaban en un medio apropiado y exponían los cultivos a agentes químicos que inhibieran las actividades víricas conocidas entonces.
Aquellos agentes químicos que reducían la cantidad de virus en el cultivo se reservaban para ensayos ulteriores.
Pero ese enfoque experimental, que procedía por aciertos y fracasos, aportaba escasa información sobre otras actividades víricas dignas de ataque, con el bloqueo consiguiente de otros empeños por desarrollar fármacos más eficaces o menos lesivos en sus efectos secundarios.

Con la genómica se saltó esa barrera.
Convertida en trampolín para el descubrimiento de objetivos nuevos en que centrar el ataque, ha abierto el camino para el desarrollo de fármacos antivíricos muy diversos. 
Con su ayuda se ha identificado la mayoría de los objetivos seleccionados en los virus desde los años ochenta.
(El término no se acuñó hasta finales de ese decenio, bastante después de que se desarrollaran algunos de los fármacos antivíricos hoy día disponibles.)
Descifrada la secuencia del virus de interés, se coteja con las ya identificadas en otros organismos, incluidos otros virus, para así averiguar su segmentación en genes.
Las ristras de nucleótidos de la secuencia que se parezcan a genes conocidos en otros organismos constituirán probablemente genes en el virus y darán lugar a proteínas con estructuras similares.
Una vez localizados los genes de un virus, pueden abordarse las funciones de las proteínas correspondientes y construir así un mapa de los pasos moleculares a través de los cuales el virus de interés se asienta y prospera en el organismo.

En ese mapa pueden aparecer resaltadas las proteínas - y los dominios en esas proteínas - que sería bueno inutilizar.
En general, los investigadores se inclinan por los objetivos cuya alteración mine la actividad del virus.
También optan por centrar su atención sobre dominios proteicos con escaso parecido con los del hombre, y evitar así un posible daño en las células sanas y otros efectos secundarios intolerables.
No desdeñan los dominios proteicos que compartan básicamente todas las cepas importantes del virus, de suerte que el fármaco abarque, en su eficacia, el arco más amplio posible de las variantes víricas.

Supongamos que se ha identificado un objetivo vírico. 
Podemos entonces recurrir a varias técnicas para crear el fármaco correspondiente, capaz de anularlo.
Podemos apelar a la ingeniería genética estándar (introducida en los años setenta) para producir copias de una proteína seleccionada y usarla después en el desarrollo de un fármaco.
Se inserta el gen correspondiente en bacterias u otros tipos de células, que sintetizan copias innúmeras de la proteína codificada.
Las moléculas de proteína resultantes pueden constituir la base de pruebas de muestreo:
sólo las moléculas que se unan a ellas recibirán una atención ulterior.

Podríamos seguir otra vía, la de analizar la estructura tridimensional de un dominio proteico y diseñar fármacos que se acoplaran de manera precisa a esa región; 
por ejemplo, construir un compuesto que inhibiera el centro activo de una enzima crucial para la reproducción del virus.
Y cabe también combinar los métodos de muestreo antiguos con los más recientes basados en la estructura.

De la aplicación de técnicas avanzadas en la industria farmacéutica se han extraído ideas para desarmar al virus en cualquiera de las etapas de su ciclo biológico.
Aunque las estrategias reproductivas difieren de forma sutil de una especie vírica a otra, en general las etapas de la replicación de los virus comprenden la unión a las células del huésped, la liberación de los genes víricos en el interior de las células, la replicación de todos los genes víricos y sus proteínas (con la ayuda de la maquinaria celular para producir las propias proteínas), la agrupación de los componentes en hordas de partículas víricas y la migración de éstas para comenzar un nuevo ciclo en otras células.

El momento ideal para atentar contra el virus es al comienzo de la infección, antes de que haya tenido tiempo de diseminarse por el organismo y provocar los primeros síntomas.
Las vacunas actúan en ese instante preciso, al estimular el sistema inmunitario para que destruya el agente patógeno en cuanto se ha introducido en el organismo.
A lo largo de su historia, las vacunas han ofrecido ese incentivo al inocular una versión debilitada o muerta del agente infeccioso, incapaz de alcanzar un número suficiente de réplicas para causar la enfermedad.
En las vacunas subunitarias hallan éstas su opción alternativa más frecuente. 
Contienen fragmentos del patógeno.

Tales trozos, incapaces de producir la infección, pueden inducir una respuesta inmunitaria protectora si se seleccionan cuidadosamente.

Para la hepatitis B se consiguió una primera vacuna subunitaria tras aislar el virus del plasma sanguíneo de personas infectadas y purificar las proteínas deseadas. 
Hoy la vacuna subunitaria para la hepatitis B se obtiene por ingeniería genética.
Se utiliza el gen de una proteína específica de la hepatitis B para fabricar copias puras de la proteína.
Con la ayuda de la genómica se desarrollan ahora vacunas adicionales para el dengue, el herpes genital y la fiebre hemorrágica, a menudo mortal, causada por el virus de Ebola.

En curso de investigación se encuentran también vacunas para prevenir o tratar el sida.
Pero los genes de su agente, el VIH, mutan con suma rapidez y se diversifican en numerosas cepas víricas.
Una vacuna que induzca una reacción contra ciertas cepas podría carecer de efecto contra otras.
Al comparar los genomas de diversas cepas de VIH, podríamos hallar secuencias compartidas por la mayoría y aplicar éstas a la síntesis de fragmentos proteicos víricos puros, que, una vez obtenidos, podrían someterse a prueba y ver si inducen protección inmunitaria frente a las cepas existentes.
Podrían diseñarse, asimismo, vacunas contra ciertas regiones de cepas prominentes.

Impedir la entrada

El tratamiento adquiere particular interés cuando no existe la vacuna idónea.
Los antivíricos curan a algunos pacientes, si bien en su mayoría se limitan a reducir la gravedad o duración de la infección vírica.
Un grupo de estas terapias frena la actividad del virus al impedir la entrada en un determinado tipo celular. 

Bajo el término entrada se amparan varios pasos, que comienzan con el anclaje del virus en un sitio de enlace, o receptor, de la célula huésped y terminan con la degradación de la cápside proteica en el interior de la célula, donde se liberan los genes del virus.
La entrada de los virus que portan una envoltura complementaria requiere un paso más.
Antes de que se produzca la eliminación de la cápside, han de fundir la envoltura adicional con la membrana celular o con la membrana de la vesícula que lo introduzca en el interior de la célula.

Se trabaja en posibles fármacos inhibidores de la entrada del VIH en las células.
El examen atento de la interacción entre VIH y su huésped favorito (leucocitos de la sangre, las llamadas células T coadyuvantes) nos indica que el virus ancla las moléculas CD4 y CCR5 en dichas células.
Aunque el bloqueo de CD4 no ha conseguido evitar que el VIH penetre en las células, el bloqueo del CCR5 podría conseguirlo.

De los cuatro antigripales iniciales, la amantidina y la rimantidina interrumpen otras partes del proceso de entrada. 
Los farmacólogos hallaron los compuestos al realizar un muestreo de productos químicos con capacidad potencial para bloquear la replicación del virus;
más tarde se demostró que inhibirían la fusión y el desprendimiento de la cápside.
Con ayuda de la información genómica se buscan también inhibidores de la fusión para el virus respiratorio sincitial (agente de complicaciones pulmonares en niños prematuros), los de la hepatitis B y C, y VIH.

Muchos resfriados podrán pronto controlarse con otro bloqueador de la entrada, el pleconaril.
Las comparaciones genómicas y estructurales han demostrado que muchas cepas comparten cierto bolsillo de la superficie de los rinovirus (responsables de la mayoría de los resfriados).
El pleconaril se acopla a este bolsillo e inhibe la eliminación de la cápside vírica.
Por lo que parece, el fármaco en cuestión actúa además contra los enterovirus, que causan diarrea, meningitis, conjuntivitis y encefalitis. 

Copia y replicación

Numerosos antivíricos comercializados, y otros en estudio, operan en la fase ulterior al desprendimiento de la cápside, cuando el genoma vírico, sea de ADN o de ARN, se libera para copiar y dirigir la síntesis de proteínas víricas.
Varios de los agentes que inhiben la replicación del genoma remedan a nucleósidos o nucleótidos.
Las enzimas que copian el ADN o el ARN incorporan tales análogos en las hebras nacientes. 
Y son precisamente esas imitaciones las que impiden que las enzimas añadan nuevos bloques a la cadena;
se aborta así la replicación del virus.

El aciclovir, el primer fármaco antivírico de probada eficacia y escasa toxicidad, constituye un análogo nucleosídico.
Se descubrió tras un muestreo de compuestos seleccionados por su capacidad obstructiva de la replicación del virus del herpes simple.
Se prescribe para el herpes genital;
moléculas con él emparentadas se indican contra otras infecciones herpéticas, como el zoster causado por la varicela y la inflamación de la retina que produce el citomegalovirus.

El primer fármaco aprobado para su uso contra el VIH, la zidovudina (AZT), es otro análogo nucleosídico. 
Ideado como anticancerígeno, pronto se advirtió que obstruía la actividad de la transcriptasa inversa, una enzima que el VIH utiliza para copiar en ADN su genoma de ARN.
Si este paso se realiza con éxito, otras enzimas del VIH unen el ADN en los cromosomas de la célula invadida, y allí el ADN integrado dirigirá la reproducción del virus.

El AZT puede acarrear efectos secundarios graves; anemia, por ejemplo.

No obstante, la investigación sobre la secuencia del gen de la enzima de la transcriptasa inversa ha permitido sintetizar otros análogos nucleosídicos menos tóxicos. 
De éstos, la lamivudina ha recibido aprobación oficial para la hepatitis B, que aplica la transcriptasa inversa para convertir copias de ARN obtenidas del ADN genómico en nuevo ADN.
Tras rigurosos análisis de la transcriptasa inversa del VIH se han logrado versiones mejoradas de una clase de inhibidores de dicha enzima que no mimetizan a los nucleósidos. 

La genómica ha puesto al descubierto nuevas posibilidades, objetivos potenciales contra los cuales intervenir para suspender la replicación del VIH.
Citemos la ARNasa H, una parte de la transcriptasa inversa que separa, del ARN, el ADN recién acuñado;
también, el centro activo de la integrasa, una enzima que agrega el ADN sintetizado al ADN cromosómico de la célula infectada.
En voluntarios infectados por el VIH se está sometiendo a prueba un inhibidor de la integrasa.

Inhibición de la síntesis de proteínas 

Todos los virus tienen, en algún momento de su ciclo biológico, que transcribir genes en hileras de ARN mensajero. 
La célula huésped las traduce en síntesis de proteínas.
Se trabaja en diversos fármacos destinados a entorpecer la etapa de la transcripción:
se pretende que eviten la unión entre los factores de transcripción y ADN vírico, de la que resulta la producción de ARN mensajero.

La genómica ha contribuido a identificar los objetivos de muchos de estos agentes.
Además, ha promovido la confección de un nuevo tipo de fármaco: la molécula antisentido.
Si la genómica demuestra que se requiere una determinada proteína para un virus, los farmacólogos pueden detener su síntesis recubriendo parte del molde de ARN con un fragmento de ADN diseñado a propósito y que se una firmemente a la secuencia de ARN seleccionada.
Se dispensa ya un medicamento antisentido, el fomivirsen, para tratar las infecciones oculares producidas por el citomegalovirus en sidosos.
Y hay en vías de desarrollo agentes antisentido dirigidos contra otras enfermedades víricas; 
uno de ellos bloquea la producción de la proteína Tat del VIH, que éste necesita para la transcripción de otros genes del virus.

Se parte del genoma vírico para identificar los puntos del ARN por donde las ribozimas puedan cortar. 
(Las ribozimas son formas enzimáticas de ARN.) 
Está en fase de prueba una ribozima en pacientes con hepatitis C; y, menos avanzados, estudios de ribozimas contra el VIH.
Algunos de estos proyectos recurren a la terapia génica, en virtud de la cual se introducen genes especialmente diseñados en células que luego producirán las ribozimas.
Otros tipos de terapia génica en relación con el VIH van encaminados a la producción de anticuerpos que buscan objetivos en las células infectadas, así como proteínas que se encadenan a secuencias génicas del virus en el interior celular.

Dentro de la célula algunos virus producen una cadena proteica que debe seccionarse para obtener proteínas funcionales. 
El VIH es uno de estos virus;
una proteasa, la enzima que lleva a cabo esos cortes.
Cuando en el curso de la investigación sobre el VIH se advirtió la presencia de esta actividad, los farmacólogos sumaron la proteasa a sus objetivos potenciales.
Así, a finales de los años noventa aparecieron los primeros inhibidores potentes de la proteasa gracias a la ayuda de un estudio estructural exhaustivo por ordenador.

Aunque los inhibidores disponibles pueden causar efectos secundarios perturbadores (acumulación de grasa en zonas indebidas), prolongan la salud general y la vida en muchos enfermos, si se toman en combinación con otros fármacos antivíricos contra el VIH.
Una nueva generación de inhibidores de la proteasa está en marcha.

Bloqueo de la migración vírica 

Aun cuando genomas y proteínas víricos se reproduzcan en el interior celular, carecerán de eficacia patógena mientras no formen nuevas partículas víricas, dotadas de potencia suficiente para escapar de la célula y emigrar hacia otras.
El zanamivir y el oseltamivir, indicados contra la gripe, intervienen en esa fase.
Desde hace tiempo se sabe que la neuraminidasa, molécula que aparece en la superficie de los dos tipos principales de gripe (A y B), facilita la huida de las partículas víricas de las células que las producen.
Por comparación genómica se descubrió la similaridad del sitio activo de la neuraminidasa entre las diversas cepas gripales; 
por investigación estructural se llegó a la creación de fármacos eficaces contra dicho centro.
Los demás medicamentos antigripales actúan contra el tipo A. 
Reforzando la respuesta inmunitaria del paciente se evita también la migración celular de los virus. 
Unas respuestas son generales;
ocurre tal cuando los fármacos frenan la propagación de distintas clases de invasores, en vez de concentrarse sobre un patógeno determinado.
Los interferones, por ejemplo, que participan en esa batalla inmunitaria, inhiben la síntesis proteica y otros aspectos de la replicación vírica en las células infectadas.
En concreto, el interferón alfa constituye un elemento importante de la terapia contra las hepatitis B y C. 
(En el caso de la hepatitis C se indica junto con ribavirina, un fármaco más antiguo.) 
Se investigan otros interferones.

Entre las respuestas inmunitarias específicas mencionaremos la producción de anticuerpos estándar, que reconocen fragmentos de una proteína de la superficie del virus invasor, se enlazan con la proteína y señalizan el virus para su ulterior destrucción por otros componentes del sistema inmunitario.
En cuanto se dispone de la secuencia génica que cifra la proteína de superficie del virus, pueden crearse anticuerpos puros, monoclonales, contra determinadas regiones del polipéptido en cuestión. 
Se expende ya en botica un anticuerpo monoclonal para prevenir el virus sincitial respiratorio en niños expuestos a la infección;
se está ensayando otro en pacientes que sufren hepatitis B.
A través de la comparación entre genomas víricos y humanos se ha sugerido una nueva estrategia antivírica.
Numerosos virus fabrican proteínas que parecen moléculas implicadas en la respuesta inmunitaria.
Algunas de tales versiones víricas desarman las defensas y salvan de la destrucción al virus. 
Los fármacos dirigidos contra esas proteínas miméticas que facilitan la huida podrían restablecer la integridad de las respuestas inmunitarias y acelerar la recuperación del organismo que sufre infecciones víricas.
Se están investigando posibles agentes sanadores.

Virus resistentes

La búsqueda de fármacos antivíricos se ha convertido en una guerra sin cuartel.
Los farmacólogos han de enfrentarse con la posibilidad de que los virus desarrollen resistencia o insensibilidad a muchos de los medicamentos, especialmente cuando se administran durante períodos prolongados, como ocurre en el sida y en bastantes casos de hepatitis B y C, enfermedades crónicas.
Cada medicina prescrita hoy contra el VIH encuentra alguna cepa resistente;
a menudo esa misma cepa es resistente a otros fármacos.
Tal resistencia surge de la tendencia de los virus - en particular de los virus ARN, sobre todo el VIH - a mutar muy deprisa.
Si una mutación capacita a la cepa para vencer algún obstáculo que impide su reproducción (un fármaco, por ejemplo), esa variante vírica prosperará pese a la barrera interpuesta.

Para mantener a raya la resistencia hasta que se encuentren vacunas eficaces, habrá que idear nuevos fármacos. 
Cuando surjan mutantes resistentes a un medicamento, la lectura del texto genético puede revelar el lugar del genoma del virus donde se ha producido la mutación y sugerir el modo en que esa mutación socava la interacción entre la proteína del virus afectada y el fármaco. 
A través de tal información el farmacólogo proseguirá la búsqueda de nuevas estructuras medicamentosas u otros medios encaminados a mantener la eficacia de la medicina a pesar de la mutación.

Se investiga en fármacos nuevos basados en la capacidad para combatir cepas resistentes a otros medicamentos.
Recientemente, los laboratorios DuPont optaron por el DPC 083, un nuevo inhibidor, no nucleosídico, de la transcriptasa inversa, para el desarrollo de fármacos encaminados a vencer la resistencia a otros inhibidores.
Los investigadores examinaron las mutaciones del gen de la transcriptasa inversa que causaba la resistencia.
Después estudiaron modelos con el ordenador para crear compuestos potenciales que inhibieran la enzima transcriptasa inversa, pese a las mutaciones. 
Recurrieron luego a la ingeniería genética para producir virus que sintetizaran las enzimas mutadas, y seleccionaron el compuesto más adecuado para limitar la reproducción de aquellos virus.
El fármaco se encuentra ahora en fase de prueba y evaluación en enfermos de sida.

Quizá tardemos algún tiempo antes de que todas las infecciones víricas graves puedan prevenirse con vacunas o tratarse eficazmente con un fármaco.
Pero la secuenciación reciente del genoma humano, aunque provisional, ha de permitirnos descubrir un grupo nuevo de proteínas que estimulen la producción de anticuerpos antivíricos o que potencien otras partes del sistema inmunitario contra los virus.


Suicidio celular, en la salud y en la enfermedad

Las células están conectadas para suicidarse.
A menudo lo hacen.
Esa apoptosis es imprescindible para el buen funcionamientos de nuestro cuerpo.
Pero, 
qué ocurre cuando se altera el proceso autodestructor normal?

Mientras usted lee este artículo, las células de su cuerpo se mueren a millones.
Pero no se asuste, la mayoría se autosacrifican para que usted sobreviva.
Investigaciones recientes indican que la salud de todos los organismos pluricelulares, incluidos los humanos, depende no sólo de que el cuerpo sea capaz de producir nuevas células, sino también de que sus células puedan autodestruirse cuando no sirven para nada o sufren una alteración.
Este proceso crítico, que ahora se denomina apoptosis, o muerte celular programada, pasó inadvertido durante décadas.
Pero en los últimos años los biólogos han realizado notables progresos en el conocimiento de cómo se lleva a cabo y controla el suicidio celular.

La motivación de muchos investigadores es doble.
Por una parte, la curiosidad científica y, por otra, el deseo de combatir algunas de las enfermedades más temidas.
La regulación aberrante de la apoptosis -que puede determinar un exceso o un defecto de suicidio celular- contribuye probablemente a patologías muy dispares, del cáncer a la artritis reumatoide pasando por el sida y la enfermedad de Alzheimer.

Quienes investigaban el desarrollo embrionario en la primera mitad del siglo XX se percataron ya de que la muerte celular no era, como se había pensado, forzosamente mala para el cuerpo.
Antes bien, resultaba imprescindible.
En los años cincuenta se demostró que ciertos organismos pluricelulares alcanzaban su morfología final eliminando determinadas células seleccionadas.
El renacuajo se desprende de su cola durante el proceso de metamorfosis en rana.
Los mamíferos pierden innumerables neuronas durante el proceso de formación del sistema nervioso.
Los microscopistas habían identificado también las principales señales indicadoras que distinguían esta muerte celular fisiológica de la mera destrucción accidental, o necrosis.

La muerte necrótica ocurre cuando una célula sufre un daño grave, causado tal vez por un golpe físico o por falta de oxígeno.
La hinchazón es una señal característica.
Los orgánulos internos, así las mitocondrias (las factorías energéticas celulares), y toda la célula se hinchan y cuartean.
Estos efectos ocurren porque los daños impiden que la célula controle adecuadamente su balance de fluidos y iones.
El agua y las partículas cargadas (especialmente iones de sodio y calcio), que en condiciones de normalidad se bombean hacia fuera, ahora entran a raudales.
Otra señal es la inflamación:
los macrófagos circulantes y otros glóbulos blancos del sistema inmunitario convergen en las células necróticas y las ingieren.
La inflamación coadyuva a limitar la infección y eliminar los restos, pero la acción y las secreciones de los glóbulos blancos pueden lesionar también el tejido normal de la vecindad, y a veces bastante.

Los científicos que estudian células que sufren apoptosis aprecian cambios muy distintos.
No observan hinchamiento.
Ven que las células que se están muriendo se encogen y se apartan de sus vecinas.
Al poco tiempo parece que hierven: se forman unas burbujas en la superficie, que desaparecen, sustituidas de inmediato por otras nuevas burbujas.
Los orgánulos internos retienen su estructura, pero el núcleo, que se altera poco en la necrosis, cambia espectacularmente durante la apoptosis.
Uno de los cambios más notables afecta a la cromatina (ADN cromosómico y proteínas), que en una situación normal está dispersa y durante el proceso apoptósico se condensa formando una o varias manchas cerca de la membrana nuclear.

En este punto, las células apoptósicas suelen ingerirse por células cercanas o por células carroñeras, que se encuentran en todos los tejidos, sin que se produzca una respuesta inflamatoria.
Las células que se están muriendo y no se ingieren pueden sufrir nuevos cambios.
El núcleo se desintegra y las células se dividen en numerosos "cuerpos apoptósicos", que pueden contener una o dos piezas nucleares.
Como antes, estos cuerpos se eliminan discretamente. (A finales de los años setenta, los estudios bioquímicos escribieron otra página de la apoptosis: la cromatina suele disgregarse en fragmentos que producen un patrón similar a los peldaños de una escalera cuando dichos fragmentos se separan por tamaños en geles sometidos a electroforesis.)

Curiosamente, ciertas células que sufren muerte celular programada no se engullen.
Hoy sabemos que persisten largo tiempo, si no indefinidamente.
El cristalino del ojo, por ejemplo, está formado por las carcasas de células que sustituyen la mayor parte de su citoplasma por la proteína cristalina, cuando mueren.
En la piel, las células denominadas queratinocitos se generan a partir de precursores de una capa más profunda, y después emigran hacia la superficie, muriendo en el camino.
En vez de cristalina, reemplazan su contenido por queratina, una proteína resistente, y adquieren una cubierta que repele el agua.
Estas células muertas constituyen la capa protectora externa de la piel, hasta que se caen, reemplazadas por otros queratinocitos.

Aunque la mayoría de los eventos apreciables que definen la apoptosis quedaron ya corroborados en los años cincuenta y se conocía su función en el desarrollo embrionario, la importancia de la apoptosis en el mantenimiento diario del organismo maduro seguiría sin reconocerse durante veinte años más.
El patólogo australiano John F. R. Kerr y sus colegas escoceses Andrew H. Wyllie y Alastair Currie abrieron un nuevo surco con un trabajo publicado en 1972.

En dicho artículo, afirmaban que el mismo tipo de muerte celular que era evidente durante el desarrollo, acontecía también en organismos maduros y durante toda la vida.
Sugerían que, a diferencia de la necrosis, donde la célula es una víctima pasiva, esta forma de muerte es activa, y requiere que la célula gaste energía en su propio ocaso.
Los investigadores proponían, además, que una iniciación inadecuada del suicidio celular, o su inhibición, podría contribuir a muchas enfermedades, cáncer incluido.
Fueron ellos quienes, aconsejados por un colaborador, adoptaron el término griego apoptosis para distinguir este tipo de muerte celular de la necrosis ("causar la muerte").
En griego clásico, apoptosis significa "caerse" ("morirse"), como caen los pétalos o las hojas en otoño.

A pesar de la profundidad de las ideas de la publicación de 1972, sus observaciones durmieron en el limbo durante más de una década, hasta que los pocos grupos que a la sazón seguían la pista de la apoptosis comenzaron a confirmar las predicciones del artículo.
Encontraron signos del carácter progresivo de la apoptosis y de que su ausencia podría favorecer el cáncer.
Los investigadores empezaron a señalar con precisión algunas de las moléculas que intervienen en el proceso y en su regulación.

Hay ahora muchos expertos empeñados en descifrar con exactitud cómo y cuándo las células se autodestruyen.
Quedan cuestiones pendientes, pero se han descubierto ya algunos principios fundamentales.
La mayoría de las células, si no todas, fabrican una serie de proteínas que se utilizan como armas autodestructivas.
Mientras una célula es útil para el cuerpo, la maquinaria letal permanece silente.
Pero si la célula resulta infectada, se torna maligna o amenaza la salud del organismo, se liberan las proteínas letales.

La apoptosis puede ponerse en marcha de varias maneras.
Por ejemplo, cuando una célula deja de percibir las señales químicas (factores de crecimiento o supervivencia) que le permiten comunicarse con otras y reafirmarse en la importancia del papel que lleva a cabo.
También puede ponerse en marcha cuando una célula recibe mensajes externos o internos que anulan las señales anteriores; o cuando las células reciben órdenes contradictorias sobre si deben o no dividirse.

En algunos tipos celulares, la puesta en marcha de los mecanismos apoptósicos es predecible.
Los queratinocitos que migran a la superficie de la piel están muertos y desaparecen aproximadamente a los 21 días de comenzar su viaje.
Sin embargo, esas mismas células, así como las que están destinadas a durar toda la vida (neuronas y células del músculo esquelético), pueden morir prematuramente si se tornan conflictivas.
Las quemaduras solares, por ejemplo, pueden provocar apoptosis en los queratinocitos que aún no han llegado a su destino en la piel.

En todos los tipos celulares y en todos los organismos pluricelulares estudiados hasta la fecha, las armas suicidas son unas enzimas que degradan proteínas.
Se denominan proteasas "de tipo ICE", porque estructuralmente se parecen a la enzima transformadora de la interleucina 1, o ICE (" interleukin-1 converting enzyme "), que fue el primer miembro del grupo que se descubrió.
Las proteasas de tipo ICE que destruyen a las células son como una colección de cuchillos afilados, que se mantienen envainados en condiciones de normalidad.
Cuando las enzimas se activan (las hojas se desenvainan), destruyen otras proteínas y, con ello, las células.
Algunas de las proteínas lisadas son componentes estructurales esenciales de la célula.
En otros casos, los cortes producidos por las proteasas conducen directa o indirectamente a la destrucción del material genético, impidiendo, por tanto, el propio mantenimiento de la célula.

A pesar de que todas las células tienen la misma maquinaria letal, las señales que llevan a la autodestrucción de unas y otras pueden ser distintas.
La facilidad y rapidez con que se activa el programa mortal puede variar también de un tipo celular a otro y de un estado de desarrollo a otro en una misma célula.
Y una célula concreta puede ser sensible a varios tipos de señales inductoras.
Entre los objetivos principales de la investigación actual está el de especificar la gama de inductores apoptósicos y descifrar cómo activan a las destructivas proteasas de tipo ICE.
Se sabe que las instrucciones que portan los inductores se transmiten a las proteasas a través de una serie de intermediarios y que diferentes inductores pueden recurrir a intermediarios distintos.
Pero, en la mayoría de los casos, se desconocen las cadenas de interacciones, o rutas de señales, que intervienen en el proceso.
Tampoco se ha avanzado en el desciframiento de las moléculas que activan directamente a las proteasas.

Podemos hacernos una idea de los progresos realizados analizando cómo se suicidan los linfocitos T en diferentes estadios de su ciclo vital.
Las células T son protagonistas estelares de la respuesta inmunitaria contra virus y otros microorganismos invasores.

Las células T se producen a partir de precursores, en la médula ósea.
Las células inmaduras emigran después al timo, donde los timocitos, nombre que entonces reciben, se especializan.
Durante este proceso despliegan en sus membranas las moléculas receptoras que más tarde permitirán a las células T maduras detectar las infecciones.
Para cumplir su función beneficiosa, las células T deben fijarse a los antígenos microbianos (proteínas marcadoras que señalan la presencia de un invasor), a través de sus receptores.
Deben también ignorar a las sustancias elaboradas por el propio cuerpo, ya que las células T autorreactivas pueden destruir los tejidos normales.
Sólo los timocitos que fabrican receptores útiles madurarán y pasarán al torrente sanguíneo, para patrullar por todo el cuerpo.

Mientras se hallan en el timo, los timocitos que no son capaces de fabricar receptores funcionales sufren apoptosis, ya que no van a ser útiles.
Los timocitos también se autodestruyen si sus receptores se unen con mucha fuerza a moléculas presentes en el timo.
Una unión de ese tipo es señal de que las células pueden más tarde atacar el tejido sano y destruirlo (autoinmunidad).

Las células T maduras que logren entrar en circulación permanecen en estado latente, a menos que encuentren el antígeno identificable por receptores celulares. Las células latentes, igual que los timocitos y muchas células más, son sensibles a otros inductores del suicidio: rayos X (en radioterapia contra el cáncer) y otros agentes que dañan el ADN.
Las lesiones inducidas provocan que las células sinteticen p53, proteína que activa el programa de suicidio.
Durante un tiempo se pensaba que, para autodestruirse, todas las células debían sintetizar p53 u otras proteínas.
La síntesis de proteínas se exige en muchos casos, aunque no siempre.

Las células T circulantes se tornan activas, esto es, proliferan y producen proteínas que promueven la inflamación, cuando sus receptores se traban con antígenos foráneos.
Esta actividad es importante mientras persista el agente infeccioso; ahora bien, una vez desaparecida la infección, las células deben morir.
Si tal no ocurriera, se acumularían y originarían una inflamación crónica (con su correspondiente hinchazón y fiebre) y, posiblemente, un estado de autoinmunidad.

Se conocen dos formas de inducir la apoptosis de las células innecesarias.
Uno de los mecanismos implica la pérdida de factores de supervivencia; en este caso, la desaparición de interleucina 2, un factor de las células T, cuando se ha eliminado el agente infeccioso.
El segundo mecanismo depende de una molécula Fas, que recientemente ha suscitado notable interés.

Las células T latentes producen pequeñas cantidades de proteína Fas, que se sitúa en la membrana celular.
Uno de los extremos de la proteína se proyecta hacia el exterior de la célula y el otro hacia dentro, pudiendo así transmitir señales al interior celular.
Cuando las células T encuentran un antígeno y se activan, fabrican más Fas, que en un comienzo no interviene.
Sintetizan también transitoriamente otra molécula de superficie: el ligando de Fas.
A los pocos días, Fas empieza a actuar.
Entonces, el ligando de Fas presente en las células activadas se une al Fas que hay en la misma célula o en otra célula T activada, en el sitio donde se ha desencadenado la infección.
Esa unión es la señal para que las células portadoras de Fas sufran apoptosis.
Por tanto, las células T activas tienen unos pocos días para llevar a cabo su tarea (erradicar la infección), muriéndose después.

Hemos dicho ya que la sensibilidad de los linfocitos T y otras células a diferentes inductores de la apoptosis puede depender del estado de las células en cada momento.
Cuando se irradia con rayos X, las células T latentes mueren enseguida, pero no así las células T activas.
Una unión fuerte entre un receptor de timocito y proteínas del timo provoca la muerte, pero la unión de los antígenos con las células T maduras circulantes provoca su activación.
Más aún: algunos tipos celulares son de por sí más proclives a la apoptosis que otros. 
¿A qué se deben tales diferencias?

Sospechamos que la evolución se las ha arreglado para que las células irreemplazables, como las neuronas y las células del músculo esquelético, opongan mayor resistencia, ya que la pérdida de esas células acarrearía consecuencias nefastas para el organismo.
Por contra, las células fácilmente sustituibles, como las sanguíneas, parecen más propensas a morir ante la menor provocación.

Se percibe con creciente nitidez que la sensibilidad está modulada fundamentalmente por la proteína Bcl-2 y otras similares, como las Bax y Bad.
Unas moléculas bloquean la apoptosis, otras la promueven.
La proporción entre aquéllas y éstas coadyuva a determinar la rapidez con que se va a producir la apoptosis.
Se desconoce, sin embargo, el mecanismo de interacción entre las moléculas implicadas y la maquinaria letal.

De la misma manera que la apoptosis es esencial para la supervivencia de un organismo, su desregulación parece estar detrás de muchas patologías, como las de etiología vírica.

Una vez dentro de las células, los virus intentan subvertir la maquinaria de síntesis de proteínas y ponerla a su servicio, para que sólo fabrique las proteínas necesarias en la producción de más virus.
Para desgracia del virus, el mero acto de detener la síntesis proteica de su hospedador basta para inducir el suicidio en muchos tipos celulares.
Si la célula hospedadora muere, se disgrega el virus con ella.
Por eso, ciertos virus han desarrollado estrategias para inhibir la apoptosis de las células que infectan.

El virus de Epstein-Barr, agente de mononucleosis y vinculado con linfomas en humanos, recurre a un mecanismo que se ha observado también en otros virus.
En efecto, produce sustancias que se parecen a Bcl-2, inhibidor de la apoptosis.
Puede fabricar también moléculas que hacen que la célula hospedadora incremente su propia síntesis de Bcl-2.
Los papilomavirus, una de las principales causas del cáncer cervical, inactivan o degradan la proteína p53, inductora de la apoptosis.
Y los virus vacunales, similares a los utilizados como vacuna de la viruela, elaboran una proteína que impide a las proteasas de tipo ICE llevar a cabo el programa de apoptosis, lo que sugiere que algunos virus humanos podrían hacer lo mismo.
De ahí que la búsqueda de terapias antivíricas se empeñe en hallar formas de bloquear la actividad de las moléculas antiapoptósicas preparadas por los virus.

Afortunadamente para los humanos y otros animales, el sistema inmunitario posee sus propias estrategias para contrarrestar esas artimañas víricas.
Una de las principales consiste en erradicar las células infectadas, utilizando para ello una subserie de linfocitos T denominados citotóxicos, o células T asesinas.
Tras unirse a las células diana, las células asesinas las bombardean con dos tipos de proteínas que, juntas, asestan un golpe mortal.
Una de las proteínas (perforina) se inserta en la membrana de la célula infectada.
Allí, forma un estructura parecida a un poro, que facilita la entrada de granzimas (véase "Células asesinas", por John Ding-E Young y Zanvil A. Cohn, en Investigación y Ciencia , marzo de 1988).
Estas enzimas pueden activar las proteasas de tipo ICE, induciendo así la apoptosis.
Pero si este intento exterminador falla, los iones de calcio que atraviesan los nuevos poros pueden colaborar con las granzimas, produciendo una muerte necrótica.

Ahora bien, la capacidad que tienen las células T de inducir apoptosis a diestro y siniestro, no sólo en las células infectadas, puede perjudicar a las células sanas que residen cerca de los tejidos infectados.
Semejante daño indiscriminado ocurre porque muchas células del cuerpo portan en sus membranas la proteína Fas, en especial cuando se infectan ellas mismas o sus vecinas.
Si las células asesinas llegan hasta las células infectadas, el ligando de Fas que sobresale de la superficie de las células T puede acoplarse con la proteína Fas presente en las células enfermas, poniendo en marcha la maquinaria apoptósica en las células infectadas.
Esta actividad es útil y aumenta la eficacia de otras tácticas inmunitarias para combatir las infecciones.
Pero el ligando de Fas presente en las células T puede acoplarse con la proteína Fas que hay en las células sanas vecinas e instar también su suicidio.
Se ha sugerido que ese efecto indiscriminado pudiera ser la explicación del daño grave que los virus de la hepatitis infligen en el hígado, aunque infecten a muy pocas células hepáticas.

La inducción apoptósica de las células sanas parece contribuir también al hundimiento inmunitario que atormenta a los enfermos de sida.
En la gente que contrae el virus de inmunodeficiencia humana (VIH), agente del sida, las células T coadyuvantes mueren.
Cuando esos linfocitos T desaparecen, las células T citotóxicas mueren también, ya que éstas necesitan las señales de crecimiento de las células coadyuvantes para impedir la apoptosis.
Cuando cae el número de células T, disminuye a su vez la capacidad del cuerpo para luchar contra las enfermedades, especialmente las infecciones víricas y parasitarias.
Sabemos que son muchas más las células coadyuvantes que sucumben que las que son infectadas con el VIH.
Resulta también evidente que muchas de las células mueren probablemente por apoptosis.
Pero se desconoce el elemento que impulsa a esa autodestrucción.

Una respuesta plausible estaría en el exceso de proteína Fas.
Recuérdese que, en condiciones de normalidad, las células T sólo fabrican proteína Fas funcional si llevan activas unos días y se encuentran ya listas para morir.
Pero las células coadyuvantes de los sidosos pueden acarrear en sus membranas grandes cantidades de proteína Fas funcional, antes incluso de que las células hayan dado con un antígeno.
Cabe esperar que tal despliegue de proteínas Fas les haga sufrir apoptosis prematuramente, cada vez que encuentran el ligando de Fas en otras células (como en las células T ya activadas contra el VIH u otros microorganismos).
Las células pueden poner en marcha también su propia muerte, sin recibir señales de células activadas, si encuentran el antígeno que sus receptores reconocen. Cuando se produce el reconocimiento antigénico, las células T fabrican ligando de Fas.
El ligando de estas células puede acoplarse con las moléculas Fas que ella misma porta y activar así el programa letal.
Y lo que es peor, esas células T estimuladas antigénicamente, que portan Fas y su ligando, pueden expandir el proceso de muerte celular prematura, induciendo el suicidio celular a diestro y siniestro.

No podemos descartar que los radicales libres de oxígeno desencaden, por su parte, el suicidio de las células T exentas de virus.
En los sidosos, las células inflamatorias de los nódulos linfoides infectados producen tales sustancias altamente reactivas, que atentan contra el ADN y las membranas celulares.
Los radicales libres causarán necrosis si provocan un daño excesivo, pero pueden inducir apoptosis si el daño es más sutil.
En apoyo de la tesis de los radicales libres, se ha observado que las moléculas capaces de neutralizar radicales libres previenen la apoptosis en células T obtenidas de pacientes con sida. (Hoy se están estudiando algunas terapias contra el sida basadas en la lucha contra la apoptosis.)

Aunque en los pacientes infectados con el VIH las células T coadyuvantes normales pueden verse inducidas al suicidio por otras células inmunitarias, en términos técnicos las células sanas no mueren por un proceso de autoinmunidad.
Se dice que ocurre autoinmunidad cuando los receptores antigénicos de las células inmunitarias reconocen antígenos específicos de las células sanas y hacen que esas células que portan tales sustancias mueran.
Pero existen también enfermedades de autoinmunidad en las que se da apoptosis.

Si el cuerpo elimina de oficio los linfocitos autorreactivos, 
¿cómo puede presentarse la autoinmunidad? La verdad es que el cuerpo permite la circulación de algunos linfocitos dotados de una baja capacidad autorreactiva.
Esas células suelen encerrar poco peligro, pero pueden volverse hiperactivas.
Lo harán, por ejemplo, si esos linfocitos reconocen también algún antígeno foráneo (presente en algún microorganismo, o en la comida).
Cuando tal acontece, proliferan y pueden llegar a atacar tejido sano con redoblado entusiasmo.

Las reacciones de autoinmunidad suelen ser autolimitadas.
Desaparecen cuando los antígenos que las desencadenan se quitan de en medio.
En algunos casos, sin embargo, los linfocitos autorreactivos sobreviven más de lo que debieran y continúan induciendo apoptosis en las células normales.
Por lo que se ha observado en animales y humanos, parece que existe un exceso de supervivencia de células autorreactivas en al menos dos síndromes de autoinmunidad crónicos, el lupus sistémico eritematoso y la artritis reumatoide.
En otras palabras, los linfocitos sufren muy poca apoptosis, y las células normales demasiada.

Se está investigando la posibilidad de que la larga vida de los linfocitos autorreactivos tenga que ver con su producción de moléculas que bloquean la unión entre el ligando de Fas (presente en las otras células) y la proteína Fas de su superficie, impidiendo así que el ligando envíe mensajes de muerte a los linfocitos.
De acuerdo con otras hipótesis, los linfocitos evitarían la apoptosis porque producen poco Fas o mucho Bcl-2, el inhibidor del suicidio.
En cualquier caso, un mayor conocimiento de cómo viven y mueren las células T debería conducirnos a la creación de estrategias que permitieran activar selectivamente el programa letal en los linfocitos específicos responsables de las enfermedades de autoinmunidad.
Pensamos, por ejemplo, en poder enviar directamente una molécula activadora de Fas (quizás el propio ligando de Fas) a las articulaciones artríticas, induciendo así la autoaniquilación de las células inmunitarias hiperactivas.

Varios tipos de tejidos parecen utilizar el ligando de Fas para no ser víctimas de la autoinmunidad.
Con el ligando de Fas en su superficie, células de testículo, ojo y posiblemente cerebro inducen una rápida apoptosis en cualquier célula T activada y portadora de Fas que se le cruce por el camino.
Se espera poder aplicar este descubrimiento a los trasplantes de órganos.
De momento, los únicos órganos y tejidos trasplantables son aquellos cuyos antígenos tipificadores de tejidos son compatibles con los de los tejidos de los receptores.
La compatibilidad debe ser alta, condición para que el sistema inmunitario admita el trasplante.
Pero si se lograse que los órganos y tejidos que se donan presentasen el ligando de Fas, podrían resistir los ataques inmunológicos del hospedador y ser adecuados por tanto para el trasplante.

En la autoinmunidad, las células inmunitarias no mueren cuando se supone que deberían hacerlo; en el cáncer son las células tumorales las que dejan de autosacrificarse.
De hecho, la definición del cáncer cada vez se acerca más a la de una patología en que se produce una excesiva proliferación de células y una pérdida de capacidad autodestructiva (véase "Así se produce el cáncer", por Robert A. Weinberg, en Investigación y Ciencia , noviembre de 1996).

Se desarrolla el cáncer cuando una célula acumula mutaciones en varios de los genes que controlan el crecimiento y la supervivencia de las células.
Si una mutación es irreparable, la célula afectada suele autodestruirse antes de volverse potencialmente peligrosa.
Pero si la célula no muere, ella o sus descendientes pueden vivir lo suficiente como para acumular mutaciones que la lleven a dividirse sin control y originar metástasis, situación en que se desprenden del tumor original y establecen masas en puntos alejados del mismo.

En muchos tumores, el daño genético parece que impide que se induzca la apoptosis, ya que sus células han inactivado el gen que cifra la proteína p53.
Esta proteína, recordemos, conduce a la activación de la maquinaria apoptósica de la célula cuando el ADN sufre daños.
En más de la mitad de todos los tumores sólidos, incluidos los de pulmón, colon y mama, falta la proteína p53 o no es funcional.

Las células que alcanzan el estado canceroso podrían eliminarse por otros inductores apoptósicos.
La tendencia de las células normales a suicidarse cuando les faltan sus factores de crecimiento normales o el contacto físico con sus vecinas constituye, a buen seguro, un mecanismo de defensa contra la metástasis; la rápida activación de la apoptosis en las células tumorales que abandonan su tejido originario erradica presumiblemente a muchas células metastásicas antes de que tengan alguna probabilidad de desarrollarse.
Pero las células cancerosas se las arreglan a veces para volverse insensibles a los efectos apoptósicos de la falta de factores de crecimiento y la pérdida de los contactos célula-célula.

Otras proteínas relacionadas con la apoptosis se han visto comprometidas también en los estados malignos.
En varios tipos de cánceres, nos referimos a determinados linfomas, se impide la muerte celular mediante una síntesis desmesurada de Bcl-2, la proteína inhibidora del suicidio.
Y hay razones para sospechar que algunos tumores se oponen a que Fas envíe señales a la maquinaria letal, o producen ligando de Fas para evitar la apoptosis mediada por el sistema inmunitario.

Por extraño que parezca, ciertas células normales fabrican niveles sustanciosos de Bcl-2.
Esta proteína parece proteger a las células cuya pérdida sería devastadora para el cuerpo, pero ese escudo tiene un precio.
Si las células en cuestión se tornan cancerosas, engendran tumores más agresivos.
Protegidas por Bcl-2, tienen una probabilidad menor de morir que otras células tumorales.
En consecuencia, pueden ser más propensas a desarrollar metástasis en tejidos que no suministran los factores de supervivencia sintetizados por sus tejidos de origen.

Consideremos los melanocitos, células productoras de melanina.
Este pigmento oscurece a otras células de la piel y previene de los efectos causados por la absorción de dosis letales de luz solar.
Si los melanocitos muriesen fácilmente, correrían peligro las otras células.
Por ello, los melanocitos fabrican grandes cantidades de Bcl-2.
Sin embargo, cuando los propios melanocitos resultan genéticamente dañados no son tan propensos a suicidarse como las demás células de la piel, pero sí lo son, cuando se vuelven malignos, a formar tumores agresivos que se propagan con celeridad.

Los estudios sobre apoptosis han empezado también a aclarar la raíz de la resistencia que muchos tumores oponen a los efectos letales de las radiaciones y la quimioterapia.
Creíase, tiempo atrás, que esas terapias causaban la destrucción del foco tumoral, provocando la muerte necrótica de las células malignas.
Hoy sabemos que la muerte de esas células suele resultar de la apoptosis, frecuentemente por la activación de p53 .
Las células que carecen de p53 o que producen niveles elevados de la proteína inhibidora Bcl-2, pueden, por tanto, volverse resistentes a los efectos de los tratamientos anticancerosos.

Se están investigando las posibilidades de las terapias genéticas para vencer la resistencia a la apoptosis.
En los trabajos correspondientes se introduce un gen p53 normal en los cánceres que lo presentan dañado, con el fin de restablecer la síntesis de proteína p53 normal.
También se han abordado las formas de impedir que los genes Bcl2 hiperactivos produzcan proteína Bcl-2.
Otros enfoques se han propuesto como meta impedir que las células reciban los factores de crecimiento específicos que promueven su supervivencia.

En contraste con el cáncer, donde ocurre muy poca apoptosis, la muerte celular que acompaña a las cardiopatías isquémicas y los accidentes cerebrovasculares causados por el bloqueo de uno de los vasos sanguíneos que alimenta un segmento del músculo cardíaco o el cerebro se debe, en buena medida, al suicidio celular. En el corazón, el bloqueo diezma a las células que dependían totalmente de los vasos sanguíneos obstruidos.
Esas células mueren por necrosis, en parte, porque son drásticamente privadas del oxígeno y la glucosa que necesitan para mantenerse y, en parte también, porque los iones de calcio, que en condiciones normales se bombean fuera de la célula, penetran a raudales y alcanzan niveles tóxicos.
Pero aquí no se acaba la destrucción.

A los pocos días, las células que rodean la zona necrótica, y que inicialmente sobreviven porque siguen recibiendo alimentos procedentes de otros vasos sanguíneos, pueden también morir.
Muchas fallecen por necrosis, alcanzadas por los radicales libres que se liberan cuando las células inflamatorias inundan la zona muerta para eliminar el tejido necrótico.
Pero otras muchas células, que han sufrido daños menos graves, se suicidan.
Si el paciente es tratado y se restablece su flujo sanguíneo, se eleva el número de células que pueden morir por necrosis o apoptosis: la reperfusión multiplica la producción de radicales libres.

Algo similar acontece en los accidentes cerebrovasculares.
Las células más afectadas mueren por necrosis.
Después, a los pocos días, la inflamación y los agentes químicos que se liberan de las células muertas (en particular el neurotransmisor glutamato) provocan más necrosis y apoptosis en las células vecinas.
Las células que desaparecen lo hacen para siempre, ya que ni las células del músculo cardíaco ni las neuronas se dividen en el cuerpo adulto.
El conocimiento de los factores que provocan la muerte hística inherente a las cardiopatías, accidentes cerebrovasculares y reperfusión, ha favorecido el desarrollo de nuevos tratamientos.
En ese contexto, podría restringirse la muerte celular mediante fármacos bloqueantes de la producción de radicales libres o mediante drogas inhibidoras de las proteasas de tipo ICE.

Habrá que atribuir a la apoptosis el motivo de la muerte celular que se registra en enfermedades marcadas por la pérdida progresiva de neuronas cerebrales, como las enfermedades de Alzheimer, Parkinson, Huntington y la esclerosis lateral amiotrófica (enfermedad de Lou Gehrig).
Se desconoce la causa exacta de esa apoptosis.
Se han propuesto varios agentes; entre ellos, los radicales libres, niveles insuficientes de factores de crecimiento nervioso y niveles excesivos de neurotransmisores.
Parece verosímil que una combinación de esos factores pudiera ser el responsable último de la autodestrucción gradual de muchas células.
Estudios con animales indican que una administración prolongada de factores de crecimiento nervioso podría actuar de protección contra la apoptosis en esas condiciones.
La pérdida del control normal del mecanismo de la apoptosis pudiera hallarse detrás de la retinitis pigmentosa (una de las causas de la ceguera) y la osteoporosis.

Nos encontramos todavía en las primeras fases de la investigación sobre la muerte celular, lo que significa que también están en sus etapas iniciales los esfuerzos empeñados en combatir las enfermedades mediante la intervención en el propio proceso autodestructor.
En ello andan numerosos laboratorios farmacéuticos, diseñando nuevos fármacos y comprobando la influencia sobre la supervivencia celular de otros ya existentes.
El conocimiento cada vez mayor de los procesos apoptósicos estimulará sin duda las líneas de investigación incoadas.


Huellas de la evolución

Los orígenes evolutivos del genoma humano, y de todos los genomas, se remontan por definición al origen mismo de la vida.
Este capítulo no intenta ser una introducción a la teoría evolutiva básica, o un panorama general de la genética evolutiva molecular per se .
Esto hace que muchas áreas fascinantes queden sin ser cubiertas, como la idea de que el RNA fuera utilizado como la molécula de información primaria antes de ser sustituido por el DNA, o que el código genético estuviera basado en un principio en dobletes, antes de evolucionar al código de tripletes que nos es familiar, etc. 
El lector interesado en estos temas puede consultar un libro de texto de genética evolutiva molecular de carácter más general.
En este capítulo intentamos centrarnos en cómo los análisis comparativos de los genomas contemporáneos han arrojado luz sobre el origen evolutivo del DNA humano.
Muchos de los datos derivan de la comparación de genomas de mamíferos, a pesar de que a veces se recurre a las comparaciones con genomas más distantes para explicar ciertas huellas de la evolución , por ejemplo para explicar el origen del DNA mitocondrial o de los intrones.
Una sección final considera las particularidades del genoma humano comparado con modelos mamíferos, fundamentalmente el ratón (un modelo importante para entender el desarrollo temprano y también las enfermedades humanas) y los primates, nuestros parientes vivientes más cercanos.

Evolución del genoma mitocondrial 

El genoma mitocondrial humano se originó probablemente tras la endocitosis de una eubacteria aeróbica y la subsecuente transferencia a gran escala de genes hacia el genoma nuclear

La organización del genoma mitocondrial humano es radicalmente diferente de la del genoma nuclear.
Los genomas mitocondriales de las células animales, incluyendo las humanas, presentan varias características de los genomas procariotas:
pequeño tamaño, ausencia de intrones, un porcentaje muy elevado de DNA codificante, la falta generalizada de secuencias repetidas y genes de rRNA comparativamente pequeños, parecidos a los procariotas.
El análisis filogenético de las secuencias de rRNA sugiere que las mitocondrias son particularmente próximas a la subdivisión xxx de las bacterias púrpuras.
En consecuencia, se cree que las mitocondrias se originaron a partir de la endocitosis de una eubacteria aeróbica con un sistema de fosforilación oxidativa (probablemente una bacteria púrpura) por parte de una célula precursora eucariota anaeróbica. 
Según la hipótesis del endosimbionte , esto sucedió probablemente cuando el oxígeno comenzó a acumularse en cantidades importantes en la atmósfera terrestre, hace unos 1500 millones de años.
En aquel tiempo, los precursores de los eucariotas eran células anaeróbicas sencillas.
El cambio a una atmósfera cada vez más aeróbica proporcionó un poderoso sistema de selección para que las células anaeróbicas adquirieran la capacidad de realizar la fosforilación oxidativa.
La endocitosis de una bacteria respiratoria constituyó entonces una gran ventaja para las células precursoras de los eucariotas:
adquiriendo el sistema de fosforilación oxidativa bacteriano pudieron promover su propia rápida expansión y evolución en la atmósfera que contenía oxígeno (ver Figura 9.1). 
El tamaño del genoma mitocondrial humano (16,6 kb) y de los genomas mitocondriales de otras células animales (típicamente menores que 20 kb) es, sin embargo, mucho menor que el de muchos genomas bacterianos.
Por ejemplo, el de Escherichia coli tiene 4,6 Mb.
Es probable, entonces, que la mayor parte de los genes aportados por el endosimbionte eubacteriano hayan sido incorporados al genoma nuclear;
el genoma mitocondrial retuvo presumiblemente una pequeña parte de los genes de la eubacteria, incluyendo los genes principales de rRNA y de tRNA y unos pocos genes codificantes de algunos de los componentes del sistema de fosforilación oxidativa.
Esto daría una explicación plausible de por qué las mitocondrias contemporáneas dependen tanto de los productos de genes nucleares (que son traducidos en el citoplasma antes de ser importados al interior de la mitocondria).

Figura 9.1

El genoma mitocondrial humano se originó probablemente después de la endocitosis de un procariota aeróbico y la subsecuente transferencia de genes a gran escala hacia el genoma nuclear.

A pesar de que el procariota endocitado era capaz al principio de sostener su propia síntesis de proteínas, al cabo de un tiempo de escala evolutiva la mayoría de sus genes fueron transferidos al genoma nuclear.

Esto dejó un pequeño genoma remanente que en la actualidad sólo puede sintetizar rRNA, tRNA y unas pocas proteínas implicadas en el sistema de fosforilación oxidativa (Oxphos).
La gran mayoría de las proteínas mitocondriales, incluyendo enzimas, proteínas ribosómicas y la mayor parte de las proteínas del sistema de fosforilación oxidativa están especificadas por genes nucleares, son sintetizadas en ribosomas citoplasmáticos y finalmente importadas al interior de las mitocondrias.

La evolución del código genético mitocondrial es probablemente el resultado de una presión de selección reducida en respuesta a una capacidad codificante muy disminuida

El código genético mitocondrial humano difiere ligeramente del código genético universal" utilizado en la expresión de los polipéptidos codificados por los genomas procariotas, eucariotas nucleares y mitocondriales de plantas.

Además, pese a que es idéntico al código genético de las mitocondrias de otros mamíferos, presenta algunas diferencias respecto del código utilizado por las mitocondrias de otros eucariotas, tales como Drosophila o levaduras.
Es muy probable que la evolución de códigos genéticos mitocondriales alterados haya surgido como consecuencia de la pérdida progresiva de potencial codificador, debida a la transferencia de genes desde el genoma endosimbionte original al genoma nuclear;
el minúsculo genoma mitocondrial de las células animales especifica la síntesis de unos pocos polipéptidos (13 en el caso humano).
La bacteria endosimbionte original utilizaría el código genético universal. 
Sin embargo, una vez que la mayor parte de su capacidad codificadora se hubo transferido al genoma nuclear, la presión de selección para conservar el código original habría disminuido -pequeñas alteraciones del código universal no tendrían va consecuencias catastróficas ya que sólo afectarían a un número muy pequeño de genes.
También es probable que los codones que han sido alterados no fueran muy utilizados en aquellas posiciones en las que los cambios de aminoácidos habrían sido perjudiciales.
En contraste, existe una enorme presión de selección para conservar el código genético universal en las genomas mayores, como el genoma nuclear-la más mínima alteración puede resultar en la pérdida de función o en una función aberrante para un gran número de productos génicos de importancia vital, lo que provocaría la muerte celular.

Evolución del genoma nuclear humano: duplicación del genoma y alteraciones cromosómicas masivas

En muchos aspectos el genoma nuclear humano es típico de los genomas nucleares de células de eucariotas pluricelulares complejos, como los mamíferos.
El genoma es de un tamaño global grande, lo que con notables excepciones está correlacionado positivamente con la complejidad del organismo.
El genoma contiene una elevada fracción de secuencias de DNA no codificantes cuya importancia sigue siendo mayormente una incógnita.
Además, una elevada proporción del DNA, tanto codificante como no codificante, es DNA repetitivo.
En contraste, los genomas bacterianos contemporáneos son mucho más pequeños -el tamaño del genoma de E. coli no llega a ser 1/1000 de un genoma de mamífero típico- y tienen poco DNA no codificante o secuencias repetidas.
Cabe suponer entonces que la evolución de los organismos pluricelulares complejos haya implicado un aumento progresivo del tamaño del genoma.
Existen varios mecanismos diferentes que probablemente hayan contribuido al gran aumento del tamaño del genoma, incluyendo duplicaciones ocasionales de todo el genoma (tetraploidización) y una variedad de reacciones de duplicación subgenómica, mucho más frecuentes, resultantes de la acción de diferentes mecanismos genéticos incluyendo entrecruzamiento desigual, intercambio desigual de cromátidas hermanas y replicación de hebras deslizadas .
En todos los casos el aumento del tamaño del genoma debió suceder sin comprometer inicialmente las funciones del DNA original. 
En lugar de ello, al proporcionar DNA adicional, mutaciones posteriores podrían haber contribuido a una relativamente rápida diversificación de las secuencias:
en cada locas duplicado hay un gen que sobra para la función de la copia original, por lo que puede divergir rápidamente debido a que no hay presión de selección para mantener su función.
En algunos casos estos genes divergentes pueden llegar a adquirir nuevas funciones que pueden ser una ventaja selectiva.
Sin embargo, en la mayoría de los casos cabría esperar que las secuencias adicionales acumularan mutaciones deletéreas y degeneraran hasta transformarse en pseudogenes no funcionales. 

Figura 9.2

La duplicación génica puede llevar a la adquisición de una nueva función o a la formación de un pseudogén.

La duplicación del gen A genera dos copias equivalentes del gen.
La presión de selección sólo se ejerce sobre una de las copias (la superior) a fin de mantener la presencia del producto génico funcional original.

La otra copia (parte inferior) continuará expresándose, pero ante la falta de presión de selección acumulará mutaciones (barras verticales) de una manera relativamente rápida. 
Puede sufrir mutaciones perjudiciales y transformarse en un pseudogén no funcional, que puede continuar expresándose al nivel del RNA durante algún tiempo, pero que a la larga será transcripcionalmente inactivo ( xxx A) Sin embargo, en algunos casos las diferencias debidas a las mutaciones acumuladas pueden llevar a la aparición de un patrón de expresión diferente o alguna otra propiedad que resulte una ventaja selectiva ( xxx ).
En caso de las duplicaciones génicas en tándem, los intercambios de secuencia entre las dos copias (debidos a mecanismos tales como el entrecruzamiento desigual, actuarán como un treno sobre la velocidad de divergencia de las secuencias de las dos copias génicas. 

La evolución del genoma humano puede haber pasado por duplicaciones de todo el genoma, pero la evidencia ha quedado oscurecida por subsecuentes reordenaciones de los cromosomas y de su DNA

La duplicación es una manera efectiva de aumentar el tamaño del genoma y es responsable de la extendida poliploidía de muchas plantas con flores contemporáneas. 
Puede ocurrir naturalmente cuando se da un fallo de la división celular posterior a la duplicación del DNA, de forma que la célula que sufre el fallo pasa a tener el doble de cromosomas que lo normal.
Las células somáticas humanas son normalmente diploides.
Sin embargo, si hay un fallo en la primera división zigótica se pueden obtener tetraploides constitucionales.
La tetraploidía puede ser dañina y suele ser seleccionada negativamente. 
Sin embargo, algunos animales, particularmente algunas especies de peces, son tetraploides naturales y por tanto resulta bastante posible que ciertas distantes células ancestrales sufrieran estos procesos de duplicación del genoma.
De esta forma, una célula inicialmente diploide podría haber pasado por un estado transitorio tetraploide;
subsecuentes inversiones, translocaciones y otras reordenaciones cromosómicas habrían provocado la divergencia de los cromosomas y restaurado la diploidía, pero ahora con dos veces el número inicial de cromosomas.

Si la evolución del genoma humano atravesó alguna remota tetraploidización, mucho tiene que haberse mezclado el DNA intragénico desde la última vez que sucedió.
Esto significa que la evidencia original de tetraploidización podría haber quedado oscurecida por subsecuentes inversiones, translocaciones y reordenaciones cromosómicas en general.
Además, los rastros de las duplicaciones génicas producto de la duplicación del genoma han quedado probablemente eliminados tras el silenciamiento de un miembro de cada par duplicado y su posterior degeneración a pseudogén.
Tras cientos de millones de años sin ejercer función alguna, los pseudogenes no procesados generados tras la posible última duplicación del genoma habrán divergido ya tanto de los genes originales que gran parte de su secuencia no será reconocida como relacionada con el gen funcional, incluso asumiendo que no se haya perdido durante reordenaciones ocasionales que provocaran delaciones de las regiones analizadas.

Utilizando técnicas de bandeo de baja resolución existen ciertos pares de cromosomas que exhiben aparentes similitudes en el patrón de bandas (por ejemplo los cromosomas 1 y 2), que inicialmente fueron tomadas como evidencias de una antigua tetraploidía.
Sin embargo, los análisis de alta resolución posteriores no apoyaron esta conclusión. 
Otra estrategia ha sido estudiar si genes no alélicos que tienen un parecido obvio tienen alguna regularidad en sus localizaciones cromosómicas.

Estos genes pueden tener secuencias parecidas (miembros de una familia génica) o funciones parecidas (participación en la misma vía metabólica, relaciones de receptor-ligando, subunidades de la misma proteína, etc.).
Esta estrategia ha llevado a la identificación en el genoma humano de varios grupos de cromosomas no homólogos pero relacionados, que representan los también llamados segmentos cromosómicos parálogos (ver Recuadro 9.1 ).

Probablemente la evidencia más importante de la existencia de segmentos parálogos proviene de la comparación de los cromosomas 12 y 17 ( xxx -la HS por Homo sapiens ) ( Figura 9.4 ).
Estos dos cromosomas contienen dos de los cuatro grupos de genes HOX , genes que contienen homeobox.
Se piensa que los genes homeobox desempeñan un papel muy importante en el establecimiento del eje ántero-posterior durante el desarrollo, y debido a su papel crucial durante el desarrollo embrionario habrían sido muy conservados durante la evolución.
Otros mamíferos poseen también cuatro grupos de genes HOX , y se piensa que el orden lineal de los genes en cada grupo dicta el orden temporal de su expresión durante el desarrollo, y también el límite anterior de su expresión a lo largo del eje ántero-posterior.
Los cuatro grupos son tan parecidos que permiten identificar claramente genes HOX parálogos , es decir, genes que están en todos los grupos pero que se parecen más entre ellos que a los genes vecinos del mismo grupo.
Amphioxus tiene un solo grupo de genes HOX , cuya organización es muy similar a la de los mamíferos(ver Figura 9.5 ).
Dado que se piensa que Amphioxus es el invertebrado más cercano a los vertebrados, es probable que el antecesor de los vertebrados tuviera un solo grupo de genes HOX.

Durante la evolución de los vertebrados se habrían producido o bien sucesivas duplicaciones del grupo HOX o bien una o dos rondas de tetraploidía que habrían contribuido a la formación de los cuatro grupos que observamos actualmente.
Los genes de la cadena a de la integrina no I (que participan en una variedad de interacciones intercelulares y entre las células y la matriz celular) se sitúan en posiciones similares a las de los genes HOX y podrían haber evolucionado con los grupos de genes HOX de forma coordinada, presumiblemente debido a su proximidad a los genes HOX en el grupo ancestral.

Figura 9.3

La duplicación del genoma puede llevar a un estado transitorio tetraploide antes de que la divergencia de cromosomas restablezca la diploidía. 

Tras la duplicación de un genoma diploide, cada par de cromosomas homólogos (por ejemplo el cromosoma 1) se transforma en un par de pares idénticos.
Sin embargo, del estado tetraploide resultante se puede volver a la diploidía debido a la divergencia entre cromosomas, por ejemplo a raíz de una delación intersticial (panel superior, a), por una delación terminal (panel inferior, c) o por una inversión (b).

Tabla 9.1

Paralogía, ortología y homología

Paralogía significa que existe un gran parecido entre las secuencias de DNA o los segmentos cromosómicos no alélicos considerados dentro de una misma especie, lo que indica que tienen una estrecha relación evolutiva que puede o no ser anterior a la especiación. 
Por ejemplo, los dos genes humanos de xxx -globina son parálogos.
En una familia de secuencias de DNA con más de dos miembros se pueden identificar diferentes grados de paralogía.
Por ejemplo, pese a que los 38 genes humanos HOX están claramente emparentados y en cierta medida pueden ser considerados parálogos, existe una relación estructural y funcional especialmente estrecha entre genes específicos de cada uno de los cuatro grupos. 
En consecuencia, en la familia de genes HOX el término parálogo se reserva por costumbre para referirse a los grupos de genes particularmente relacionados. 

Ortología significa que existe un gran parecido en las secuencias de DNA o los segmentos cromosómicos considerados de especies diferentes .
Por ejemplo, el gen principal determinante del sexo en humanos es SRY , y su ortólogo en ratón es Sry .
En familias génicas con miembros estrechamente relacionados puede ser difícil reconocer ortólogos verdaderos entre especies.

Homología el término se utilizó inicialmente para describir la relación entre segmentos cromosómicos alélicos.
Posteriormente ha sido cada vez más utilizado en sentido amplio para definir cualquier tipo de relación indicativa de un origen evolutivo común, sea dentro de una misma especie o entre especies diferentes.

Figura 9.4

xxx y xxx parecen tener segmentos cromosómicos parálogos.

En la figura se indican las posiciones aproximadas de algunos de los genes emparentados que se sitúan sobre los cromosomas humanos 12 y 17.
El tamaño de las barras marca el margen de incertidumbre sobre la posición exacta de cada miembro de un grupo de genes;
probablemente la mayoría de ellos estén agrupados en posiciones muy próximas.
Obsérvese que el gen de la aldehído deshidrogenasa, xxx , ha sido localizado de una manera muy laxa sobre el cromosoma 17, pero está claramente emparentado con el gen xxx situado en xxx .

Figura 9.5

La organización del grupo de genes HOX en los mamíferos y en Amphioxus sugiere la posibilidad de una o dos rondas de duplicaciones de un genoma ancestral.

Los grupos parálogos indicados están formados por genes con patrones de expresión muy parecidos y presumiblemente funciones similares.
En el momento de escribir este libro se habían aislado 10 genes HOX Amphioxus , el invertebrado considerado como el pariente más próximo a los vertebrados.
Se supone que los genes equivalentes de Drosophila se organizaron en un único grupo en un genoma ancestral, antes de una translocación que generó los grupos Ultrabithorax (Ubx) y Antennapedia ( Antp ).
El antecesor de los vertebrados tenia probablemente 13 genes HOX , pero la pérdida de genes individuales tras la duplicación de los grupos ha llevado a que cada uno de los grupos HOX de mamíferos carezca de uno o más de los genes originales.
Los recuadros rosas indican los grupos de genes con homeoboxes claramente relacionados.

Durante la evolución de los genomas de los mamíferos tuvieron lugar numerosas e importantes reordenaciones cromosómicas

Además de la duplicación del genoma existen varios mecanismos posibles de duplicación del DNA subgenómico, que resultan en el intercambio entre cromosomas no homólogos (translocaciones), en los intercambios desiguales entre homólogos o entre las cromátidas hermanas de un mismo cromosoma y de reacciones de transposición con copia del DNA.
Algunos de estos mismos mecanismos pueden producir también la pérdida de material genético.
Además, otros mecanismos (inversiones cromosómicas, transposiciones no duplicativas del DNA y translocaciones equilibradas) pueden no generar ni pérdida ni ganancia de material.
La evolución en el tiempo del genoma de los mamíferos parece haber atravesado sucesivas duplicaciones subgenómicas y también reordenaciones sin producirse pérdida neta de DNA.
Por ejemplo, se tienen buenas evidencias de la existencia de grandes regiones parálogas sobre los dos brazos del cromosoma 1 humano, y en menor medida sobre los diferentes brazos de otros cromosomas (ver Figura 9.6 ).
Las supuestas duplicaciones de gran escala podrían haber sido el fruto de duplicaciones de cromosomas completos por fusión robertsoniana o por duplicaciones subcromosómicas seguidas de inversiones pericéntricas .

Las comparaciones entre los genomas contemporáneos de la especie humana y de otros mamíferos sugieren también que las reordenaciones de gran escala han sido fenómenos frecuentes, y que la evolución fenotípica y el cariotipo podrían no estar acoplados.
Por ejemplo, el muflón de la India ( Muntiacus Muntjak ) tiene sólo tres tipos de cromosomas, muy grandes, mientras que su pariente más cercano, el muflón chino ( Muntiacus reevesi ), tiene 23 cromosomas diferentes.
El cariograma humano es bastante diferente al del ratón, e incluso el grupo de ligamiento del cromosoma X, muy conservado, muestra señales claras de numerosas diferencias organizativas entre las dos especies (ver Figura 9.10 ). 
Los grandes simios son extremadamente cercanos a los humanos, pero presentan diferencias citogenéticas claras que son consecuencia de varias inversiones, de una translocación exclusiva del linaje humano y de otra exclusiva del linaje de los gorilas.
Los monos del viejo mundo también son parientes cercanos de los humanos pero, exceptuando los gibones, han sufrido numerosas reordenaciones cromosómicas desde su separación del linaje humano.

Figura 9.6

El mapa de genes del cromosoma l sugiere la presencia de grandes regiones duplicadas en ambos brazos.

Las barras verticales señalan las posiciones cromosómicas aproximadas de los genes situados en el mapa. 
Los duplicados son los siguientes:
TRN, xxx y TRNL , parecido a xxx (del inglés TRN-Like );
TRN, xxx y xxx , parecido a xxx , RNA pequeño nuclear xxx y xxx , pseudogenes 1-4 del RNA pequeño nuclear xxx ;
xxx , adenilato quinasa 2 y xxx , guanilato quinasa 1; 
xxx , secuencias 1 y 2 parecidas a la glutámico-oxalacético transaminasa 2;
xxx , secuencias 1 y 2 parecidas a la cadena pesada de la ferritina;
xxx y xxx , factores de coagulación III y V;
xxx , proteínas del complemento xxx y finalmente xxx , proteína de unión a xxx del complemento, receptores 1/2 de componente del complemento.

El Proyecto Genoma Humano

Historia, organización y objetivos del Proyecto Genoma Humano

El Proyecto Genoma Humano es un proyecto internacional cuyo objetivo final es obtener una descripción completa del genoma humano a través de la secuenciación del DNA.
Una vez que se ha completado ya la secuenciación del genoma mitocondrial humano, el genoma que se investiga es el genoma nuclear.

Dada la escala del esfuerzo que el proyecto supone, representa lo que podríamos considerar el primer proyecto de "gran ciencia" en biología.

El Proyecto Genoma Humano fue concebido a partir de la necesidad de un proyecto de gran escala para desarrollar nuevos métodos de detección de mutaciones

Un catalizador importante del desarrollo del Proyecto Genoma fue un workshop que tuvo lugar en Alta, Utah, en diciembre de 1984.
Apoyado parcialmente por el Departamento de Energía norteamericano (DOE), el workshop iba dirigido a evaluar la situación de las técnicas de detección y caracterización de mutaciones, y a proyectar direcciones futuras para superar las limitaciones del momento.
En él se discutieron los papeles crecientes de las nuevas tecnologías de DNA, fundamentalmente las emergentes metodologías relacionadas con la clonación y la secuenciación de genes.

A pesar de que habían estado utilizándose durante una década, los esfuerzos de los laboratorios individuales de intentar clonar y caracterizar genes uno por uno se consideraron una pérdida relativa de los recursos y tiempo de los científicos.

Debido a la percepción de la existencia de tantos obstáculos técnicos se llegó a la conclusión de que los métodos serían incapaces de medir mutaciones con sensibilidad suficiente, a no ser que se pusiera en marcha un programa enormemente grande, complejo y caro.
Un informe posterior en Technologies for Detecting Heritable Mutations in Human Beings (tecnologías para la detección de mutaciones heredables en seres humanos) originó la idea de un proyecto genoma humano por la DOE, y en marzo de 1986 esta agencia respaldó una reunión internacional en Santa Fe, Nuevo México, para evaluar la factibilidad y la deseabilidad de ordenar y secuenciar clones de DNA que representaran el genoma humano en su totalidad.
Prácticamente todos los participantes concluyeron que tal proyecto sería factible y que constituiría un logro extraordinario para la biología.

Tras innumerables discusiones con la comunidad científica estadounidense, la DOE respondió a la reunión de Santa Fe con la elaboración de un informe sobre la Iniciativa del Genoma Humano ( Report on the Human Genome Initiative ) en la primavera del año 1987.
- generación de mapas físicos refinados de los cromosomas humanos;
- desarrollo de tecnologías de apoyo y de instalaciones para la investigación en genoma humano;
- expansión de redes de comunicación y de capacidad computacional y de almacenamiento de información en bases de datos
.
Una vez que se comenzó la implementación de este programa en un pequeño número de proyectos piloto, diversas organizaciones de los Estados Unidos iniciaron sus propios estudios de política y estrategia.
En el año 1988 aparecieron dos informes muy difundidos emitidos por la US Office of Technology Assessment y por el National Research Council, y los National Institutes of Health (NIH) americanos crearon la Office of Human Genome Research (que más tarde sería renombrada National Center for Human Genome Research) para coordinar todas las actividades que estuvieran relacionadas con el genoma dependientes del NIH en cooperación con otras organizaciones gubernamentales americanas. 
El mismo año el congreso de los Estados Unidos aprobó oficialmente un proyecto genoma humano americano de 15 años de duración, a comenzar en 1991.
El presupuesto pedido para este proyecto fue estimado en alrededor de 3000 millones de dólares.

Organización del Proyecto Genoma Humano

El Proyecto Genoma Humano americano sigue siendo el principal soporte de la investigación que se realiza a nivel internacional en esta área.
La coordinación de este proyecto ha sido confiada a la DOE (a través de la Office of Energy Research, de la Office of Health and Environmental Research y de la Office of Human Genome Program) y al US Department of Health and Human Services [Public Health Service, National Institutes of Health (NIH) y National Center for Human Genome Research]. 
Los NIH tenían un interés natural muy claro en el Proyecto Genoma Humano, al haber sido uno de los principales apoyos para la investigación en genética y biología molecular.
Además, se consideraba que la DOE tenía un papel sumamente importante a desempeñar, una especie de continuación de su programa tradicional de investigación genética dirigido a mejorar su capacidad de valorar los efectos de la radiación y de agentes químicos relacionados con la energía sobre la salud humana.
Reconociendo la complementariedad de las actividades de las dos instituciones, la DOE y los NIH acordaron coordinar sus actividades individuales relacionadas con el genoma.
La actividad dependiente de los NIH se canaliza fundamentalmente a través del National Center for Human Genome Research, mientras que las actividades de la DOE quedan representadas fundamentalmente por programas pluridisciplinarios que están siendo llevados a cabo en el Lawrence Berkeley National Laboratory, en Los Alamos National Laboratory y en el Lawrence Livermore National Laboratory.

El Proyecto Genoma Humano es actualmente un verdadero proyecto internacional, y existen importantes programas ya establecidos en el Reino Unido, en Francia y en Japón.
Siguiendo el ejemplo de los Estados Unidos existen países europeos como Dinamarca, Francia, Alemania, Italia, Holanda, el Reino Unido y algunos países de la antigua URSS que han establecido programas nacionales, al igual que la Unión Europea a nivel transnacional.
Además, los países nórdicos han propuesto una iniciativa de cooperación en la investigación del genoma que les permitirá contribuir de manera conjunta al esfuerzo de cartografiado del genoma humano.

Fuera de Europa y los Estados Unidos se han establecido proyectos genoma nacionales en Australia, Canadá, Japón, Corea y Nueva Zelanda.
Se ha creado la Organización del Genoma Humano ( HUGO , de Human Genome Organization ), que coordina los diferentes esfuerzos que se están realizando a nivel nacional, facilita el intercambio de recursos de investigación, estimula el debate público v ofrece consejo sobre las implicaciones de la investigación en genoma humano (McKusick, 1989).

Concebida en 1988, la HUGO se administra actualmente a través de tres centros: HUGO Europa (con sede en Londres), HUGO América (con sede en Bethesda, EUU) y HUGO Pacífico (Tokio).

La secuencia completa de nucleótidos del genoma humano es sólo uno de los varios objetivos del Proyecto Genoma Humano

La principal justificación del proyecto Genoma Humano es la adquisición de información fundamental relativa a nuestra constitución genética, que pueda aumentar nuestra comprensión científica básica sobre la genética humana v el papel de varios genes en la salud y en la enfermedad.
El principal estimulo científico que ofrece el Proyecto Genoma Humano tiene relación con la construcción de mapas genéticos y físicos de alta resolución del genoma humano, un preludio del mapa físico definitivo, el de la secuencia completa del genoma humano.
Gran parte de este trabajo se está realizando en unos pocos centros principales dedicados al cartografiado. 
También existe una extensa colaboración con la investigación centrada en el cartografiado de genes de enfermedades, que se realiza actualmente en numerosos laboratorios distribuidos por todo el mundo (ver Figura 13.1 ).
Los principales objetivos de estos 15 años incluyen un compromiso para obtener mapas y secuencias completas de los genomas de diversos organismos modelo, y desarrollar técnicas auxiliares incluyendo análisis de datos (ver Tabla 13.1 ).

También se está dedicando un presupuesto considerable a los aspectos éticos, legales y sociales, y al apoyo de la transferencia de tecnología a la comunidad médica.

Al comenzarse el proyecto Genoma Humano se reconoció claramente que la tecnología de secuenciación disponible en aquel momento no podría permitir cumplir con el objetivo principal (obtener la secuencia completa del genoma humano en 15 años) con las limitaciones presupuestarias que se impusieron. 
Las mejoras en la tecnología han sido graduales, no espectaculares, de forma que los costos de la secuenciación del DNA han ido cayendo progresivamente, pero sin aumentar enormemente la eficiencia de la técnica.
En el momento de escribirse este libro las regiones más largas de secuencia del DNA humano que estaban a punto de publicarse tenían menos de 0,2 Mb, con la sola excepción de una secuencia en el cluster del gen TCR B.
A pesar de ello en organismos modelo se han podido secuenciar segmentos mucho más largos.
Como consecuencia de las limitaciones tecnológicas percibidos el énfasis inicial del provecto ha sido puesto en la construcción de mapas genéticos y físicos de alta resolución, más que en la secuenciación a gran escala.
No se espera que se encare este último aspecto hasta que se disponga de un número significativo de cóntigos que abarquen cromosomas completos.
Mientras tanto, el progreso de los provectos de secuenciación a gran escala del genoma de organismos modelo (véase más adelante) ha sido escudriñado cuidadosamente a fin de obtener beneficios de su experiencia antes de comenzar la secuenciación a gran escala del genoma humano.
Como consecuencia de la experiencia de la secuenciación de los genomas modelo se espera con cierto optimismo poder lograr la secuenciación completa del genoma humano mucho antes de la fecha objetivo inicial del año 2005.

Figura 13.1

Principales estrategias científicas utilizadas en el Proyecto Genoma Humano.

El principal impulso científico del Proyecto Genoma Humano comienza con el aislamiento de clones genómicos y de cDNA humanos (utilizando clonación basada en sistemas celulares o en PCR).
Estos clones se utilizan para construir mapas genéticos y físicos de alta resolución, como paso previo a la obtención del mapa físico definitivo, la secuencia completa de nucleótidos de las 3000 Mb del genoma nuclear.
Es inevitable que el proyecto acabe interactuando con la investigación sobre el cartografiado y la investigación de genes de enfermedades humanas.
Además existen proyectos auxiliares, como el estudio de la variabilidad genética (el Proyecto Diversidad del Genoma Humano); los proyectos genoma de organismos modelo y la investigación sobre las implicaciones legales, éticas y sociales del proyecto.
Los datos que se producen son canalizados a bases de datos de mapa y secuencia, lo que permite el rápido acceso y análisis de datos por vía electrónica.
EST: etiqueta de secuencia expresada; STS, sitio de secuencia etiquetada.

Tabla 13.1

Objetivos a 15 años del Proyecto Genoma Humano

Construcción de un mapa genético de alta resolución del genoma humano.


Producción de varios mapas físicos de todos los cromosomas humanos, y del DNA de organismos modelo seleccionados, con éntasis en mapas que hagan el DNA accesible para un mayor análisis por parte de los investigadores.


Determinación de la secuencia completa del DNA humano y del DNA de los organismos modelo seleccionados.


Desarrollo de la capacidad para colectar almacenar distribuir y analizar los datos que se produzcan.


Creación de las tecnologías apropiadas necesarias para cumplir estos objetivos.


Mapas genéticos humanos

El primer mapa genético humano fue publicado en 1987, y se basaba en marcadores RFLP

Desde hace décadas que se dispone de mapas genéticos clásicos de organismos experimentales como Drosophila y ratón, mapas que además se han ido refinando continuamente.
Están construidos cruzando diferentes mutantes a fin de determinar si dos loci génicos estaban ligados o no.
Durante la mayor parte de este período los genetistas humanos habían sido meros espectadores envidiosos, ya que la idea .le construir un mapa genético humano se consideraba en general inviable. 
A diferencia de los organismos experimentales, el mapa genético humano no podía jamás basarse en genes debido a que la frecuencia de apareamiento entre dos individuos que sufren de anomalías genéticas distintas es extremadamente pequeña.
Todo lo que se podía hacer era establecer un mapa genético basado en marcadores polimórficos , que no necesariamente debían estar relacionados con enfermedades o con genes. 
El mapa genético humano era posible de obtener en la medida que los marcadores presentaran segregación mendeliana y fueran suficientemente polimórficos de forma que se pudieran registrar los recombinantes en un número razonable de meiosis.
El problema era que hasta hace poco no se disponía de marcadores polimórficos adecuados. 

Los marcadores genéticos humanos clásicos eran polimorfismos proteicas, principalmente el grupo sanguíneo y marcadores de proteínas séricas.
Ambos polimorfismos son poco frecuentes y no muy informativos.
Hacia 1981 se habían podido obtener mapas de ligamiento muy parciales, y sólo en el caso de algunos cromosomas. 
Sin embargo, la identificación de polimorfismos basados en DNA llevó a una revisión radical de las ideas, y durante la primera parte de la década de los 80 se oyeron serias discusiones acerca de la posibilidad de construir un mapa genético humano por primera vez.
Botstein et al. (1980) argumentaban que era posible construir un mapa de ligamiento completo del genoma humano empleando RFLP.
Era claro que la obtención de un mapa de ligamiento completo del genoma humano era un objetivo muy deseable.

Además de proporcionar un marco de trabajo para el estudio de la naturaleza de la recombinación en humanos, otras áreas podrían beneficiarse:
- Localización de genes : cualquier gen para el que pudiera tipificarse un poli morfismo podía situarse inmediatamente sobre el mapa genético, y obtenerse su localización cromosómica.
Desde el punto de vista médico, esto era muy atrayente:
los análisis de ligamiento podrían emplearse en familias en las que el gen de una enfermedad estuviera segregando, lo cual permitiría situar en el mapa, por primera vez, a los genes causantes de muchas condiciones hereditarias.

- Clonación de genes : la localización génica proporcionaría puntos de partida para los esfuerzos de clonación de genes (ver clonación posicional ).

- Diagnóstico : el diagnóstico prenatal o presintomático de los genes de enfermedades hereditarias mediante el análisis de ligamiento se vería enormemente facilitado si se dispusiera de muchos marcadores de DNA en los alrededores del locas de la enfermedad.


Era casi inevitable que la toma de conciencia de que se podía obtener un mapa genético humano completo disparara serios esfuerzos para construir uno.
En 1987 se publicó el primero de estos mapas, basado en el empleo de 403 loci polimórficos, incluyendo 393 marcadores RFLP. 
La empresa fue mayúscula:
el estudio requirió la tipificación de todos y cada uno de los miembros de 21 familias de tres generaciones para cada locas polimórfico, y el análisis posterior de los datos de ligamiento por ordenador.
Los resultados arrojaron 23 grupos de ligamiento (segmentos lineales de marcadores ligados) que corresponden a los 22 autosomas y al cromosoma X (es posible construir un mapa genético de la región pseudoautosómica del cromosoma Y pero no del resto, ya que no participa en procesos de recombinación.
A pesar de la importancia que tuvo este logro existían serias limitaciones respecto a lo que podría considerarse por comparación lo deseable en un mapa genético idealizado (ver Tabla 13.2 ).
La distancia media entre marcadores ( xxx 10 cM) era todavía considerable.
Más importante era la principal limitación, que e] mapa estaba basado en marcadores RFLP, no muy informativos y de difícil tipificación (ver Recuadro 12.1 ).

Tabla 13.2

Hacia un mapa genético ideal

Densidad de marcadores
El espacio medio entre marcadores polimórficos debe ser considerablemente inferior a 1 cM, de forma que prácticamente todas las regiones del genoma contengan un marcador inmediatamente vecino.

Tipificación de marcadores
Cada marcador debe ser de sencilla tipificación, y la información y las instalaciones para la tipificación deben ser de fácil disponibilidad.

Utilidad de marcadores
La heterozigosis de cada marcador debe ser alta, preferentemente cercana a 1,0.
Esto implicará que cada marcador será informativo en un gran porcentaje de meiosis diferentes.

Orden de marcadores
La utilidad del mapa genético depende claramente de la confianza en que el orden secuencial de marcadores de un grupo de ligamiento es el correcto.
Los análisis de ligamiento genético no pueden proporcionar una certeza absoluta en relación al orden de los marcadores.
En lugar de ello hace falta realizar un estudio estadístico que dé una probabilidad relativa de cierto orden.
La confianza en el orden de marcadores depende de hallar la mayor cantidad posible de recombinantes. 
Para un mapa de alta resolución, esto significa evaluar un gran número de meiosis informativas.

Familias de referencia
Debe utilizarse un único grupo de familias de referencia;
las muestras provenientes de cada miembro de una familia de referencia utilizadas para la construcción del mapa deben estar disponibles, de manera que diferentes laboratorios puedan colocar nuevos marcadores sobre el mapa y continuar mejorándolo.

Utilidad del mapa
Los nuevos marcadores deben ser integrados en un único mapa cuya resolución aumente continuamente;
debe evitarse el empleo de mapas genéticos diferentes basados en grupos de marcadores distintos.

Recientemente se han podido obtener mapas genéticos humanos de alta resolución, fundamentalmente gracias al empleo de marcadores microsatélites

El siguiente hito en la construcción de mapas genéticos humanos sino de la mano del empleo de marcadores mucho más polimórficos que ]os marcadores RFLP.
Los polimorfismos del DNA minisatélite hipervariable eran una alternativa, pero limitada.
Esto se debe a que su distribución no es general a lo largo del genoma, sino que tienen una tendencia marcada a aparecer cerca de los telómeros.

Los marcadores microsatélites también denominados STRP , de las iniciales inglesas de Short Tandem Repeat Polymorphisms ( polimorfismos cortos repetidos en tándem ), tienen la ventaja de ser abundantes, estar disperses a lo largo del genoma, ser muy informativos y fáciles de tipificar.
Centrándose en este tipo de marcadores, los investigadores del laboratorio Généthon en Francia fueron capaces de generar rápidamente un mapa de ligamiento de segunda generación del genoma humano. 
Este tour de force supuso la selección de repeticiones CA/TG adecuadamente polimórficas, su adscripción a cromosomas específicos a través de la tipificación de paneles de híbridos de células somáticas humano-roedor y la realización de un análisis estadístico del ligamiento entre marcadores de cromosomas individuales.
Se emplearon 813 marcadores de los cuales 605 presentaban una heterozigosis superior a 0,7.
Estos marcadores fueron organizados en 23 grupos de ligamiento.

El importante avance del laboratorio Généthon fue posible gracias al soporte económico aportado por la organización francesa de la distrofia muscular, la Association Française Myopathique ( AFM ) que permitió una estrategia de investigación tipo factoría, empleando métodos automatizados de gran escala.
Con posterioridad se han producido mapas con un número de marcadores genéticos superior, especialmente marcadores microsatélites, y con una resolución aún mayor.
Se han construido varios mapas genéticos diferentes (basados en diferentes conjuntos de marcadores).
El problema con esto es doble. 
En primer lugar, no todos los mapas genéticos han utilizado las mismas familias de referencia.
En segundo lugar, los diferentes mapas genéticos se basan en grupos de marcadores distintos, y las relaciones de ligamiento que existen cuatro marcadores empleados en mapas diferentes no tienen por qué ser conocidas.

Habiendo detectado estas limitaciones la comunidad investigadora internacional reconoce cada vez más la importancia de la colaboración:

- Familias de referencia.
El grupo de familias de referencia más ampliamente distribuido es el que deriva del CEPH ( Centre d'Études du Polymorphisme Humaine ), una colaboración de más de 100 laboratorios independientes.

- Integración de mapas.
El estudio de las relaciones de ligamiento entre los marcadores utilizados en los diferentes mapas genéticos proporciona un mapa integrado. 
El mapa publicado por Murray et al. en 1994 es un ejemplo, ya que presenta unas 4000 STRP (en su mayoría provenientes del laboratorio Généthon) más unos 1800 marcadores adicionales, fundamentalmente RFLP, que en su conjunto aportan una densidad promedio de marcadores de uno por cada 0,7 cM.


La resolución del mapa de Murray et al. (1994) era considerablemente superior a la resolución media de 2-5 cM que se deseaba obtener al cabo de cinco años del inicio del proyecto.
Sin embargo, será difícil avanzar más en la resolución de los mapas debido al poco éxito obtenido en la identificación de nuevos marcadores microsatélites;
a mentido ocurre que marcadores microsatélites recientemente descubiertos acaban siendo idénticos a otros previamente caracterizados. 
Además, existe una percepción generalizada de que la resolución de marcadores actual es suficiente p ara tener el marco de trabajo necesario para construir un mapa físico completo del genoma humano.
El mapa genético publicado por Dib et al. en 1996 representa una de las ultimas versiones del mapa de Généthon, y el esfuerzo principal ha sido recientemente reconducido hacia la construcción de mapas físicos de alta resolución del genoma humano.

Mapas físicos del genoma humano 

Se están construyendo varios mapas físicos diferentes del genoma humano

Al igual que el mapa genético, un mapa físico del genoma humano estará formado por 24 submapas que corresponden a los 24 tipos de cromosomas diferentes.
Los mapas genéticos del genoma humano que se han compuesto hasta el momento representan todos el mismo concepto: conjuntos de marcadores polimórficos ligados (grupos de ligamiento) que corresponden a los diferentes cromosomas.
Sin embargo, en los mapas físicos existe menos uniformidad, en el sentido que se puede construir varios tipos de mapa físico.
En cierto sentido el primer mapa físico del genoma humano se obtuvo cuando las técnicas de bandeo citogenético fueron empleadas no sólo para diferenciar a los cromosomas, sino también para poder diferenciar las regiones subcromosómicas entre sí. 
Pese a que la resolución es muy baja (una banda cromosómica de una preparación de 550 bandas contiene unas 6 Mb de DNA), los mapas de esta clase han sido de gran utilidad como armazón para ordenar las secuencias de DNA humano mediante técnicas de hibridación in situ de cromosomas.

Existe otro grupo de mapas que se han obtenido cartografiando puntos de rotura cromosómica naturales (empleando paneles de híbridos de células somáticas que contienen fragmentos derivados de cromosomas de translocación y deleción;
o bien cartografiando los puntos de rotura artificial utilizando híbridos de radiación.
Sin embargo, aquí también hay limitaciones de resolución, ya que la distancia media entre las roturas vecinas en estos paneles de células híbridas puede ser bastante elevada para grandes partes del genoma.
En consecuencia, es deseable poder disponer de mapas físicos de resolución más elevada.
Está claro que el mapa físico definitivo, el de mayor resolución, será el que llegue al nivel del par de bases individual, es decir, el que contenga la secuencia completa de los nucleótidos que forman el genoma humano.
Dado que esto no se logrará hasta dentro de un cierto tiempo existen esfuerzos para la construcción de mapas físicos de resolución intermedia.
En algunos pocos cromosomas humanos se ha logrado obtener mapas completos de RH y de enzimas de restricción de corte infrecuente. 
Un ejemplo es el cromosoma 21, del que se dispone de un mapa de restricción Notl para el brazo largo completo.

Además, gran parte del esfuerzo dedicado en la actualidad al cartografiado físico va dirigido a la obtención de mapas de las secuencias de DNA codificante, con lo cual lo que se está produciendo son mapas de transcripción exhaustivos.

Tabla 13.3

El genoma nuclear humano puede cartografiarse utilizando diferentes tipos de mapas físicos 

Tipo de mapa.
Ejemplos/metodología .
Resolución.


Citogenético .
Mapas de bandas cromosómicas.
Una banda media contiene varias Mb de DNA.


Mapas de puntos de rotura cromosómicos.
Paneles de híbridos de células somáticas que contienen fragmentos de cromosomas humanos derivados de cromosomas con translocaciones o deleciones naturales.
Mapas de híbridos de radiación (RH) monocromosómicos. .
Mapas de RH del genoma completo.
La distancia entre los puntos de rotura adyacentes en un cromosoma suele ser de varias Mb.
La distancia entre los puntos de rotura suele ser de muchas Mb.
La resolución puede llegar a 0,5 Mb.


Mapa de restricción .
Mapas de restricción de rare cutters , o enzimas de corte infrecuente, p. ej. mapas Notl .
varios centenares de kb.


Mapa de cóntigos.
Clones YAC solapantes.
Clones cosmídicos solapantes.
El tamaño medio de un inserto en YAC es de varios cientos de kb.
El tamaño medio de un inserto en un clon cosmídico es 40 kb.


Mapa de sitios de secuencia etiquetada (mapa de STS).
Requiere disponer de información previa sobre la secuencia de clones ya ordenados, de manera que se puedan ordenar los STS.
Un objetivo es tener un espaciado promedio de 100 kb.


Mapa de etiquetas de secuencia expresada (mapa de ETS).
Requiere la secuenciación de los cDNA y la localización posterior del cDNA sobre otros mapas físicos.
El menor espaciado promedio posible es aproximadamente 40 kb.


Mapa de secuencia de DNA.
Secuencia completa de nucleótidos del DNA cromosómico.
1 bp .


Figura 13.2

Se están construyendo varios tipos de mapas físicos de los cromosomas humanos.

La figura muestra la integración de varios mapas físicos del brazo largo del cromosoma 21 humano. 
Al lado del mapa citogenético estándar, sobre la izquierda, se han representado las posiciones de las roturas del cromosoma 21 observadas a partir del estudio, fundamentalmente, de translocaciones del 21 (6;21. 4;21, 3;21, 2;21, etc.).
El mapa de STS se muestra dos veces para facilitar las comparaciones con los otros mapas, entre los que se incluyen el mapa de ligamiento genético de Chumakov et al. (1992), el mapa de restricción Notl basado en PFGE de Ichikawa et al. (1993) y el mapa de híbridos de radiación (medido en centiRays tras una exposición a 8000 red). 
Reproducido de Nature , vol. 359, página 385 con autorización. xxx 1992 Nature, Macmillan Magazines Limited.


LA NEUROPATÍA HEREDITARIA ÓPTICA DE LEBER COMO MODELO DE ENFERMEDAD MITOCONDRIAL

INTRODUCCIÓN

En la evolución de la Genética como ciencia se han desarrollado una serie de preceptos iniciales que posteriormente han tenido que ser modificados o ampliados con el advenimiento de nuevos conocimientos.

Así por ejemplo la clásica herencia mendeliana que fue base teórica sustentadora de la transmisión de caracteres hereditarios durante varios lustros, se hizo insuficiente para explicar numerosas heredopatías que se fueron conociendo. 

Existe un grupo particular dentro de las patologías heredadas que se caracteriza por su herencia materna, o sea que aunque los pacientes afectos pueden ser de ambos sexos, siempre es la madre la que transmite los caracteres y ningún varón afecto lo hace.
Este hecho se debe la presencia en el óvulo materno de gran número de mitocondrias con ADN mitocondrial (ADNmit.).
El espermatozoide por su parte tiene un pequeño número de estos orgánelos que no llega al citoplasma del cigoto.
Así la información procedente del ADN de las mitocondrias de la madre es la que se expresa en el individuo.
En estas circunstancias las leyes de segregación enunciadas y vigentes desde hace más de dos siglos no explican el mecanismo adecuado.

Desde el año 1963 se conoce de la existencia de ADN dentro de la mitocondria, sin embargo no es hasta la década de los años 70 en que "el otro genoma humano" se comienza a considerar como posible responsable de enfermedades genéticas humanas que tendrían un patrón de herencia materna .

Una gran variedad de enfermedades degenerativas, que involucran cerebro, corazón, músculos, riñón, glándulas endocrinas, sistema nervioso periférico, etc., resultan de mutaciones del ADNmit.
La primera de estas mutaciones fue descubierta en 1988 por Wallace y su equipo de trabajo y produce precisamente un fenotipo de neuropatía hereditaria óptica de Leber.
Subsecuentes mutaciones patógenas descritas en este ADNmit. se asocian a síndromes raros como MELAS ( Mitocondrial encephalopathy lactic acidosis and stroke-like episodes ), MERRF ( Myoclonus epilepsy with ragged red fibres ), Kearns-Sayre, etc .

La Neuropatía Hereditaria Óptica de Leber (NHOL) fue descrita por primera vez en 1858 por Von Graefe y fue nombrada así en reconocimiento a Theodore Leber por su investigación publicada en los Archivos Von Graefe de Oftalmología en 1871, con una descripción detallada de la enfermedad.

Esencialmente esta enfermedad se caracteriza por su comienzo temprano, en la segunda o tercera década de vida como una forma de neuropatía óptica subaguda, con pérdida de la visión grave y rápida asociada a escotoma cecocentral en ambos ojos, presentando particularidades que obedecen a su mecanismo de producción mitocondrial. 

En nuestro país existen antecedentes sobre el estudio de la NHOL , lo que unido a las interesantes peculiaridades de esta entidad motivan que realicemos esta revisión.

Enfermedades Mitocondriales

Las enfermedades mitocondriales constituyen un diverso grupo de desórdenes producidos por desarreglos genéticos, estructurales o bioquímicos de la mitocondria.
Dado que una disfunción mitocondrial puede afectar virtualmente todos los órganos, los especialistas de diversas ramas de las Ciencias Médicas se ven en contacto con estas entidades. 

El conocimiento del papel del ADNmit. en algunas enfermedades ha evolucionado rápidamente desde 1988, cuando fueron descubiertas sus primeras mutaciones.

Estas fueron subsecuentemente identificadas en una variedad de enfermedades y el papel patogénico del daño acumulativo del ADNmit. ha sido explorado en muchas enfermedades comunes que se desarrollan en etapas tardías de la vida o en el proceso de crecimiento.

Las mutaciones del ADNmit. se dividen en dos grandes grupos, sustitución y delección - inserción de bases .
Estas mutaciones producen alteraciones en el producto génico ya sea por un cambio de amino ácidos (a.a.) que afectan la proteína codificada por una secuencia de este ADNmit. o por alteraciones de la síntesis de las proteínas por dañar los ARNt o ARNr .

Se reporta que ninguna de las delecciones impide los orígenes de la replicación de ambas cadenas.
Las inserciones pueden causar duplicación de varias regiones del ADNmit. .

Estructura y función del ADNmit.

Es esencial para entender la enfermedad mitocondrial examinar los rasgos generales del ADNmit.
La mitocondria genera energía para la célula mediante la producción de ATP por fosforilación oxidativa.
Este orgánelo contiene su propio ADN conocido como " el otro genoma humano ", es de doble cadena, circular, codifica para 13 subunidades proteicas de cuatro complejos bioquímicos y para 24 ARN ( los ribosomales xxx y xxx y 22 de transferencia) que se requieren para el traslado intramitocondrial de las unidades de codificación de las proteínas.
Esta molécula tiene 16569 pares de bases y su genoma está completamente mapeado.
En todos los genes de este ADN han sido halladas mutaciones.

La mitocondria, que probablemente se desarrolló como orgánelo a partir de organismos independientes que se convirtieron en parte de la célula, es capaz de replicar, transcribir y traducir su ADN independientemente del ADN nuclear, sin embargo la función nuclear y la mitocondrial son interdependientes. 
Por una parte el ADN nuclear codifica para subunidades proteicas de la fosforilación oxidativa y gran cantidad de componentes macromoleculares requeridos en la estructura y función mitocondrial.
Las proteínas codificadas y el ARNmit. procedentes del núcleo deben ser importados a la mitocondria.
Por otra parte durante la fosforilación, la energía de la célula (ATP) se obtiene mediante un gradiente electroquímico;
este proceso depende de los ARNt mitocondriales que están codificados por el ADNmit..

Existen varios rasgos del ADNmit. que pueden estar relacionados a su frecuente asociación con mutaciones y enfermedades. 
El ADNmit. muta con 10 veces más facilidad que el nuclear y no tiene intrones, así que una mutación al azar lesiona con frecuencia una secuencia codificante.

Adicionalmente, esta molécula no tiene mecanismos de reparación efectivos, ni histonas protectoras y está expuesta a los radicales libres de oxígeno liberados por la fosforilación.

El ADNmit. es heredado por la vía materna y no recombina. 
Aquí la mutación se acumula secuencialmente a través de líneas maternas.
Cada mitocondria tiene de dos a diez ADN y cada célula tiene múltiples mitocondrias.
Así en una célula pueden coexistir mitocondrias normales y mutantes.
Esta condición se llama Heteroplasmia y permite que persista una mutación que de otra manera sería letal.
Por otra parte la Homoplasmia es la presencia total de ADNmit. normal o mutado. 
A través del proceso de segregación replicativa, las proporciones de moléculas mutantes y normales pueden ser cambiadas al segregarse a las células hijas.
Así más que las leyes de Mendel son las de la genética poblacional las que gobiernan el ADNmit..

La proporción de ADNmit. mutante requerido para que ocurra un fenotipo deletéreo se somete al "efecto umbral", que varía entre personas, entre sistemas de órganos y aún dentro de los tejidos.
Este efecto depende del delicado balance entre el suministro oxidativo y su demanda. 

Neuropatía Óptica Hereditaria de Leber (NOHL)

La herencia de la NHOL es mitocondrial.
Leber reconoció los dos hallazgos inusuales del patrón de la enfermedad, la transmisión materna y la propensión a afectar varones.
Imai y Mariwceki sugieren en 1936 la herencia citoplasmática y Erikson en 1972 sugiere una posible mutación de ADNmit.
En 1988 Wallace y cols. identificaron una mutación en el par de bases 11778 del ADNmit. que es exclusivo de familias con NOHL.

Subsecuentemente se han reportado varias mutaciones nuevas. 
La prevalencia de la enfermedad es aproximada a 1 en 50,000, comportándose de manera similar a la de la Atrofia Óptica Dominante (tipo Kjer).

La evaluación de la patogenicidad del ADNmit. mutado en NHOL es de extrema importancia para determinar el grado de afección que produce cada mutación.

Todas las mutaciones identificadas como causantes de la enfermedad son puntuales y se detectan por amplificación de la región relevante del ADNmit. y digestión con endonucleasas para las que la mutación causa pérdida o ganancia de un sitio de corte.

Criterios de Patogenicidad de las mutaciones en NOV

En 1995 Riordan-Eva y Harding enunciaron los siguientes criterios aplicados para establecer patogenicidad de una mutación individual:

- Identificar la mutación en un número de familias conocidas de NHOL, preferiblemente de origen independiente con pedigríes grandes y bien documentados mostrando un patrón claro de transmisión materna.

- Identificar la mutación exclusivamente en familias afectas y no en sujetos sanos o control o con otro tipo de neuropatía óptica.

- La mutación no debe coexistir con otras mutaciones patogénicas.

- La mutación debe ser heteroplásmica en algunas personas de las familias afectas, dado que la heteroplasmia no parece ser un hallazgo de polimorfismos inocuos.

- La mutación debe cambiar un residuo de a.a. altamente conservado

- Estos criterios no son absolutos, pero mientras más de ellos se cumplan, más certeza de patogenicidad de la mutación en estudio habrá


De aquí surgen los conceptos de mutación primaria, secundaria e intermedia para NHOL.

Las mutaciones primarias son aquellas que se han identificado solamente en familias con la enfermedad, nunca en sujetos control. 
Existen numerosas mutaciones primarias en esta entidad. 
El rol causal de la mutación de Wallace (A11778G), que es la más común en las familias estudiadas en Europa, Estados Unidos, Japón y Australia, fue originalmente establecido por la identificación de la mutación en dos pedigríes diferentes, una familia negra en USA y una blanca en Europa.
La mutación no se vio en los controles y era heteroplásmica para cientos de sujetos. 
Resultó una sustitución de una arginina altamente conservada por histidina en la NDH4 (Complejo I de la NAD deshidrogenasa) en la cadena respiratoria.
Como la arginina y la histidina son básicas las dos el cambio es relativamente conservativo.

Otras mutaciones primarias son: 3460, 14484, 4160, 4136 y 5244, todas ellas con cambios en a.a de los polipéptidos del Complejo I. La 9438 y 9804 afectan la subunidad III del citocromo C.

Aproximadamente el 90% de los pacientes de Leber estudiados en el mundo tienen las mutaciones 11778, 3460 o 14484. 

Las mutaciones secundarias incluyen un grupo numeroso de mutaciones identificadas en las familias NOHL y en familias control, pero con más frecuencia en las primeras.

Estas mutaciones encuentran asociación con otras secundarias o con primarias, siendo difícil acceder a su significado en la producción de la enfermedad.
Estas son: 3394, 4136, 4216, 4917, 5244, 13708, 7444, 15812. 
Se ha sugerido que en ausencia de una primaria de las tres mayores (11778, 3460, 14484), la NOHL solamente ocurre si hay múltiples mutaciones secundarias o bien una primaria de las otras asociada a estas, entonces las secundarias afectarían las manifestaciones de la enfermedad.

Una mutación intermedia es la 15257, que sustituye por asparagina el aspártico altamente conservado en apocitocromo del complejo III, frecuentemente asociada a la 13708 o la 14484. 
Recibe su nombre del hecho de que en diversos estudios se ve asociada a alguna de las tres primarias mayores pero no tiene descrita influencia en los hallazgos clínicos de la NOHL de los pacientes.

Recientemente se han descubierto nuevas mutaciones como por ejemplo una que actúa sobre el gen de la subunidad mitocondrial ND5, se localiza en la posición 13513 y es probable que sea patogénica produciendo un fenotipo solapado entre un cuadro de NHOL y uno de MELAS.
Algunas estarían relacionadas con la enfermedad de Parkinson y otras producen cuadro clásico aislado .

Distribución por sexos de la NOV

La heteroplasmia ha sido implicada como la base de la penetrancia incompleta en NHOL (50% en hombres y 90% en mujeres con heteroplasmia a las mutaciones primarias).

Un elemento interesante es el predominio masculino en NOHL. 
Aún continúa inexplicado si bien existe una teoría del " gen de susceptibilidad de pérdida visual ligado al X " pero es controvertida y en muchos casos ha sido refutada al analizar los datos.

Lo que sí ha sido demostrado por algunos autores es que existen diferencias entre hombres y mujeres en cuanto al curso clínico de la enfermedad.
Se plantea una mejor tasa de recuperación para hombres, con una edad promedio de comienzo menor para las mujeres y la presencia en la mayoría de ellas de su madre afecta.

Sin embargo si la edad de comienzo es temprana (primera o segunda década de vida), con independencia del sexo el pronóstico de recuperación es peor.

CUADRO CLÍNICO CLÁSICO

En el cuadro clínico clásico el mayor síntoma es la pérdida aguda de la visión central, bilateral. 
Hay gran variedad de hallazgos asociados o no.
Clásicamente la enfermedad ocurre como "trueno en cielo despejado", sin pródromos, aunque pude haber cefaleas migrañosas en el 33% de los pacientes.
La pérdida de la visión central inicialmente es monocular, no dolorosa y se desarrolla en pocas horas o días.
El paciente describe visión borrosa con la luz diurna que mejora a la puesta del sol, pero la curva de adaptación a la oscuridad es subnormal.

Puede haber visión borrosa transitoria con ejercicio físico o con calor (signo de Uhtroff).
Se describen también relampagueos inespecíficos de luces y colores. 

La visión de colores se afecta severamente en el curso temprano de la enfermedad con discromatopsia axial adquirida al rojo y al verde.

El Fondo de Ojo (F/O) descubre un disco hiperhémico de bordes irregulares e inflamación de las fibras nerviosas subyacentes.
No hay goteo de fluoresceína en la cabeza del nervio pero sí hay microangiopatía telangectásica circumpapilar, con dilatación irregular de los capilares pre y peripapilares.
Los grandes vasos son tortuosos. 
Hay gran variabilidad al F/O que oscila desde la normalidad (42%) hasta neuroretinitis florida con hemorragia del disco. 

En un intervalo de semanas a meses ( xxx ) ocurre lo mismo con el otro ojo.

Es poco frecuente que se alargue el intervalo (reportado hasta 8 años) y mucho menos frecuente que persista monocular. 

Los tests de campo visual demuestran típicamente un gran escotoma central, reflejando inclusión preferencial del paquete maculopapular.

La pérdida de visión es permanente.
La atrofia no afecta la cúpula, se desarrollará empezando por el campo temporal con gradual empeoramiento de la agudeza visual (A.V.) en los meses subsiguientes.
El resultado final es variable, pero la estabilidad de la función visual persiste.
Sin embargo en algunos casos hay recuperación espontánea aunque incompleta, uni o bilateral.

El análisis molecular sugiere relación entre la mutación mitocondrial específica y la recuperación visual.
La 14484 tiene mejor pronóstico en tasa de recuperación con una A.V. de 20/60 en ambos ojos en el 37% de los casos.
La mutación 3460 tiene tasa variable pero llega al 22% de los pacientes recuperados en algún grado.
La 11778 es la de peor pronóstico, solo un 5% de recuperación.

La mejoría siempre es más probable si la pérdida de la visión no ocurre en edades tempranas. 
El tiempo que media entre comienzo y final de recuperación es variable, puede ser de 10 o más años, siendo muy rara una recaída.

Independientemente de la asociación entre algunas mutaciones y la tasa de recuperación de la A.V. se plantea que esta última sigue el curso clínico de la enfermedad sin que haya muestras evidentes de que alguna terapéutica pueda propiciar su aparición en menos tiempo.

En términos de distribución en la población típicamente se afectan hombres jóvenes entre 15-35 años.
La edad media de comienzo en hombres es de 25-34 y en mujeres de 23-26 con rangos variables.

Síndrome Leber Plus

Existen algunas enfermedades que con frecuencia se han visto asociadas a la NHOL.
De este hecho surge la denominación del Síndrome Leber plus.

Enfermedades asociadas

- De tipo Neurológico:
Esclerosis múltiple, encefalopatía infantil subaguda, tremor, disartria, ataxia, signos de cordón medular posterior y espasticidad.
Etiología no bien definida.

- De tipo Cardíaco:
Síndrome de preexcitación (Long-Ganong-Levine), intervalo Q-T prolongado, combinación de Q profunda con T alta y otros.


Asesoramiento Genético

El asesoramiento genético (AG) en NHOL no es tan exacto, al igual que la transmisión de una mutación no está siempre asociada con la transmisión de la enfermedad.
Este fenómeno es común a todas las entidades con mutaciones del ADN mitocondrial.

Aunque la heteroplasmia puede ser un factor determinante en la penetrancia de la enfermedad, muchos pacientes y sus parientes no afectados en familias de Leber tienen cantidades muy elevadas de ADNmit. mutante ( xxx ) y muchos varones de riesgo permanecen no afectados en este contexto .

Por esta razón el análisis del ADNmit. en parientes de riesgo no es muy útil, particularmente en varones jóvenes.

El hallazgo de heteroplasmia en una mutación relevante siempre crea ansiedad familiar.
Esto es particularmente aplicable para tener hijos, y es también una situación de difícil manejo desde el punto de vista ético que por tanto se debe analizar cuidadosamente.

Aunque es posible que algunos varones afectos presenten mutaciones frescas, lo cual es sugerido por la ausencia de mutación en la madre u otros parientes de esta línea, esto solo ha sido descrito para una mutación reciente (13730) la que tiene significado patogénico incierto.

No hay publicados aún datos concernientes al riesgo de recurrencia en pacientes de NHOL genéticamente definidos y no está claro como estos pueden variar según la mutación.
Pero existen análisis preliminares de 87 pedigríes con las mutaciones 11778, 3460 y 14484 para la enfermedad tomados del Hospital Nacional de Neurología y Neurocirugía de Londres que sugiere los siguientes riesgos: 

- Hermanos 34%
- Hermanas 14%
- Sobrinos 42%
- Sobrinas 9%
- Primos Hermanos 40%
- Primas hermanas 2%

Estos riesgos pueden ser modificados tomando la penetrancia en relación con la edad media de comienzo. 
Para las tres mutaciones más importantes es de aproximadamente 20 años para varones y 28 para hembras. 

Estudios para otras mutaciones (14484) muestran también una gran diferencia en el riesgo de recurrencia entre varones y hembras de diferentes grados de parentesco.

Las dificultades referidas antes sobre el uso de análisis molecular de ADNmit. en parientes de riesgo, son también aplicables al diagnóstico prenatal molecular.
Dado que no es conocido que proporción del ADNmit. mutante en vellosidades coriónicas reduce el riesgo genético, o como esta proporción puede cambiar durante el desarrollo pre y postnatal, la única opción en mujeres portadoras de este ADNmit. alterado y que deseen evitar el embarazo de alto riesgo es la del aborto de fetos varones. 
Sin embargo sus hijas tienen también un riesgo tangible de ser afectadas, por lo que hasta el momento no es posible el control de un embarazo normal para garantizar que no haya problemas de este tipo en la descendencia.
De forma alternativa se podrían usar técnicas de reproducción asistida, pero se trata de un campo aún por explorar en estas entidades.

NHOL y estilos y hábitos de vida

La existencia en los pacientes con la entidad, de hábitos tóxicos nocivos como el alcoholismo y el hábito de fumar ha propiciado estudios epidemiológicos en este sentido.
Se sugiere que si bien estos hábitos no están presentes en muchos de los casos al debut de la enfermedad por su temprana edad de aparición y no pueden ser asociados consecuentemente con la edad de aparición, si es estadísticamente significativa su asociación con otras variables como el empeoramiento progresivo de los síntomas y la ausencia de regresión de estos.
La explicación biológicamente plausible es que por el consumo de estos tóxicos se producen sustancias de las llamadas toxinas exógenas que son capaces de adelantar y empeorar el curso clínico de numerosas enfermedades mitocondriales al sobrecargar la función de la mitocondria.

La dieta de los pacientes es otro de los tópicos de difícil valoración dada su heterogeneidad en las diversas zonas geográficas estudiadas, no obstante existe el consenso de que los alimentos ricos tiocianatos deben ser evitados en exceso y complementar la alimentación con un suplemento vitamínico que incluya la ubiquinona (coenzima Q); lo que teóricamente debería mejorar la progresión de los síntomas.
No obstante al tratarse de un tópico poco explorado existen detractores de la utilidad de esta última terapia.

ANÁLISIS MOLECULAR

En cuanto al análisis molecular propiamente dicho gran cantidad de técnicas han sido usadas para estudiar mutaciones en el ADNmit entre ellas: PCR, Southern blotting y clonaje.
En particular para la NOHL donde las mutaciones que se presentan son puntuales, con mayor predominio de las transiciones que transversiones, el procedimiento más utilizado en los ensayos de rutina para el diagnóstico es la amplificación mediante la reacción en cadena de la polimerasa (PCR) y digestión con enzimas de restricción específicas que crean un sitio nuevo o desaparecen un sitio que ya existía para la enzima en presencia de la mutación.
Los resultados se visualizan con una electroforesis de ADN.

Es llamativa una situación específica en lo referente a individuos sanos de familias con NHOL en los que los estudios moleculares arrojan homoplasmia al ADNmit. mutado sin la más leve traza de ADNmit. sano independientemente del tipo de estudio aplicado.
Para corroborar el resultado se ha utilizado la cuantificación de heteroplasmia utilizando el SSCP ( single- strand conformation polymorphism analysis ) o un Southern Blot post PCR o bien métodos de marcaje radiactivo que sensibilicen la técnica.

En consecuencia este resultado de ausencia de heteroplasmia en sangre periférica en niveles suficientemente altos para ser detectables no se explica por la sensibilidad del test, sobre todo si tenemos en cuenta la uniformidad múltiples resultados en este sentido.
Según datos de algunos biólogos moleculares que trabajan en esta línea investigativa este es un fenómeno común y no explicado que se presenta por ejemplo en todas las familias estudiadas en Canadá a la mutación 14484 en las que se utilizó inicialmente un procedimiento rutinario y posteriormente otros de los referidos más específicos.

Se han propuesto numerosas hipótesis que tratan de explicar este hallazgo pero las más aceptadas se basan en dos mecanismos diferentes.
En uno se plantea la segregación genética al azar ocurriendo durante la ovogénesis temprana que propiciaría la aparición de líneas homoplásmicas en la descendencia de progenitoras heteroplásmicas haciendo este fenómeno común.
En el segundo se plantea el hallazgo de una selección negativa contra células de sangre periférica mutadas en su ADNmit. en la posición 3460 que propició la disminución paulatina de la tasa de heteroplasmia a este nivel en un estudio de seis años.

No podemos afirmar que en tales pacientes haya ocurrido una selección negativa contra células sanguíneas no mutadas en su ADNmit, lo que sería poco probable, pero sería provechoso en futuras investigaciones poder contar con muestras de otros tejidos del paciente, como su pelo, piel, mucosas; fluidos como la saliva y la orina o bien muestras post mortem de tejidos del nervio óptico que al ser estudiados en su conjunto brinden una información más completa del comportamiento de la heteroplasmia en estas familias.
Tales tomas de muestras aportarían con su estudio elementos que aclararían en la NHOL un hallazgo comprobado en otras entidades como MERRF, MELAS, Kearns Sayre en los que se plantea una selección específica de tejidos como elemento modulador en la patogénesis de las mutaciones del DNAmit.. 


La proteómica en el horizonte

Avanzado ya el estado de desarrollo de la genómica, la ciencia se apresta a catalogar las proteínas de nuestro organismo y a descubrir su mutua interrelación.
El progreso de la proteómica habrá de contribuir a la aparición de fármacos nuevos más eficaces. 

El genoma humano pronto saldrá del primer plano. 
La investigación se concentra ahora en el proteoma, el conjunto de las proteínas elaboradas por las células y los tejidos de nuestro cuerpo.
El genoma -información genética completa del organismo- sólo ofrece las recetas para sintetizar proteínas.
Estas son los verdaderos ladrillos y la argamasa con que se fabrican las células; 
asumen, además, la mayor parte del trabajo.
Y son las proteínas las que diferencian los diversos tipos de células.
Aunque todas las células compartan esencialmente el mismo genoma, pueden tener distintos genes en actividad y, por tanto, elaborar diferentes proteínas; 
del mismo modo, las células enfermas a menudo sintetizan proteínas que no se encuentran en células sanas, y viceversa.

Se comprende, pues, el nuevo afán por catalogar todas las proteínas humanas y descubrir sus interacciones mutuas.
Con ello se pretende diseñar fármacos mejores y con menos efectos secundarios.

Alcanzar esa meta no será un juego de niños. 
El estudio de las proteínas entraña más dificultades que el de los genes.
Las empresas del ramo siguen esforzándose en aportar a esta tarea las técnicas e instrumentos más perfeccionados.
Sin embargo, la carrera ya ha comenzado:
al menos una compañía promete descifrar de aquí a tres años el proteoma humano, paso importante para configurar la multitud de interacciones que existen entre las proteínas individuales.
Mientras tanto, por ceñirnos a los Estados Unidos, hay programas federales que subvencionan investigaciones académicas sobre los proteomas de las células cancerosas y del suero, residuo líquido de la sangre humana al coagularse.

Se han logrado ya avances importantes.
En enero dos equipos de investigadores anunciaron haber cartografiado las interacciones de todas las proteínas de la levadura del pan, modelo socorrido para el estudio de la biología celular.
Otros comunicaron en febrero haber preparado, mediante técnicas proteómicas, una prueba segura para la detección precoz del cáncer de ovario. 

La proteómica va a provocar una gran actividad comercial. 
El análisis de inversiones realizado por Frost & Sullivan estima que el mercado mundial de instrumentos, suministros y servicios de proteómica llegará en 2005 a 6360 millones de euros, habiendo partido de 795 millones nada más en 1999, y sin incluir ahí los beneficios generados por los fármacos y diagnósticos desarrollados por efecto de las técnicas proteómicas.
La proteómica también puede ser vital para el futuro de la industria farmacéutica.
El ramo invirtió 34.000 millones de euros en investigación y desarrollo durante 2000, pero sólo se aprobaron 30 fármacos en ese año. 

Del genoma al proteoma

El término "proteoma" fue acuñado en 1994 por Marc R. Wilkins, de Proteome Systems en Sydney, para designar la totalidad de proteínas codificadas por un genoma.
La exacta definición de "proteómica" varía según los autores, pero suele admitirse que engloba tres actividades principales: la identificación de todas las proteínas sintetizadas por una célula, tejido u organismo; la determinación del modo en que interaccionan estas proteínas para formar redes semejantes a circuitos eléctricos, y el trazado de las estructuras tridimensionales precisas de las proteínas como medio para encontrar su talón de Aquiles, es decir, dónde puedan ser desactivadas o activadas por la acción de fármacos.

Estas tareas tal vez parezcan sencillas.
Cuando se anunció la práctica terminación del Proyecto Genoma Humano daba la impresión de que, al conocer la secuencia de los aproximadamente 3000 millones de letras de código o pares de bases de ADN presentes en el genoma humano -y sobre todo las secuencias concretas de las unidades codificadoras de proteínas (los genes)- se llegaría a comprender las proteínas en sí mismas.
Mas, por desgracia, el proteoma es mucho más complicado que el genoma.

El "alfabeto" del ADN consta de cuatro bases químicas denominadas por sus letras iniciales: adenina (A), citosina (C), guanina (G) y timina (T).
Las proteínas, por el contrario, están formadas por 20 bloques constructivos llamados aminoácidos.
Los genes especifican qué aminoácidos han de agruparse para constituir una determinada proteína.
Pero aun conociendo la secuencia de aminoácidos de una proteína, de ello no se deduce necesariamente cuál es su función ni con qué proteínas se vincula. 
Tampoco puede predecirse su estructura tridimensional con absoluta certeza.
A diferencia de los genes, que son lineales, las proteínas se pliegan y adoptan formas que a veces se apartan de lo esperado.

Por si fuera poco, las células suelen modificar las proteínas añadiéndoles azúcares, lípidos o ambos, de maneras también difíciles de prever.
Para producir una proteína cifrada por un gen recién descubierto no basta con ensamblar las cadenas de aminoácidos en el orden que impone el gen;
a menudo es preciso cerciorarse de que se incorporan las adecuadas modificaciones de lípidos y azúcares.
Y para determinar el comportamiento de una proteína, se ha de tener en cuenta que algunas proteínas se disuelven en agua, mientras que otras normalmente sólo actúan en medios lipídicos o tienen regiones que están imbricadas en membranas celulares rellenas de grasa.

Pero no termina ahí la complejidad.
Aunque suele admitirse que el genoma contiene del orden de 40.000 genes, lo cierto es que una célula típica elabora cientos de miles de proteínas distintas.
La cabal comprensión del proteoma exige aprender las características de todas esas proteínas.
No basta con manejar los datos obtenidos del Proyecto Genoma Humano, que finalmente echan por tierra el viejo dogma de que un gen codifica una sola proteína; 
está claro que puede dar origen a una multitud de proteínas diferentes.

Pese a lo complicado del asunto, la investigación se muestra optimista.
Aunque desconocemos del 30 al 50 por ciento de las proteínas humanas, contamos con medios para identificar con bastante rapidez las que componen nuestro organismo.

Proceso fabril de las proteínas

Cuando se quiere descubrir las proteínas presentes en células o tejidos seleccionados, suele recurrirse a dos técnicas: la electroforesis en gel bidimensional y la espectrometría de masas.
En geles de dos dimensiones se añade una mezcla de proteínas al borde del gel fino que separa las proteínas en una dirección en razón de su tamaño y en la dirección perpendicular en razón de su carga electroquímica. 
Al venir la proteína caracterizada por su tamaño y su carga, a cada proteína corresponderá un punto discreto en el gel.
Podemos recortar de los geles puntos individuales para identificar, mediante otras técnicas, las proteínas que contienen.
Y comparando las configuraciones de puntos que presentan los geles obtenidos de dos tejidos diferentes pueden buscarse proteínas que fabrique un tejido, pero no el otro.

La espectrometría de masas emplea imanes o campos eléctricos para discernir las proteínas con arreglo a las masas de sus átomos constitutivos.
Los resultados aparecen representados en crestas de un gráfico.
Sin embargo, ni la técnica del gel bidimensional ni la espectrometría de masas son soluciones ideales.
Los geles son especialmente difíciles de manejar y no pueden distinguir las proteínas muy grandes ni las muy pequeñas, ni las que sobresalen a través de membranas; 
la espectrometría de masas es muy cara y a veces falla en la detección de proteínas raras.

Pese a todo, varias empresas preparan versiones refinadas de estos métodos para utilizarlos en procedimientos industriales, que recuerdan las que hicieron posible el Proyecto Genoma Humano. 
La pieza fundamental del proyecto fue el ABI 3700, el secuenciador de ADN desarrollado por Applied Biosystems.
En enero, presentó Applied Biosystems su analizador proteómico 4700, a la par que anunciaba un acuerdo con Perkin Elmer y Millipore sobre la producción de un sistema automatizado para tratamiento y análisis de geles bidimensionales.
Se espera que la automatización permita realizar en días lo que antes necesitaba meses o años.
Pero queda por ver si estos nuevos sistemas van a ser estándares de la proteómica.
La materia es demasiado vasta para que se imponga un solo sistema o método de trabajo. 

Entretanto, las empresas Myriad Genetics, GeneProt, Large Scale Biology y MDS Proteomics se han incorporado a este campo con plantas proteómicas de diseño propio, algunas de las cuales utilizan técnicas robóticas tomadas de la industria de automoción.
El año pasado Myriad anunció que se había aliado con Hitachi y Oracle en una operación de 210 millones de euros para descifrar en tres años el proteoma humano completo, programa que se ha iniciado oficialmente en enero de este año. 
Celera, por su parte, ha asignado cerca de 1140 millones de euros a sus trabajos en proteómica.

Los críticos de estos grandes proyectos aducen que no hay un proteoma humano único:
como ejemplo, el páncreas produce un conjunto de proteínas muy diferente del que elabora el cerebro.
Existen, además, numerosas variables (la simple ingesta reciente de un vaso de vino) capaces de afectar los tipos de proteínas que genera el organismo.
Cada estado -salud más o menos buena, mayor o menor cantidad de fármaco- crea un proteoma diferente. 

En resumen, la enumeración de las proteínas humanas lleva a extremos insospechados.
Para comprender su actuación en el organismo y poder desarrollar fármacos útiles, es preciso conocer cómo varían las proporciones de proteínas de un tipo de célula a otro y en el interior de una misma célula.
Necesitamos saber, asimismo, de qué manera colaboran las proteínas en las diversas actividades de una célula.

Escuchando en la red

La compañía de Moran se ha concentrado en esta última tarea: examinar cómo se engarzan entre sí las proteínas para formar cadenas de reacciones bioquímicas o construir máquinas moleculares como el huso, la que parte en dos la célula en el momento de la división celular.
"Las proteínas se ensamblan en redes", afirma Moran.
"Si sólo pudiéramos hacer una pregunta sobre una proteína, sería con qué otras proteínas interacciona."

En un estudio reciente, científicos de MDS Proteomics y la Universidad de Toronto -junto con los de un grupo independiente de Cellzome y del Laboratorio Europeo de Biología Molecular- presentaban una nueva estrategia para localizar cientos de interacciones proteínicas en la levadura.
Su enfoque consiste en juntar a los centenares de genes de levadura escogidos unos fragmentos de ADN que codifiquen "etiquetas" adhesivas.
De este modo pueden aislarse las proteínas elaboradas por los genes modificados, unidas a cualesquiera proteínas que se les hayan pegado, sin más que triturar la levadura y hacer pasar la lechada resultante por una columna de perlas microscópicas que sólo puedan unirse a las etiquetas adhesivas.
Tras haber procesado los complejos de proteínas en un espectrómetro de masas y analizado los resultados, se descubrió que más del 90 por ciento de los complejos aislados contenían proteínas de función desconocida.
Y más aún, hasta el 80 por ciento de las proteínas interaccionaban al menos con otra proteína, lo que muestra la complejidad de la red bioquímica intracelular.

MDS Proteomics proyecta ahora aplicar esta técnica al proteoma humano.
Dado que el proteoma de la levadura se completó en unas pocas semanas, la compañía apuesta por conseguir de aquí a un año una instantánea inicial del proteoma de una célula humana.
No está claro, sin embargo, qué tipo de célula humana van a estudiar ni en qué condiciones.

En los Estados Unidos el sector público presta también atención a la proteómica.
Se ha concedido a Samir M. Hanash, de la Universidad de Michigan, la dirección de la Organización del Proteoma Humano (HUPO), que coordina los proyectos de proteoma en el sector público.
(El Proyecto Genoma Humano coordinó a los laboratorios académicos que descifraban el genoma humano.)
Uno de los primeros objetivos de la HUPO será determinar las proteínas presentes en el suero sanguíneo.

El Instituto Nacional del Cáncer (NCI) y la Agencia de Alimentación y Farmacia (FDA) trabajan en el desarrollo, por medios proteómicos, de tratamientos anticancerígenos mejor dirigidos y diagnósticos más certeros.
En el programa, anunciado en julio de 2001, se analizarán células tumorales de distintos pacientes para obtener una lista de proteínas presentes en células cancerosas mas no en células sanas.
Asimismo, buscarán "marcadores" de proteínas que se correlacionen con los cánceres más agresivos, con el refinamiento consiguiente de las pruebas diagnósticas.

Junto con otros investigadores, Emanuel Petricoin, codirector del programa NCI/FDA, acaba de demostrar las interesantes perspectivas de la aplicación de la proteómica al diagnóstico de tumores.
En un trabajo aparecido en febrero explicaba que podían compararse las configuraciones de proteínas presentes en el suero sanguíneo de pacientes afectadas o no por cáncer de ovario.
Resultado de esta comparación fue la identificación correcta de las 50 mujeres que sufrían cáncer de ovario, sin dejar ni una;
no tuvo más fallo que dar tres falsos positivos entre las mujeres que no sufrían cáncer.

La catalogación y la cartografía de las interacciones entre proteínas suponen las dos terceras partes de la proteómica;
igual importancia reviste la determinación de las formas de las proteínas.
La técnica clásica es la cristalografía por rayos X, en la cual se purifican las proteínas, se las hace crecer en cristales y se bombardean luego los cristales con rayos X.
Analizando cómo rebotan los rayos X en los átomos componentes de una proteína, se infiere el ensamblaje de la misma y su estructura tridimensional.

Las formas que han de venir

La cristalografía de rayos X fue en un tiempo una industria muy restringida, que exigía el acceso al haz de rayos X de un sincrotrón.
Estos enormes anillos, a veces de kilómetros de diámetro, se han venido utilizado históricamente para acelerar las partículas atómicas.
En el curso de ese proceso se producen los rayos X.
Pero hoy día los avances técnicos en el dominio de los láseres de rayos X han posibilitado la aparición de aparatos de sobremesa, idóneos para laboratorios.

Dos compañías -Syrrx y Structural GenomiX (SGX)- han llevado la cristalografía por rayos X al terreno industrial.
A semejanza de las compañías que automatizaron el proceso de descubrimiento de las proteínas, Syrrx se ha inspirado en técnicas de la automoción. 
Hasta el punto de haber traído a su planta de San Diego consultores de General Motors para automatizar su planta de 7800 metros cuadrados, en la que todo -desde la purificación de la proteína hasta la cristalización- se realiza sobre una cadena de montaje.
Además de sus propios láseres de rayos X, la compañía dispone de un trayecto de haz reservado en la Fuente Luminosa Avanzada del Laboratorio Lawrence en Berkeley.
Structural GenomiX tiene un acuerdo semejante con la Fuente Fotónica Avanzada del Laboratorio Nacional Argonne, donde ha construido un trayecto de haz.

Esta información estructural podría ser objeto de protección.
La compañía inglesa Oxford GlycoSciences aduce que puede reclamar patente sobre una porción notable del genoma y el proteoma humano utilizando datos proteómicos.
En diciembre pasado la compañía presentó solicitudes de patente para 4000 proteínas humanas, lo que podría trastornar la definición de propiedad intelectual en biotécnica.
En otros tiempos, las empresas buscaban patentar secuencias de ADN y la única proteína que supuestamente iban éstas a codificar.
Pero como un mismo gen puede elaborar toda una serie de proteínas, tal vez tendrían más valor las reivindicaciones basadas en las propias proteínas y ofrecerían además un camino para esquivar las patentes de secuencias de ADN en poder de la competencia.
Si esto fuera así, los tribunales serían un terreno de liza más en el que los genes tendrán que dejar paso a las proteínas.


Terapia génica contra el cáncer 

La inserción de genes podría, en teoría, detener el crecimiento tumoral e incluso frenar el sida

Se estima que en 1997 en los Estados Unidos se diagnosticarán 1,38 millones de nuevos casos de cáncer.
Con las técnicas actuales, cirugía, radioterapia y quimioterapia, sólo se pueden curar aproximadamente la mitad de todos ellos.
Este dato tan poco halagüeño ha promovido una serie de investigaciones dirigidas al desarrollo de estrategias adicionales para el tratamiento de la enfermedad, basadas en los mecanismos biológicos subyacentes.
Los investigadores centran su atención en las terapias génicas, esto es, en la introducción en el organismo de genes con capacidad potencial para combatir los tumores.

Los investigadores consideraron en un primer momento la terapia génica para remediar los efectos de instrucciones genéticas deficientes causadas por mutaciones.
Este tipo de alteración se transmite a las generaciones siguientes.
La mayoría de los tumores, sin embargo, no se han contraído por herencia, sino que tienen su origen en mutaciones adquiridas, producidas por factores externos (citemos el humo del tabaco y la radiación a altas dosis) o por simple mala suerte.
Con el tiempo, estas mutaciones se acumulan en las células y terminan por incapacitarlas para controlar de forma correcta su propio crecimiento, fundamento último del cáncer.

En general, las terapias génicas proporcionan instrucciones en forma de secuencias de ADN a las células enfermas, con el fin de que produzcan proteínas de carácter terapéutico.
Este tipo de abordaje es posible porque tanto los virus y las bacterias, como los vegetales y los seres humanos compartimos el mismo código genético.
Se ha aprendido mucho en muy poco tiempo sobre los mecanismos en virtud de los cuales determinados genes rigen los procesos fundamentales de la vida y sobre la forma en que contribuyen al desarrollo de tumores.
Y puesto que los genes de una especie puede leerlos y descifrarlos otra, nos es permitido transferir genes de una célula a otra y de una especie a otra, en la búsqueda de nuevas variantes terapéuticas.

Existen varios enfoques de la terapia génica experimental en el tratamiento del cáncer.
Algunos se basan en la introducción en las células cancerosas de genes que den lugar a la producción de moléculas tóxicas.
Cuando tales genes se expresan (esto es, cuando la maquinaria celular los utiliza para la síntesis de proteínas), las proteínas resultantes matan a esas mismas células tumorales.
Otras estrategias apuntan a la corrección o compensación de mutaciones genéticas adquiridas.
También hay planteamientos que persiguen la activación de los procesos encargados, de suyo, de la reparación de tales fallos.
Y todo un universo de ideas están brotando de la inquisición en diversas líneas:
métodos de que se valen los tumores para evitar su reconocimiento y destrucción por el sistema inmunitario, vías que siguen para su diseminación a partir del foco originario, medios gracias a los cuales organizan su vascularización y caminos por los que alcanzan una serie de condiciones que les permiten resistir y diseminarse. 

Aunque la mayoría de los enfoques mencionados no han pasado siquiera las pruebas clínicas preliminares de seguridad y eficacia, cabe presumir que con tal bagaje conceptual llegaremos a un mejor tratamiento del cáncer en el futuro. 

Las técnicas de terapia génica, además de abrir el horizonte a nuevos tratamientos, han mostrado ya su rentabilidad en la valoración de los tratamientos que se aplican.
En los últimos años los médicos han recurrido con frecuencia al trasplante de médula ósea en el tratamiento de aquellos tumores que se han mostrado resistentes a las terapias tradicionales, como los estadios avanzados de la leucemia.
Esta forma de cáncer afecta a los leucocitos, células sanguíneas que se producen en la médula ósea.
Antes del trasplante, se extrae médula ósea de pacientes afectos de leucemia que se encuentren en período de remisión y se almacena. 
Los pacientes reciben altísimas dosis de radio o de quimioterapia, suficientes para eliminar cualquier posible tumor residual.
Semejante tratamiento bastaría para destruir también la médula ósea sana, por lo que no sólo se eliminaría el cáncer sino que, además, se provocaría la muerte del individuo. 
El trasplante evita esta circunstancia al recibir el paciente una transfusión de su propia médula ósea sana.

En teoría, este procedimiento debería curar la leucemia.
Pero a menudo la enfermedad recidiva.
Los clínicos se han preguntado repetidas veces a qué puede deberse.
Dos son las posibilidades que se barajan: 
que la dosis de radiación administrada sea insuficiente para matar todas las células cancerosas, o bien que pase inadvertida la presencia de células leucémicas en la médula ósea, supuestamente libre de enfermedad, extraída durante un período de remisión. 
Para poder diferenciar estas dos posibilidades, se buscó un marcador permanente e inocuo, con el que poder identificar tras el reimplante las células de la médula ósea extraída.
A finales de 1991, Malcolm K. Brenner, del Hospital Pediátrico San Judas de Menfis, Tennessee, insertó en la médula ósea extraída de los pacientes una secuencia de ADN bacteriano que no se encuentra en el genoma humano.

Brenner sabía que la detección de ADN bacteriano en la sangre y en la médula ósea del paciente tras el transplante significaría que los elementos celulares sanguíneos se habían originado de la médula reimplantada.
Más importante aún:
la presencia del marcador en cualquier tumor recidivante demostraría que la fuente de las células tumorales era esa misma médula ósea transplantada.

Esto último es lo que, en efecto, han comprobado Brenner y otros en algunos casos, lo que hace necesaria una revisión crítica de las indicaciones del transplante de médula ósea.
Así, se reconoce ahora que en algunos tipos de cáncer debe indicarse antes de su implante un tratamiento adicional de la propia médula ósea almacenada, con el fin de eliminar todo resto de células cancerosas.
Las investigaciones sobre marcadores genéticos de médula ósea, idénticas a las descritas, son de gran ayuda a la hora de encontrar la mejor solución. 
Estos estudios permiten comparar diversos métodos de purificación de la médula ósea de tumores residuales.
Se están empleando diferentes marcadores genéticos para identificar muestras de médula ósea, purificada de formas diversas.

Se puede así valorar en qué medida una médula ósea purificada según un procedimiento concreto facilita el tratamiento posterior del paciente.

Ante un cáncer que recidiva, es posible determinar si proviene o no de una médula ósea cuya purificación se acometió de una determinada manera.

Vacunas génicas

En lo que se refiere al tratamiento de los tumores, hace más de tres décadas que los investigadores intentan hallar la forma de estimular al sistema inmunitario para que se enfrente al cáncer;
nos referimos a la inmunoterapia o vacunoterapia.
Y no sin razón, si se tiene en cuenta que la inmunidad es una reacción sistémica que podría, en principio, eliminar todas las células cancerosas del organismo, incluidas aquellas que emigran desde su lugar de origen o las que reaparecen tras años de remisión clínica.
El problema de esta alternativa radica en que el sistema inmunitario no siempre reconoce a las células cancerosas como tales.
Son muchos los tumores que consiguen escapar a la detección por el sistema inmunitario.

Recientemente, las investigaciones realizadas en el campo de la inmunología básica han descubierto medios para desenmascarar estos cánceres.
En particular, existe la posibilidad de marcar las células tumorales con ciertos genes que las hagan más visibles para el sistema inmunitario.

Una vez estimulado, no es infrecuente que el sistema inmunitario señale también células tumorales sin marcar. 

La respuesta inmunitaria pone en marcha muchas células y substancias químicas distintas que colaboran en la eliminación de microorganismos invasores o de células alteradas.
Las células anormales acostumbran presentar en su membrana ciertas proteínas, llamadas antígenos, que difieren de las encontradas en las células sanas.
Cuando el sistema inmunitario se activa, los linfocitos B producen cierto tipo de moléculas, los anticuerpos, que circulan por el organismo y se unen a antígenos extraños.
De esa guisa marcan a los portadores de éstos para que los destruyan otros componentes del sistema inmunitario.
Otro tipo celular, los linfocitos T , que también reconocen antígenos extraños, acaban con las células portadoras de antígenos específicos o estimulan a las células T asesinas para que lo hagan. 
Las células T y las B se comunican entre sí mediante las proteínas que secretan: las citoquinas.
Existen además otras células, como las células presentadoras del antígeno y las células dendríticas, que auxilian a los linfocitos T y B en la detección de antígenos de células cancerosas o infectadas.

Actualmente se está sometiendo a prueba una estrategia de terapia génica que se basa en la modificación de células cancerosas mediante la introducción de genes codificadores de citoquinas.
Se extraen primero células tumorales, en las que se insertan genes que cifran citoquinas tales como la interleuquina 2 (IL-2, es decir, factor de crecimiento de linfocitos T ) o el activador de células dendríticas, conocido como factor estimulador de colonias de granulocitos y macrófagos (GM-CFS).
Luego, se reimplantan estas células en la piel o el músculo del paciente, donde empezarán a segregar citoquinas, que despiertan la atención del sistema inmunitario.
En teoría, las células alteradas deberían instar una febril actividad de los elementos celulares inmunitarios en el mismo lugar de reimplante.
Las células activadas, ya alertadas sobre la presencia de células tumorales, circularían por todo el organismo y atacarían a otros tumores.

En algunos casos, estas vacunas tumorales genéticamente modificadas inducen la detección del cáncer por el sistema inmunitario;
se han observado algunas respuestas clínicas llamativas.
Con todo, la investigación clínica no ha pasado de la fase preliminar.
En general, no se ha comparado la respuesta a estas variantes terapéuticas con lo observado tras un tratamiento tradicional sin más. 
Las pautas de respuesta varían de un tipo de cáncer a otro y entre distintos pacientes con el mismo tipo de cáncer; 
no son predecibles.

Dicha investigación, además, presenta el inconveniente de que los pacientes abordados se encontraban en estadio de cáncer diseminado.
A menudo, tales sujetos han recibido un tratamiento intensivo que ya ha debilitado su sistema inmunitario.
Por tanto, aun cuando las vacunas génicas activaran la inmunidad en estos individuos, la respuesta emitida apenas si resultaría perceptible.
Es de suponer que las vacunas génicas produzcan mayores beneficios en pacientes con cargas tumorales mínimas y una inmunidad robusta.
Pero antes de ensayar con pacientes de este tipo, debemos esperar a que concluyan las pruebas con grupos más seriamente enfermos para establecer los riesgos asociados al tratamiento.
El desarrollo de nuevas terapias contra el cáncer es un proceso largo y complejo.

Afín a la anterior es la terapia génica que se basa en antígenos predominantes en las células cancerosas.
En los últimos tres o cuatro años se ha avanzado bastante en la identificación de los antígenos producidos por células tumorales.
Se han descubierto, además, los genes que determinan estos antígenos asociados a tumores;
de manera particular, los que se manifiestan en la forma más grave de cáncer de piel, el melanoma maligno.
Con la descripción de algunos de tales antígenos, empieza a vislumbrarse la posibilidad de sintetizar una vacuna que proteja del cáncer, de forma similar a como existen vacunas para el tétanos o la polio.
Semejante planteamiento podría aplicarse también en el tratamiento de tumores ya existentes.

Inmunización preventiva

Al igual que ocurría con las vacunas mediante citoquinas, las antioncológicas basadas en antígenos requieren la transferencia de genes.

Su efectividad es mayor si se administran a células de fácil acceso para el sistema inmunitario.
A este respecto, Philip L. Felgner, de Vical en San Diego, y Jon A. Wolff, de la Universidad de Winsconsin, han observado, junto con sus respectivos equipos, que la inyección directa, en el músculo de ratones, de un fragmento de ADN que determina un antígeno exógeno provoca una potente respuesta inmunitaria frente al antígeno.
La explicación de esta reacción no reviste mayor complejidad:
el ADN foráneo penetra en las células musculares o en las vecinas y dirige la producción de pequeñas cantidades de la proteína exógena correspondiente. 
Las células presentan la proteína sintetizada a los linfocitos B y T circulantes. 

Estas células inmunitarias, una vez sensibilizadas, recorren luego el organismo dispuestas a atacar a las células tumorales portadoras de ese mismo antígeno.

Esta estrategia básica ha revolucionado la obtención de vacunas para la prevención de muchas enfermedades infecciosas. 
Para verificar la eficacia de una inmunización con ADN en el tratamiento del cáncer, se administran directamente los genes de los antígenos que acaben de identificarse, ya sea mediante una vacuna o partículas de adenovirus inactivadas, ya sea mediante sistemas de transporte no víricos, como el ADN desnudo.
En la actualidad, las pruebas se realizan en pacientes con tumores muy diseminados.
Obviamente, en estos casos la vacuna de ADN ya no está en disposición de evitar la enfermedad, pero los estudios deberán demostrar si los antígenos cumplen el requerimiento mínimo de inducir una respuesta inmunitaria en el organismo.
Se comprobará, asimismo, si las vacunas de ADN pueden tratar tumores ya diagnosticados.
Por ahora, dado lo avanzado de la enfermedad en los pacientes estudiados, resulta difícil interpretar los resultados obtenidos.

Existe otro modelo de terapia génica, también basado en anticuerpos, que está en período de prueba en pacientes y en el laboratorio.
Los anticuerpos presentan regiones de gran variabilidad individual que las convierten en moléculas exquisitamente específicas.
Pueden distinguir la menor diferencia entre antígenos extraños o mutados y autoantígenos muy parecidos.
Algunas células cancerosas presentan, en sus membranas externas, moléculas de anticuerpos específicos; así, en los linfomas de células B, encargadas de la producción de anticuerpos.
Cada linaje o clon de células sintetiza el mismo anticuerpo, por lo que en todo cáncer que tenga su origen en estas células se podrá encontrar la misma molécula específica en la membrana.
El anticuerpo, que actúa como un marcador molecular único, podría permitir la diferenciación de las células tumorales de otras parecidas que también sean productoras de anticuerpos, pero no tumorales.

En algunos casos, los investigadores han conseguido sintetizar anticuerpos antiidiotipo, esto es, dirigidos contra los anticuerpos de las membranas de ciertas células cancerosas.
Aunque a veces se consigan tratamientos eficaces o incluso respuestas espectaculares en pacientes sometidos a tratamiento con anticuerpos antiidiotipo, su síntesis es laboriosa y limita su empleo. 
Las técnicas de transferencia de genes han ofrecido recientemente nuevas opciones.
Habida cuenta de que los anticuerpos vienen determinados por genes, se han preparado vacunas antiidiotipo de ADN que incorporan el ADN que determina el marcador crítico del cáncer (el idiotipo).
Esta secuencia de ADN se ha unido al gen de la citoquina GM-CSF.
Hasta ahora sólo se ha experimentado en animales de laboratorio con esta vacuna reforzada, pero parece muy prometedora.

Aún otra terapia reforzada, todavía en fase de estudio inicial, combina anticuerpos y linfocitos T.
Los linfocitos T de ciertos pacientes sí son capaces de reconocer las células cancerosas, si bien sólo identifican las del propio paciente o, en una pequeña fracción de casos, las de aquellos individuos que presenten el mismo tipo de cáncer y el mismo tipo de tejido.
Rara vez se encuentran anticuerpos humanos contra tumores.
Por contra, si se inyectan en un ratón células tumorales humanas, su sistema inmunitario fabricará anticuerpos que reaccionarán fuertemente contra esas mismas células tumorales.
En algunos casos los anticuerpos del ratón reaccionan in vitro uniéndose a prácticamente todas las células de un cáncer, incluso aunque se hayan tomado de individuos distintos con el mismo tipo de cáncer.
Estos anticuerpos murinos, empero, no destruyen las células cancerosas en humanos y, en los pocos casos en que sí lo logran, la respuesta tiene corta duración: 
el organismo produce a su vez anticuerpos que inactivan los anticuerpos de ratón.

Por este motivo, los oncólogos han buscado afanosamente la manera de conjugar la selectividad de los anticuerpos anticáncer de ratón con la capacidad de aniquilación de las células T humanas.
La técnica del ADN recombinante ofrece las herramientas precisas.
Se han aislado a partir de células del ratón los genes que determinan los anticuerpos anticáncer humano y se han recombinado parte de aquellos con segmentos de los genes que determinan el receptor que las células T asesinas emplean para identificar a sus objetivos.
Este receptor modificado faculta a la célula T asesina, a menudo incapaz de reconocer los tumores, para identificar lo que los anticuerpos de ratón, con menor capacidad de discriminación, ya han detectado. 
Se ha comprobado que las células T asesinas provistas de receptores de células T quiméricos eliminan de forma harto eficaz células tumorales in vitro .
Y se han puesto en marcha los primeros ensayos clínicos que emplean este método en el tratamiento no sólo del cáncer, sino también en infectados por el VIH, el agente etiológico del sida, y por otros patógenos.

Otras terapias génicas

Además de la inmunoterapia, se dispone de varias estrategias basadas en la genética con las que combatir los tumores.
No es vano el interés puesto en la identificación de las alteraciones concretas del ADN que producen cáncer.
Se ha establecido que ciertas mutaciones van asociadas a formas de cáncer específicas. 
Otras mutaciones aparecen en muchas formas distintas de cáncer.
Se diferencian los siguientes tipos de mutación:
algunas activan ciertos genes, conocidos como oncogenes, que inducen un multiplicación descontrolada de las células;
otras son la causa de la pérdida de los frenos fisiológicos del crecimiento celular.
Estas últimas afectan a los genes supresores de tumores. 

El gen supresor de tumores que con mayor frecuencia se encuentra alterado es el xxx , cuya producto proteico regula el ADN durante la división celular.
Si el ADN sufre cualquier alteración, la proteína xxx puede suspender la división celular hasta que se repare el ADN o inducir el suicidio celular (apoptosis).
Cuando se introduce una copia normal de este gen en células tumorales de un cultivo de tejido, se observa que dichas células siguen un patrón de crecimiento más regular o que inician procesos de apoptosis.
Cualquiera de las dos circunstancias sería interesante en el tratamiento del cáncer y ello explica el empeño puesto en el desarrollo de métodos que permitan la inserción de xxx normal en tumores presentes en el organismo.

Pero quedan muchas barreras por superar.
Las técnicas actuales de inserción de un gen en un órgano o una población celular son ineficaces.

Tampoco se dispone de métodos que faciliten la extensión de los efectos producidos por una inserción local a otras regiones del cuerpo, lo que limita la aplicación de las terapias génicas a tumores con localización única. 

Por otra parte, se ha observado en animales una mejora significativa tras la administración de xxx , se introdujera éste en el sistema circulatorio (en complejos lipídicos que faciliten su absorción celular) o se inyectara directamente en el tumor (gracias a virus modificados que actúan como lanzaderas de los genes).
En un ensayo clínico aún en fase inicial, se han descrito algunas regresiones locales.
En teoría hay una limitación importante a la transferencia de genes que activen genes supresores de tumores o neutralicen oncogenes: 
el gen corrector debe entrar en todas y cada una de las células tumorales.
Si no se logra eso, las células sin modificar continuarán multiplicándose desenfrenadamente. 
Con la técnica actual es imposible corregir los genes de cada célula tumoral, ni siquiera los de tumores localizados.
Si bien los tratamientos adicionales pueden ayudar a corregir un número mayor de células tumorales, la transferencia repetida de genes vía virus modificados no se ve realizable.
Con frecuencia, el sistema inmunitario reconoce a la segunda ocasión al virus y lo destruye antes de que pueda cumplir su cometido.

Sucede en ocasiones que los efectos beneficiosos de la inserción inicial aparecen en células que no han pasado por la corrección genética.

Diversos estudios de terapia génica experimental, aplicada al cáncer, informan de la aparición de un fenómeno de "solidaridad".
Se invoca este fenómeno para explicar por qué un tratamiento es capaz de destruir un número de células mayor que el atribuible a la cantidad de células en las que se ha inducido la expresión de cierto nuevo gen.
Los investigadores que han descrito este tipo de discrepancia en ensayos terapéuticos con xxx no son capaces de explicarla.
En buena lógica, si los genes xxx generaran un efecto de solidaridad, ya no tendría de entrada por qué desarrollarse un cáncer.
Este fenómeno es objeto de intensa investigación en relación con otros tratamientos, como la terapia de "suicidio génico", por la que un gen introducido en las células cancerosas las sensibiliza a alguna droga que previamente carecía de efecto anticanceroso. 

En la aplicación inicial de la terapia de suicidio génico, Edward H. Oldfield, Zvi Ram y Ken Kulver y el autor introdujeron, en células tumorales del cerebro, el gen productor de quinasa de timidina (tk) de un herpesvirus. 
En las células infectadas por el virus herpes simplex, esta enzima puede convertir el ganciclovir, un fármaco sin actividad tóxica, en un metabolito, o producto secundario, que actúa como un potente agente antivírico.
Se observó que este mismo metabolito tóxico podía destruir células cancerosas en división y que, en algunos tumores, acababa también con las células malignas vecinas.
Este producto ejercía su efecto de solidaridad gracias a su difusión, desde la célula en que aparecía, a través de las uniones de intervalo. (Denominadas en inglés gap junctions , estas uniones son canales intercelulares que permiten el paso de sustancias.) En el ensayo clínico original, aproximadamente una cuarta parte de los pacientes con tumores cerebrales respondieron al tratamiento.
Los clínicos experimentan otras terapias de suicidio génico mediado por distintas substancias anticancerosas, algunas de las cuales puede que presenten el fenómeno de solidaridad.


Genética y cáncer

CONCEPTOS DEL CAPÍTULO

En la actualidad se reconoce al cáncer como una anomalía genética en el ámbito celular, que implica la mutación de un pequeño numero de genes.
Muchos de estos genes actúan normalmente suprimiendo o estimulando la continuidad del ciclo celular, y la pérdida o inactivación de estos genes da lugar a una división celular descontrolada y a la formación de tumores.
Los factores ambientales y los virus juegan también un papel importante en las alteraciones genéticas que son necesarias para transformar células normales en cancerosas.

Aunque a menudo se considere como una sola enfermedad, el cáncer es realmente una serie compleja de enfermedades que afectan a un amplio rango de células y tejidos.
La primera conexión entre el cáncer y la genética se propuso a principios del siglo XX, y esta idea ha servido como una de las bases de la investigación sobre el cáncer. 
Las mutaciones que alteran el genoma o la expresión génica se consideran como un rasgo común de todos los cánceres.
En algunos casos, tales mutaciones inciden en la línea germinal y se heredan.
Muy a menudo, las mutaciones aparecen en las células somáticas y no pasan a la generación siguiente a través de las células germinales.
A veces, una mutación hereditaria tiene que venir acompañada por una mutación somática en el locas homólogo, dando lugar a homozigosis. 
Cualquiera que sea la situación, el cáncer se considera ahora como una anomalía genética en el ámbito celular. 

Las alteraciones genómicas asociadas con el cáncer pueden implicar cambios a pequeña escala, como la sustitución de un solo nucleótido, o a gran escala, como reordenaciones cromosómicas, ganancia o pérdida de cromosomas o incluso la integración de genomas virales en el cromosoma. 
Las alteraciones genómicas a gran escala son un rasgo común del cáncer; la mayoría de los tumores en la especie humana se caracterizan por cambios cromosómicos visibles.
Algunos de estos cambios cromosómicos, particularmente en la leucemia, son tan característicos que pueden utilizarse para diagnosticar y clasificar la enfermedad y para realizar una predicción precisa acerca de la gravedad y del curso de la enfermedad.

Las formas de cáncer familiar se han conocido hace unos doscientos años.
En muchos de estos casos, se pueden establecer patrones de herencia no muy bien definidos. 
Sin embargo, en un pequeño número de casos se puede establecer un patrón de herencia mendeliano, dominante o recesivo, lo que indica la naturaleza hereditaria del cáncer.

Debido a que la mutación es la causa subyacente del cáncer, siempre habrá una proporción básica de cánceres, ya que hay una tasa de fondo de mutación espontánea.
Además de esta proporción básica, los factores ambientales que promueven mutaciones también juegan un papel en el desarrollo del cáncer.
Los factores ambientales, como la radiación ionizante, los productos químicos y los virus, son agentes cancerígenos y actúan casi todos ellos generando mutaciones.
Dado que las mutaciones juegan un papel en el cáncer, en este capítulo exploraremos respuestas a una serie de preguntas acerca de cómo las mutaciones convierten a las células normales en tumores malignos, que genes mutantes tienen mayor probabilidad de dar lugar al cáncer y cuántas mutaciones se necesitan para dar lugar al cáncer.

Un método para responder a estas cuestiones es considerar qué propiedades de las células cancerosas las distinguen de las células normales y preguntarnos qué genes controlan estas propiedades.
Las células cancerosas tienen dos propiedades en común:
(1) una multiplicación incontrolada y (2) la capacidad para extenderse o producir metástasis desde su localización original a otras localizaciones corporales.
La división celular es el resultado del paso de las células por el ciclo celular; las células cancerosas pierden el control sobre el ciclo celular y proliferan rápidamente.
Las investigaciones sobre el control genético del ciclo celular están proporcionando ahora ideas sobre el origen del cáncer.

La metástasis de las células cancerosas está controlada por productos génicos que se localizan en la superficie celular v la genética de la metástasis está relacionada con la comprensión de cómo las células interactúan con la matriz extracelular y con otras células mediante las moléculas de la superficie celular.
Aunque este campo está menos desarrollado que el estudio del ciclo celular, está comenzando a proporcionar algunas ideas sobre los fenómenos secundarios de la progresión del tumor.

En este capítulo consideraremos las relaciones entre genes y cáncer, con énfasis en las relaciones entre el ciclo celular y las anomalías genéticas asociadas con el cáncer.
Examinaremos también las relaciones entre las mutaciones y el cáncer, la identificación de los genes que, cuando matan, inician la transformación de las células y estimaremos el número de mutaciones implicadas en la formación de tumores.
Estudiaremos también las relaciones entre los cambios cromosómicos y el cáncer y el papel de los factores ambientales en la génesis del cáncer.

El ciclo celular y el cáncer 

Como se describió en el Capítulo 2, el ciclo celular representa la secuencia de fenómenos que ocurren entre divisiones mitóticas en las células eucariotas. 
En años pasados, los trabajos sobre el control del ciclo celular se llevaron a cabo principalmente por dos grupos: 
(1) genéticos, trabajando con levaduras, especialmente Saccharomyces cerevisiae y Schizosaccharomyces pombe , y
(2) biólogos del desarrollo estudiando óvulos recién fecundados de organismos como ranas, erizos de mar y salamandras.

En los últimos años estos grupos han identificado y caracterizado, con éxito, genes implicados en el ciclo celular y su trabajo está ahora convergiendo y solapando con temas importantes de la biología del cáncer, especialmente con estudios de factores de crecimiento y de los genes que suprimen o promueven la formación de tumores. 
Esta convergencia ha tenido un efecto sinérgico, dando lugar a nuevas ideas sobre el proceso que controla la división celular y de cómo la regulación del ciclo celular está acoplada con la transcripción de genes concretos. 
Debido a estos desarrollos recientes, es necesario dedicar algún tiempo para describir lo que se conoce acerca de los pasos del ciclo celular y de los genes que regulan la progresión del ciclo.
Luego entraremos en la discusión de la genética del cáncer.

El ciclo celular

Resumiendo lo esencial, el ciclo celular progresa desde un periodo de replicación del DNA cromosómico (fase S) hasta la segregación de los cromosomas en dos núcleos en la mitosis (fase M).
Intercalados entre estos dos estadios hay dos períodos de reposo, el G1 y G2.
Juntos, G1, S y G2, constituyen la interfase del ciclo celular (Figura 21.1).

La fase G1 comienza después de la mitosis; en este momento se produce la síntesis de muchos elementos citoplásmicos, como ribosomas, enzimas y orgánulos derivados de membranas. 
En la fase S tiene lugar la replicación del DNA, produciéndose una copia duplicada de cada cromosoma.
Luego, hay un segundo periodo de crecimiento y síntesis, denominado G2, como preludio de la mitosis.

Debido a que la mitosis se produce rápidamente, normalmente en menos de una hora, la célula se encuentra la mayor parte del ciclo celular en interfase.
Sin embargo, la duración del ciclo celular (el periodo entre dos divisiones mitóticas) puede variar ampliamente entre células en el ciclo biológico de un organismo y entre tipos celulares diferentes del mismo organismo.
Por ejemplo, las células animales presentan in vivo ciclos que van de unos pocos minutos a unos meses.
La mayor parte de esta variación se puede situar en el periodo que dura G1.
El tiempo necesario para completar S y G2 permanece relativamente constante en muchos tipos celulares. 

Mientras que algunas células, como las meristemáticas de vegetales o las dérmicas de la especie humana, se dividen continuamente, otros tipos celulares, como las células nerviosas, se retiran de la fase G1 y entran en un estado permanente de no división llamado G0.
Otros tipos celulares, como los glóbulos blancos, pueden ser reclutados para volver del periodo G0 y reentrar en el ciclo celular.

En conjunto, estas observaciones sugieren que el ciclo celular está regulado estrictamente y depende de la historia biológica y del estado diferenciado de una célula dada.
En los últimos años se ha acumulado una gran cantidad de información acerca de la regulación genética del ciclo celular.
Un resumen del conocimiento actual acerca de la regulación genética del ciclo celular servir como preludio al estudio de la genética del cáncer.

Puntos de control y control del ciclo celular

A medida que la investigación en el ciclo celular de muchos organismos ha convergido, está comenzando a surgir un modelo universal del ciclo celular y de su regulación. 
Aunque no se conocen los detalles de todos los fenómenos moleculares e incluso del número exacto y de la secuencia de los pasos, probablemente todas las células eucariotas utilizan una serie de vías bioquímicas comunes para regular los sucesos del ciclo celular.
Si se mantienen los aspectos universales del modelo normal del ciclo celular, los resultados obtenidos del estudio de levaduras o de óvulos de almejas se pueden utilizar para comprender y predecir los acontecimientos en células humanas normales y en células mutantes que se han convertido en cancerosas.
Por ello, los descubrimientos en la investigación del ciclo celular seguirán produciéndose probablemente con gran rapidez y espectacularidad.

El esquema que está surgiendo indica que el ciclo celular está regulado en la transición G2/M y en punto de la fase G1.
Se conoce muy bien lo que sucede en la transición G2/M, pero el punto regulador en la fase tardía de G1, varias horas antes de iniciarse la fase S, todavía no se conoce tan bien.

En cada uno de estos puntos, se toma una decisión para continuar o detener la progresión a lo largo del ciclo celular.
Esta decisión está controlada mediante la interacción de dos clases de proteínas. 
Una clase es un grupo de enzimas llamadas proteínas quinasas , que fosforilan selectivamente proteínas diana.
Aunque en la célula existe un gran número de proteínas quinasas diferentes, sólo unas pocas están implicadas en la regulación del ciclo celular.
En el segundo grupo, las proteínas implicadas en el control de la continuación del ciclo celular se llaman ciclinas .
Estas proteínas, que se identificaron en embriones de invertebrados en desarrollo, se sintetizan y se degradan en un patrón sincrónico a lo largo del ciclo celular (Figura 21.2).
Varias ciclinas, como la C, D1, D2 y E, se acumulan durante la fase G1, persistiendo sólo la D1 después de la fase S.
La ciclina A se acumula hacia el final de la fase S y persiste hasta la transición G2/M; la ciclina B se acumula durante la fase G2 y persiste hasta el final de la fase M.
El acoplamiento de una quinasa y de una ciclina da lugar a una molécula reguladora que controla el paso de la célula de G2 hacia M y desde G1 hacia S.

El inicio de M en muchas células eucariotas está controlado por una quinasa llamada CDK1 (quinasa dependiente de la ciclina), que se caracterizó bioquímicamente en óvulos maduros de anfibios y se identificó genéticamente en levadura como el producto del gen cdc2 .
Esta proteína tiene una secuencia altamente conservada en todos los eucariotas examinados; su función es necesaria para entrar en la fase M.

Varios acontecimientos marcan el paso desde G2 hacia la mitosis (M), como la condensación de la cromatina para formar cromosomas, la degradación de la membrana nuclear y la reorganización del citoesqueleto.
Los acontecimientos principales en esta transición están regulados por la formación de un complejo activo CDK1/ciclina B. 
Cuando el CDK1 se une a la ciclina B, el componente CDK1 cataliza la fosforilación, que provoca la desaparición de la membrana nuclear y la reordenación del citoesqueleto. 
También se fosforita la histona H1, que puede jugar un papel en la condensación de la cromatina (Figura 21.3).
No está clara la función de la ciclina B en este complejo, pero puede controlar la localización celular o la especificidad de la molécula diana.
Aunque varios experimentos sugieren que la ciclina A está también implicada en la progresión desde G2 hacia M, no están claras Sus funciones.

Mientras que la acción de la ciclina B y de CDKl son responsables del paso hacia M, parece que varias ciclinas, incluyendo a la D y a la E, pueden hacer que la célula pase de G1 hacia S.
Ciertos experimentos indican que la quinasa CDK1 es también activa en fosforilación en Gl, pero que en lugar de combinarse con la ciclina B (que no está presente), se combina con las ciclinas de G1, dirigiendo la fosforilación de sustratos proteicas específicos del estadio G1.

En total, se ha identificado casi una docena de ciclinas diferentes y se están describiendo un número creciente de ciclinas dependientes de quinasas, indicando que en el ciclo celular existen puntos múltiples de control, o que estas quinasas y ciclinas tienen funciones múltiples.

Regulación del ciclo celular y cáncer

Las mutaciones que interrumpen cualquier paso de la regulación del ciclo celular son candidatas para el estudio de genes que dan lugar al cáncer.
Por ejemplo, las mutaciones de los genes que codifican a las quinasas y las ciclinas pueden ser importantes en la generación de transformaciones malignas de las células.
Se están acumulando pruebas de que el punto de control G1 es aberrante en muchas formas de cáncer, y se cree que los mutantes de las ciclinas y quinasas de G1 son los mejores candidatos de genes que provocan cáncer. 
Recientemente se ha demostrado que una forma de la ciclina D, llamada D1, es idéntica al producto génico que está sobreexpresado en ciertas formas de leucemia.

En la siguiente sección resumiremos lo que se sabe acerca de la genética de algunos cánceres y, siempre que sea posible, relacionaremos esta información con lo que se ha descubierto acerca del ciclo celular y de su regulación. 

Genes y cáncer

Los estudios genéticos de varios tipos de cáncer diferentes han permitido la identificación de un pequeño número de genes que deben estar matados a fin de disparar el desarrollo del cáncer o para mantener la multiplicación de las células malignas.
Está claro que las dos propiedades principales del cáncer, la división celular incontrolada y su capacidad de metástasis, son el resultado de alteraciones genéticas.
Como se mencionó anteriormente, estas alteraciones pueden implicar inestabilidad genómica a gran escala, que da lugar a la pérdida de cromosomas, a reordenaciones o a la inserción de secuencias de DNA foráneo (a menudo viral) en loci de cromosomas humanos.
Puede haber también alteraciones a pequeña escala implicadas en cambios en la secuencia nucleotídica, o modificaciones más pequeñas que pueden alterar sólo la cantidad del producto de un gen o el momento en el que el producto de un gen es activo.
Al considerar el cáncer como una anomalía genética en el ámbito celular surgen dos cuestiones relacionadas: 
(1) ¿hay alelos mutantes que predispongan a un organismo, o a un tipo celular específico, al cáncer? 
, y (2) si es así, cuántos sucesos mutacionales son necesarios para provocar el cáncer?

Genes que predisponen al cáncer 

Realmente hay genes que predisponen a las células a convertirse en malignas, y es posible identificar familias en las que ciertos tipos de cáncer son hereditarios.
Muchos estudios han identificado familias con una alta frecuencia de ciertos tipos de cáncer, como cáncer de mama, de colon o de riñón.
Sin embargo, en muchos casos es difícil identificar un patrón claro y simple de herencia.
Un ejemplo de tal predisposición es la herencia del retinoblastoma (RB), un cáncer de las células retínales del ojo.
El retinoblastoma se da con una frecuencia variable de 1 en 14.000 a 1 en 20.000, y muy a menudo aparece a la edad de 1 a 3 años.

Tabla 21.1

Predisposición a padecer tumores heredables de manera dominante

Gen de predisposición a tumores.
Cromosoma.


Aparición temprana familiar del cáncer de mama.
17q.


Cáncer de colon hereditario sin pólipos.
2p.


Melanoma familiar.
9p.


Neoplasia endocrina múltiple, tipo 1.
11q .


Neoplasia endocrina múltiple, tipo 2.
22q.


Neurofibromatosis, tipo I.
17q.


Neurofibromatosis, tipo 2.
22q.


Poliposis adenomatosa familiar.
5q.


Retinoblastoma .
3q.


Síndrome de Gorlin .
9q.


Síndrome de Li-Fraumeni .
17p.


Síndrome de von Hippel-Lindau.
3p.


Tumor de Wilms.
11p.


Se conocen dos formas de retinoblastoma.
Una es la forma familiar (cerca del 40 por ciento de los casos), que se hereda como una carácter autosómico dominante, aunque la mutación en sí misma es recesiva, lo que se explicar más adelante.
Debido a que el carácter es dominante, aquellos que heredan el alelo mutante RB (el 50 por ciento de los miembros de la familia) están predispuestos a desarrollar tumores en la retina; de hecho el 90 por ciento de estos individuos desarrollan tumores, normalmente en ambos ojos.
Además, aquellos miembros de la familia que hayan heredado el alelo mutante están predispuestos a desarrollar otras formas de cáncer, como el osteosarcoma, un cáncer de hueso, incluso cuando no desarrollan el retinoblastoma.

También se conoce una segunda forma de retinoblastoma, que explica el 60 por ciento de los casos.
Esta forma no es familiar y los tumores se desarrollan espontáneamente. 
La forma esporádica se caracteriza por la aparición de tumores sólo en un ojo, y su inicio se da mucho más tarde que la forma familiar.

Se conocen otros tipos de herencia familiar del cáncer dominante (Tabla 21.1).
El tumor de Wilms (WT) , un cáncer hereditario de riñón, autosómico dominante y el síndrome de Li-Fraumeni , una rara situación autosómica dominante que predispone a un cierto número de cánceres diferentes, incluido el cáncer de mama.

¿Cuántas mutaciones son necesarias?

El estudio de genes que predisponen a los individuos al cáncer ha permitido a los científicos estimar el número y la secuencia de sucesos mutacionales necesarios para disparar diferentes formas de cáncer.
Al estudiar los dos tipos de retinoblastoma, Alfred Knudson y sus colegas desarrollaron un modelo que requiere la presencia de dos copias mutantes del gen RB en la misma célula retinal para desarrollar el tumor (es decir, el alelo mutante es recesivo).
En la forma familiar, se hereda un alelo mutante RB, que se encuentra en todas las células corporales, incluidas las celarlas de la retina (Figura 21.4).
Si el alelo normal del gen RB marta en una célula retinal, se dar lugar a la formación del tumor.
Los individuos que llevan una mutación heredada del gen RB están predispuestos a desarrollar retinoblastoma, necesitándose sólo un suceso mutacional adicional para dar lugar a la formación del tumor.
Sin embargo, esto no ocurre en todos los casos; alrededor del 10 por ciento de los que heredan el gen mutante RB no desarrollan cáncer.
Este modelo explica de qué manera la mutación puede actuar como un carácter recesivo (ambos alelos debe ser inactivos para observar el fenotipo mutante), y cómo el llevar un alelo mutante actúa como un carácter dominante en predisponer a un individuo al cáncer (se necesita sólo una mutacional adicional).

En los casos esporádicos tienen que darse dos mutaciones independientes, una en cada uno de los alelos del gen RB de la misma célula retinal, para que se desarrolle un tumor. 
Como cabría esperar, estos hechos son menos frecuentes y ocurren a una edad más tardía, por lo que la forma esporádica de retinoblastoma es más probable que ocurra sólo en un ojo, y a una edad más tardía que la forma familiar.

Estudios similares sobre la predisposición a otros tipos de cáncer han llevado a la conclusión de que el número de mutaciones necesario para el desarrollo del cáncer va de 2 hasta, quizá, 20 (Tabla 21.2). 

Tabla 21.2

Número de mutaciones asociadas con algunos cánceres

Cáncer.
Lugares cromosómicos .
Número mínimo de mutaciones requerido .


Retinoblastoma.
13q.
2.


Tumor de Wilms.
1lp.
2.


Cáncer de colon.
5p, 12p, 17p, 18q.
4-5.


Cáncer de pulmón de células pequeñas.
3p, llp. 13q, 17p.
10-15.


Genes supresores de tumores

En general, la mitosis puede estar regulada de dos maneras: 
(1) por genes que actúan normalmente deteniendo la división celular y
(2) por genes que normalmente funcionan promoviendo la división celular.

La primera clase, llamados genes supresores de tumores , inactiva o reprima el progreso a través del ciclo celular y de la división celular resultante.
Estos genes y/o sus productos génicos deben estar ausentes o inactivos para que tenga lugar la división celular.
Si estos genes quedan completamente inactivados o se pierden por mutación, se pierde el control sobre la división celular, y la célula comienza a proliferar de un modo incontrolado.

Los genes de la segunda clase, llamados protooncogenes , funcionan normalmente promoviendo la división celular. 
Estos genes pueden "activarse" o "desactivarse", y cuando están "activos" promueven la división celular. 
Para detener la división celular, estos genes y/o sus productos génicos, tienen que inactivarse.
Si los protooncogenes quedan permanentemente activados, entonces se da una división celular incontrolada, dando lugar a la formación de un tumor. Las formas mutantes de los protooncogenes se conocen con el nombre de oncogenes .

Consideraremos algunos ejemplos de cómo las mutaciones en genes supresores de tumores pueden dar lugar a la pérdida del control sobre la división celular y al desarrollo del cáncer.
Después de esta discusión, consideraremos los protooncogenes y los oncogenes, de qué manera los alelos normales actúan regulando la división celular y cómo actúan los oncogenes mutantes para promover la formación de un tumor.

Figura 21.1

El ciclo celular está controlado en dos, y posiblemente más, puntos de control, uno en la transición G2/M y otro al final de la fase G I, antes de entrar en la fase S

Estos puntos de control implican interacciones entre proteínas transitorias, llamadas ciclinas, y quinasas, que añaden grupos fosfato a las proteínas.
La fosforilación de las proteínas diana dispara una cascada de sucesos que permiten el progreso a través del ciclo celular.

Figura 21.2

Expresión relativa de los momentos y de las cantidades de ciclinas durante el ciclo celular

D I se acumula tempranamente en G1 y se expresa a un nivel constante a lo largo de gran parte del ciclo.
La ciclina C se acumula en G I, alcanza un máximo y declina hacia la mitad de la fase 5.
Las ciclinas D2 y E comienzan a acumularse hacia la segunda mitad de G I, alcanzan un máximo justo antes de comenzar S y luego declinan en los comienzos de G2.
La ciclina A aparece tardíamente en G I, se acumula a lo largo de 5, alcanza un máximo en G2 y se degrada rápidamente cuando comienza la fase M.
La ciclina B aparece hacia mediados de la fase S, tiene un máximo en el periodo de transición G21M y se degrada rápidamente. 

Figura 21.3

La transición de G2 a M está controlada por CDKI y la ciclina B

Estas moléculas interactúan para formar un complejo que añade grupos fosfato a componentes celulares que destruyen la membrana nuclear (polipéptidos laminas A, B y C), reorganizan el citoesqueleto (caldesmon) e inician la condensación de los cromosomas (histona Hl). 
La ciclina B puede especificar la localización celular o las moléculas diana.
Se cree que otras ciclinas (especialmente la ciclina A) están implicadas en esta transición, pero sus funciones son desconocidas. 

Figura 21.4

En casos espontáneos de retinoblastoma (izquierda), una única célula sufre dos mutaciones en el gen del retinoblastoma, dando lugar al crecimiento y a la división celular incontrolada, produciendo la formación de un tumor

En los casos familiares de retinoblastoma (derecha). una mutación se hereda y se encuentra en todas las células, una segunda mutación en el locas del retinoblastoma en cualquier célula de la retina da lugar a un crecimiento celular incontrolado y a la formación de un tumor.


Extremos pegajosos y una nueva creación 

James Watson, en su libro El ADN recombinante, Un curso breve, recuerda que en 1966, a pesar del hecho de que el código genético había sido completamente descifrado, la biología molecular parecía estar en un impasse.
A menos que surgiesen métodos radicalmente nuevos para permitir a los científicos aislar y estudiar genes individuales, parecía haber una escasa perspectiva de nuevos avances espectaculares.
Según Watson, para evitar «estar haciendo tiempo», varios investigadores eminentes dejaron la genética molecular por la neurobiología. 
Sin duda se refería, entre otros, a Francis Crick, quien se pasó al campo de la biología evolutiva y al estudio del cerebro.

Crick creía que las bases de la biología molecular habían sido bien establecidas y sólo restaba la extensa tarea de «llenar los muchos detalles». 
No previó, y no podría haberlo hecho, que una verdadera avalancha de descubrimientos pronto inundaría a la comunidad científica e iniciaría un período de excitación y logros que ha tenido pocos paralelos en la historia de la investigación biológica.

Como ha ocurrido en tantos otros avances en las ciencias de la vida, estos adelantos espectaculares surgieron no como el resultado de un gran programa de investigación organizado, con responsabilidades asignadas, sino del estudio voluntario, en laboratorios independientes, de criaturas de apariencia inocua y, para el lego, más bien insignificantes.

Algunos críticos, de hecho, creen encontrar en el Proyecto Genoma Humano los defectos de que está demasiado orientado por metas y que podría distraer a individuos y fondos de otras áreas vitales, menos espectaculares; más adelante continuaremos discutiendo el tema.

La historia de la ciencia es un rico mosaico de líneas individuales de investigación, a menudo sin ningún fin evidente más allá del conocimiento en y por sí mismo.
Naturalmente se siguen tendencias y una conclusión puede, con frecuencia, adelantar la investigación de otra distinta.

Las observaciones aisladas, las especulaciones y los datos aparentemente no relacionados pueden o no desaparecer en la oscuridad. 

Pueden, también, resurgir precisamente en el momento que son necesarios para un nuevo campo, quizá nunca vislumbrado por los investigadores originarios.
Tal fue el caso, una vez más, en la biología molecular, durante finales de la década de los 60, cuando diferentes descubrimientos convergieron y la breve sensación de estancamiento se transformó rápidamente en la percepción de una oportunidad sin precedentes.

Recordemos nuestro relato del descubrimiento de los virus, incluyendo los bacteriófagos, o «fagos», los virus que atacan bacterias.
Los más intensamente estudiados de éstos habían sido los fagos que infectan a la Escherichia coli .
Esta humilde bacteria, aislada por primera vez de las heces de un niño en 1888, por el doctor Theodore Escherich, podría parecer que carece del brío necesario como para transformarse en un actor famoso en el drama de la historia de la genética.
De hecho, con la excepción del organismo humano, esta célula, con forma de bastón, de menos de l micra de longitud, ha sido estudiada más intensamente que ningún otro organismo vivo.
La E. coli es nuestra compañera constante.
Vive en cientos de miles de millones en el intestino grueso' en compañía de otras 400 o más especies bacterianas.
Estas bacterias son una amenaza para nosotros sólo debido a la posibilidad de una infección que resulte de un trauma con perforación del intestino y liberación de su contenido en la cavidad abdominal, normalmente estéril.
La función importante de estas bacterias es proteger de la invasión de organismos causantes de enfermedades en el contenido intestinal.
Tales intrusos son rutinariamente dominados por estas bacterias «amistosas».

Los bacteriófagos denominados fagos T par son virus que contienen ADN que infecta a las E. coli .
Recuérdese que los virus no son organismos vivientes, sino más bien ácidos nucleicos, ya sea ADN o ARN, en- vueltos en proteína. 
Pueden propagarse sólo ingresando en una célula viviente y utilizando el metabolismo celular para hacer copias de si mismos.
Sin embargo son extraños, parásitos sin vida que pasan de una célula a otra, pereciendo eventualmente, si es que puede usarse ese término, si no pueden encontrar y entrar en una célula y forzarla a hacer su voluntad. 
Hacer que las células no sean hospitalarias con los virus dañinos es la base de muchas vacunas.

Un ejemplo espectacular es el de la vacuna para el virus de la viruela.
Como resultado de un esfuerzo mundial por erradicar la viruela, una enfermedad viral que durante siglos ha matado y desfigurado a millones de personas, la Organización Mundial de la Salud montó un esfuerzo masivo para identificar y vacunar a todas las personas en contacto con todo paciente de viruela conocido.
Los humanos son los únicos anfitriones conocidos de este virus.
En octubre de 1975 se informó del último caso de viruela en el mundo. 
El virus ya no podía encontrar células sin mecanismos de defensa adecuados y el flagelo de la viruela desapareció del planeta.
Se espera que el aislamiento de genes durante el Proyecto Genoma Humano dé lugar al desarrollo de muchas vacunas, inobtenibles mediante los métodos convencionales, que permitan hacer desaparecer virus perniciosos. 

Figura 1

Los dos lados de la doble hélice son grupos de azúcar (S) y fosfato (P) alternantes unidos entre sí

Los lados se mantienen unidos mediante enlaces de hidrógeno entre pares de bases nitrogenadas (A-T o G-C) que están ligadas a los azúcares.

Figura 2

Un nucleótido consiste en un fosfato, un azúcar desoxirribosa y una de cuatro bases nitrogenadas posibles (A, C, G o T)

Una de estas últimas se asocia al carbono I' del azúcar.
En el ADN, los nucleótidos adyacentes están unidos por el fosfato entre el átomo de carbono 5' del azúcar de un nucleótido y el átomo de carbono 3' del azúcar en el nucleótido asociado.
En el ADN, un lado de la doble hélice termina en un extremo 3' mientras que el otro lado, alineado en la dirección opuesta (antiparalela), termina en un extremo 5'.

Para mediados de los sesenta, la descomposición rápida del ADN y el ARN en soluciones mediante la trituración de células en el laboratorio era un fenómeno conocido. 
La destrucción de estos ácidos nucleicos fue rastreada en las «nucleasas», enzimas hallados en muchos tipos de células.
Estos enzimas dividían los ácidos nucleicos en muchos fragmentos aleatorios. 
Sin embargo, los medios mediante los cuales una célula podría reunir fragmentos de ADN eran desconocidos.
Entonces, en 1967, se halló un enzima que podía hacer justamente eso.
Sorprendentemente, este enzima, la ADN ligase, fue descubierta casi simultáneamente por cinco grupos de investigación que trabajaban en forma independiente en laboratorios distintos.

Antes de continuar, será útil repasar y elaborar algunos de los detalles de la estructura del ADN que deben ser claramente comprendidos.
Visualicemos la doble hélice de ADN, simplemente, como una es- calera.
Una escalera tiene dos laterales y una serie de escalones.
Los escalones, en esta analogía, son dos secciones unidas, cada una un par complementario de bases nitrogenadas, AT o GC.
Los laterales son los «espinazos» de azúcar desoxirribosa (S) y fosfato (P) de la hélice: son una cadena continua de S-P-S-P-S-P, y así siguiendo.
Según el protocolo químico, los cinco carbonos del azúcar desoxirribosa están numera- dos para poderlos distinguir. 
En este esquema, se los lista como los átomos de carbono 1' a 5'.
El átomo de carbono 5' se proyecta desde el resto de la molécula de azúcar, en forma de anillo.
Un grupo hidroxilo (OH) está conectado con el carbono 3'.

Para continuar la analogía con la escalera, se puede decir que el lateral izquierdo corre de arriba abajo y el lateral derecho de abajo arriba.

El resultado es que el lateral izquierdo termina en el tope con un grupo fosfato saliendo del átomo de carbono 5' y en el fondo con un grupo hidroxilo saliendo desde el átomo de carbono 3'.
El lateral derecho es justamente lo opuesto. 
Las consecuencias prácticas para la molécula son que esta orientación le permite mantener su forma de doble hélice simétrica y funcionar normalmente. 
Tenga esta estructura en mente mientras continuamos nuestra historia.

En 1970, un grupo que trabajaba en el laboratorio de Har Gobind Khorana, entonces en la Universidad de Wisconsin, encontró que un enzima ADN ligase producido por la E. coli podía juntar aleatoriamente los extremos de trozos de ADN completamente separados.
La reacción era ineficiente, ya que dependía de que los extremos de las cadenas entraran en contacto directo.
Para hacer el proceso más eficiente, se necesitaría un modo de mantener unidos los dos extremos para que la ligase pudiese actuar. 

Un método semejante fue diseñado rápidamente en Stanford, en el laboratorio de Peter Lobban y A. Dale Kaiser, y también en el de David Jackson, Robert Simons y Paul Berg.
El primer grupo usaba ADN ligasa para unir los trozos de ADN cortados de bacteriófagos.
El segundo grupo de científicos usó una nueva técnica para unir ADN de bacteriófagos con ADN de un virus animal. 

Este procedimiento involucraba los siguientes pasos básicos. 
El ADN era tratado primero con un enzima hecho por el fugo bacteriano «lambda».
El enzima era una «exonucleasa», así llamado debido a que funciona cortando nucleótidos del extremo de una molécula de ADN.

Sólo recorta el extremo 5' del nucleótido y como resultado deja un sólo 3' proyectado en cada extremo de la molécula lineal de ADN.
A estos extremos 3' se asociaba una corta serie de nucleótidos idénticos que contenían adenina, mediante la actividad de otro enzima. 
Otro trozo de ADN es tratado de un modo similar, excepto que se agregan nucleótidos que contienen timina en lugar de adenina.

Cuando estas dos muestras de ADN se mezclaban, las «colas» complementarias de los nucleótidos con A y T se unían mediante enlaces de hidrógeno.
Esto combinaba los fragmentos antes separados en largas cadenas interconectadas. 
Se agregaba entonces la ADN ligase para formar enlaces entre el azúcar y los grupos fosfato.
Las dos cadenas de ADN se convertían en una sola.

Era ciertamente intrigante que se pudiese cortar el ADN en fragmentos heterogéneos impredecibles y pegarlos aleatoriamente de nuevo.
Sin embargo, para poder avanzar en la organización del ADN y sus genes -esto es, la determinación de secuencias precisas de nucleótidos- era necesario hallar nucleasas muy específicas.
La opinión prevaleciente era que esa capacidad de cortes específicos en el ADN no existía en la naturaleza.

El único indicio de la posibilidad de que existiesen nucleasas más específicas, provino de observaciones, comenzadas en 1953, de que cuando las moléculas del ADN de la E. coli eran introducidas en otra forma apenas diferente de E. coli rara vez funcionaban genéticamente. 
En seguida eran descompuestas en fragmentos menores. 

Esto era, en apariencia, parte de un sistema que había evolucionado en las bacterias para protegerse contra la entrada de ADN extraño.
Además de todas las formas más obvias de competencia en la naturaleza, hay una lucha invisible constante en el mundo microscópico, en este caso entre las bacterias y los bacteriófagos.
La selección natural de Darwin se recrea aquí a una escala minúscula.

Todos los organismos han desarrollado alguna forma de defensa, incluyendo el simple expediente de crecer rápidamente. 
Esto asegurará que algunos sobrevivirán por el mero peso de su número, a pesar del ataque constante por parte de predadores.
Las bacterias son ejemplos fundamentales de esto.
Además de su notable capacidad para multiplicarse con rapidez, lo que en algunas especies permite que una población duplique su tamaño cada 20 minutos, producen enzimas que destruyen el ADN que ingresa como resultado de la infección de algún fugo o por algún otro medio.

Estos enzimas han sido hallados por una serie de investigadores, en diversos laboratorios, a mediados y finales de los sesenta. 
Los denominaron «endonucleasas de restricción» o, simplemente, «enzimas de restricción», ya que cortan o «restringen» los ácidos nucleicos y fueron producidos dentro de la célula.
En 1970, Hamilton Smith, de la Universidad Johns Hopkins, continuó desarrollando su descubrimiento accidental de que la bacteria Haemophilus influenzae rápidamente descompuso el ADN de un fugo, pero no dañó su propio genoma.
El estudio de extractos libres de células de este organismo mostró que la degradación del ADN era causada por un enzima de restricción, que él y sus colegas aislaron y denominaron Hind II.
Éste fue el primero de lo que llegaría a conocerse como enzimas de Tipo II.
Su uso revolucionaría la biología molecular.

Hind II era significativamente diferente de otros enzimas de restricción aislados con anterioridad, en el sentido de que era «específico de un lugar».
Eso significa que tema la característica muy importante de ser capaz de reconocer una secuencia de bases nitrogenadas específica y cortar el ADN sólo en ese lugar.

Aún más extraordinario era el enzima EcoR I, aislado poco después por Robert Yoshimori, en el laboratorio de Herbert Boyer, en la Escuela de Medicina de la Universidad de California en San Francisco.
Este enzima indujo rupturas que estaban separadas por varios nucleótidos.

Cada corte era siempre hacia el extremo 5' C de una cadena. 

Este enzima corta al ADN sólo entre G y A en los lados opuestos de la hélice.
La hélice se separa en ese punto, dejando dos extremos sobresalientes con secuencias de nucleótidos complementarias.
Éstos se conocen como «extremos pegajosos» debido a que las secuencias complementarias pueden pegarse fácilmente de nuevo formando enlaces de hidrógeno.

En noviembre de 1972, Janet Mertz y Ronald W. Davis, de Stanford, informaron que el ADN viral dividido por EcoR I volvía a unirse mediante enlaces de hidrógeno y podía quedar unido permanentemente mediante el tratamiento con ADN ligase.
Las posibilidades eran asombrosas.
Usando este enfoque, podía el ADN de un organismo ser recortado y vuelto a empalmar con el ADN de otro organismo?

En otras palabras, podrían las barreras naturales, que impiden el intercambio de información genética entre organismos no relacionados, ser superadas?
Estas barreras han sido construidas durante millones de años de evolución.
Las especies se identifican y determinan por los genes que portan.
En organismos que se reproducen sexualmente, las especies se definen literalmente por la capacidad de sus miembros para cruzarse entre sí.
Esta persistencia de la unicidad genética, en lugar del caos que de otro modo resultaría, se extiende por todo el mundo viviente. 
Podía esto superarse?

La respuesta era sí.
El primer paso hacia ello fue dado en 1973.

A.C.Y. Chang y Stanley Cohen, en la Escuela de Medicina de la Universidad de Stanford, y Herbert Boyer y Robert Helling, en la Escuela de Medicina de la Universidad de California en San Francisco, informaron acerca de la unión de moléculas de ADN biológicamente funcionales de dos organismos diferentes. 
Llamaron a su molécula compuesta un ADN «quimera», debido a su similitud conceptual con la mitológica Quimera, una criatura con cabeza de león, cuerpo de cabra y cola de serpiente.

En este experimento pionero, los fragmentos del ADN de la E. coli fueron combinados e insertados en otras células de E. coli , en donde el ADN híbrido funcionó normalmente.
El ADN quimérico llegó a ser conocido como ADN recombinante, o rADN, y el proceso de división, fusión y replicación de la quimera como «ingeniería genética».

Nótese que en este primer experimento con el rADN, el ADN no había sido intercambiado entre especies, sino de una E. coli a otra.
Usando los procedimientos de su primer éxito, el grupo de investigación continuó con la unión del ADN de la peligrosa bacteria Staphylococcus en la E. coli .
Dados estos éxitos, surgió naturalmente la pregunta: podían los genes animales también ser introducidos en las bacterias y se replicarían y funcionarían allí?
Pronto obtuvieron una respuesta.
Usando ADN de un sapo, unieron un segmento de un gen del sapo con el ADN de la E. coli y colocaron el nuevo ADN en células de la bacteria.
La mitología había cobrado vida.
Una quimera animal-bacteriana había sido creada en el laboratorio.
Las barreras naturales entre los reinos vivientes habían sido superadas. 

Figura 3

El enzima Hind II corta completamente la doble hélice del ADN en un lugar específico 

Figura 4

EcoR I corta entre G y A en lados opuestos de la doble hélice de ADN

Figura 5

Los «extremos pegajosos» son bases nitrogenadas expuestas

Incorporados en estos tres extraordinarios experimentos, simples en principio y expuestos más arriba de forma sumaria, están los métodos fundamentales del Proyecto Genoma Humano.
Debemos ahora explayarnos acerca de ellos en este contexto, con el fin de facilitar una clara comprensión de estos procedimientos.

Ya hemos explicado la acción de los enzimas de restricción. 
Desde que las propiedades del enzima Hind II original han sido descritas y demostradas por Huntington Smith, en 1970, varios cientos de formas de estos enzimas han sido aisladas y purificadas, cada una con sus propios puntos específicos de incidencia en la molécula de ADN.
Estos enzimas reconocen una secuencia de cuatro a seis nucleótidos, y la mayoría forman un corte escalonado en lados opuestos de la doble hélice, como en el caso de la EcoR I. Así, dejan extremos pegajosos de bases nitrogenadas no apareadas. 
Ahora, veinte años más tarde, se puede simplemente examinar un catálogo de suministros de biología molecular, llamar por teléfono y pedir cualquiera de estos enzimas.

Como las bases en la doble hélice del ADN son complementarias, el patrón escalonado de corte significa, como hemos señalado, que la mitad de los extremos pegajosos son complementarios de la otra mitad.
Por ende, teóricamente, el ADN de cualquier otra fuente -plantas, animales, bacterias, etc.- cortado con el mismo enzima de restricción tendrá estos mismos extremos pegajosos complementarios accesibles.
Cuando se mezcla las dos muestras de ADN bajo condiciones apropiadas y se agrega ADN ligase, las bases complementarias se unirán mediante enlaces de hidrógeno y la ligase completará la reparación de los espinazos helicoidales.
En semejante mezcla, el ADN de una fuente puede fusionarse con el ADN de la misma fuente o con ADN extraño.

Los nombres peculiares de los enzimas de restricción se obtienen de la siguiente manera: las tres primeras letras se toman de los nombres latinos de la bacteria de la que fueron aislados, el género provee la primera letra y la especie las dos siguientes.
Haemaphilus influenzae se transforma en Hin y Escherichia coli en Eco.
A veces se incluye una designación para indicar una forma específica (cepa) de una bacteria, tal como Haemaphilus cepa Rd en Hind o E. coli cepa RY13 en EcoR.

Como, con frecuencia, un organismo puede producir más de un enzima de restricción, cada uno recibe un número al ser aislado, como en Hind II o EcoR I. Cortar y empalmar las moléculas de doble cadena del ADN con enzimas ya era posible en 1972.
Pero, para hacer más que simplemente existir en un tubo de ensayo como curiosidad, estas quimeras debían ser colocadas dentro de una molécula de ADN que pudiera replicarse y funcionar en una célula particular.
La célula de elección ha sido la célula bacteriana, por varias razones.

Primero, las bacterias pueden ser cultivadas bajo condiciones controladas, rápidamente y en cantidades enormes.
En una noche, unas pocas celarlas se multiplicarán en, literalmente, miles de millones Es muy importante comprender que una célula bacteriana, por lo general, se reproduce simplemente copiándose a sí misma.
Suponiendo que ninguna mutación ocurriese en las células, todos los descendientes de esa única célula serían idénticos.
Una tal población de células que se originan a partir de una sola célula se denomina «clon», y el proceso de producir ese clon se denomina «clonación» de la célula.

El ADN en una célula bacteriana típica existe en dos formas.
Una es el cromosoma bacteriano individual que, a diferencia de los cromosomas de nuestras células, tiene la forma de una molécula circular.
El ADN de todos los otros organismos puede ser comparado con una larga cinta.
En las bacterias, los extremos de la cinta están unidos, formando un círculo.

Además del ADN del cromosoma bacteriano, el ADN también existe en las bacterias en la forma de plásmidos. 
Éstos, como el cromosoma bacteriano, también son moléculas circulares de ADN, pero mucho más pequeñas.
Cuando la célula bacteriana se divide, el cromosoma bacteriano se replica y uno de los cromosomas pasa a la nueva célula.
Del mismo modo, cada uno de los plásmidos se replica y la mitad son entregados a la generación siguiente.
Los plásmidos son moléculas de ADN únicas, independientes y autorreplicantes que pueden existir sólo dentro de la célula bacteriana viviente.

El número de plásmidos es de unos 30 por célula, y tipos específicos de bacterias pueden tener uno por célula y otros varios cientos.
La E. coli tiene 4, 5 millones de pares de nucleótidos en su cromosoma, mientras que sus plásmidos pueden tener sólo unos pocos miles. 
El genoma humano consta de 4.000 millones de pares de nucleótidos.

Los plásmidos pueden ser aislados fácilmente de las bacterias rompiendo las células con enzimas que pueden descomponer la pared celular.
La mezcla resultante es centrifugada.
El ADN cromosómico más pesado, denominado ADN «genómico», junto con fragmentos de la célula, irá al fondo.
Esto deja una suspensión, relativamente limpia, de plásmidos, cerca de la superficie del tubo centrífugo.

Estos pequeños círculos de ADN no son realmente vitales para la supervivencia de la bacteria.
Los plásmidos pueden ser eliminados de una célula bacteriana y la célula funcionará normalmente.
Sin embargo, algunos plásmidos contienen genes que confieren una marcada ventaja a la célula bajo ciertas condiciones.
Por ejemplo, el veneno fatal del tétanos es un producto de genes que hay en los plásmidos de la bacteria del tétanos.
La E. coli tiene plásmidos que causan una forma de la infame «diarrea del viajante».

Probablemente los genes de los plásmidos más ampliamente estudiados sean los que confieren resistencia a antibióticos específicos.
Ciertas bacterias pueden producir enzimas, codificados en los genes plásmidos, que pueden descomponer antibióticos tales como la penicilina, ampicilina, tetraciclina o cloramfenicol.
En la naturaleza, esto da a la bacteria un mecanismo de defensa contra los antibióticos naturales.
En los tejidos de un paciente infectado, las bacterias con estos plásmidos pueden ser inmunes a la administración de antibióticos terapéuticos. 
Estas infecciones resistentes se han convertido en un problema médico importante.

Esta descripción aparentemente esotérica de la vida bacteriana contiene otro elemento clave de nuestra historia.
Los plásmidos bacterianos son usados como moléculas de ADN en las que otros fragmentos de ADN recortados por un enzima de restricción pueden ser colocados. 

Volviendo a nuestro sistema original, si cortamos cualquier ADN con un enzima de restricción y cortamos plásmidos con el mismo enzima, mezclamos los plásmidos recortados y el ADN cortado en presencia de ADN ligase, se formarán quimeras de plásmidos más ADN extraño.

Estos plásmidos quiméricos, colocados en bacterias receptivas, «vivirán», esto es, se replicaran y producirán, cualquier producto genético que sus genes constituyentes estén codificando.
Si el ADN extraño en el plásmido híbrido porta el código apropiado para la proteína x, entonces se producirá la proteína x.
Este fenómeno sorprendente demuestra la notable unidad de todos los sistemas vivientes.
Las bacterias, organismos primitivos antiguos, tienen en lo fundamental la misma bioquímica celular que las ranas, ratones, árboles o seres humanos.
Dado el código casi universal de las instrucciones genéticas, escritas como secuencias de bases nitrogenadas, la célula bacteriana, bajo condiciones apropiadas, se dedicará a la fabricación de proteínas humanas, tan fácilmente como si fueran un producto bacteriano.

Consideramos de nuevo el primer experimento de ADN recombinante, hecho por Cohen y sus colegas, a la luz de este conocimiento más específico.
Al hacerlo, explicaremos los varios pasos necesarios para realizar este experimento engañosamente simple.
Por favor, tenga en mente que los métodos usados en este experimento, ahora clásico, son herramientas indispensables en el cartografiado y secuenciamiento del genoma humano.

En la Universidad de Stanford, Stanley Cohen ya había aislado una amplia variedad de plásmidos.
Uno de ellos, xxx , tenía la información genética necesaria para replicarse en la E. coli y conferir resistencia al antibiótico tetraciclina. 

Uno de los privilegios de aislar plásmidos es que uno puede bautizarlos.
El plásmido xxx era el plásmido (p) centésimo primero (101) aislado por Stanley Cohen (SC).
Aun cuando los plásmidos pueden ser pequeños, su aislamiento y utilización pueden a veces traer grandes recompensas.
Stanley Cohen compartió el premio Nobel con Herbert Boyer y Paul Berg por sus experimentos revolucionarios con el ADN recombinante.

Cohen y sus colegas habían descubierto que cuando dividían el plásmido xxx mediante la EcoR 1, se cortaba sólo en un lugar, transformando as' el plásmido circular en una forma lineal.
Volvieron a formar los plásmidos en círculos uniendo los extremos mediante la acción de la ADN ligase.
Los volvieron a poner en la E. coli donde los plásmidos se replicaron como es usual y fueron entonces pasados a las generaciones sucesivas de dicha bacteria.

Pudieron colocar los plásmidos en las células bacterianas por el proceso de transformación.
Recordemos la descripción original de Fred Griffith de la transformación bacteriana, en 1928.
En su experimento, los fragmentos de ADN de células muertas de Streptococcus entraban en las células vivas y les conferían la capacidad de producir cápsulas protectoras.
Esa transformación ocurre ocasionalmente en la naturaleza en unos pocos tipos de bacterias, entre ellos el Streptococcus de las anginas.

En 1970, Morton Mandel y A. Higa, de la Escuela de Medicina de la Universidad de Hawai, hallaron que el tratamiento de la E. coli con calcio permitía, de algún modo, que las bacterias tomasen ADN extraído de los virus.
En este estado receptivo, las células bacterianas se denominan «competentes», lo que significa que el ADN en la solución que rodea a las células será absorbido a través de la pared y se mezclará con el contenido de la célula.
La E. coli no es naturalmente capaz de la transformación, pero el grupo de Cohen utilizó esta técnica de calcio y pudieron introducir plásmidos en la E. olí. 
En este proceso, primero se incuba la E. coli , de modo que se forme una población densa y rápidamente creciente de células saludables. 
Las células son centrifugadas y vueltas a suspender en cloruro de calcio, y luego mezcladas con la muestra de ADN. 

Después de haber estado sometidas a incubación a diferentes temperaturas, se distribuye las células en un medio de crecimiento sólido apropiado para las E. coli , pero que también contiene tetraciclina, a una concentración que ordinariamente mataría las células.
Aquellas células bacterianas que han recogido uno o más plásmidos que confieren resistencia al antibiótico, sobrevivirán y se multiplicarán. 
Como las bacterias se multiplican tan rápidamente, después de una incubación nocturna se observará grupos separados, cada uno con millones de células, sobre la superficie del medio.

Éstos pueden ser colocados en un medio fresco de crecimiento para aumentar su número aún más. 

Esencial a este procedimiento es que cuando las bacterias son distribuidas sobre el medio de crecimiento, cada célula capaz de crecer se reproducirá.
A medida que cada célula resultante de las reproducciones sucesivas se amontona con sus ancestros, forman una población de millones de células que aparecen como una pequeña mancha sobre la superficie del medio sólido.

Esta mancha es una «colonia», y una colonia es un clon, una población de células derivadas de una sola.
Todas las células en este clon son idénticas, por lo menos en teoría, de modo que cultivar una colonia en un frasco o tubo puede proveer una vasta población de células con el mismo genoma.

La transformación bacteriana ocurre en tan solo una célula entre un millón durante este procedimiento de laboratorio.
Sin embargo, usando esta técnica, esas pocas células y sus descendientes, en la forma de colonias, pueden ser seleccionadas fácilmente y directamente propagadas en tantas células como sean necesarias para un propósito particular.

Stanley Cohen y sus colegas colocaron plásmidos xxx en las E. coli por transformación y las colonias de células resistentes fueron separadas del medio sólido con tetraciclina.
El paso siguiente era ver si un trozo de ADN extraño podía ser insertado en los plásmidos.
La EcoR I fue usada para abrir el ADN de otro plásmido de E. coli , que tenia resistencia al antibiótico kanamicina.

Entonces mezclaron estos plásmidos con los plásmidos xxx , también en forma lineal, y unieron los extremos pegajosos.
Los nuevos plásmidos híbridos fueron colocados en la E. coli .
Algunas de las bacterias sobrevivieron en bandejas que contenían un medio de tetraciclina y kanamicina, una combinación que ordinariamente hubiera sido letal.

Los plásmidos que habían resultado de su mezcla eran quimeras de xxx y un segundo fragmento de ADN del otro tipo de plásmido, que portaba en sus genes la información para producir la resistencia a la kanamicina. 
Esto demostró, lo que era muy importante, que el plásmido xxx podía servir de portador para introducir otro segmento de ADN en las E. coli .
Muchos otros portadores semejantes han sido desarrollados desde este primer ejemplo.
Estos portadores se denominan «vectores».

Era posible que los genes de otras especies bacterianas se combinaron con un portador mediante esos métodos?
Quizás ese ADN bacteriano, siendo extraño a las E. coli , tendría genes que no podrían funcionar en tales células. 
El plásmido pl258 , extraído del Staphylococcus aureus , portaba la resistencia a otro antibiótico, la penicilina.
Este plásmido fue dividido con EcoR I y combinado, como se ha descrito más arriba, con xxx .
Este nuevo híbrido transformó las E. coli en células que eran ahora resistentes a la tetraciclina y a la penicilina. 

Finalmente, en esta excitante serie de experimentos, se planteó la pregunta: Pueden incluso los genes de células animales ser implanta dos en las bacterias y funcionarían allí?
Seleccionando un fragmento de ADN de sapo, lo fusionaron al xxx e introdujeron la combinación en la E. coli .
Entonces seleccionaron colonias transformadas por su supervivencia en un medio que contenta tetraciclina.
Como el nuevo gen no podía ser seleccionado convenientemente por sí mismo, el xxx , con su gen resistente a la tetraciclina, era un vector efectivo para transportarlo a la E. coli y también para la selección. 

El producto concreto, fabricado por la actividad del nuevo gen del sapo, era ARN, el que hormonalmente sería usado para construir una parte del ribosoma de la célula del sapo.
En este caso la E. coli reaccionó a la introducción del plásmido recién creado produciendo obedientemente ARN ribosómico del sapo.
El primer híbrido animal-bacteriano había sido creado.

Figura 6

Los fragmentos de ADN se unirán a los plásmidos cortados por los «extremos pegajosos» producidos por los enzimas de restricción

Vueltos a colocar en las bacterias, los nuevos genes en los plásmidos pueden ser capaces de funcionar como lo harían normalmente en las células de las que provinieron.

Los clones entran en escena

Los cromosomas, vistos a través de la lente del microscopio, ofrecen poca información al espectador.
Los genes individuales que forman los cromosomas siguen siendo invisibles, así como una vista de la Tierra desde el espacio revela los continentes, pero ninguno de sus habitantes.

La localización precisa de los genes está oculta en algún lugar de esos bastones inertes, separados, teñidos y fijados en el portaobjetos.
Debemos dejar atrás el mundo de la visión para encontrar los genes.

En un primer nivel de análisis, podríamos tener éxito en rastrearlos hasta su «zona de residencia», uno de los 46 cromosomas humanos.

Este tipo de «cartografiado» básico ya ha sido hecho con casi 5.000 genes.

Buscando más profundamente, podríamos llegar al bloque específico en cuya vecindad reside.
El bloque estará sobrepoblado, pero será, en términos biológicos, un lugar cromosómico con una precisión de un millón de pares de bases nitrogenadas respecto de su localización real en un cromosoma particular.
Relativamente pocos genes están, por ahora, en esta categoría.

Aun más difícil de alcanzar sería la meta última de conocer la localización exacta de cada gen, su domicilio.
Para realizarlo se necesitaría «secuenciar» el genoma humano completo, esto es, determinar la secuencia de bases nitrogenadas de todos los nucleótidos del ADN humano.
Sin embargo, eso es precisamente lo que el Proyecto Genoma Humano se propone hacer.

El resultado de dicho análisis sería una cadena de 3.000 millones de letras A, T, G 0 C, cada una representación de la base nitrogenada de un nucleótido.
Una impresión de simplemente esas letras llenaría 200 volúmenes del tamaño de la guía telefónica de Manhattan. 
Dentro de esa enorme lista estaría la secuencia de todo gen humano, como también las secuencias de los grandes trechos de ADN sin función conocida actualmente. 

Aun así, la posesión de semejante secuencia sólo sería el comienzo.

Conocer la secuencia no nos diría por sí misma la localización de todos nuestros genes.
Los científicos todavía querrán saber más. 
Querrán descubrir no sólo la posición sino también la función de cada gen y cómo esa función se regula y controla.

Por ejemplo, las células de los músculos, del hígado o del cerebro, cómo realizan sus funciones individuales y altamente especializadas, usando este mismo conjunto de genes?
Qué genes se hacen activos o cesan de estar activos en estas células y cómo se realiza esto?
Además, cómo se desarrolla la miríada de tipos de células en el cuerpo, a partir de tan sólo el huevo humano fertilizado y sus 46 cromosomas, la mitad de los cuales han sido aportados por la madre y la otra mitad por el padre?

La forma más simple de cartografía es mediante los métodos clásicos del análisis de la herencia de características visibles en las historias familiares.
Un cartografiado y secuenciamiento más detallados, como el que se está haciendo en el Proyecto Genoma Humano, así como las preguntas acerca del funcionamiento y desarrollo correspondientes, necesitan y utilizan las herramientas de la biología molecular que hemos mostrado en el capítulo anterior.

A comienzos de 1990, el Departamento de Energía y el Instituto Nacional de la Salud publicaron las metas específicas del Proyecto Genoma Humano, con especial énfasis en los primeros cinco años, que se extendían desde el año fiscal de 1991 a 1995.
La publicación detalla las metas científicas y el marco administrativo propuesto para su obtención.

Subyacente a todos los objetivos está el requerimiento absoluto de una provisión de genes individuales y de grupos de genes para un análisis detallado.

Supóngase que un genetista en 1970 hubiese querido investigar la estructura y función de los genes humanos que controlaban la síntesis de la hemoglobina, la molécula portadora de oxígeno que da color a los glóbulos rojos.
Podría querer saber la secuencia de bases nitrogenadas de dichos genes o quizá determinar qué cambios ocurren en los genes para causar la enfermedad genética talasemia, en la cual uno de los cuatro polipéptidos que forman la molécula de hemoglobina no se produce.

La pregunta no podía responderse a menos de que pudiera proveerse una cantidad suficiente del gen como para estudiarlo en detalle, probablemente un miligramo.
Eso suena como un cantidad irrisoria, pero se torna monumental cuando se considera que se la debe aislar del ADN humano completo.
El ADN involucrado en un gen de hemoglobina es menos de una parte en un millón del genoma humano.
Se necesitaría, por ende, una cantidad inobtenible de ADN, como material de partida.

Y aun suponiendo que se pudiese reunir una tal masa de ADN humano un problema aún más difícil: cómo separar el gen de interés de todo el resto del ADN?

El clonado de genes resuelve actualmente estos problemas. 
Clonar un gen significa, simplemente, obtener una muestra diminuta y pura del gen y producir muchos más, como si se tuviese un documento y se hicieran muchos otros idénticos, fotocopiándolo.
El «fotocopiado» de genes se realiza, primero, uniendo unos pocos de los genes con vectores como los plásmidos y, luego, insertando los vectores, que ahora portan los genes, en las bacterias u otras células apropiadas.

Ya hemos visto los principios básicos del clonado de genes.
En ese proceso, el así llamado ADN extraño, el ADN que hemos tomado de un organismo' es insertado en la molécula vector, por ejemplo un plásmido, para crear una quimera de ADN.
La construcción de tales compuestos o moléculas recombinantes artificiales también debe ser denominada ingeniería genética o «manipulación de genes».
Este procedimiento también ha sido llamado «clonado molecular» o «clonado genético», porque una población de bacterias genéticamente idénticas, todas con el ADN deseado, pueden ser cultivadas en gran número, copiando así el ADN tantas veces como células se dividan.

Lo que es muy significativo es que estas quimeras, dentro de las células bacterianas, pueden no sólo copiarse a sí mismas sino también producir concretamente un producto genético especifico en grandes cantidades. 
Este enfoque ya ha sido utilizado en la producción comercial de insulina humana, la hormona del crecimiento y la proteína antiviral interferón.

Todas ellas pueden actualmente fabricarse en bacterias, porque los genes humanos que regulan su síntesis han sido aislados y clonados en las bacterias.
Las celarlas bacterianas, cultivadas en gran número, obedientes a las órdenes de sus genes, fabrican ahora un producto génico humano. 
Los productos génicos humanos derivados de genes aislados y clonados durante el Proyecto Genoma Humano, sin duda, serán usados también para el beneficio de la humanidad, particularmente en la cura y prevención de enfermedades.

Sin embargo, nuestra preocupación aquí es la de examinar el proceso del clonado de genes en un grado suficiente como para permitir una comprensión clara de su uso esencial en el cartografiado y secuenciamiento del genoma humano.

Veamos primero la disposición del ADN dentro del cromosoma e introduzcamos el concepto de cómo la función del ADN es regulada en la célula viviente.
Las células vivientes son de dos tipos fundamentales, procariotas y eucariotas.
Las células procariotas son bacterias, mientras que las eucariotas incluyen todos los otros tipos de células hallados en los organismos vivientes.

Una diferencia fundamental entre estas dos categorías es que las bacterias no tienen una membrana que separe el ADN cromosómico del resto del contenido de la célula, como en las eucariotas.
En otras palabras, las células bacterianas no tienen núcleo.
Además, las bacterias tienen aproximadamente un milésimo del ADN de una célula eucariota típica.
El ADN de las células eucariotas es una doble hélice larga y delgada dentro del núcleo de la célula.
En contraste, el ADN bacteriano está simplemente concentrado en una región central de la célula.
Este genoma bacteriano es un solo cromosoma, una doble hélice en forma de anillo, como una serpiente de ADN con su cola en la boca.
Tiene muy poca proteína asociada con él, en contraste con el genoma humano, en el cual las proteínas constituyen una parte significativa del cromosoma.
Las células bacterianas también pueden contener anillos de ADN más pequeños, los plásmidos, que son independientes del cromosoma en su replicación y función.
El patrón básico de replicación del ADN y la traducción de los mensajes genéticos en proteínas son, en muchos aspectos, los mismos para los procariotas y eucariotas, pero quedan diferencias importantes.

La comprensión de estas diferencias es particularmente significativa en el clonado de genes.
Recuérdese que, en este procedimiento, con frecuencia colocamos genes eucariotas en una célula bacteriana.
La supervivencia y función de esos genes introducidos depende del control de ciertos parámetros que reflejan estas diferencias entre los procariotas y los eucariotas.

Cuando se observan los cromosomas humanos con el microscopio parecen bastones oscuros y torcidos.
Normalmente, se los examina después de que han sido preservados en alcohol, teñidos con tinturas y extendidos en un portaobjetos. 
Más aún, para poder verlos claramente estos cromosomas deben ser atrapados en el acto de la división celular (mitosis).
Sólo en este breve período en la vida de la célula los cromosomas, normalmente alargados y casi invisibles en la célula no dividida, se retuercen y condensan en sus típicas formas romas Cada uno de los 23 pares de cromosomas adopta una forma única en este proceso, de modo que se puede preparar un «cariotipo» de los cromosomas humanos.
Éste surge del fotografiado de los cromosomas de una célula en división y su ordenamiento en 23 pares separados según su tamaño y forma.
Es una ayuda valiosa en el diagnóstico de desórdenes como el de un cromosoma parcial o faltante. 
Obviamente, la presencia de dos cromosomas X o un X y un Y determinan al individuo como macho o hembra y da pie a un método rutinario de diagnóstico prenatal del sexo.

Los cromosomas humanos son aproximadamente un 60% de proteína y un 40% ADN.

También hay algo de ARN, debido a la síntesis del ARN mensajero.
Como se hizo notar con anterioridad, el ADN del cromosoma existe como una larga cadena continua con forma de doble hélice.
Un típico cromosoma humano contiene unos 140 millones de pares de nucleótidos. 
Estirado, el cromosoma tendría una longitud de aproximadamente 5 centímetros.
Obviamente, se necesita una enorme cantidad de torsiones para hacer que quepan los 46 cromosomas en el pequeño núcleo de una célula.

Si pudiésemos desenroscar suavemente un cromosoma humano, encontraríamos que se parece a una serie de cuentas en un hilo, siendo el hilo el ADN y las cuentas las proteínas. 
Cada 200 nucleótidos la hélice de ADN envuelve un complejo de «histonas» que son pequeños polipéptidos.
Estas proteínas están cargadas positivamente y el ADN está cargado en forma negativa, por lo que es fuertemente atraído por el grupo de proteínas.
También existen otras contorsiones del ADN, ya que la cadena de ADN e histonas se envuelve en «superespiras». 

Esta condensación es necesaria para que los cromosomas puedan pasar intactos de una célula a la siguiente durante la división celular.
La duplicación de la molécula de ADN ocurre cuando está en el estado parcialmente desenroscado.
Cuando los cromosomas se condensan en pequeños bastones gruesos por enroscamiento, ya se han replicado y aparecen como pares unidos, estando cada cromosoma todavía ligado a la nueva réplica, en una región denominada centrómero.

En términos de la función, necesitamos hacer unas pocas distinciones más allá de la generalidad de que el cromosoma es una molécula de ADN hecha de genes que dirige la síntesis de polipéptidos.
Para aumentar la complejidad del análisis del ADN humano y otros eucariotas, aproximadamente del 10 al 25% del total del ADN está formado por secuencias cortas de cinco a diez nucleótidos que se repiten en tándem miles de veces. 
En los cromosomas, estas repeticiones en tándem están localizadas en los centrómeros, lugar en donde los dos cromosomas siguen ligados después de la replicación, hasta que se separan al dividirse la célula. 

Aparentemente, esta área del ADN tiene un papel estructural, más que genético, en la célula y de algún modo opera durante la replicación y la separación.

Ya hemos descrito cómo, después de que las preocupaciones iniciales acerca de la seguridad del ADN recombinante se calmaron y los investigadores comenzaron a usar vectores seguros para estudiar en serio la estructura de los cromosomas, en 1977, se hizo un descubrimiento sorprendente.
Los patrones de fragmentos producidos por enzimas de restricción en el ADN cromosómico eucariota revelaron que había regiones dispersas dentro de los genes que no eran parte del gen en absoluto.
Eran secuencias de nucleótidos que parecían no tener ninguna función.
En cambio, un gen bacteriano es simple.
No está interrumpido, de modo que el código del gen puede ser interpretado directamente a partir de la secuencia de aminoácidos del polipéptido cuya síntesis dirige.

Nadie había supuesto que los eucariotas fuesen diferentes, pero lo eran.
Los genes de eucariotas con frecuencia están dispuestos en trozos, separados por los llamados códigos «sin sentido».
Walter Gilbert, de la Universidad de Harvard, denominó a estas secuencias intermedias «intrones» y a la secuencia del código genético verdadero «exones».
Añada unas docenas de dígitos extra al azar aquí y allá dentro de su número telefónico.
Su número serían los exones y todo el resto serían los intrones.

Algunos genes no tienen intrones; otros tienen muchos. 
Nadie sabe con seguridad por qué tenemos intrones. 
Curiosamente, por razones aún no conocidas, el gen humano, en el cual han sido hallados, hasta la fecha, la mayoría de ]os intrones, es el gen cuya falla causa la distrofia muscular de Duchenne.
Tiene por lo menos de 60 a 100 intrones.
Cuando el ARN se prepara para llevar la información del código genético fuera del núcleo, el ARN mensajero es sintetizado a lo largo de la longitud de un gen completo, los intrones forman parte del ARN también.

Entonces los intrones son recortados enzimáticamente en un proceso conocido como empalme del ARN.
Esto deja al ARN mensajero con una copia del código del gen y no de los intrones.

Este fenómeno apunta a dos problemas asociados al clonado de genes.
Uno es que no es posible inferir a partir de la secuencia de aminoácidos en una proteína la secuencia exacta de bases nitrogenadas en el segmento de cromosoma que lo codifica.
Los intrones no se reflejarán en las secuencias de aminoácidos, ya que han sido eliminados del mensaje.

Además, se debe manipular las condiciones experimentales durante el clonado de modo que las bacterias sean capaces de manejar la presencia de intrones en el ADN, ya que las bacterias no tienen intrones en sus cromosomas.

Más todavía, cómo hacen las células, procariotas y eucariotas, para que ciertos genes estén activos y otros no?
Las células deben ser capaces, por lo menos, de «reprimir» algunos de sus genes, si no resultaría un caos.
Todas nuestras células, excepto el esperma y los óvulos, tienen un conjunto completo de genes.
Las células del ojo tienen genes para la función del riñón y las células del corazón tienen genes para el color del cabello.
Las células han desarrollado, de algún modo, mecanismos para reprimir los genes que codifican enzimas que no son necesarios en un momento dado y para activarlos cuando son necesarios.
La dilucidación de estos sistemas sigue siendo un área principal de investigación en la biología molecular. 
Francamente, los científicos, por ahora, saben muy poco acerca de cómo operan semejantes sistemas complejos. 
Sin embargo, podemos describir unas pocas generalidades relevantes.

François Jacob y Jacques Monod, del Instituto Pasteur, en París, fueron los pioneros, en los cincuenta y sesenta, del estudio de estas cuestiones, con su magistral análisis, ganador del premio Nobel, de cómo la E. Coli fabrica enzimas que descomponen las moléculas de lactosa (el azúcar de la leche).
Los productos de esta descomposición son usados como fuente de nutrición por la bacteria.
Entre otras conclusiones, hallaron que para que el ARN mensajero pueda ser sintetizado en el gen, el enzima ARN polimerasa debe ligarse al ADN cerca del comienzo del gen, en una región llamada promotora.
El enzima, entonces, comienza rápidamente a unir el ARN mensajero necesario para sintetizar los enzimas que se requieren para descomponer la lactosa.

La célula eucariota tiene requerimientos de control genético de una complejidad todavía mayor.
Los genes no sólo deben ser activados y desactivados para operar las funciones básicas de la célula, sino que estos sistemas también deben responder a señales provenientes del entorno de la misma.

Esto es especialmente importante en células que se están diferenciando en diversos tipos.
Un huevo humano fertilizado se transforma en un bebé, una criatura hecha de billones de células de muchos tipos.
De algún modo, cada etapa del desarrollo dispara la expresión genética necesaria para la etapa siguiente.

En las células humanas, con sus 100.000 genes, en cualquier momento dado sólo un 1% del genoma está expresándose realmente.
El otro 99% permanece desactivado debido a la acción de sistemas que sólo ahora estamos empezando a descubrir mediante las técnicas de la biologia molecular.
Para nuestros propósitos, adoptaremos la generalización de que los sistemas reguladores involucrados son similares a los de las bacterias... pero existen complejidades adicionales.
Nuevamente, la mayoría de las cuestiones que rodean esta complicada área permanecen sin respuesta. 

Existen diferencias entre las secuencias promotoras de las bacterias y las de los eucariotas, grupo que incluye al hombre. 
Los promotores para la síntesis de ARN mensajero en el genoma humano no necesariamente serán reconocidos por las bacterias.
Para obtener la expresión de una secuencia de ADN humano insertada en la E. coli , por ejemplo, puede ser necesario situar el gen humano cerca de un promotor bacteriano.

Además, para lograr la expresión del ADN eucariota en las bacterias, es necesario agregar una zona sobre la molécula de ADN recombinante que le permita ligarse a los ribosomas bacterianos.
Esta zona de ligamiento no está presente normalmente en el ADN eucariota.
En resumen, no basta con incluir los genes del ADN humano en el clonado de genes, al menos si uno se propone usarlos para que se expresen en la síntesis de polipéptidos. 
Todo un conjunto de accesorios deben fabricarse para que acompañen al gen en cuestión, lo que hace la ingeniería genética más complicada de lo que podría parecer a primera vista.

Si el científico está interesado sólo en obtener grandes cantidades de genes, en lugar de producir un producto génico, basta con copiar el gen en el vector.

Figura 1

Cariotipo de un ser humano masculino 

Nótense los cromosomas sexuales, el X grande y el Y más pequeño.
Cada cromosoma aparece duplicado porque el cariotipo se hace después de que los cromosomas se han replicado, antes de la mitosis, pero cuando aún no se han separado.

Figura 2

Un cromosoma, en la etapa en la que puede verse con un microscopio, es una molécula de ADN sumamente retorcida que está enroscada alrededor de grupos de moléculas de proteínas

Ahora, habiendo visto algo de las complejidades que hay que solucionar, hagamos algunas consideraciones prácticas acerca del clonado de genes humanos.
Cualquier procedimiento de clonado del ADN tiene cuatro partes básicas:

1 un método para producir fragmentos de ADN
2 la unión de estos fragmentos de ADN «extraños» al vector
3 un medio de introducir este recombinante en una célula anfitriona donde pueda replicarse
4 un método para «filtrar» o seleccionar un clon de células que haya adquirido el recombinante.
Seguimos esos pasos en nuestro relato del primer experimento en ADN recombinante, a comienzos de la década de los setenta.


Supóngase que queremos seguir esos pasos básicos para clonar genes humanos.
Debido al tamaño y complejidad del genoma humano, se desarrollan permanentemente métodos y estrategias más complejas para realizar esa tarea.

Hay dos enfoques básicos hacia el clonado de un gen específico.
Tienen que ver con el material de partida.
El primer enfoque implica el clonado de todos los fragmentos de ADN a partir de una muestra del mismo cortada en pedazos por enzimas de restricción.
Luego los trozos son colocados en las bacterias, son clonados y después se seleccionan las bacterias con los diversos fragmentos de ADN, usualmente por su resistencia a antibióticos.
Entonces, se seleccionan los clones con el gen concreto en el que se está interesado, usando medios que explicaremos en seguida.
Esta práctica de clonar todos los fragmentos de ADN de una muestra se denomina «shotgunning».

Por ejemplo, podríamos fragmentar el ADN humano en su totalidad con un enzima de restricción, insertar cada fragmento o pequeños grupos de fragmentos en un vector apropiado, clonarlos y luego intentar aislar los clones que contienen un gen específico.
Esto es aproximadamente equivalente a pasar la edición dominical del New York Times por una trituradora de papel, hacer algunos millones de fotocopias de cada trozo, mezclar todo y buscar los pedazos con el resultado del partido de los Mets.

Supongamos que los fragmentos de ADN resultantes de la fragmentación de los enzimas sea, en promedio, de unos 1.000 a 2.000 pares de bases de largo o, para usar el término convencional, de 1 a 2 «kilobases» kb de largo. 
Como el ADN humano contiene aproximadamente 3.000 millones de pares de bases, la concentración de cualquier fragmento específico de ADN sería de menos de uno en un millón, haciendo así muy engorroso el problema.
Habría un número limitado de trozos de la página de deportes en nuestra pila de papel triturado y su número sería superado en mucho con una cantidad enorme de fragmentos extraños. 

Este problema puede, al menos, aliviarse un poco clonando al azar fragmentos de un tamaño mayor, por ejemplo, de unos 20 kb.
Esta fragmentación podría hacerse rompiendo en forma mecánica el ADN, pero más comúnmente éste es tratado con uno o a veces una mezcla de dos enzimas de restricción.
Se permite que este proceso continúe sólo hasta un estado de digestión parcial, produciendo así fragmentos que están en el rango de los 10 a 30 kb.
Éstos pueden ser tratados para separar los fragmentos que son de unos 20 kb, que pueden insertarse en un vector simple.
Se ha calculado que aproximadamente un millón de clones serían necesarios para asegurar, con una probabilidad del 90-95%, que cada uno de los fragmentos de 20 kb estén incluidos.
Ajustemos las cuchillas de nuestra trituradora de modo que los trozos producidos sean mayores.
Por lo menos tendremos menos que examinar.

Sin importar qué método exacto usemos, una colección de fragmentos de ADN aleatorios colocados en un vector y clonados se denomina una «biblioteca de genes» o a veces una «biblioteca genómica» o «banco de genes».
Esta biblioteca es un conjunto completo de clones que contienen la totalidad del genoma.
Nuestra exposición sobre los clones, hasta ahora, se ha referido a ellos en la forma de colonias bacterianas transformadas.
Colocaremos todos los trozos idénticos fotocopiados de nuestro periódico en, digamos, un millón de carpetas. 
La biblioteca de genes es un recurso valioso de fragmentos de ADN humano para cartografiar y secuenciar.
Una vez establecida, sirve de medio para mantener una provisión consistente de ADN para llevar a cabo investigaciones.

El segundo enfoque del clonado de un gen específico está en marcado contraste con el uso de clones genómicos. 
Esto implica la creación de segmentos de ADN mucho más específicos, denominados ADN complementario o cADN.

Saca partido del hecho de que, debido al modo en que están hechas las proteínas, la secuencia de bases nitrogenadas del ARN mensajero es un complemento directo de la secuencia del gen concreto.

Debido a ese hecho, el cADN puede sintetizarse usando su mARN.

El «Dogma Central» de que el ADN hace ARN y el ARN hace proteínas, experimentó una repentina transición, de una sentencia universal a una generalidad, en 1970.
En ese año, Howard Temin y David Baltimore, de forma independiente, descubrieron el enzima «transcriptasa inversa».
Se la halló en ciertos tipos de virus, los «retrovirus», que contienen ARN en lugar de ADN.
Los retrovirus incluyen patógenos importantes, como ciertos virus de hepatitis y leucemia, como también el infamante agente del sida, conocido oficialmente como virus de inmunodeficiencia humana (VIH).

En esta forma de ataque viral, el ARN, y no el ADN, entra en la célula eucariota.
El enzima transcriptasa inversa usa el ARN del virus como modelo para formar una sola cadena complementaria de ADN, fabricando así, literalmente, un lado de la doble hélice.
Ésta, entonces, sirve de patrón para la formación de una segunda cadena complementaria con ella, formando así un ADN de doble cadena.
Este ADN entonces entra en el ADN cromosómico de la célula anfitriona, donde es replicado durante las divisiones celulares.
Más tarde (en el caso del sida, a veces muchos años después), este ADN se torna activo y dirige la síntesis de nuevas partículas retrovirales infecciosas que abandonan la célula y entran en otras.

Para realizar la síntesis de mARN a cADN en el laboratorio, tiene sentido obtener el mARN de celarlas en las cuales el gen que se está tratando de clonar esté altamente activo. 
Por ejemplo, las células en el páncreas contienen concentraciones relativamente altas de mARN que ayudan a fabricar la insulina, y las células que maduran transformándose en glóbulos rojos son ricas en mARN porque están llevando el mensaje del gen encargado de la fabricación de la hemoglobina.
Como resultado, el asilamiento del mARN de tales células provee una muestra enriquecida de mARN específico.
Un mARN de hemoglobina casi puro puede prepararse fácilmente a partir de glóbulos rojos inmaduros.
Con la transcriptasa inversa el cADN puede prepararse y usarse como fuente de ADN, para posterior clonado.

Ahora que tenemos una fuente de ADN para clonar, ya sean fragmentos grandes o moléculas de cADN mucho más específicas, debemos seleccionar un vector de clonación apropiado para el ADN humano.
Ya hemos descrito el uso de plásmidos, tales como el xxx , en los días tempranos de la revolucionaria técnica del ADN recombinante.

Después de sus éxitos iniciales, Boyer y sus colegas desarrollaron un conjunto de vectores muy populares, conocidos como la serie pBR.

Cuando se crean plásmidos para ser usados como vectores de clonación, varias características son deseables.

Una es la capacidad del plásmido para funcionar como un «plásmido de alto copiado», esto es, con capacidad de copiarse a sí mismo repetidamente en la célula, a veces tanto como mil veces.
Esto, por supuesto, aumenta en gran medida el producto final de los genes portados por esos plásmidos.
Además, el plásmido debiera tener un sistema para mostrar la presencia de las inserciones de ADN extraño.
Recordemos que el primero de tales marcadores era la resistencia a un antibiótico.

Diversos factores resistentes a antibióticos han sido producidos en vectores de plásmidos.
Uno de estos plásmidos, el pBR322 , ha sido muy usado.
Tiene un alto número de copiado y tiene dos genes para la resistencia a los antibióticos, la tetraciclina y la ampicilina.

Las modificaciones del pBR322 fueron usadas por el grupo de Boyer, en 1977, en su síntesis del gen humano que fabrica la hormona que regula el crecimiento.
El gen fue clonado y la hormona está ahora disponible para la comunidad médica. 
En agosto de 1990, los investigadores informaron que, en una prueba clínica, el tratamiento de personas de edad avanzada con la hormona que regula el crecimiento revierte espectacularmente diversas características del envejecimiento, incluyendo la piel y el tono muscular.

Aunque, por lo común, son usados para transportar fragmentos relativamente pequeños de ADN, los plásmidos están limitados por el tamaño del ADN que pueden transportar.
Los plásmidos pueden llevar secuencias de ADN de hasta 12 kb.

Segmentos mayores pueden ser transportados por la molécula de ADN derivada del bien conocido fugo lambda.
Éste ha sido manipulado para que sea particularmente útil para el análisis del genoma humano.

Tales vectores han sido denominados por su creador, Fred Blattner, fagos «Caronte».
Caronte, otra figura mitológica, es el barquero de la laguna Estigia.
Así como Caronte transporta las almas al bajo mundo, el fago Caronte transporta el ADN a las células bacterianas. 
El uso más común de los fagos Caronte es en la construcción de bibliotecas genómicas.
Los fagos han sido ingeniosamente diseñados para que acepten sólo ADN entre los 12 y los 20 kb.
Eso asegura que clones con cantidades insignificantes no llenen la biblioteca.
Aun así, a 12 o 20 kb por clon se necesitan aproximadamente un millón de clones para completar una biblioteca.

Otros vectores han sido desarrollados para propósitos especializados, tal como los vectores «cósmidos», para transportar fragmentos de ADN de hasta 40-50 kb.
Desde hace poco, una forma relativamente nueva de vector está siendo usado cada vez más en el análisis del genoma humano.
Es el «YAC», o cromosoma artificial de la levadura, desarrollado en 1987 por David T. Burke, George F. Carie y Maynard V. Olson, de la Escuela de Medicina de la Universidad de Washington.
Una ventaja importante del clonado de genes en la levadura es la capacidad de clonar grandes trozos de ADN.
Muchas secuencias de interés del ADN humano son bastante grandes.
Por ejemplo, el gen para el factor de coagulación sanguínea VIII cubre aproximadamente 190 kb o más o menos el 0.1% del cromosoma humano X, y el gen de la distrofia muscular de Duchenne cubre más de un millón de bases.

El YAC es ADN ingeniosamente producido mediante la reunión de las partes funcionales esenciales de un cromosoma natural de levadura.

Se empalma a él un fragmento de ADN humano y se inserta el cromosoma en una célula de levadura, donde el YAC se reproduce durante la división celular.
El resultado es un clon de ADN dentro de células de levadura en lugar de células bacterianas.
El uso de levadura se ha extendido tanto en el análisis genético que James Watson, en su libro de texto acerca de la genética molecular, La biología molecular del gen , llama a la levadura «la E. coli de las células eucariotas».

El plan quinquenal del Departamento de Energía/Instituto Nacional de Salud de 1990, incluye un esfuerzo importante por mejorar los vectores YAC disponibles y los procedimientos de clonado del YAC, particularmente en lo que respecta al aumento del tamaño de los fragmentos de ADN que pueden ser clonados, como también a nuevas mejoras en los métodos para separarlos, posteriormente, de la levadura.

Sin importar el método específico elegido para la preparación de las bibliotecas de genes específicos de interés, a los científicos les queda un problema analítico final.
Un método conveniente es necesario para separar y analizar los muchos miles de fragmentos en la biblioteca de genes, decidiendo qué células transformadas contienen el gen deseado y separando ese gen del resto del ADN de la muestra.
Qué carpetas contienen el resultado de los Mets?
Hay un millón y todas se parecen.

Enfermedad, diagnóstico y terapia 

Nuestros cromosomas pueden alojar un total de más de 100.000 genes.
La mayor parte de las instrucciones que emanan de estos códigos de ADN inician e integran una compleja variedad de reacciones químicas características de un organismo saludable y normal.
Los genes dirigen la síntesis de los enzimas y éstos, a su vez, controlan el metabolismo de la célula, que con frecuencia funciona normalmente, salvo algunas notables excepciones.

Algunos genes anidados en nuestros cromosomas pueden dañarse y fallar, pero seguir activos.
Pueden causar enfermedades físicas y mentales, raras pero devastadoras.
Por ejemplo, el 30% de los pacientes jóvenes admitidos en los hospitales pediátricos en América del Norte tienen enfermedades que pueden ser atribuidas directamente a causas genéticas.
Reconocemos cada vez más que las fallas genéticas también contribuyen a la aparición de muchas enfermedades comunes, tal como el cáncer, enfermedades del corazón y la diabetes. 

La necesidad de comprender cómo nosotros y los otros seres vivientes funcionamos en los niveles celular y molecular ha dado un empuje fundamental a la investigación científica. 
Las enormes posibilidades inherentes al cartografiado y secuenciamiento de los genomas ha alimentado estos estudios con una sensación de urgencia, acrecentada por una nueva esperanza de aliviar el sufrimiento humano.

Incluso un leve error genético puede descarrilar la producción de proteínas, provocando enfermedades y deformidad.
Se cree que más de 3.000 enfermedades hereditarias se deben a aberraciones en un solo gen.
Muchas más están influenciadas por combinaciones de genes. 
Los participantes principales en el PGH están calificados por su experiencia y orientación para dedicar una particular atención a aquellos genes alterados que son los agentes de la enfermedad.

El Departamento de Energía tiene un mandato del Congreso para controlar el daño hereditario debido a la exposición a bajos niveles de radiación.
El Laboratorio Nacional de Los Alamos del DOE ha sido el hogar del GenBank, la principal base de datos de secuencias de ADN de los Estados Unidos, desde 1983.
Una porción principal de la investigación en genética humana y los avances en la metodología del ADN han sido sustentados por el financiamiento de la NIH.
El Instituto Médico Howard Hughes (HHMI) ha financiado durante mucho tiempo la investigación biomédica en los mecanismos básicos de la genética y en las enfermedades genéticas, además de proveer el financiamiento de las bases de datos de la Biblioteca de Cartografía del Gen Humano y de la «Herencia Mendeliana del Hombre». 
El HHMI también colabora con el Centro para el Estudio del Polimorfismo Humano (CEPH), cuyo cuartel general se encuentra en París, Francia.

Además, tradicionalmente, el estudio de los patrones de herencia de enfermedades relacionadas con genes específicos ha facilitado el cartografiado de los genes en sus posiciones relativas, en cromosomas específicos.
No es sorprendente, pues, que muchos de los éxitos iniciales que surgieron de las iniciativas que desencadenaron el PGH, han sido los que localizaron y caracterizaron genes de enfermedades humanas.

La búsqueda de la localización de nuestros genes acaba de comenzar.

Sólo unos 5.000 han sido asignados a lugares aproximados en nuestros cromosomas.
Sin embargo, en los ochenta y noventa, la tecnología emergente de cartografiado y secuenciamiento -posibilitada por la revolución de la biología molecular de los setenta- ya ha producido algunos descubrimientos notables acerca de la localización y función de una variedad de desordenes genéticos devastadores.
Se ha hecho posible el diagnóstico, se han aislado genes enfermos y ahora puede buscarse, en forma razonable, un camino hacia la prevención, el tratamiento o la cura.
Este capítulo es la historia de cómo surgieron algunos de esos descubrimientos y de qué significa esto para la condición humana. 

ENFERMEDADES GENÉTICAS

La historia de la humanidad consiste en una serie de luchas para obtener el control sobre nuestro entorno, particularmente en la medida en que afecta nuestra seguridad y comodidad.
El desarrollo de la agricultura, la vivienda, el transporte y todos los logros que contribuyen a la definición de la civilización, ha buscado controlar la naturaleza y, con demasiada frecuencia destructivamente, torcerla según nuestra voluntad.

Cualquiera que sea el control que podamos haber ganado sobre las enfermedades es, fundamentalmente, un producto del siglo XX.

Puede idealizarse, si se desea, al «buen salvaje», pero la vida para la especie humana ha sido una experiencia marcada por el sufrimiento, prácticamente sin alivio, a manos de agentes invisibles.
Cualquier comprensión del papel de las bacterias y los virus en las enfermedades es un fenómeno reciente.
También lo ha sido el descubrimiento de que las enfermedades heredables están regidas por los genes, que son transmitidos de una generación a la siguiente.
Los efectos de tales enfermedades surgen de causas que no provienen de nuestro entorno, sino de lo profundo de nuestras mismas células.

Puede decirse que a partir de mediados de la década de los cuarenta nos hemos introducido en la era de la medicina moderna.
A los antibióticos, primero en forma de penicilina, que sólo estuvo disponible cerca del fin de la Segunda Guerra Mundial, le siguió un número creciente de medicaciones antibacterianas y de inmunizaciones contra patógenos bacterianos y virósicos.
Los hombres tienen, por primera vez, medidas preventivas y agentes terapéuticos efectivos contra las enfermedades infecciosas. 

Las enfermedades genéticas presentan un panorama totalmente diferente.
Los efectos de la anemia de células falciformes, la deficiencia de adenosina deaminasa (ADA), el retinoblastoma, el síndrome de Tay-Sachs, la enfermedad poliquística de riñón, la fibrosis cístico, la enfermedad de Huntington y una multitud de otras dolencias sólo se prestan, a lo sumo, a medidas paliativas.
Son causadas por genes cuya propia existencia, hasta hace poco, nos era desconocida.
Aun después de que fuera revelado el papel de esos genes y su arquitectura molecular, éstos siguieron siendo tan remotos e inaccesibles como siempre.
Luego, ocurrió el milagro de la biología molecular y se comenzó a tener conciencia de que el gen podría llegar a ser manipulado directamente.

Puede causar algún asombro que esta posibilidad haya cautivado la imaginación de los científicos? 
El PGH es un asalto frontal al destino genético asignado a cada ser humano.
El cartografiado y secuenciamiento de los genes intenta conducir al conocimiento que nos permita el diagnóstico, tratamiento, cura y prevención de las enfermedades genéticas, que menguan la vida de tantas personas.

Podemos, legítimamente, plantearnos la pregunta de si estos fines se alcanzan mejor mediante un esfuerzo de investigación a una escala tan grande como la del PGH, pero tendríamos dificultades en cuestionar el atractivo intelectual y emocional de la oportunidad de obtener el control, en cierta medida, sobre nuestra herencia genética.
En otro capítulo encararemos las cuestiones éticas planteadas por semejante poder sobre la herencia humana.
Aquí explicaremos los aspectos científicos, qué se ha hecho y que nos aguarda.

Examinemos brevemente unos pocos conceptos necesarios para seguir los detalles de la investigación de la enfermedad genética.
Un gen, a través del mensajero intermediario del ARN, dirige la síntesis de una cadena de aminoácidos.
Esta cadena es un polipéptido que puede, de por sí o en conjunción con uno o más polipéptidos, asumir una configuración tridimensional compleja y transformarse en una proteína. 
Algunas de estas proteínas actúan como enzimas, catalizadores que regulan las reacciones químicas de la célula viviente.
Cuando estudiamos la función de un gen nos estamos concentrando en un pequeño rincón de una vasta red de reacciones metabólicas interrelacionadas. 

Un gen defectuoso puede no poseer ya el código correcto, en la forma de una secuencia de bases nitrogenadas para un polipéptido particular.
El mensaje incorrecto puede expresarse en una secuencia incorrecta de aminoácidos y la proteína puede no funcionar ya de catalizador.
En consecuencia, una reacción química particular queda bloqueada y, como corolario, todas las reacciones que normalmente habrían ocurrido como resultado de esa reacción inicial, quedan detenidas del mismo modo.
Un caso típico sería la situación en la cual el gen correcto para convertir el aminoácido fenilalanina en tirosina está ausente. 
El resultado es la fenilcetonuria (PKU), que causa daño cerebral y un tamaño reducido de la cabeza.

Como se ha comentado varias veces, tenemos dos copias de la mayor parte de nuestros genes en el núcleo de cada célula, con la excepción de los gametos, que sólo conservan una copia después de su formación en la meiosis.
Los dos genes de un par (los alelos) ocupan el mismo lugar en ambos miembros de un par de cromosomas, uno heredado del padre y el otro de la madre.
Si los genes son idénticos, la persona es homocigótica para esos alelos y si difieren, es heterocigótica.

Un gen que causa un defecto puede ser dominante o recesivo, dependiendo de la enfermedad en cuestión.
Si es dominante, puede ejercer su efecto aun en la condición heterocigótica; si es recesivo, sólo en la condición homocigótica.
Si el gen recesivo de la enfermedad está presente en un marco heterocigótico, esto es, apareado con un gen dominante, quedará silencioso y sin expresión.
La persona heterocigótica para ese gen es un «portador».
Sólo cuando dos genes recesivos, uno de cada uno de los dos portadores, se combinan en el acto de fertilización entre el espermatozoide y el óvulo, cada uno con 23 cromosomas, para reconstituir el complemento completo de los 46 cromosomas humanos, pueden aparecer los síntomas de la enfermedad.

El sexo de los seres humanos está determinado por la presencia de dos cromosomas sexuales.
La hembra tiene dos cromosomas X mientras que el macho contiene un X y un Y en cada célula.
Los cromosomas X son homólogos, pero el X tiene muchos loci que no están presenten en el cromosoma Y, más pequeño.
Los genes defectuosos en el X tienen, pues más posibilidades de expresarse en el macho, debido a la falta de un alelo correspondiente.
No son funcionales en la hembra a menos que sea homocigótica recesiva.

La mayoría de las enfermedades genéticas causadas por defectos en un solo gen se debe a alelos recesivos. 
Como deben juntarse dos de éstos, en el estado homocigótico, se puede comprender por qué los matrimonios entre dos personas cercanamente emparentadas tienen mayor probabilidad de reunir dos genes recesivos, ya que el gen en cuestión estaría distribuido dentro de ese grupo familiar.
Se ha establecido de los matrimonios de primos directos dan cuenta de aproximadamente el 20% de los niños albinos y del 53% de los niños con la enfermedad fatal de Tay- Sachs.
Aun cuando la mayoría de las enfermedades genéticas recesivas conocidas son bastante raras, en conjunto representan una enorme carga de sufrimiento humano.

Existen varias enfermedades bien conocidas atribuibles a alelos dominantes.
La acondroplasia (un tipo de enanismo), la enfermedad de Huntington y la braquidactilia (dedos muy cortos) son causadas por la presencia de un alelo dominante.
También la hipercolesterolemia, caracterizada por niveles elevados de colesterol en sangre, lo que resulta en ataques cardíacos. 
Se la considera el trastorno genético más común heredado en forma mendeliana.
Afecta a una de cada 500 personas.

Otros trastornos están ligados al sexo.
La distrofia muscular de Duchenne y la hemofilia resultan de genes del cromosoma X, lo que da cuenta del hecho de que tales condiciones sean muy raras en las mujeres.

Necesitarían dos cromosomas X afectados para desarrollar la enfermedad.
Nótese que aunque hay una gran categoría de enfermedades atribuidas a defectos de un solo gen, algunas fatales, otras con un grado variable de gravedad, muchas enfermedades genéticas se deben a una complejo de genes interactuantes. 

Las enfermedades multifactoriales complejas, tales como la aterosclerosis o enfermedad vascular, son ejemplos prominentes. 
También, algunas formas de cáncer requieren la presencia de genes causantes del cáncer y la de defectos en los genes supresores de tumores.
Los problemas causados por múltiples genes son mucho más difíciles de rastrear y de analizar, de modo que la mayor parte de la atención se ha concentrado en los defectos de un solo gen, que son también los más susceptibles de tratamiento.

Ya hemos introducido los conceptos de cartografiado y secuenciamiento en capítulos anteriores.
Consideremos ahora varios ejemplos sobresalientes de cómo tales avances han sido utilizados para buscar los genes de la enfermedad.
También son ejemplos de «genética inversa».

Esto significa que se puede encontrar, por lo menos, la localización aproximada de un gen o genes causantes de una enfermedad de la que no se tiene ninguna idea del defecto bioquímico específico involucrado.

Esto contrasta con una enfermedad como la anemia de células falciformes, en la cual existe un defecto obvio en los glóbulos rojos, que son el transporte de la hemoglobina, lo que señala un probable defecto dentro del gen de la hemoglobina.
Estos ejemplos de genética inversa pueden ser considerados como un paradigma de lo que los científicos pueden esperar conseguir mediante el PGH.

Examinemos el reciente trabajo detectives que ha contribuido a nuestra comprensión actual de varias enfermedades genéticas humanas importantes.
El tratamiento directo de las fallas genéticas subyacentes a estas condiciones no es posible en la actualidad.
Concluiremos con una discusión de las bases científicas de la esperanza de que dicho tratamiento pueda estar a nuestro alcance.

Fibrosis cística

En el capítulo 1 bosquejamos la historia de la búsqueda del gen de la fibrosis cística.
La fibrosis cística (FC) es una enfermedad hereditaria que afecta los pulmones, el páncreas y las glándulas sudoríparas.
Los síntomas aparecen en la infancia y se caracterizan por una infección crónica del pulmón, una función pancreática anormal y un alto contenido de sal en la transpiración.
Esto se debe a un defecto en ciertos poros de la membrana celular que no admiten la entrada del cloruro en las células.
El resultado es un mucus grueso que obstruye los pasajes respiratorios en los pulmones y los conductos del páncreas y el hígado, interfiriendo con la respiración y la digestión.
Aquellos que tienen FC usualmente sucumben de infecciones respiratorias. 

La FC afecta a 1 entre 1.800 blancos y a I entre 17.000 negros.
Uno de cada dos blancos porta el gen recesivo y la FC está presente cuando hay una combinación homocigótica de dos genes recesivos.
La supervivencia promedio de las personas con FC es de 25 años.

Después de siete años de búsquedas intensivas, un equipo de investigadores canadienses y estadounidenses localizó este gen recesivo.
En 1985, Lap-Chee Tsui y John R. Riordan, del Hospital de Niños de Toronto, lo situó en el cromosoma 7 mediante un análisis de ligamiento génico, usando los RFLP.
Los patrones de herencia de cientos de familias afectadas por la fibrosis cístico fueron estudiados.
Los investigadores encontraron dos zonas de marcadores que estaban localizados a cada lado de un área en el cromosoma 7, gracias al hecho de que estos marcadores solían ser heredados junto con la enfermedad. 

El área cromosómica flanqueada por los marcadores fue aislada por una combinación de híbridos de células de humano y roedor y cartografía genética mediante enzimas de restricción, asistida por gel-electroforesis. 
Se clonaron muchos trozos de fragmentos de ADN de una biblioteca genómica ordenada por fluorescencia, específica del cromosoma 7.

En 1988, la distancia entre los marcadores había sido «reducida» a 1,5 millones de pares de bases. 
Restaba aún la tarea formidable de buscar, dentro de esa larga secuencia de bases nitrogenadas, el gen deficiente. 

Esto se realizó mediante «caminatas» y «saltos» cromosómicos.

«Caminar» a lo largo de un cromosoma es un término usado para describir una técnica que a veces nos permite aislar una secuencia génica cuando su localización aproximada es conocida.
Se comienza por el segmento de ADN que contiene el gen y una longitud adicional de ADN que contiene un área que siempre se hibrida con una sonda particular.
La sonda se usa como punto de partida para tratar de aislar el gen específico de la enfermedad.
Se aísla un clon de una biblioteca genómica que contenga un segmento del genoma que corresponde a la sonda. 
Se aísla una porción del clon que esté más lejos del sitio de hibridación con la sonda y se lo usa para reexaminar la biblioteca en busca de nuevos clones que se superpongan con él, pero que estén aun más lejos que la primera sonda.

Se repite este proceso muchas veces y se «camina» hacia el área del gen en pasos de unas 20 kilobases.
Es un proceso lento y tedioso que puede acelerarse mediante «saltos».

Éstos se refieren a la práctica de usar enzimas de restricción que cortan el ADN de un modo infrecuente, generando así fragmentos mayores y proveyendo, entonces, distancias de sondeo más grandes.
El uso de PAGE y YAC permite el manejo y la clonación de estos fragmentos grandes.
Una diferencia clave entre caminar y saltar es que en la segunda modalidad los fragmentos de ADN, más grandes, toman la forma de círculos, y el ADN en el área donde el círculo se cierra se corta y clona, reuniendo así secuencias que originalmente estaban alejadas sobre el ADN.
Estos ADN clonados del punto de clausura forman una biblioteca de «saltos».

En el caso de la búsqueda del gen de la FC, fue posible realizar saltos de hasta 100 kb usando este atajo.

En enero de 1989, los investigadores, ahora con la participación del doctor Francis Collins, del Instituto Médico Howard Hughes, y la Universidad de Michigan, habían reducido la búsqueda a un trecho de unos 300.000 pares de bases, suficientemente grande como para cubrir varios genes.
Entonces, volvió a dominar la caminata, para poder moverse a lo largo del ADN en pequeños pasos, de modo de no correr el riesgo de saltarse el gen de la FC.
Otros varios enfoques fueron usados en estas etapas finales.
Por ejemplo, todo el ARN mensajero hecho por las células que están en las glándulas sudoríparas del hombre fue extraído y se preparó cADN.
Éste fue usado como sonda para localizar un gen grande que se extendía a lo largo de aproximadamente 250.000 bases, en un mosaico compuesto por 24 exones separados por intrones no codificantes.
Resultó que al gen le faltaban sólo tres bases nitrogenadas que codificaban la inserción de fenilalanina en la posición 508 de un polipéptido.
El gen de la fibrosis cística había sido encontrado.

La noticia estuvo en la portada de todos los diarios del mundo.
Aumentó la esperanza de la posibilidad de pruebas diagnósticas precisas de los portadores y quizás, incluso, de algún tratamiento para los que sufren la enfermedad. 
Sin embargo, resultó que el 70% de los genes de FC defectuosos no tenían el código para la fenilalanina, mientras que el 30% restante tenía por lo menos alguna otra forma de mutación, aún no analizada.
A medida que pasaron los meses, luego de la euforia inicial los investigadores de todo el mundo hallaron, para su creciente sorpresa y descontento, que más de 40 mutaciones diferentes eran capaces de provocar la FC, haciendo así imposible el desarrollo de una prueba de selección prenatal que detectase todas las formas del gen.

Existen sorprendentes diferencias raciales y geográficas. 
En Dinamarca, por ejemplo, el 90% de las mutaciones de la FC implican la pérdida de la fenilalanina, mientras que en Israel sólo el 30% de las mutaciones de FC involucran esta pérdida.
Esto ha retardado el desarrollo de una prueba de selección prenatal satisfactoria, la cual, para ser precisa debería detectar la mayoría de las mutaciones causantes de la FC.

La multiplicidad de mutaciones que puede causar la FC también complica el problema de comprender cómo el defecto génico causa la enfermedad.
Los investigadores todavía no saben qué proteína específica codifica el gen, aunque la hipótesis más plausible es que se trata de una proteína llamada CFTR, o «regulador de conductancia transmembrana de la fibrosis cístico». 
Esa proteína es necesaria para el control del pasaje del cloruro a través de la membrana celular.

El 20 de septiembre de 1990, los diarios publicaron la espectacular noticia de que dos grupos de investigadores, usando virus como vectores, hablan introducido genes normales en células pulmonares con fibrosis cístico en crecimiento, en el laboratorio.
Los genes comenzaron a fabricar las proteínas de las que carecían los pacientes de fibrosis cística. 

Esto puede mostrar el camino para traducir el resultado en una terapia, en años venideros, quizás introduciendo estos genes normales en virus no nocivos que serían inhalados mediante un pulverizador nasal.

Enfermedad de Huntington

Igualmente esperanzador y frustrante ha sido el notable progreso en la búsqueda de la causa genética de la enfermedad de Huntington (EH), un trastorno relativamente raro.
Rastreado hasta el efecto de un solo gen dominante, el comienzo de este trastorno trágico es insidioso.
Ocurre una degeneración gradual de las células nerviosas en el cerebro, que puede comenzar ya a la edad de dos años o a los 80, pero, usualmente, exhibe sus síntomas mortales a mitad de la vida.
La enfermedad logró notoriedad durante los años sesenta cuando llevó a la muerte al popular cantante folk Woody Guthrie.

La duración de la enfermedad puede ser de hasta 25 años, durante los cuales hay una pérdida gradual del control sobre los músculos voluntarios, causando espasmos primero, luego grandes movimientos al azar seguidos por la demencia y finalmente la muerte.
Trágicamente, las personas con EH mantienen una inteligencia social, una conciencia de que ya no pueden controlar sus facultades físicas y son incapaces de comunicar sus necesidades y sentimientos a los otros.

A diferencia de la mayoría de las otras enfermedades genéticas, la EH, inexplicablemente, tiene un inicio retardado, haciéndose evidente, en muchos casos, mucho después de que el individuo afectado haya procreado.
Como la responsabilidad le corresponde a un gen dominante, cada niño de un padre que tiene la enfermedad tiene una probabilidad del 50% de heredarlo. 

Antes de 1983, el diagnóstico de un padre, luego de que los síntomas se habían hecho indudables, sólo informaba a sus descendientes acerca de esa probabilidad. 
Desde esa época la presencia del gen ha sido detectable con un marcador RFLP.

El camino que condujo a este marcador comenzó mucho antes de 1983.
En 1955, Americo Negrette, un médico que trabajaba en una base militar cerca del lago Maracaibo, en Venezuela, documentó la enfermedad entre los habitantes de un grupo de pequeños poblados cercanos.
Al principio, el doctor Negrette pensó que muchos de los lugareños, que había observado transitar tambaleantes por las calles, estaban ebrios.

Cuando finalmente se le explicó que estaban enfermos, comenzó a tomar historias clínicas detalladas y determinó que se trataba de la EH, o como se la denominaba entonces, la «corea» de Huntington, lo que significa «movimiento de baile».

Nancy Wexler, una psicóloga clínica, tenía un interés profesional y personal en la EH.
Su madre había sucumbido a la enfermedad, de modo que Wexler y su hermana corrían peligro.
Su padre, el doctor Milton Wexler, había creado la Fundación para las Enfermedades Hereditarias, dedicada a encontrar una cura para la EH, además de financiar investigaciones sobre otras enfermedades heredables.
Nancy Wexler y sus colegas recomendaron, con éxitos, al NIH que iniciase un estudio de esta población venezolana.

Cuando el grupo de investigación fue a Venezuela por primera vez, en 1979, su intención era buscar a alguien que fuese homocigótico para el gen.
Una vez que iniciaron seriamente el trabajo de campo, en 1981, sin embargo, la tecnología de ADN recombinante se había expandido al punto de que su énfasis cambió desde sólo estudios clásicos de ligamiento génico a la recolección de muestras sanguíneas de las que extraerían el ADN para análisis con RFLP.
Resultó que el árbol genealógico que descubrieron iba más allá de sus expectativas.
Incluía más de l0.000 personas y resultó ser una valiosa fuente para la comprensión de la herencia en general, además de proveer una importante referencia de ligamiento genético para otras enfermedades.

La EH es un ejemplo perfecto de lo que se conoce en genética como el «efecto fundador».
Que se refiere a la alta frecuencia de un gen en una población que comenzó o fue «fundada» por un pequeño número de personas, por lo menos una de las cuales portaba el gen en cuestión.
El cruzamiento dentro de la población, inicialmente limitada, difundió el gen.
El grupo venezolano ha sido rastreado hacia atrás por siete generaciones hasta una mujer cuyo padre, un europeo, portaba el gen defectuoso. 

Hay otra población estudiada intensamente en la región sudeste de Australia, donde los registros muestran que por lo menos 432 australianos enfermos de la EH han heredado el gen de una viuda inglesa, una tal señora Cundick, que, con sus trece hijos de sus dos matrimonios emigró a Australia en 1848.

James Gusella, Michael Conneally y un gran equipo de colaboradores del Hospital General de Massachusetts habían pasado una década intentando encontrar un gen ligado a la herencia de la EH, pero sin resultado.

Casi inmediatamente después de haber comenzado a buscar un marcador RFLP asociado a él, tuvieron éxito. 
Encontraron una sonda de hibridación, que llamaron G8, entre la primera docena de las que probaron.
Esta sonda hibridó con un RFLP que es muy cercano al gen de la EH.
El gen resultó estar localizado en el cromosoma 4, como reveló el análisis mediante líneas germinales de células híbridas de humano y ratón. 
Sólo la línea germinal de células que contenta el cromosoma 4 hibridó con la sonda G8.

El patrón de herencia es algo complicado, ya que hay cuatro variaciones posibles de los RFLP que pueden hallarse en las familias afectadas.
Cada persona recibe dos patrones RFLP, uno de cada uno de sus padres.
Las hibridaciones de fragmentos, obtenidos por enzimas de restricción, del ADN de un individuo venezolano que mostraba el patrón llamado «C» tienen la enfermedad o la desarrollarán más tarde en su vida.
En contraste, otras familias en otros países pueden tener el tipo «A» de patrón de RFLP.
Existen variaciones, pero el tipo de RFLP sigue siendo consistente dentro de un agrupamiento familiar. 

Hasta ahora, los científicos están a unos pocos millones de bases del gen.
Desafortunadamente, la secuencia dentro del área en la que estaría localizado el gen posee un patrón de bases que los enzimas de restricción tienden a cortar en muchos fragmentos pequeños, haciendo que sea muy difícil construir un cartografiado detallado de la región.
Deben hallarse nuevos RFLP más cercanos al gen y que lo flanqueen a ambos lados.
Entonces, una vez que la búsqueda haya sido restringida a un trozo de ADN de aproximadamente 500 kb 0 menos, el salto y caminata por el cromosoma podrá proceder con posibilidades razonables de éxito.

La naturaleza exacta del gen de la EH, a diferencia de la del gen de la FC, sigue siendo un misterio, pero con los marcadores RFLP el diagnóstico es posible.
El niño de un padre con EH tiene ahora la opción de hacerse un análisis de RFLP para determinar la presencia de este mortal gen dominante.
Debiera la persona someterse a la prueba y descubrir que tiene o no el gen defectuoso?
Si no posee el gen de la EH, esto significa que ni esa persona ni sus hijos están en peligro (suponiendo que el otro padre también sea negativo).
Si se lo encuentra, la persona puede sufrir la misma pérdida progresiva de integridad física de la que ha sido testigo en su padre.

Las probabilidades de pasar el gen a los hijos son de un 50%.
Los RFLP, el salto cromosómico, el PCD y otras técnicas de la genética molecular adquieren una nueva importancia para aquellos para los que son el único medio de aislar y clonar el gen, que aguarda en su ADN como una bomba de tiempo.
Cuando el gen sea aislado y su función comprendida, los científicos esperan que sea posible alguna forma de terapia genética.

Enfermedad de Alzheimer

Los datos recolectados en la red familiar masiva venezolana han sido usados para estudiar todo un conjunto de enfermedades genéticas.
Se han hecho mapas normales de localizaciones de marcadores de genes usando las muestras familiares venezolanas. 
Estos mapas normales son, entonces, comparados con los mapas cromosómicos de otros grupos familiares que exhiben otras enfermedades de interés, familias que con frecuencia son demasiado pequeñas para realizar un cartografiado productivo.

Entre los genes que las muestras venezolanas han ayudado a cartografiar destaca el de la enfermedad de Alzheimer (EA). 
Esta condición presenta un interjuego complejo y frustrante de factores genéticos y otros con frecuencia no determinados.
Aunque la genética tiene sin duda un papel, y hasta un 50% de los miembros de algunas familias desarrollan la enfermedad, esto da cuenta de menos de una persona entre 20 de las afectadas.

La EA puede afligir a más del 10% de las personas de 65 años o más y a cerca del 50% de las de 80 años o más.
Es una enfermedad degenerativa del sistema nervioso central caracterizada por trastornos de la memoria y la función intelectual que, con frecuencia, provocan una incapacidad mental y física profunda, que puede requerir de cuidado institucional.
La muerte ocurre usualmente dentro de los 5 a 10 años del inicio de los síntomas.
Se estima que la EA produce un costo de unos 25 mil millones de dólares, sólo en los Estados Unidos.
Por supuesto, el costo en sufrimientos de los pacientes y sus familias es incalculable.

No se ha encontrado ningún tratamiento efectivo ni para prevenir ni para detener el progreso de la EA.
En 1987, un equipo internacional de investigación de 22 miembros informó haber rastreado el defecto genético causante de la EA de relación familiar al cromosoma 21. 
Es interesante que este sea el mismo cromosoma implicado en el síndrome de Down, una forma muy común de retraso mental.
Casi siempre es provocado por la presencia de un cromosoma 21 extra en cada célula.
Las personas con síndrome de Down, si viven lo suficiente, suelen desarrollan la EA.
Nuevamente, la promesa contenida en el hallazgo de marcadores ligados al gen de la EA es que estreche la búsqueda del propio gen.

Una comprensión del defecto de la EA a nivel molecular podría posibilitar la definición de los caminos bioquímicos implicados en la iniciación y el progreso de las características químicas y neuropatológicas de la enfermedad.
Los trastornos cerebrales son muy difíciles de tratar.

La mayoría de los fármacos, aunque pueden circular libremente en la sangre, no pueden dejar los vasos sanguíneos y entrar en el tejido cerebral.
Las neuronas, el tipo de célula que forma buena parte del cerebro, no pueden ser aisladas y tratadas sin provocar serios daños.

Sin embargo, los experimentos con animales indican que es posible implantar fibroblastos (células de tejido conectivo) genéticamente alterados en el cerebro, para que estimulen la liberación de un factor de crecimiento neuronal del tipo de neuronas cuyo mal funcionamiento está asociado con la pérdida de memoria en la enfermedad de Alzheimer. 

Podría ser un sistema útil de introducción de genes, cuando los genes causantes sean finalmente aislados. 

Pueden encontrarse indicios de cómo los factores ambientales afectan la EA no genética.
Entretanto, mientras la caza del gen continua, el Instituto Nacional para el Envejecimiento estima que hasta 14 millones de estadounidenses tendrán la enfermedad de Alzheimer para mediados del próximo siglo.


ADN: modelos y significado

Mediante la observación, la experimentación y la especulación inspirada, lo que al principio eran nociones vagas e incipientes acerca del gen se afinaron cada vez más.
Sin embargo, a pesar de las conclusiones precisas que se estaban obteniendo acerca del gen -su localización en el cromosoma y su papel clave en la vida de la célula y en la herencia- la arquitectura molecular y la función exacta del gen seguían siendo misterios a mediados del siglo XX.
Para poder contar la historia de la comprensión de estos misterios, debemos introducir aquí algunos detalles técnicos.
Estos detalles giran en torno a la molécula de ADN y sus actividades dentro de la célula.

De hecho, son los tecnicismos que debemos aprehender para comprender cuál es la interpretación actual del genoma humano.
Deben entenderse para poder apreciar, más allá de simples generalidades, cómo la información creciente acerca del genoma puede, primero, ser interpretada significativamente y, luego, aplicada para manipular genes con propósitos científicos y médicos.
Tal comprensión se ha hecho un común denominador entre los biólogos.
También debe serlo para todos aquellos que deseen tener un conocimiento claro acerca de cómo y por qué el conocimiento derivado de este análisis tendrá un impacto tan profundo no sólo sobre los hombres sino también sobre todos los sistemas vivientes.

Con el trabajo de Watson y sus colegas comenzó una época dorada de la biología molecular.
Surgirían nuevas tecnologías capaces de hacer avanzar el conocimiento de las funciones del gen.
El Proyecto Genoma Humano sólo ha sido posible gracias a esta serie de desarrollos.
Sus herramientas y metodologías son en gran medida las iniciadas entre los años 50 y 70.
La nueva era comenzó con lo que se considera la síntesis más significativa en las ciencias biológicas desde el trabajo de Charles Darwin.
Nos referimos a la historia de la «doble hélice».

James Dewey Watson fue siempre brillante.
Había sido uno de los «niños prodigio» en un programa de radio durante la guerra y había ingresado en la Universidad de Chicago a la edad de quince años.
Aunque no podía saberlo en esa época, la Universidad era entonces la sede de la primera «pila atómica» del mundo.
Como parte del Proyecto Manhattan, la «pila», muy secreta, era un apilamiento gigante de bloques de grafito que rodeaban barras de uranio.
La pila clandestina, ignorada hasta por el rector de la universidad, había sido montada bajo las gradas de un estadio de fútbol inutilizado.

El 2 de diciembre de 1942, bajo el ojo vigilante de Enrico Fermi, diseñador de la pila, las barras de control de cadmio fueron cuidadosamente quitadas y el uranio altamente radiactivo produjo la primera reacción en cadena atómica. 
Sólo funcionó 4,5 minutos a medio vatio de potencia, pero conduciría a la tecnología que hizo posible el desarrollo de la bomba atómica sólo tres años después.
Entre el pequeño grupo de científicos presentes en ese día histórico estaba Leo Szilard, un físico teórico húngaro y uno de los principales arquitectos de la bomba.
En 1947, Szilard, que se había opuesto vigorosamente al uso de la misma, abandonó la física por la biología. 
Fue a Cold Spring Harbor para unirse al «grupo de fagos» de Luria y Delbrück.

El interés de Watson, sin embargo, había sido siempre la biología.

Obtuvo su título en 1946 y se quedó un año adicional para tomar clases de zoología.
En 1944 leyó por casualidad un libro recién publicado por Erwin Schrödinger: ¿Qué es la vida? El libro había sido inspirado por un artículo breve escrito por Max Delbrück acerca de las propiedades físicas del gen.
Schrödinger especuló acerca de la naturaleza del gen a la luz de los principios establecidos de la química y la física.
El tema del libro es que los genes son los componentes clave de las células vivientes y que para comprender qué es la vida debemos saber cómo actúan.

Watson, en seguida, «quedó polarizado hacia descubrir el secreto del gen».

Cuando Watson llegó a la escuela de graduados de la Universidad de Indiana, en 1947, su académico más prominente era Herman Mullen, quien acababa de ganar el premio Nobel por su trabajo con las mutaciones inducidas por los rayos X.
Aunque atraído a Indiana por la presencia de Muller, Watson fue influenciado allí, fundamentalmente, por Salvador Luria y, a través de él, por Max Delbrück y el «grupo de fagos».
Luria y Delbrück habían huido de Europa cuando la guerra se hizo más cercana, Luria de Italia y Delbrück de Alemania.
Luria asignó a Watson a un proyecto de investigación acerca de los efectos de los rayos X sobre los fagos, y éste completó su doctorado en mayo de 1950, a los veintidós años.
Después de unas últimas seis semanas en Cold Spring Harbor, partió hacia Europa.

Su experiencia en el laboratorio de Herman Kalckar rápidamente se transformó en un desastre.
La indiferencia de Watson hacia la química del ácido nucleico no se curó por el hecho de estar en Europa.
Se sentía cada vez más frustrado al advertir que su permanencia en Copenhague con Kalckar no lo aproximaría más a la naturaleza del gen.

Entonces, mientras asistía a una conferencia en Nápoles, su suerte cambió.
Entre los conferenciantes estaba Maurice Wilkins, del King's College de Londres.
Wilkins repasó sus datos preliminares acerca de la estructura molecular del ADN, que había obtenido mediante el uso de la cristalografía con rayos X.
Básicamente, esta técnica implica exponer las moléculas, en una forma cristalina, a los rayos X.
Algunos de estos rayos golpean los átomos y son desviados de la sustancia cristalina. 
Una rotación del cristal cambia los ángulos en los que los rayos X inciden y la imagen de los rayos desviados puede captarse en película.
Este patrón de «difracción» puede suministrar información acerca de la configuración tridimensional de los átomos que forman la molécula.

La técnica es precisa y exige una interpretación habilidosa para examinar las moléculas, de un modo que ninguna otra técnica permite.
Watson había estado preocupado por el hecho de que la estructura de los genes pudiera ser frustrantemente irregular.
Ahora, sabía que podían cristalizarse, esto es, debían tener una estructura regular que podría ser analizada de un modo sistemático.
Mientras Wilkins hablaba.
Watson decidió unírsele en su trabajo del ADN, en Inglaterra.

Wilkins, un físico de Nueva Zelanda, había ido a los Estados Unidos durante la Segunda Guerra Mundial para trabajar en la separación del uranio para el Proyecto Manhattan.
Pronto dejó el estudio del núcleo atómico para trabajar en el núcleo de la célula, como tantos físicos que decidieron volcar sus habilidades hacia la comprensión de los problemas biológicos. 
Watson, incapaz de obtener una invitación para trabajar directamente con Wilkins, consiguió, sin embargo, y gracias a los esfuerzos de Luria, una invitación para unirse al grupo de Max Perutz en el Laboratorio Cavendish, en la Universidad de Cambridge, cerca del King's College.

En esa época se hallaba en marcha un esfuerzo concertado, encabezado por Perutz, para analizar moléculas biológicas grandes.
Su grupo estaba particularmente interesado en la cristalografía con rayos X de la proteína de hemoglobina.
El Cavendish estaba entonces dirigido por sir Lawrence Bragg, uno de los inventores de la cristalografía. 
Había sido la sede del primer triturador de átomos y el acelerador original de Ernest Rutherford todavía se encontraba allí.
Ahora, el Cavendish sería la escena de otra revolución científica extraordinaria. 
ya había liberado la sorprendente energía encerrada dentro del átomo.
Finalmente había llegado el momento de abrir los misterios del gen para liberar su potencial.

Como herederos de un pasado en el cual los norteamericanos arrojaron bombas atómicas, sabemos demasiado bien cuánta destrucción en nuestros congéneres provocó ese triunfo de la física.
También nos permitió una comprensión profunda de la naturaleza del universo y la materia.
Esa comprensión llevó a extraordinarios desarrollos, como los ordenadores, los aviones supersónicos, los cohetes y los láser.
Estos adelantos, aunque en sí mismos moralmente neutros, pueden ser usados con propósitos sabios o destructivos.

En forma similar, una nueva frontera de la biología molecular está ante nosotros.
Sólo ahora, con el Proyecto Genoma Humano y toda la investigación que inspirará, estamos aproximándonos a una comprensión íntima de nuestros genes.
Junto con ella vendrá un poder sobre la vida humana que debe ser usado sabiamente. 
Habrá mucho más por decir acerca de este tema en los capítulos siguientes, de modo que volvamos al drama que nos ocupa: el descubrimiento revolucionario de la estructura y esencia del gen.

Durante su primer día en Cambridge, Watson conoció al entusiasta Francis Crick y los dos se entendieron inmediatamente. 
No sólo tenían intereses similares sino que, como Crick escribiría en su retrospectiva de 1988, What Mad Pursuit : «(teníamos) una cierta arrogancia juvenil una crueldad y una impaciencia junto con un pensamiento torpe (que) nos surgía naturalmente a los dos».
Por su parte, Watson comenzaba su libro más conocido, La doble hélice , en 1968 con la frase: «Nunca he visto a Francis Crick en una actitud modesta».

En esa época Francis Crick tenía 35 años y era todavía un estudiante graduado.
Era otro convertido a la biología desde la física y se había unido a Perutz en Cavendish para trabajar en la estructura de las proteínas.
Como Watson, Crick había decidido con anterioridad que sólo un análisis molecular detallado del ADN podría descubrir la verdadera naturaleza del gen.
Pronto, estuvieron de acuerdo en colaborar en base a que Crick uniese sus conocimientos de cristalografía con rayos X y de física a los conocimientos de Watson en genética.

Como entusiastas de talento, pero virtuales desconocidos en el mundo de la ciencia, tenían un formidable competidor. 
Linus Pauling, profesor en Cal Tech, era considerado el químico más adelantado en el mundo.
Era el autor del libro clásico La naturaleza del enlace químico, y recientemente había descrito la estructura de la primera proteína analizada mediante cristalografía con rayos X, llamándola una «hélice alfa».
Por ello quería decir que la molécula de proteína en estudio tenía una forma helicoidal, esto es, una configuración con un diámetro constante en toda su longitud, como un trozo de alambre que estuviese enrollado en torno a un cilindro.

La insistencia de Pauling en que el conocimiento de la química de los átomos y los enlaces de las moléculas grandes de los sistemas vivientes sería suficiente para descubrir sus misterios ya había sido la inspiración de Watson y Crick.
Parecía ser sólo cuestión de tiempo que el gran Pauling descubriese la forma exacta de la molécula de ADN.
Con la temeridad de la juventud, decidieron ganarse a Pauling en su propio juego.

Para aumentar su frustración, el trabajo en la molécula de ADN en Cambridge estaba ahora centrado en el laboratorio de Maurice Wilkins, en el King's College.
En 1950, sir John Randall, director de la sección biofísica, había invitado a Rosalind Franklin, que entonces estaba trabajando en Paris como especialista en análisis de moléculas mediante rayos X, a que fuera al King's College y que estableciese una unidad de difracción por rayos X.
Wilkins ya había realizado algunos análisis del ADN mediante rayos X, pero, sabiendo que no tenía los conocimientos necesarios para avanzar mucho más había acordado con Randall que debía emplearla para seguir donde él se había detenido.
La historia de Rosalind Franklin es un capítulo complejo y oscuro en la historia del ADN.

Rosalind Franklin, una mujer de treinta años, brillante, analítica e independiente, era percibida generalmente como una anomalía en la atmósfera de club masculino del King's College.
Poco después de su llegada comenzó a trabajar bajo la impresión perfectamente lógica de que el ADN era ahora su territorio.
Pronto se sintió abiertamente molesta ante el trato de Wilkins hacia ella, como asistente y no como colega.
Los dos, como Wilkins lo formuló discretamente, «nunca congeniaron». 
La relación entre Wilkins y Franklin y las dificultades que ella tuvo que enfrentar están documentadas cuidadosa y reflexivamente en el libro de Anne Sayre Rosalind Franklin y el ADN .
Arthur Cooper, del Newsweek , dijo del libro en su comentario: «Cualquiera que lea La doble hélice le debe a Franklin leer su historia también».

El bestseller de Watson, La doble hélice , publicado por primera vez en 1968, es un ejemplo de chauvinismo masculino.
Watson se refiere a Franklin como «Rosy» y se pregunta «cómo sería si se quitase las gafas e hiciese algo distinto con su cabello».
En el epílogo de su libro admite que sus impresiones iniciales de ella «frecuentemente eran erróneas» y advirtió «algunos años demasiado tarde, las luchas que debe enfrentar una mujer inteligente...»

Francis Crick, en su retrospectiva de aquellos años, What Mad Pursuit , enfocó el tema de forma algo diferente.
Notó que había «restricciones irritantes-no se le permitía tomar café en una de las salas reservada sólo para los hombres-, pero éstas eran relativamente triviales o así lo parecían en la época».
No se puede imaginar que sean triviales para la parte excluida.

Privados de cualquier oportunidad oficial para atacar el problema de la estructura del ADN, Watson y Crick, de todos modos, procedieron a emular el trabajo de Pauling sobre las proteínas. 
En base a lo que podían aprender de los datos de rayos X, ya reunidos por Wilkins y Franklin, se preguntaron qué átomos en la molécula de ADN era más probable que estuviesen asociados.
Todo lo que tenían que hacer, según Watson, con la confianza de la retrospección, era «construir un conjunto de modelos moleculares y entonces empezar a jugar...»

A comienzos de los años cuarenta los científicos habían pensado erróneamente que el ADN era una molécula pequeña y relativamente simple.
El experto principal en ácidos nucleicos en la década de los veinte había sido Phoebus Levine, del Instituto Rockefeller.
Había descrito la molécula de ADN como una serie regularmente repetida de bloques de construcción llamados nucleótidos, cada uno compuesto por azúcar, fosfato y una base nitrogenada, en la cual los cuatro tipos diferentes de nucleótidos se siguen uno a otro en un orden fijo, en conjuntos repetidos de cuatro.
Esto formaría un polímero, una larga cadena hecha de muchos bloques de construcción individuales.
La hipótesis del «tetranucleótido», como se la llamó, dejaba poco lugar para la variación de la molécula de ADN.
Esto fortaleció la candidatura de las proteínas como material genético. 

Se sabía que las proteínas eran polímeros definidos, cuyos bloques de construcción eran aminoácidos. 
Éstos se asociaban entre si como cuentas en un hilo.
Se sabe que comúnmente hay veinte tipos diferentes de aminoácidos en los sistemas vivientes.
A diferencia de los conjuntos hipotéticamente monótonos repetidos una y otra vez en los ácidos nucleicos, una proteína tendría muchas más posibilidades de variabilidad. 

Esto se debe a que los 20 aminoácidos podían ser dispuestos en literalmente millones de secuencias diferentes. 
Una proteína consiste en una o más cadenas individuales, «polipéptidos», usualmente formados por varios cientos de aminoácidos.
Las cadenas polipeptídicas están ligadas entre sí por enlaces químicos y todo el complejo está contorsionado en una forma aproximadamente esférica o alargada como un filamento.

El análisis de rayos X de numerosas proteínas había mostrado que cada proteína particular tiene una forma específica invariante.
Si se la calienta, algunos de los enlaces de la proteína se rompen.
Ésta, entonces, literalmente, se despliega, perdiendo su forma precisa normal.
Esto se conoce como «desnaturalizar» la proteína.
Un ejemplo familiar de esto ocurre cuando se calienta la clara del huevo, que está formada por la proteína albúmina.
Esta desnaturalización cambia la clara del huevo de una forma líquida a una sólida, reflejando el cambio en la forma de las moléculas de albúmina. 

Como los científicos hallaron que las proteínas desnaturalizadas ya no pueden funcionar como enzimas, conjeturaron que la función de una proteína depende de su estructura tridimensional exacta.
Por ende, si los genes estaban hechos de proteína, parecía lógico que cada gen tuviera su propia y única organización tridimensional, exactamente como los enzimas.
¿Cómo podía una tal estructura complicada funcionar como gen?
¿Cómo podía sintetizar proteínas con formas diferentes, para ser usadas como enzimas?
¿Cómo podía ese tipo de gen complejo copiarse una generación tras otra con pocos errores?

Hacia finales de la década de los cuarenta se habían hecho varios descubrimientos importantes que clarificaban la química del ADN.
Los científicos habían establecido que las moléculas de ADN eran bastante largas y ciertamente más complejas que la secuencia invariante descrita por Levine.
Los bloques de construcción del ADN, los nucleótidos, consistían, cada uno, en un azúcar (desoxirribosa), un grupo fosfato y una de entre cuatro moléculas orgánicas complejas denominada bases nitrogenadas.

Aunque es difícil de visualizar, debemos comprender que las moléculas ocupan espacio en todas las dimensiones. 
Tienen formas específicas, donde los átomos constituyentes están asociados entre sí en una variedad de ángulos.
Los fisicoquímicos se especializan en estudiar estas configuraciones y han construido modelos tridimensionales a escala para un gran número de moléculas.
Las fuerzas internas que mantienen juntas las moléculas obedecen a ciertas leyes físicas bien definidas que imponen restricciones estrictas a la forma de las mismas.

Esto significa, por ejemplo, que una molécula de agua o una molécula de glucosa sólo pueden adoptar una cierta forma específica.
Sin embargo, aun si se tuviese un análisis exhaustivo de todas las diversas partes de una molécula altamente compleja, todavía quedan numerosas configuraciones posibles que podrían parecer razonables a cualquiera que intente construir un modelo tridimensional de esa molécula.
Tomando las partes de la transmisión de un automóvil, ¿de cuántos modos podrían montarse para que el motor funcione tal como se lo diseñó?
Sin importar las posibilidades teóricas, sólo una disposición sería la correcta.

Irwin Chargaff, un refugiado austríaco y más tarde químico en la Universidad de Columbia, había hecho un descubrimiento problemático hacia finales de la década de los cuarenta.
Descubrió que la composición del ADN de diferentes organismos era bastante similar, en ciertos aspectos.
Dos de las cuatro bases nitrogenadas componentes del ADN, las purinas llamadas adenina (A) y guanina (G), estaban hechas de sendos anillos de átomos similares, uno de cinco lados y el otro de seis.
Los anillos estaban hechos de carbono, hidrógeno, oxígeno y nitrógeno.
Las otras dos bases eran las pirimidinas citosina (C) y timina (T), ambas anillos de seis lados.
Más todavía, Chargaff determinó que la cantidad de adenina en una muestra de ADN dada era siempre igual a la cantidad de timina.
Del mismo modo, la cantidad de guanina era igual a la de citosina.
Sin embargo, había una diferencia crítica entre el ADN aislado de diferentes especies de organismos.
Aunque la cantidad de A era igual a la de T y la de G a la de C, las cantidades relativas de A + T y G + C variaban mucho.
No había ninguna explicación clara.

Las bases nitrogenadas, con sus cantidades relativas acordes a lo que fue conocido como las reglas de Chargaff, se sabía que estaban asociadas a una larga cadena de grupos del azúcar desoxirribosa y fosfato alternantes.
Una de las cuatro bases nitrogenadas posibles estaba enlazada con cada desoxirribosa. 
Un nucleótido consistía en una combinación de desoxirribosa, fosfato y base nitrogenada.

Watson y Crick podían ver que la solución al problema del ADN podría llegar a ser más truculenta que la de la hélice alfa descrita por Linus Pauling.
En la hélice alfa, una sola hilera de aminoácidos asociados se enroscaban en forma de muelle.
La forma se mantenía intacta mediante «enlaces de hidrógeno» que ligaban las espiras cercanas de la cadena helicoidal.
Un enlace de hidrógeno se forma cuando un átomo de hidrógeno ligado a un átomo cargado negativamente es atraído hacia otro átomo negativo.
En una célula viviente, estos átomos negativa mente cargados son usualmente el oxígeno y el nitrógeno. 
Un enlace de hidrógeno es relativamente débil y se rompe fácilmente, pero en el caso de las proteínas y otras moléculas complejas pueden haber muchos de estos enlaces, y el total de fuerzas atractoras es entonces mucho más fuerte.

Los datos iniciales de Wilkins usando rayos X habían indicado que la molécula de ADN era más ancha de lo que debería ser si consistiese sólo en una cadena de nucleótidos.
Eso le había llevado a la conclusión de que el ADN era quizás una espiral compuesta de varias cadenas de nucleótidos que se trenzaban entre sí.
Las cadenas podrían ser mantenidas juntas por enlaces de hidrógeno o quizás por enlaces entre los grupos fosfato.

Watson y Crick partieron del supuesto de que los nucleótidos estaban hechos por un «espinazo de azúcar-fosfato», una larga cadena no ramificada alternante de desoxirribosa y fosfatos.
Cada una de las unidades de desoxirribosa del «espinazo» tenía una base nitrogenada asociada. 
La secuencia de bases nitrogenadas, cada una asociada a una molécula de desoxirribosa, se suponía era bastante irregular.
Si la secuencia de bases era regular, como ATGCATGCATGC, entonces la molécula de ADN sería una serie homogénea de nucleótidos sin nada para distinguir un gen (fuera lo que fuere) de otro.
Por otro lado, si el orden de las bases era irregular, tal como AATGCTACT, entonces este tipo de disposición de las bases podía proveer una increíble variabilidad a la molécula de ADN. 

Lo que se necesitaba era fotografías de rayos X con mayor definición.
Rosalind Franklin debía dar una conferencia acerca de su trabajo en los últimos seis meses, y la atención de Watson y Crick se enfocó en sus últimos descubrimientos.
Watson, que había asistido a la charla con Maurice Wilkins, detalló sus observaciones a Crick, quien advirtió el hecho de que sus imágenes con rayos X habían sido realizadas usando ADN hidratado.
Eso significaba que el ADN estaba en la denominada forma B, un término descriptivo acuñado por Franklin, es decir que contenía muchas moléculas de agua y era así más fibrosa por naturaleza que la forma cristalina previamente vista por Wilkins.
El problema con la presencia de tanta agua era que la claridad de las imágenes obtenidos era pobre, haciendo que el resultado fuese difícil de interpretar Usando el recuerdo de Watson de la cantidad de agua en las muestras Crick determinó que el ADN podía estar compuesto por dos, tres o cuatro cadenas de nucleótidos.
Era «meramente» cuestión del modo en que dichas cadenas estaban enroscadas en torno al eje.

Unos pocos días después construyeron un modelo de espiral con tres miembros usando figuras de chapa metálica con la forma y el tamaño de los átomos constituyentes, con alambres, tuercas y bulones para mantenerlas unidas entre sí.
Las bases nitrogenadas miraban hacia afuera y la espiral se mantenía unida en el centro mediante enlaces entre grupos fosfato.
Al mostrar orgullosamente su construcción a Wilkins y Franklin para su aprobación, quedaron desolados al saber que Watson había malentendido la cantidad de agua en su muestra por un factor de 10.

Para empeorar las cosas, la creciente tensión causada por el sentimiento de propiedad que el King's College tenía sobre el ADN y los trabajos no oficiales sobre el mismo realizados por Watson y Crick, se tradujeron en una orden oficial desde arriba.
Sir Lawrence Bragg indicó en términos nada vagos que de allí en adelante debían dejar tranquilo el problema del ADN.
Watson, con su agudeza característica, escribió acerca de esta moratoria: «Quedarnos quietos tiene sentido, porque estábamos hasta las orejas de modelos basados en núcleos de azúcar y fosfato».
Su construcción activa de modelos fue detenida, pero sus conversaciones y especulaciones continuaron sin mella.

Entre tanto, Watson había recibido una larga carta de Alfred Hershey, desde Cold Spring Harbor, llena de excitantes novedades acerca de sus últimos experimentos.
Él y Martha Chase habían establecido que la característica fundamental de la infección de bacterias por virus era la inyección del ADN del virus en la bacteria.
El ADN dominaba las operaciones de la célula bacteriana, haciendo que construyera nuevos virus.
Era una demostración poderosa de que el ADN era el material genético principal. 
No mucho después, un químico norteamericano se detuvo en Cavendish para una breve visita.
Erwin Chargaff, ahora uno de los expertos mundiales en la química del ADN, no se sintió muy divertido cuando conoció a los dos desconocidos que proclamaban haber estado en la carrera por ser los primeros en comprender la estructura tridimensional del ADN.
Quedó doblemente desconcertado al hallar que Crick había olvidado justo en ese momento las fórmulas para las bases nitrogenadas.
Se fue muy poco impresionado, pero las reglas de Chargaff entraron ahora en sus cálculos. 
Esto resultaría crucial para su éxito. 

Una mañana, llegó un manuscrito del renombrado Linus Pauling en el cual detallaba sus conclusiones acerca de la estructura del ADN.
Había sido enviado a su hijo Peter, que era en esa época, irónicamente, compañero de oficina de Crick y Watson.
Su desconsuelo se transformó en incredulidad a medida que devoraban el informe y advirtieron que Pauling había cometido un error fundamental en la química.
Los químicos ya habían establecido hacía mucho tiempo que los grupos fosfato de los nucleótidos se ionizaban, esto es, liberaban hidrógeno en su entorno.
Esta propiedad de ser un «donante de hidrógeno» hacía que la molécula de ADN fuese un ácido.
Increíblemente, Pauling había dibujado el grupo fosfato en la espiral de tres miembros con los átomos de hidrógeno todavía asociados.
La carrera no estaba terminada de ningún modo.

Watson corrió al King's College con las noticias de Pauling.
Se topó con Rosalind Franklin, quien trabajaba sola en su laboratorio.
Había estado ofendida con él durante bastante tiempo, debido a su continua insistencia en darle clases acerca de la interpretación de la evidencia surgida de la cristalografía por rayos X.
De acuerdo con el relato de Watson, que es el único registro de este encuentro, mientras él blandía el manuscrito de Pauling excitadamente, ella se ofuscó. 
Su enojo estaba aumentado por el hecho de que el manuscrito que él tenía en su mano contenía información que ella hacia un mes había solicitado infructuosamente al laboratorio de Pauling.

Watson rápidamente retrocedió a través de la puerta abierta, cuando Wilkins apareció y lo empujó hasta su oficina.
Con un espíritu de camaradería quizá generado por su común animosidad hacia Rosalind Franklin, Wilkins mostró a Watson una imagen de un patrón de rayos X, una de dos excelentes fotografías de la forma B del ADN que Franklin había tomado diez meses antes. 
Watson escribió acerca de la experiencia: «En el instante en que vi la imagen mi boca se abrió y mi pulso comenzó a acelerarse».

Tenían ya los nuevos hechos críticos necesarios. 
La forma B del ADN era una hélice que tenía un intervalo regular de repetición de 34 angstroms de distancia entre cada espira (un angstrom es un diez mil millonésimo de metro).
Esto era precisamente 10 veces la distancia de 3,4 angstroms entre un nucleótido y el siguiente.
La propia espiral era un cilindro regular de 20 angstroms de ancho.

Watson ya era consciente de algunas de estas medidas.
Si Wilkins discutió o no las otras, en ese momento, todavía no está aclarado.
El hecho es que mostró a Watson la imagen sin el conocimiento de Rosalind Franklin. 
Ella ya había deducido que las bases nitrogenadas tenían que estar en el centro de la espiral con el espinazo de azúcar-fosfato en el exterior.
Sus propias notas dicen que «los resultados sugieren una estructura helicoidal... que contiene 2, 3 o 4 cadenas de ácidos nucleicos coaxiales por unidad helicoidal y que tienen los grupos fosfato cerca del exterior».
Wilkins le había mostrado a Watson la fotografía de Rosalind Franklin que sería crucial para la construcción del modelo molecular correcto.

Al enterarse del fiasco de Pauling y convencido de que Watson y Crick, ahora armados con su nueva información, estaban preparados para completar su síntesis, Bragg los autorizó a retomar la construcción del modelo. 
Siguieron días de frustración.
Un mes después de que Watson viera la forma B del ADN en el laboratorio de Wilkins, le entregaron a Crick un documento que había sido hecho circular por sir John Randall.
Contenía un informe de Rosalind Franklin, en el que daba las mediciones críticas de la forma B, pero no la fotografía. 
Crick, con una comprensión mucho más profunda que Watson de la cristalografía por rayos X, extrajo de los números de Franklin el último trozo del rompecabezas. 

Crick sugirió que los espinazos de azúcar-fosfato de la doble hélice deberían estar alineados de modo de que corriesen en direcciones opuestas, un modo denominado antiparalelo.
Esta orientación era necesaria para que los dos lados de la hélice, reunidos por los pares de bases, se ajustasen al diámetro conocido de 20 angstroms. 

Provisto de esta nueva revelación, Watson duplicó sus esfuerzos.
El momento del «eureka» llegó. 
Repentinamente advirtió que un par adenina y timina, cuando se los mantiene juntos mediante dos enlaces de hidrógeno, serían de un tamaño idéntico al par guanina y citosina mantenidos unidos por tres de tales enlaces.
¡Las reglas de Chargaff! Si estos pares AT y GC fuesen colocados en un modelo de doble hélice hecho de dos espinazos entrelazados de azúcar-fosfato de modo que los pares de bases tuviesen una posición plana, sus dimensiones permitirían precisamente que diez pares de bases cupiesen en cada giro completo de 34 angstroms de las espiras helicoidales.
Al hacerlo, cada par de bases no estaba alineado con los pares inmediatamente por encima y por debajo, sino levemente desplazado del siguiente como los escalones en una escalera de caracol.
Cada uno estaba rotado 36 grados respecto de los pares adyacentes.
Todo encajaba.

Este apareamiento se ajustaba a la configuración de dos espinazos de unidades de azúcar-fosfato unidas por estos pares de bases nitrogenadas.
La doble hélice era «diestra».
Si uno pudiese mirar hacia abajo desde el eje longitudinal, los dos espinazos de azúcar-fosfato girarían en un sentido antihorario al aproximarse al observador. 
El modelo estaba completo.
Según Watson: «Era demasiado hermoso para no ser verdadero».

Un artículo de 900 palabras fue enviado por James Watson y Francis Crick el 2 de abril de 1953 a los editores de Nature , una revista científica internacional muy importante.
Comenzaba modestamente: «Deseamos sugerir una estructura para... el ADN.
Esta estructura tiene características novedosas que son de considerable interés».
Las «características novedosas» de la molécula incluirían al mismo gen.

«Esta estructura», cuyo análisis había sido durante mucho tiempo la provincia de los químicos, a los que se unieron más tarde los cristalógrafos por rayos X, finalmente, había sido descubierta por dos mentes creativas que no estaban entre sus filas.
No habían usado ni tubos de ensayo, ni incubadoras, ni sustancias químicas; sólo el brillante poder intelectual de síntesis. 
Una parte significativa de los datos que habían entrado en su síntesis procedían de Rosalind Franklin. 
Ella había tomado la fotografía por rayos X definitiva y sus cálculos fueron vitales en la formación del modelo final.
¿Hubiera ella, eventualmente, hallado la respuesta?
Es muy probable.

Sin embargo, no sería elegida para compartir la gloria de Watson Crick y Wilkins, quienes recibieron el premio Nobel en 1962 ni siquiera en forma póstuma.
Rosalind Franklin murió de cáncer en 1958, a la edad de 37 años.
Según Watson, «continuó trabajando a un alto nivel hasta pocas semanas antes de su muerte». 

Figura 1

La doble espiral del ADN

Las dos líneas enroscadas representan las cadenas de azúcar-fosfato.
Nótese que su disposición antiparalela está indicada por un 5' y un 3' en los extremos opuestos.
Véase el capítulo 5 para más detalles.

Mapas y marcadores

La búsqueda de los genes humanos comienza, bastante lógicamente, con un mapa.
Los mapas ordinarios van desde los que indican los puntos sobresalientes de grandes áreas hasta los más útiles y detallados que guían al visitante casual hacia su nuevo destino. 

Para registrar con precisión las fronteras, las carreteras, las ciudades y pueblos de un estado se requiere que una enorme cantidad de información esté a disposición del cartógrafo.
Para magnificar las dimensiones de una ciudad en la forma de un mapa de calles individuales se requiere aún más datos.
El cartografiado de cromosomas es el proceso de determinar la posición relativa de los genes u otros hitos reconocibles sobre los cromosomas, y requiere información obtenido de la genética tradicional y la biología molecular.

Si se parte de un punto de referencia tal como un edificio prominente, un buen mapa de la ciudad permitirá encontrar una calle particular.

El establecimiento de hitos reconocibles sobre los cromosomas puede conducir a la localización de genes especificas sobre ellos.
El Proyecto Genoma Humano intenta desarrollar mapas que finalmente conduzcan a los científicos a la localización de cada gen en todo cromosoma humano (como también la de los genes en varios genomas no humanos). 

¿Para qué servirían esos mapas?
¿Es realmente necesario dedicar años de esfuerzo al intento de determinar las localizaciones cromosómicas de los genes? 
¿Por qué no limitarse a aislar y estudiar los genes como unidades individuales y dejar la construcción del mapa para más adelante, si fuera posible?

Esto suena lógico, pero si continuamos con la analogía del mapa podremos ver por qué esa objeción, aunque en apariencia plausible, no es válida.
Recuérdese que el genoma humano es un cordón de ADN, de dos metros de largo en su estado no enroscado, que comprende de 50.000 a 100.000 genes.
Así como sería imposible localizar una casa particular en una cierta calle de una ciudad no conocida sin la asistencia de un mapa, así también necesitamos, primero, un mapa cromosómico, antes de poder determinar la localización aproximada de los genes individuales. 

Mostrando la extraordinaria utilidad de, incluso, un mapa incompleto de los cromosomas, los mapas preliminares del genoma humano ya construidos nos han permitido localizar los genes ligados a diversas enfermedades humanas.
Hasta agosto de 1990, han sido localizadas las posiciones relativas de casi 5.000 genes. 
Entre ellos están los genes responsables de dolencias tan devastadoras como la hemofilia, la fibrosis cística, la anemia de células falciformes y la enfermedad poliquística del riñón.

El cartografiado de estos genes ha conducido en algunos casos a pruebas que diagnostican una enfermedad y permiten un estudio detallado del gen causante, lo que podría conducir al tratamiento o la cura.

Se deduce que a medida que crezca la resolución de los mapas genéticos podremos localizar la mayoría de los miles de genes que juegan algún papel en la enfermedades humanas.

Debe reconocerse que el ímpetu principal del cartografiado de los genes radica en su potencial para mejorar la salud humana. 
Además, en el área de la ciencia básica, una comprensión detallada del genoma del hombre, como también del de otras especies, profundizará nuestra comprensión de las preguntas fundamentales acerca de la organización del genoma, el control de la función de los genes, el crecimiento y desarrollo celular y la biología evolutiva.
Por ejemplo, ¿por qué envejece nuestro cuerpo?
¿Por qué tenemos un intervalo de tiempo tan breve antes de que nuestras células y tejidos se desgasten?
Quizá los genes nos lo dirán. 

Finalmente, los mapas son un sine qua non para determinar la secuencia de bases nitrogenadas del genoma, el código que detalla las instrucciones de las actividades de la vida. 
Una vez que se obtenga el código en forma completa, según James Watson «estaremos interpretando hasta dentro de más de mil años».

Con ello quiere decir que aun cuando conozcamos la secuencia y localización de los genes, éste es sólo el primer paso.
Debe seguirle un proceso prolongado de determinación de las funciones e interrelaciones de estos genes en la vida del organismo humano.

Para explicar el cartografiado y su papel bajo la égida del Proyecto Genoma Humano, necesitaremos repasar brevemente la topografía y los movimientos de los cromosomas para poder seguir la lógica que hay detrás de los protocolos de cartografiado.

Se recordará que cada célula humana contiene 46 cromosomas, excepto las células espermáticas del hombre y los óvulos maduros de la mujer, cada uno de los cuales contiene 23.
Los 46 cromosomas son en realidad 23 pares.
Sólo uno de estos pares determina el sexo del ser humano.
Los cromosomas sexuales se denominan XX en las mujeres y XY en los hombres.

Un miembro de cada par de cromosomas se origina en el padre (el cromosoma paterno) del individuo y el otro (el cromosoma materno) en la madre.
Durante la vida de la célula los dos miembros de cada par no están físicamente unidos.
Se habla de ellos como de pares, no porque estén unidos o sean idénticos, sino porque son «homólogos». 
Esto significa que todo a lo largo de los dos cromosomas hay genes que rigen la misma característica en la misma posición (locus) de cada cromosoma.
Dicho simplemente, si el cromosoma materno tiene un gen para color de ojos en el locus B, por ejemplo, entonces el cromosoma paterno de ese par también tendrá un gen para el color de ojos en la misma posición.

Como hemos visto en nuestra exposición de la genética básica, estos pares de genes, también llamados alelos, no siempre son genes idénticos.
Ambos genes pueden ser dominantes, ambos pueden ser recesivos o uno puede ser dominante y el otro recesivo.
Se recordará que un gen recesivo funcionará sólo si el otro miembro del par también es recesivo, mientras que un gen dominante siempre lo hará.

Durante la división celular (mitosis), cuando una célula se divide en dos, cada cromosoma es copiado, resultando en un total de 92 cromosomas, la mitad de los cuales pasan a la nueva célula.
Cuando ciertas células son transformadas en células espermáticas o en óvulos, esta duplicación del número de cromosomas también ocurre, pero existen dos divisiones celulares. 
Por ende, a partir de una célula original con 46 cromosomas, se producen cuatro células (los gametos), cada uno con 23 cromosomas.
Esta reducción del número de cromosomas es críticamente importante como fuente de variabilidad en la herencia, porque cada gameto termina teniendo uno de cada uno de los 23 pares originales. 
Cuál, ya sea de origen materno o paterno, es un evento aleatorio.
Entonces, cuando el esperma y el óvulo se unen, una combinación completamente única de 46 cromosomas se forma, aun cuando sigue habiendo 23 pares.

Ahora, viene la clave de la forma tradicional de cartografiado de cromosomas.
Aunque pueda parecer extraño, en las etapas tempranas de la producción del esperma y los óvulos, los cromosomas emigran dentro del núcleo de modo que cada par de cromosomas se alinea uno al lado del otro y los dos miembros del par, literalmente, se enroscan entre si.
Nadie sabe qué fuerzas atraen mutuamente a estos socios.
Después de su estrecho pero breve abrazo, se separan.
Al hacerlo, con frecuencia, intercambien trozos en un proceso llamado «sobrecruzamiento».
Sorprendentemente, los cromosomas se rompen y vuelven a reunir con trozos del otro miembro del par.
Si un cromosoma fuese rojo y el otro blanco, después del sobrecruzamiento y el intercambio de trozos ambos cromosomas serian en parte rojos y en parte blancos.

Esto altera el orden original de los genes en cada cromosoma. 
Ya hemos señalado que los genes en el mismo sitio especifico en ambos miembros de un par cromosómico con frecuencia son diferentes.
Este intercambio de trozos entre los miembros de estos pares crea cromosomas con nuevos grupos de genes, agregando así otra fuente de variabilidad al descendiente.
Recuérdese que el niño hereda sólo un miembro de cada uno de los 23 pares originales de cromosomas.

El concepto de disposición lineal de los genes sobre los cromosomas data del tiempo de Mendel.
Sin embargo, la primera demostración experimental del sobrecruzamiento fue realizada por Alfred Sturtevant, en 1913.
Sturtevant produjo el primer mapa cromosómico, en el cual calculó las posiciones relativas de seis genes, incluyendo los de ciertos colores del cuerpo y tamaños de ala sobre el cromosoma X de la mosca de la fruta, Drosophila.
En este organismo, como en el hombre, la hembra tiene dos cromosomas X y los machos tienen un cromosoma X y uno Y. El tipo de cartografiado que Sturtevant inició, y que todavía se hace actualmente, aunque con técnicas mucho más sofisticadas, es el cartografiado por «ligamiento genético».
Este tipo de mapa muestra la disposición de los genes u otros segmentos reconocibles de ADN llamados «marcadores» a lo largo del cromosoma.
Este mapa se construye aprovechando el fenómeno de sobrecruzamiento.
Esto significa, simplemente, que si dos genes, que llamaremos A y B, están cerca uno del otro sobre el mismo cromosoma, por lo general se heredan juntos.
El único modo en que esto no ocurriera sería que ese cromosoma fuese a cruzarse con el otro miembro de su par, en este caso en algún punto entre A y B. Estos dos genes terminarían entonces en cromosomas separados. 

Es evidente que cuanto más separados están los genes sobre un cromosoma, mayor es la probabilidad de que ocurra un cruzamiento entre ellos.
Si se puede determinar la frecuencia de cruzamiento, se puede trazar un mapa que indicará las posiciones relativas de los genes sobre un cromosoma.
Usando ese mapa no se puede decir «el gen A está en este sitio exacto sobre el cromosoma».
Sólo se puede decir: «El gen A esta localizado lejos del gen B, pero cerca del gen C, etc.».

Como un solo miembro de cada par de cromosomas termina normalmente en una célula espermática o en un óvulo, entonces A y B no se heredarán juntos si entre ellos ocurrió un sobrecruzamiento.
Supóngase que haya un modo de detectar la presencia de A y B. Se podría buscar estos genes en los niños, padres o incluso abuelos y se podría rastrear el patrón de sobrecruzamientos en estas generaciones. 
Si se hace esto con un cierto número de genes, entonces se termina teniendo un mapa de ligamiento genético. 
Las distancias entre los genes en un mapa genético humano típico son grandes.
En promedio, son de aproximadamente diez millones de pares de bases nitrogenadas de longitud, lo que es más o menos el 5% del tamaño de un cromosoma.

Una limitación del cartografiado genético es la necesidad de tener que rastrear genes específicos u otros fragmentos de ADN en varias generaciones de familias humanas.
A pesar de tales dificultades, fue precisamente el desarrollo de las herramientas de cartografiado genético, posibilitadas por el advenimiento de la biología molecular, lo que condujo a la consideración de la factibilidad del Proyecto Genoma Humano.

En 1980, David Botstein del Instituto Tecnológico de California, señaló que se podría construir un mapa de ligamiento completo del genoma humano usando enzimas de restricción para cortar el cromosoma en patrones cuya herencia podría ser seguida en familias.

En 1987, cuando un mapa parcial ya había ayudado a estrechar la búsqueda de varias enfermedades importantes relacionadas con los genes, el comité del National Research Council (NRC) del National Institutes of Health pidió un esfuerzo inmediato para desarrollar un mapa de ligamiento genético como meta de un proyecto concertado sobre el genoma.

Varios miembros del comité comenzaron con serias reservas acerca de la utilidad de una iniciativa especial en la «genómica», un término acuñado en 1986 por Thomas Roderick, del Laboratorio Jackson en Bar Harbor, Maine, para referirse al cartografiado, secuenciamiento y otros procesos de análisis de genomas complejos.
El término sobrevivió en el titulo de la revista Genomics , una publicación periódica internacional acerca del cartografiado y secuenciamiento de genes, que fomenta los análisis de genomas humanos y de otros genomas complejos.
A pesar de sus dudas iniciales, el concepto de un esfuerzo a gran escala para crear un mapa y una secuencia completas del genoma humano, dentro de los siguientes quince años, también sobrevivió como una recomendación unánime.

El comité del NRC recomendó que los investigadores cartografiasen primero y secuencias en después, por dos razones.
Primero, se preveían rápidos adelantos en la cartografía debido a nuevas técnicas moleculares, mientras que el secuenciamiento requeriría de mejoras en la tecnología, anticipadas pero aún no disponibles. 
Segundo, tener un mapa es esencial para un secuenciamiento eficiente.

El comité también recomendó que debían estimularse los desarrollos tecnológicos desde el comienzo. 
Los métodos existentes de cartografiado y secuenciamiento no eran suficientes para alcanzar las metas anunciadas en el período propuesto de quince años.
También, sugirieron que se estudiasen organismos no humanos modelos, debido al valor que semejante información tendría para el análisis y comprensión del genoma humano.

Un informe subsiguiente, de la Oficina de Análisis Tecnológico, en 1988, repitió las recomendaciones generales acerca del cartografiado y secuenciamiento de los genomas del hombre y otros organismos.
En un esfuerzo por reducir las críticas de que la empresa era demasiado masiva y sin precedentes, afirmaban que «no hay un solo proyecto del genoma humano sino muchos proyectos».

Resultó que el intento por describir esta empresa como un complejo de series paralelas, pero difusas, de programas tuvo corta vida.
A medida que se sucedieron las reuniones organizativas entre los departamentos, agencias e individuos liderantes, se desarrolló un consenso que se expresa, sucintamente, en el título del plan quinquenal de las DOE/ NIH, dado a conocer en febrero de 1990.
Se lee: «La comprensión de nuestra herencia genética, el Proyecto Genoma Humano: Los primeros cinco años, Años fiscales 1991-1995».
Según el documento, «se cree que un proyecto coordinado centralmente y concentrado en objetivos específicos es el modo más eficiente y el menos costoso» de lograr los objetivos de cartografiado y secuenciamiento.

El Congreso recibió estas recomendaciones en febrero de 1990, en respuesta a su petición de un informe que describiese un plan de gastos y una estrategia óptima para cartografiar y secuenciar el genoma humano.
En términos del cartografiado, las recomendaciones se habían hecho más específicas.
Las metas, en un lenguaje que descifraremos en breve, se formularon así:

1. Realizar un mapa genético humano completamente conectado con marcadores espaciados en promedio de dos a cinco centimorgans.
Identificar cada marcador por un STS.

2. Reunir mapas STS de todos los cromosomas humanos con la meta de tener marcadores espaciados a intervalos de aproximadamente cien mil pares de bases [nitrogenadas].

3. Generar conjuntos superpuestos de ADN clonado o marcadores cercanamente espaciados y ordenados no ambiguamente con continuidad, sobre longitudes de dos millones de pares de bases para partes grandes del genoma humano.


¿Cuáles son los significados de estos nuevos términos, centimorgans y STS?
¿Cómo pueden satisfacerse estas ambiciosas metas?
Necesitamos observar un poco más de cerca el cartografiado para responder estas preguntas.

La primera meta se refiere a la construcción de un mapa genético de ligamiento detallado.
Como hemos explicado, un mapa semejante usa la frecuencia de sobrecruzamiento como medida de la distancia entre dos genes u otros segmentos detectables de ADN.
Una tasa de sobrecruzamiento del 1% entre dos puntos de un cromosoma ha sido arbitrariamente denominada una «unidad de mapa».
Así, dos genes sobre el mismo cromosoma se dice que están a diez unidades de mapa de distancia si los experimentos muestran una frecuencia de sobrecruzamiento del 10% entre ellas.
Esta unidad de mapa se expresa como un centimorgan, o cM, en honor al gran genetista Thomas Hunt Morgan.

Este método de cartografía tiende a producir una distancia de mapa imprecisa entre los genes que están muy separados, debido a los múltiples sobrecruzamientos que con frecuencia ocurrirán entre ellos.
Los mapas de ligamiento genético más precisos son aquellos para los cuales las observaciones se han hecho sobre los valores de sobrecruzamiento entre tantos puntos diferentes sobre el cromosoma como sea posible.

Realizado con los datos de sobrecruzamientos en laboratorio hechos entre más de 1.000 generaciones de Drosophila, el mapa de ligamiento genético de la mosca de la fruta es uno de los más completos para un organismo, con la excepción de la bacteria E. Coli .
Tenemos mapas excelentes, también, de los ratones de laboratorio, el maíz y la Neurospora. 
Obviamente, los sobrecruzamientos genéticos planificados no son posibles en humanos y el número de descendientes de cualquier apareamiento es relativamente pequeño.
En comparación con el intervalo de vida de la Drosophila, el período de tiempo requerido para que haya otra generación en los humanos sólo permite, en el mejor de los casos, el estudio de tres generaciones, los niños, sus padres y sus abuelos.
Como consecuencia de ello, hasta hace poco muy pocos genes humanos hablan sido colocados en un mapa de ligamiento genético y la mayoría estaban en el cromosoma X. 

Es fácil ver por qué el cromosoma X se presta al cartografiado.
El sobrecruzamiento entre los cromosomas X puede ocurrir sólo en la hembra, ya que ella es XX. 
Los cromosomas X e Y del macho son sólo parcialmente análogos.
Son el único par de cromosomas humanos que difiere mucho en su tamaño, ya que el Y es mucho más pequeño.
Como consecuencia de ello, hay machos genes en el cromosoma X, más grande, que no tienen un gen correspondiente en el Y. Por ende, aun si un gen en el cromosoma X fuera recesivo, podría estar activo, debido a que no habría posibilidad de que un gen dominante en el cromosoma Y reprimiese su actividad.
La hemofilia y ciertas formas de ceguera al color, que se dan casi exclusivamente en los hombres, son ejemplos clásicos de genes que están, obviamente, en el cromosoma X debido a su patrón de herencia.
Tales trastornos son posibles en las mujeres sólo si ambos cromosomas X tienen los mismos genes recesivos, una coincidencia muy rara.

Ya hemos señalado que cuanto más cercanos entre sí están los genes sobre un mismo cromosoma, tanto menos frecuente será su sobrecruzamiento.
Por ende, en lugar de intentar seguir la herencia de solamente genes enteros, se puede estimar la distancia entre un gen de interés particular y cualquier otra secuencia de ADN que sea identificable (un «marcador») en el mismo cromosoma. 
Este procedimiento es muy importante, porque el problema principal del ligamiento genético está en la detección de la presencia de los genes en las células de una persona. 
Esta identificación se hace más fácil si se rastrean los genes que causan una enfermedad genética detectable.
Estos genes pueden seguirse durante varias generaciones si los síntomas de la enfermedad son obvios. 

Examinemos un ejemplo práctico del uso de un marcador. 
Si las personas que desarrollan una enfermedad genética casi siempre heredan la misma versión de un cierto marcador, el gen de la enfermedad y el marcador deben estar muy cerca entre sí en el mismo cromosoma.
Para poder correlacionar el marcador y el gen de la enfermedad se deben cumplir dos condiciones. 
Primero, el marcador debe ser detectable y, segundo, debe hallarse en varias formas distinguibles.
Si las dos formas del marcador en el par cromosómico fueran idénticas, entonces los cruzamientos entre el gen de la enfermedad y el marcador no serían rastreables en la descendencia.
Para usar una hipótesis fantasiosa, supóngase que el gen de la enfermedad estuviese cerca de un marcador púrpura. 
Si el sitio correspondiente en el otro miembro de ese par cromosómico fuese también púrpura, entonces un intercambio por sobrecruzamiento entre los dos puntos resultaría en que ambos cromosomas tuviesen un punto púrpura.
Si el punto correspondiente no fuese púrpura sino amarillo, el sobrecruzamiento sería fácilmente detectable. 

Hasta hace poco, solamente algunos marcadores satisfacían estos criterios.
La primera asignación de un gen a un cromosoma no sexual específico ocurrió en 1968, cuando los científicos mostraron que la herencia de cierto grupo de proteínas de la sangre estaba ligado al cromosoma número 1.
Sólo unas pocas docenas de marcadores eran conocidos entonces, y por la carencia de tales marcadores la mayor parte del genoma humano permanecía inaccesible.

Debe resaltarse aquí que la decisión de cartografiar el genoma humano surgió en realidad mucho antes de la iniciativa actual de coordinación a gran escala.
El científico que tuvo la mayor influencia en esos tempranos años fue Víctor McCusick, de la Universidad Johns Hopkins.
En 1968, comenzó a trabajar en su libro La herencia mendeliana del hombre.
En 1971, había 15 genes que habían sido cartografiados en los cromosomas humanos.
La novena y última edición de 1990, todavía compilada y editada por McCusick, lista 4.937 genes cartografiados.

Figura 1

Los genes-representados aquí por letras-vienen en pares llamados alelos

Estos alelos están en la misma posición (locus) sobre un par de cromosomas homólogos.

Figura 2

Cada célula comienza con 46 cromosomas

Los cromosomas se replican dando lagar a 92 por célula.
En la mitosis, los cromosomas se dividen igualmente en dos células idénticas.
En la meiosis, ocurren dos divisiones celulares, dando lugar a 4 gametos, cada uno con 23 cromosomas.
Cada uno de estos cromosomas es un miembro de un par homólogo original. 

A finales de los sesenta, aun antes de los descubrimientos de la biología molecular de la década siguiente, se desarrolló un método que expandía la posibilidad de correlacionar genes específicos con cromosomas particulares.
La técnica usa células humanas y de roedores en cultivos.

También incluye el uso de un virus con una propiedad muy útil.
La mayoría de los virus tienen un punto específico de asociación con la célula. 
Un virus «Sendai» tiene varios puntos de asociación, de modo que puede simultáneamente asociarse a dos células diferentes, si están lo suficientemente cerca.
Un virus es tan pequeño, en comparación con las células, que esta asociación con dos células adyacentes muy cercanas causa, con frecuencia, que las membranas de las dos se fusionen y ambas se mezclen en una sola célula. 
Si suspensiones de células tumorales de ratón y de tejido conectivo humano se mezclan en la presencia del virus Sendai, el virus puede causar la formación de células híbridas de hombre y ratón.
La vieja pregunta «¿Eres un hombre o un ratón?
» puede ahora responderse por «Ambos».
Cuando se los mira en un microscopio, los cromosomas humanos y los del ratón pueden distinguirse con facilidad.

Afortunadamente para la técnica experimental, los cromosomas humanos son eliminados gradualmente, a medida que las células híbridas se dividen.
Una manipulación cuidadosa de esta característica puede dar lugar a una población de células que contenga un solo cromosoma o incluso una parte de un cromosoma humano.
Eventualmente, pueden desarrollarse líneas germinales de células que contengan, en total, todos los cromosomas humanos.
Estudiando tales células, entonces, puede buscarse ADN humano que no se encuentre ordinariamente en las células de los ratones.
La presencia de tal ADN en una célula híbrida significa que se ha descubierto parte del genoma. 
De los aproximadamente 5.000 genes cartografiados hasta ahora, casi 1.000 han sido localizados por este proceso de fabricación de células híbridas.

El virus Sendai había dado nueva vida a la cartografía del genoma pero, a pesar de este enfoque ingenioso, su uso todavía estaba limitado a la asignación de algunos genes a ciertos cromosomas, algo análogo a determinar que una ciudad concreta está en un país específico.

La revolución del ADN recombinante de la década de los setenta proveyó las herramientas para producir avances sin precedentes en la cartografía de ligamientos genéticos.
Reconociendo el potencial del nuevo enfoque molecular, en 1978, Raymond White, de la Universidad de Utah, dedicó su laboratorio a desarrollar un conjunto de marcadores basado en la detección de secuencias únicas de ADN.

Él y otros pronto desarrollaron una estrategia que condujo a la posibilidad de cartografiar los genes humanos a la escala prevista por el Proyecto Genoma Humano.
Se basa en la variación normal entre secuencias de bases nitrogenadas en pares de cromosomas.
Una diferencia en estas secuencias entre los dos cromosomas ocurre, en promedio,cada 200 a 500 pares de bases.

Se recordará que los enzimas de restricción cortan los nucleótidos en secuencias de bases nitrogenadas específicas.
Eso significa que una variación en la secuencia del ADN que cree o elimine un punto donde un enzima puede cortar, alterará la longitud de algunos de los fragmentos de ADN, cuando ese ADN sea tratado con el enzima. 
Esta variación en los puntos de corte del enzima de restricción entre las mismas áreas de los pares de cromosomas crea un «polimorfismo de longitud de los fragmentos por restricción» (RFLP) (pronunciado «riflips» por los expertos).
Este fenómeno ha llevado la cartografía de ligamiento genético a una nueva era.
Los RFLP proporcionan una provisión enormemente rica de marcadores detectables.

Si consideramos el genoma humano completo, sin embargo, un enzima de restricción típico tiene millones de puntos de corte en los que trabajar.
¿Cómo puede uno 0 unos pocos fragmentos de diferentes tamaños ser localizados entre millones que son iguales?
La respuesta yace, una vez más, en el uso de la gel electroforesis. 
Los fragmentos cortados del genoma son extendidos en un gel según su tamaño.
Entonces, una sonda de ADN puede ser aplicada para seleccionar una mancha específica en el surtido de fragmentos.

Si las bandas radiactivas aparecen en diferentes lugares sobre los patrones de gel del ADN de diferentes individuos, la sonda de ADN ha detectado una variación en el patrón de corte del enzima entre el ADN de esos dos individuos.
Muchos cientos de sondas deben ser examinadas antes de encontrar una útil.
Cuando se halla una, se tiene un sistema poderoso de marcadores, la sonda y el RFLP que detecta.
Puede entonces rastrearse el patrón de herencia del RFLP, en lugar de apoyarse en el rastreo de una enfermedad.
Los RFLP están dispersos abundantemente a través de todo el genoma humano y han multiplicado, en muchas veces, los marcadores disponibles para cartografiar nuestros cromosomas. 

Los RFLP han sido una herramienta útil en la provisión de un diagnóstico definitivo de diversas enfermedades, incluyendo la anemia de células falciformes y la enfermedad de Huntington.
Además del diagnóstico, el ligamiento entre un marcador, como un RFLP, y un gen de una enfermedad es el primer paso crucial en el estrechamiento de la búsqueda de ese gen y un medio para aislar grandes cantidades de un segmento cromosómico específico con el propósito de aislar y caracterizar al propio gen.

El conjunto individual de marcadores RFLP de una persona puede ser usado para producir una «huella digital genética». 
Estas «huellas» han resultado ser muy útiles en la medicina forense, ya que la probabilidad de que dos personas tengan el mismo conjunto de marcadores RFLP es infinitesimalmente pequeña.
Los tribunales, actualmente, aceptan la evidencia basada en la identificación de individuos a través de un tal análisis del ADN.

Los científicos estiman que serán necesarios 3.000 marcadores bien espaciados para lograr un mapa completamente ligado, con los marcadores a una distancia promedio de un centimorgan. 
Cada centimorgan cubre unos 30 a 40 genes.
Para los primeros cinco años del Proyecto Genoma Humano se fijó como meta el desarrollo de un mapa en el cual los marcadores estén separados por un mínimo de dos a cinco centimorgans, lo que requeriría de 600 a 1.500 marcadores.
Tener marcadores a intervalos iguales sobre los cromosomas, sin embargo, requiere un conjunto mucho mayor de marcadores aleatorios sobre el mapa.
También, debe recolectarse el ADN de cientos de individuos en docenas de familias grandes y deben determinarse las características de cada marcador.

Una innovación reciente ha hecho que el uso del RFLP sea aún más efectivo.
En muchos puntos del cromosoma humano, el ADN tiene secuencias simples de bases que no codifican proteínas y se repiten muchas veces. 
El origen y significación de estas «repeticiones en tándem» es un misterio, pero han resultado ser de mucho uso en el cartografiado de ligamiento.
El número de repeticiones en un locus dado puede variar de unos pocos a unos cientos de copias.
Si un enzima de restricción corta cerca de estas repeticiones en tándem de número variable, los fragmentos resultantes variarán en su longitud de la misma forma.
La variación en el número de repeticiones en tándem (VNTR) es tan común que las probabilidades de que un individuo tenga diferentes versiones de ellas en cromosomas homólogos es grande.

Incluso, se han desarrollado métodos más nuevos de cartografía.
En 1988, David Cox y Richard Myers, de la Universidad de California, en San Francisco, anunciaron que habían diseñado un enfoque totalmente nuevo de la cartografía de ligamiento genético que denominaron «cartografía híbrida por radiación». 
En lugar de buscar cuán frecuentemente dos marcadores quedan separados debido al sobrecruzamiento, su método busca cuán frecuentemente los marcadores quedan divididos si los cromosomas son bombardeados con rayos X.
Los ensayos iniciales mostraron que este enfoque puede conducir a una resolución veinte veces mayor que el cartografiado de ligamiento de la actualidad. 

Figura 3

El patrón de los trozos de ADN cortados por el mismo enzima de restricción varía con la fuente del ADN

La variación (RFLP) se ve en las velocidades de traslación a lo largo de un gel de electroforesis, debido al tamaño diferente de los fragmentos.

Aunque la cartografía de ligamiento genético provee una herramienta útil para estrechar la búsqueda de genes, no basta para poder aislar concretamente un gen y aislarlo para su estudio.
Este tipo de especificidad debe provenir de un segundo enfoque, la «cartografía física». 
El mapa físico provee la distancia física real entre dos hitos sobre el cromosoma.
Para analizar el mapa físico, necesitamos explicar el concepto de STS, cuyo uso se aplica tanto a la cartografía física como a la genética.

La cartografía física depende de la tarea compleja y difícil de intentar determinar el orden exacto en el cual los trozos de ADN que están siendo analizados habían existido originalmente en el cromosoma.
La distancia entre puntos en un cromosoma se mide en términos de longitud física, tal como números de pares de nucleótidos.
El mapa físico último seria el orden exacto de los 3.000 mil millones de bases nitrogenadas de los nucleótidos que constituyen el genoma humano.

Los mapas físicos pueden realizarse de diversos modos, algunos de los cuales datan de los años iniciales del análisis genético humano mientras que otros aprovechan los desarrollos más recientes de la biología molecular.
Hay dos categorías principales de mapas físicos.
El primero es el de los mapas «citogenéticos», que exhiben un patrón de hitos que son realmente visibles en los cromosomas.
Aquí, la herencia de los genes está ligada a la herencia de las áreas de los cromosomas que pueden verse a través del microscopio.

Se han desarrollado diversas técnicas de teñido que destacan ciertas regiones o «bandas» en los cromosomas. 
Cada uno de los 46 cromosomas resulta tener un patrón intrincado y único de bandas.
No se conoce la base molecular de estas bandas, que se dan también en otros organismos.
Se supone que se deben al plegamiento producido por la interacción del ADN y las proteínas.
Una sola banda representa unos 10 millones de pares de bases. 
Casi 1.000 bandas han sido detectadas en los cromosomas humanos mediante teñido y microscopía lumínica. 
Una sola banda representa un promedio de 100 genes.

Se ha desarrollado un mapa estandarizado de bandas en los cromosomas humanos, que ha resultado útil para correlacionar cambios visibles. aunque con frecuencia sutiles, en la estructura cromosómica de ciertas enfermedades genéticas. 
Por ejemplo, algunos cánceres están asociados a la producción de secuencias de ADN anormales que aparecen como cambios en los patrones de banda.
También, la detección de una «elisión», la pérdida de una pequeña parte de un cromosoma, se ve facilitada mediante la búsqueda de pérdidas en las bandas. 


Genética del cáncer

Los datos disponibles en la actualidad indican que aproximadamente el 30-40% de la población humana morirá de cáncer. 
Se está registrando un aumento de la frecuencia de enfermedades neoplásicas, en gran parte debido a la proporción creciente en nuestra población de los grupos de edad más avanzada.
Como se describe en este capítulo, la etiología del cáncer es una mezcla de componentes genéticos y ambientales.
En la actualidad, gracias a los espectaculares avances en genética y biología molecular se han descubierto los elementos moleculares básicos del cáncer y se ha obtenido un esbozo esquemático de los sucesos celulares que originan el cáncer.
Estos logros tendrán importancia crucial para el control del cáncer, proporcionando los cimientos necesarios para la obtención de terapias significativamente mejores y quizá, de medidas preventivas.

El cáncer es un conjunto de trastornos que comparten la característica común de un crecimiento celular incontrolado que origina una masa de células denominada neoplasia (del griego "nueva formación") o tumor .
Las neoplasias malignas pueden invadir tejidos vecinos y, por último, metastatizar (diseminarse) a lugares distantes en el organismo.
Las neoplasias malignas y las benignas se diferencian por su capacidad de invasión y metástasis.
Los tumores pueden clasificarse según el tipo hístico en que se originan.
Los principales tipos de tumores incluyen los de tejido epitelial ( carcinomas ), tejido conjuntivo ( sarcomas ), tejido linfático ( linfomas ), células de la glía del sistema nervioso central ( gliomas ), y órganos hematopoyéticos (leucemias).
Las células que componen un tumor suelen derivarse de una única célula progenitora, constituyendo un clon único ( monoclonal ).

Las características biológicas básicas de la carcinogénesis (desarrollo del cáncer) son evidentes.
En el transcurso de nuestras vidas, la mayoría de nuestras células continúan creciendo y diferenciándose.
De este modo se forman, por ejemplo, las capas epiteliales de nuestros pulmones y del colon, y los precursores de las células de nuestros sistemas inmunológicos.
Las células primordiales relativamente indiferenciadas producen un gran número de células hijas que repueblan y renuevan nuestras capas defensivas gastadas.
Mediante la integración de la información aportada por una compleja gama de señales bioquímicas' las nuevas células son diseñadas para encargarse por sí mismas de interrumpir su crecimiento y diferenciarse en el tipo celular apropiada para su herencia y circunstancias (fig. 9-1).

En ocasiones, algunas de estas células pierden su capacidad de respuesta adecuada a las señales ambientales e internas.
No se diferencian y continúan dividiéndose sin limitaciones.
Las descendientes de estas células se convierten en las fundadoras de las neoplasias, capaces de una evolución posterior en un cáncer invasivo y metastásico.
Nuestro objetivo es conocer con detalle los errores producidos en estas peligrosas células, para detectarlos lo antes posible y, finalmente, interferir en su desarrollo, eliminándolos de sus sistemas huéspedes. 

Todas las células del organismo están "programadas" para desarrollarse, crecer, diferenciarse y morir en respuesta a un sistema complejo de señales bioquímicas. 
Los cánceres se originan cuando alguna célula se libera de estas restricciones y se produce la proliferación de su descendencia anormal.

Figura 1

En respuesta a señales ambientales, una célula puede continuar dividiéndose, continuar diferenciándose o morir (apoptosis)

CAUSAS DE CÁNCER

Consideraciones genéticas

Los sucesos genéticos constituyen la base principal de la carcinogénesis.
Es posible inducir cáncer en modelos animales lesionando unos genes específicos. 
En los sistemas de cultivo celular, se puede invertir un fenotipo cancerígeno introduciendo en la célula copias normales de genes.
La mayoría de sucesos genéticos que causan cáncer se producen, durante la vida del individuo, en sus tejidos somáticos.
La frecuencia de estos sucesos puede resultar alterada por la exposición a mutágenos, lo que establece un vínculo con los carcinógenos ambientales.
Estos sucesos genéticos ocurren en las células somáticas y, por lo tanto, no se transmiten a las generaciones futuras. 
Aunque son sucesos genéticos , no se heredan .
Sin embargo, es posible que en las células germinales se originen mutaciones predisponentes al cáncer.
De este modo se produce la transmisión de genes causales de cáncer de una generación a la siguiente, creando familias que poseen una elevada frecuencia de cánceres específicos (fig. 9-2).

Estas "familias con cáncer", aunque muy poco frecuentes, demuestran que la herencia de un gen alterado puede causar cáncer. 
La herencia en estas familias de un alelo mutante parece suficiente para causar una forma específica de cáncer: 
casi todos los individuos que heredan el alelo mutante desarrollarán el tumor.
El retinoblastoma, un cáncer ocular de la infancia, es un buen ejemplo.
Como se estudió en el capítulo 4, quienes heredan una versión mutante del gen del retinoblastoma cuentan con una probabilidad próxima al 90% de desarrollar uno o más retinoblastomas.

Aunque la transmisión del cáncer como trastorno monogénico es relativamente infrecuente, existen pruebas de la existencia de un patrón más general de acumulación de ciertos tipos de cáncer en las familias.
En algunos tipos de cáncer, como los de mama y colon, el diagnóstico de cáncer en un pariente en primer grado de un individuo determinado tiene como resultado que su riesgo de desarrollo de cáncer sea varias veces superior. 
No conocemos la base genética de esta aparente relación familiar del cáncer.
Sin embargo, es posible que la responsabilidad recaiga sobre la transmisión genética de formas alteradas de genes específicos. 

El grado en que estos mecanismos -mutaciones de la línea germinal (heredadas) frente a mutaciones en células somáticas- contribuyen al cáncer en seres humanos es una cuestión fundamental.
Si las predisposiciones hereditarias son determinantes significativos del riesgo de un individuo de adquirir una forma específica de cáncer, finalmente sería posible identificar los individuos con riesgo.
Un análisis más exhaustivo en poblaciones definidas con riesgo elevado permitiría la detección y la intervención precoces, logrando un mejor pronóstico para los individuos y una reducción de la morbididad y mortalidad en la población. 

La causa básica del cáncer es una alteración de ciertos genes específicos.
Por lo general, las mutaciones en estos genes se acumulan en las células somáticas durante años, hasta que una célula pierde un número critico de mecanismos de control del crecimiento e inicia la formación de un tumor.
Sin embargo, si la lesión se produce en células de la línea germinal se puede transmitir una forma alterada de uno de estos genes a la descendencia y predisponerla al cáncer.

Figura 2

Árbol genealógico de un cáncer de colon familiar.

Los símbolos sombreados representan los individuos diagnosticados de cáncer de colon.

Consideraciones ambientales

¿Cuál es la función del entorno en la carcinogénesis?
A nivel celular, el cáncer es intrínsicamente genético.

Las células tumorales se originan cuando ocurren ciertos cambios, como mutaciones, en los genes responsables de la regulación del crecimiento de la célula.
Sin embargo, gran número de factores ambientales pueden modificar la frecuencia y las consecuencias de estas mutaciones. 
Por ejemplo, se ha comprobado que muchas sustancias químicas que provocan mutación en animales de experimentación también provocan cáncer y, por lo tanto, son carcinógenos .
Así pues, ni la genética ni el entorno son determinantes únicos de la carcinogénesis; 
ambos desempeñan funciones en este proceso.

Dos líneas adicionales de argumentación corroboran la idea de que la exposición a agentes ambientales puede alterar de forma significativa el riesgo de cáncer de un individuo.
La primera es que se ha identificado cierto número de agentes ambientales con propiedades carcinogénicas. 
Por ejemplo, se ha comprobado, tanto en datos epidemiológicos como en experimentos de laboratorio, que el humo de los cigarrillos provoca cáncer de pulmón.
También se han comprobado las funciones de otros agentes ambientales en cánceres específicos (p. ej., el polvo de uranio en el cáncer de pulmón entre los mineros y la exposición a asbesto en el cáncer de pulmón y el mesotelioma). 

La segunda línea de argumento se basa en comparaciones epidemiológicas entre poblaciones con estilos de vida diferentes.
Muchos tipos de cáncer presentan frecuencias distintas en poblaciones diferentes.
Por ejemplo, el cáncer de mama es prevalente en Europa septentrional y América, pero relativamente raro entre las mujeres de países en vías de desarrollo.
Sin embargo, suele ser difícil averiguar si estas diferencias reflejan los distintos estilos de vida o las distintas frecuencias génicas.

El examen de poblaciones genéticamente similares bajo estilos de vida diferentes proporciona la oportunidad de valorar los componentes genético y ambiental del cáncer. 
Por ejemplo, los estudios epidemiológicos entre poblaciones emigrantes de Japón han obtenido importantes hallazgos respecto al cáncer de colon.
Este tipo de cáncer es relativamente raro entre la población japonesa que vive en Japón, pero es el segundo tipo de cáncer más frecuente en Estados Unidos.
A la inversa, el cáncer gástrico es frecuente en Japón, pero relativamente raro en Estados Unidos.
Estas estadísticas no pueden diferenciar, por sí solas, las influencias ambientales y genéticas en las dos poblaciones.
Sin embargo, debido al gran número de japoneses que han emigrado, primero a Hawaii y después al territorio continental de Estados Unidos, podemos observar lo que sucede en las tasas de cáncer gástrico y de colon entre los emigrantes.
Debe resaltarse que los emigrantes japoneses han conservado una identidad genética, efectuando la mayoría de matrimonios en el seno de su propia comunidad.

En la población global de Estados Unidos, el riesgo de cáncer de colon durante la vida se aproxima al 5%; 
en Japón, el riesgo es 10 veces menor, tan sólo del 0,5%.
Entre los japoneses de primera generación en Hawaii, la frecuencia de cáncer de colon ha aumentado varias veces, sin alcanzar todavía la frecuencia del territorio continental de Estados Unidos, pero es superior a la registrada en Japón.
Entre los japoneses de segunda generación en el territorio continental de Estados Unidos se registran tasas de cáncer de colon del 5 %, igual a la media de Estados Unidos.
Al mismo tiempo, el cáncer gástrico se ha convertido en una enfermedad relativamente rara entre los japoneses-americanos.

Estas observaciones señalan con fuerza la existencia de una función importante del entorno o del estilo de vida en la etiología del cáncer de colon.

¿Podemos entonces asumir que los factores genéticos no desempeñan ninguna función en el cáncer de colon?
Existe el hecho de que, en el entorno norteamericano, algunos individuos padecerán cáncer de colon y otros no.
Esta diferencia puede deberse a las diferencias en este entorno (p. ej., variación dietética), así como a las diferencias en la predisposición genética:
genes heredados que aumentan la probabilidad de que un individuo presente el cáncer.
Sin embargo, para explicar la diferencia en la frecuencia de cáncer de colon entre los japoneses que viven en Estados Unidos y en Japón, se argumenta que por las características ambientales en Japón los genes de predisposición son menos penetrantes.
Además, se ha sugerido con fuerza un componente genético, que justifica que el riesgo de un individuo aumente varias veces cuando uno de sus parientes en primer grado sufre cáncer de colon.
Es probable, pues, que el riesgo de cáncer sea una combinación de genética y entorno, con interacción entre ambos componentes.

Se sabe que los factores ambientales desempeñan funciones importantes en la carcinogénesis.
Ya que, en un mismo entorno, algunos individuas desarrollan cáncer y otros no, es evidente que el riesgo de cáncer depende de la interacción entre factores hereditarios y componentes ambientales.

GENES HEREDITARIOS EN EL CÁNCER 

Control genético del crecimiento y diferenciación celular

Los cánceres se forman cuando se pierde la regulación del crecimiento y la diferenciación de las células. 
Durante las dos últimas décadas se han identificado más de 100 genes causantes de cáncer, que codifican sustancias que normalmente regulan el proceso celular de crecimiento y proliferación.
La caracterización de las actividades y las interacciones de estos productos génicos ha revelado un cuadro cada vez más detallado de la regulación normal del crecimiento y la diferenciación celular y de los mecanismos, inducidos por episodios carcinogénicos, de desregulación de estos procesos.

En la actualidad se conocen las características fundamentales de este proceso (fig. 9-3).
En primer lugar, gran parte de la regulación celular está mediada por señales exteriores que llegan a la célula mediante factores de crecimiento polipeptídicos (p. ej., factor de crecimiento derivado de plaquetas, factor de crecimiento epidérmico, hormonas esteroideas, y otros) producidos en otras células.
Cada factor de crecimiento interactúa con un receptor de factor de crecimiento específico localizado en la superficie celular.
La unión con su factor de crecimiento activa el receptor, desencadenando cambios en algunas moléculas que emiten mensajes al núcleo celular en un proceso denominado transducción de señales.
Estas moléculas de transducción de señales incluyen proteinquinasas , enzimas que pueden alterar la actividad de sus proteínas dianas al fosforilarlas.
La etapa final de la vía de transducción de señales es la regulación de la transcripción del DNA en el núcleo.
Los componentes de la cascada transducción de señales interactúan con las proteínas de unión al DNA nuclear ( factores de transcripción ), que regulan la actividad de los genes específicos que influyen sobre el crecimiento y la proliferación celular.
Los genes que codifican estas proteínas de unión al DNA nuclear incluyen myc, fosy jun.

1 factores de crecimiento polipeptídico que transmiten las señales de una célula a otra;
2 receptores de factores de crecimiento en la superficie celular;
3 moléculas de traducción de señales que activan una cascada de reacciones de fosforilación en la célula,
4 factores de transcripción nuclear.


Tras varias rondas de división celular, las células reciben, normalmente, unas señales extracelulares que les indican que cesen su crecimiento y diferenciación en células especializadas.
Las señales pueden provenir de ciertos polipéptidos, hormonas esteroideas, o del contacto directo con las células adyacentes.
Las señales se transducen al núcleo de la célula receptora donde, alterando los patrones de transmisión entre los genes que controlan las etapas del ciclo celular, inician su progresión por una vía de diferenciación. 

Por lo tanto, la célula es capaz de integrar e interpretar el gran número de señales que recibe de su entorno. 
El procesamiento de estas señales determina que las células crezcan y se dividan, o bien que cesen su crecimiento y diferenciación.
El patrón de sucesos que ocurren en una célula específica es determinado por su historia y localización, que dicta la producción instantánea de productos genéticos de la célula.

Una célula cancerosa puede surgir de una población de células en crecimiento, a causa de la acumulación de varias mutaciones independientes en los genes que codifican los factores que se han mencionado.
Estos sucesos tan sólo ocurren en muy raras ocasiones, pero las células afectadas no responden a las señales de diferenciación y continúan creciendo.
Además, parece que los cánceres suelen ser el resultado de una serie progresiva de episodios que aumentan el grado de desregulación en una línea celular:
por último, surge una célula cuyas descendientes se multiplican sin restricciones apropiadas.
Unos cambios adicionales proporcionan a estas células la capacidad para invadir los tejidos adyacentes y de formar metástasis.
El requerimiento de más de una mutación ha sido englobado bajo el término " concepto de impactos múltiples en la carcinogénesis ".
Un ejemplo de este concepto lo proporciona el cáncer colorrectal, donde se ha observado que es necesario un número de episodios genéticos para completar la progresión desde un crecimiento benigno hasta una neoplasia maligna (fig. 9-4).

Las mutaciones pueden ocurrir en cualquiera de las etapas que intervienen en la regulación del crecimiento y la diferenciación celular.

Cuando estas mutaciones se acumulan en una línea celular, la desregulación progresiva del crecimiento produce finalmente una célula cuya descendencia formará un tumor.

Figura 3

Principales características de la regulación celular: los factores de crecimiento externos (proteínas y hormonas esteroideas, como el factor de crecimiento epidérmico) se unen a la membrana que contiene los receptores de factor de crecimiento en la superficie celular. 

Éstos activan las vías de transducción de señales, en que participan genes como ras .
A su vez, los componentes de la vía de transducción de señales interactúan con los factores de transcripción nuclear, como myc y fos, que pueden unirse a regiones reguladoras en el DNA.

Figura 4

Vía del cáncer de colon.

La pérdida del gen APC (poliposis coli adenomatosa) transforma el revestimiento de tejido epitelial de los intestinos en tejido hiperproliferante.
La hipometilación del DNA, la activación del protooncogén k-ras y la pérdida del gen DCC (delecionado en el cáncer de colon), intervienen en la progresión hasta adenoma benigno.
La pérdida del gen p53 y otras alteraciones intervienen en la progresión hasta carcinoma maligno y metástasis.

El gen de cáncer hereditario comparado con el gen alterado somáticamente

Si bien hace mucho tiempo que se describieron casos de cáncer familiar, hasta principios de la década de los setenta no se comenzó a conocer la relación entre las aberraciones genéticas hereditarias y los episodios carcinogénicos que ocurren en el tejido somático. 
En 1971 el análisis de A.G. Knudson acerca de los retinoblastomas, una enfermedad ya mencionada como modelo de cáncer hereditario, le condujo a exponer una hipótesis que abrió una nueva ventana en el mecanismo de la carcinogénesis.
En la variedad genética del retinoblastoma, es probable que un progenitor esté afectado y, en ese caso, existe una probabilidad del 50% de transmisión genética a cada uno de sus descendientes.
En la forma "esporádica", ninguno de los progenitores está afectado y no existe ningún riesgo adicional para otro descendiente.
Una característica diferenciadora fundamental entre ambas formas es que el retinoblastoma hereditario suele ser bilateral (afecta a ambos ojos), mientras que en el retinoblastoma esporádico sólo suele existir un único tumor y, por lo tanto, únicamente se afecta un ojo.

Knudson razonó que pueden ser necesarias al menos dos mutaciones para que se origine un retinoblastoma. 
Una de las mutaciones alteraría el gen del retinoblastoma; 
si esto sucede en la línea germinal, estaría presente en todas las células de un niño que recibiese el alelo mutante.
La segunda mutación sería un suceso genético inespecífico adicional en una célula ya alterada.
La hipótesis de un segundo episodio era necesaria para explicar por qué sólo una pequeña fracción de los retinoblastos de un individuo que hubiese heredado el gen del retinoblastoma mutante originan en realidad tumores.
La hipótesis de Knudson se conoce como el modelo de carcinogénesis de dos impactos .

Por lo tanto, el retinoblastoma hereditario estaría causado por la herencia de uno de los "impactos" genéticos como una mutación constitucional (es decir, una mutación presente en todas las células del organismo).
Los individuos que heredan un impacto requieren únicamente un episodio mutacional adicional en un único retinoblasto para que ésta siembre un clon tumoral.
En los casos esporádicos (no hereditarios), ambas mutaciones deberían ocurrir de forma independiente en el mismo retinoblasto, una combinación muy improbable de episodios incluso en un millón de células del tejido diana.
Por esta razón, es improbable que el niño que desarrolla un retinoblastoma mediante esta vía somática de dos impactos presente más de un tumor. 
Sin embargo, el individuo que hereda un gen mutante de retinoblastoma sólo necesita un único impacto genético adicional en un retinoblasto para que se desarrolle un clon tumoral.
Knudson argumentó que un episodio de este tipo era probable que ocurriese en varias células del total de un millón de retinoblastos, explicando así la bilateralidad del retinoblastoma hereditario. 

Un importante corolario de esta hipótesis es que los genes heredados en la forma mutante en los síndromes tumorales familiares son los mismos genes que los del cáncer generado por una mutación somática.
Conociendo la naturaleza de los alelos mutantes hereditarios en los cánceres familiares, aumentaremos también nuestro conocimiento acerca de la vía somática del cáncer común.
La afirmación inversa también es cierta:
el conocimiento de la naturaleza de los genes mutantes somáticamente puede ayudarnos a comprender las formas heredadas.

La teoría de Alfred Knudson de dos impactos de la carcinogénesis en el retinoblastoma se ha convertido en el paradigma de un modelo para describir el modo en que la herencia de un gen alterado predispone al portador del gen al desarrollo de cáncer.
La teoría establece que una célula puede iniciar un tumor únicamente cuando contiene dos alelos dañados;
por esta razón, una persona que herede una copia de un gen mutante de retinoblastoma debe experimentar una segunda mutación somática para desarrollar uno o más retinoblastomas.
También pueden ocurrir dos mutaciones somáticas en un único retinoblastoma de un feto no predispuesto, originando un retinoblastoma esporádico.

El gen del retinoblastoma y los supresores tumorales

¿Cuál es la naturaleza del segundo impacto? 
De nuevo con el retinoblastoma como paradigma, en principio parecían aceptables dos posibilidades:

1' rend="il la segunda mutación afectaría la función de un gen distinto al del retinoblastoma, y la presencia de dos genes mutantes en la misma célula originaría el tumor, o

2' rend="il el segundo suceso inactivaría o alteraría la copia restante (normal) del gen del retinoblastoma. 


El segundo impacto podría implicar, en algunas ocasiones, la pérdida regular de un cromosoma o una extensa delación de una región cromosómica específica que contiene el gen del retinoblastoma.
Estas pérdidas cromosómicas a gran escala serían detectables con marcadores de DNA, si la segunda posibilidad fuese la correcta. 

Las pérdidas de marcadores del DNA junto al gen del retinoblastoma ( RB ), en la actualidad identificado y localizado en el cromosoma 13, ocurren en más del 50% de los retinoblastomas.
Varios mecanismos, incluyendo mutación puntual, deleción y recombinación somática, pueden producir una pérdida de este tipo (fig. 9-5).
La observación de la pérdida de DNA demuestra que el segundo impacto, que ocurre en el feto durante el período de rápida división y proliferación de los retinoblastos, ha eliminado el alelo restante de este gen. 
Esto significa que una célula con un alelo mutante RB y un alelo normal RB no pueden originar un tumor.
Por lo tanto, el producto del gen normal, aun cuando sólo presente una única copia, evita la formación del tumor.

Se ha acuñado el término supresor tumoral para describir el gen RB y una lista creciente de otros genes asociados con cáncer (tabla 9-1).

Los supresores tumorales tienen la sorprendente característica de que las mutaciones heredadas son alelos dominantes en el individuo (los individuos heterocigotos suelen desarrollar la enfermedad), pero alelos recesivos a nivel celular (las células heterocigotas no originan tumores). 
Esta aparente contradicción se resuelve al tener en cuenta que sólo es necesaria una única célula tumoral, entre una población de millones de células normales, para iniciar un tumor en el individuo.
Por lo tanto, en los individuos que han heredado el primer impacto, un segundo impacto que ocurra en una célula ocasionará el tumor.
Debido al gran tamaño de la población diana de retinoblastos, los individuos heterocigotos desarrollan, por término medio, varios retinoblastos homocigotos para una mutación RB y cada uno de estos retinoblastos origina un retinoblastoma.

El descubrimiento de que los retinoblastomas se originan cuando ambos alelos del mismo locus en el cromosoma 13 se inactivan en el mismo retinoblasto suscitó el concepto de genes supresores tumorales.
El producto de estos genes suprime la formación del tumor al controlar el crecimiento celular y lo hace incluso en el caso de que una célula contenga sólo una versión normal del gen.

Debido a la función crucial que tienen los supresores tumorales en la prevención de la formación del tumor, su estudio tiene una considerable importancia médica. 
Si conocemos el mecanismo por el cual el organismo suprime de forma natural el tumor, finalmente podremos desarrollar terapias médicas más eficaces de prevención y tratamiento de los tumores.

Otra clase de genes cancerígenos: oncogenes

Una segunda clase específica de genes que pueden provocar cáncer es la formada por los oncogenes (es decir, genes cancerígenos).
La mayoría de oncogenes se originan a partir de los protooncogenes , unos genes que intervienen en los cuatro mecanismos básicos de regulación del crecimiento celular antes mencionados (es decir, los factores de crecimiento y sus receptores, las moléculas de transductoras de señales y los factores de transcripción nuclear).
Cuando un protooncogén experimenta una mutación se convierte en un oncogén, un gen cuyo producto puede alterar el crecimiento y la diferenciación celular normal.
Cuando un oncogén provoca que una célula siga un proceso de crecimiento no regulado, se dice que estas células se ha transformado .
A diferencia de la mayoría de supresores tumorales, los oncogenes suelen ser dominantes a nivel celular: 
tan sólo es necesaria una única copia del oncogén mutado para contribuir al proceso multifásico de progresión tumoral.
En esta sección estudiaremos tres métodos utilizados en la identificación de oncogenes específicos.

Definición retroviral

Los retrovirus , un tipo de virus RNA, son capaces de utilizar la transcriptasa inversa para transcribir RNA en DNA.
De este modo, un retrovirus puede insertar sus genes en el DNA de una célula huésped.
Los primeros oncogenes identificados provienen del estudio de los retrovirus que suscitan el desarrollo de cáncer en sistemas animales.
Estos retrovirus son portadores de versiones alteradas de genes estimuladores del crecimiento en las células.
En una fase precoz de la infección, un retrovirus puede haber incorporado un oncogén del genoma de su huésped.
Cuando el retrovirus invade otra célula, puede transferir el oncogén al genoma del nuevo huésped, transformando de este modo la célula. 

Figura 5

Las personas que heredan una mutación RB son heterocigotas para la mutación en todas las células de su organismo.

El segundo impacto se produce durante el desarrollo embrionario y puede consistir en una mutación puntual, una deleción, una pérdida del cromosoma normal y la duplicación de uno anormal, o una recombinación somática.
Cada proceso origina homocigosis para el alelo RB mutante y, por lo tanto, desarrollo de tumor.

Herencia multifactorial

y enfermedades comunes

Los capítulos anteriores se han centrado en las enfermedades causadas por un único gen o por anomalías de un único cromosoma.
Se han efectuando grandes progresos en la identificación de mutaciones específicas causantes de estas enfermedades, permitiendo estimaciones más precisas de los riesgos y, en ciertos casos' un tratamiento más efectivo de la enfermedad.
Sin embargo, estos trastornos sólo representan una pequeña porción de la carga total de la enfermedad genética humana.

La mayoría de las malformaciones congénitas no están causadas por genes aislados o por defectos cromosómicos. 

Muchas enfermedades comunes en el adulto, como el cáncer, la cardiopatía la diabetes, tienen algún componente genético pero, una vez más, no suelen deberse a genes aislados ni a anomalías cromosómicas.
Estas enfermedades, cuyo tratamiento ocupa globalmente la atención de la mayoría de profesionales de la salud, son el resultado de una compleja interacción de múltiples factores genéticos y ambientales.

PRINCIPIOS DE LA HERENCIA

MULTIFACTORIAL

Modelo básico

Los rasgos cuya variación se debe a los efectos combinados de múltiples genes se denominan poligénicos (muchos genes).
Si se considera que los factores ambientales también causan variación en el rasgo, se utiliza el término multifactorial .
Muchos rasgos cuantitativos (los que, como la presión sanguínea, se miden en una escala numérica continua) son multifactoriales. 

Dado que están causados por efectos aditivos de muchos factores genéticos y ambientales, estos rasgos tienden a seguir una distribución normal, o «en forma de campana», en las poblaciones.

Utilicemos un ejemplo para ilustrar este concepto.

Comenzando con el caso más simple, supongamos (de forma poco realista) que la estatura está determinada por un único gen con dos alelos.
A y a.

El alelo A tiende a originar personas altas, mientras que el alelo a tiende a originar personas bajas.
Si no existe dominancia en este locus , los tres genotipos posibles, AA , Aa , y aa , producirán tres fenotipos: 
alto', intermedio y bajo.

Supongamos que las frecuencias de gen de A y a son, cada una de 0.5.
Si estudiamos una población de individuos, observaremos la distribución de la estatura de la figura 10-1, A.

Supongamos ahora, de modo algo más realista, que la estatura está determinada por dos loci en lugar de uno.
El segundo locus también tiene dos alelos, B alto) y b (bajo), que afectan la estatura del individuo exactamente del mismo modo que los alelos A y a .
Ahora existen nueve genotipos posibles en nuestra población xxx .
Un individuo puede tener cero, uno, dos, tres o cuatro alelos « altos », de modo que ahora existen cinco fenotipos distintos.
Aunque la distribución de estatura en nuestra población todavía no es la normal, se le aproxima más que en el caso anterior de un único gen.

Ampliemos ahora nuestro ejemplo, de modo que muchos factores genéticos y ambientales influyan sobre la estatura, ejerciendo cada uno de ellos un pequeño efecto.
Existen entonces muchos fenotipos posibles, cada uno de ellos algo diferente, y la distribución de la estatura se aproxima a la curva en forma de campana de la figura 10-1, C . 

Debe resaltarse que los genes individuales que determinan un rasgo multifactorial, como la estatura, siguen los principios mendelianos de segregación y distribución independiente, igual que cualquier otro gen.

La única diferencia es que muchos de ellos actúan juntos para influir sobre el rasgo.

La presión sanguínea es otro ejemplo de rasgo multifactorial.

Existe una correlación entre las presiones sanguíneas (sistólicas y diastólicas) de los progenitores y las de sus hijos.
Existe una buena evidencia de que esta correlación se debe en parte a los genes.
No obstante, la presión sanguínea también resulta influenciada por factores ambientales, como la dieta y el estrés.
Uno de los objetivos actuales de la investigación genética es la identificación y la medición de las funciones relativas que los genes y el entorno tienen en la etiología de las enfermedades multifactoriales.

Muchos rasgos están influidos por múltiples factores genéticos y ambientales.
Estos rasgos se denominan multifactoriales.
Cuando pueden medirse en una escala continua, siguen a menudo una distribución normal.

Figura 10-1

A, distribución de la estatura en una población, asumiendo que la estatura es controlada por un único locus con genotipos AA , Aa y aa,B , distribución de la estatura, asumiendo que la estatura es controlada dos loci.

Ahora existen cinco fenotipos en lugar de tres y la distribución comienza a parecerse más a la distribución normal.
C, distribución de la estatura, asumiendo que múltiples factores, cada uno de ellos con un pequeño electo, contribuyen al rasgo (modelo multifactorial).

Modelo de umbral

Cierto número de enfermedades no siguen la distribución en forma de campana. 
En realidad parece que están presentes o ausentes en los individuos y no siguen los patrones esperados para las enfermedades monogénicas.
Una explicación habitual para estas enfermedades es que existe una distribución de predisposición subyacente para la enfermedad en una población.
Los individuos situados en el extremo bajo de la distribución tienen una escasa probabilidad de desarrollar la enfermedad en cuestión (es decir, tienen pocos factores alélicos o ambientales que causarían la enfermedad).

Los más próximos al extremo alto de la distribución cuentan con más factores genéticos y ambientales causantes de la enfermedad y, por lo tanto, es más probable que desarrollen la enfermedad.

En el caso de las enfermedades que pueden estar presentes o ausentes, se considera que se debe traspasar un umbral de predisposición para que la enfermedad se exprese.
Debajo de este umbral, el individuo permanece normal;
encima del umbral, aquél está afectado por la enfermedad.

Una enfermedad que corresponde a este modelo de umbral es la estenosis pilórica, un trastorno que se presenta poco después del nacimiento y está causado por estenosis u obstrucción del píloro, la región situada entre el estómago y el intestino.
La enfermedad provoca vómitos crónicos, estreñimiento, pérdida de peso y desequilibrio electrolítico, aunque a veces cura de forma espontánea o bien puede corregirse mediante cirugía.
La prevalencia de estenosis pilórica se aproxima a 3 de cada 1.000 nacidos vivos de raza blanca.
Es mucho más frecuente en varones que en mujeres, afectando a 1 de cada 200 hombres y a una de cada 1.000 mujeres.
Se cree que esta diferencia en la prevalencia refleja dos umbrales en la distribución de la predisposición, uno inferior en los hombres y otro superior en las mujeres (Fig.10-2).
El umbral inferior para los hombres indica que son necesarios menos factores patológicos para generar la enfermedad en los éstos.

El concepto de umbral de la predisposición puede explicar el patrón de riesgos de recurrencia de la estenosis pilórica observado en la tabla 10-1.
Obsérvese que los hombres, que cuentan con un umbral inferior, siempre presentan un riesgo superior al de las mujeres.

Sin embargo, el riesgo en hermanos también depende del sexo del probando.
Es superior cuando el probando es hombre.
Ello refleja que las mujeres, que tienen un umbral superior de predisposición, deben exponerse en comparación con los hombres a más factores causantes de enfermedad para llegar a desarrollarla.

Por lo tanto, una familia con una mujer afectada tendrá más factores de riesgo genéticos y ambientales, produciendo un riesgo superior de recurrencia de estenosis pilórica en la futura descendencia.
Podría esperarse que la categoría de mayor riesgo fuese la de los hombres parientes de probandos mujeres tal y como se demuestra en la tabla 10-1.

Se ha observado un patrón similar en un reciente estudio sobre el autismo infantil, un trastorno de la conducta en que la relación hombre-mujer es aproximadamente de 4:1.
Como era de esperar en un trastorno multifactorial, el riesgo de recurrencia para hermanos de probandos hombres (3,5 %) era notablemente menor que el de los hermanos de probandos mujeres (7 %).
Cuando se invierte la proporción de sexo (es decir, más mujeres afectadas que hombres), podría esperarse un riesgo superior de recurrencia cuando el probando es hombre.

Se cree que cierto número de otras enfermedades congénitas corresponde a este modelo.
Incluyen labio leporino aislado [1] y/o fisura palatina (LL/FP) , defectos del tubo neural (anencefalia y espina bífida), pie zambo (talipes) y algunas otras formas de cardiopatía congénita.
Además, muchas de las enfermedades comunes del adulto, como la hipertensión, la cardiopatía coronaria, el ictus, la diabetes mellitus (tipos I y II) y algunos cánceres, están causadas por complejos factores genéticos y ambientales, y, por lo tanto, pueden considerarse enfermedades multifactoriales.

El modelo de umbral es aplicable a muchas enfermedades multifactoriales.
Asume que existe una distribución de predisposición subyacente en una población y que es necesario que se sobrepase el umbral de esta distribución para que se exprese una enfermedad.

[1] En este contexto, el término "aislado" indica que se trata de la única manifestación patológica observada (es decir, la característica no forma parte de un conjunto principal de signos, como en el labio leporino y/o fisura palatina secundarios a trisomía 13).

Tabla 10-1

Riesgo de recurrencia (%) de estenosis pilórica, subdividida por género sexual de probandos y parientes afectados*

Figura 10-2

Distribución de predisposición en una población de una enfermedad multifactorial.

Para que un individuo sea afectado por la enfermedad, debe sobrepasar el umbral de la distribución de predisposición.
Esta figura muestra dos umbrales, uno inferior para los hombres y otro superior para las mujeres (como en la estenosis pilórica).

Riesgos de recurrencia y patrones de transmisión

Mientras los riesgos de recurrencia pueden calcularse con confianza para las enfermedades monogénicas (50% para las enfermedades autosómicas dominantes típicas, 25 % para las enfermedades autosómicas recesivas, etc.), la situación es más compleja en las enfermedades multifactoriales.
Ello se debe a que no se suele conocer el número de genes que contribuyen a la enfermedad, a que no se conoce la constitución alélica precisa de los progenitores, y a que el grado de efectos ambientales puede variar de forma considerable.
Para la mayoría de las enfermedades multifactoriales se han derivado riesgos empíricos (es decir, riesgos basados en la observación directa de los datos).
Para calcular riesgos empíricos se examina una larga serie de familias en las que un niño ha desarrollado la enfermedad (el probando). 

Después se estudian los hermanos de cada probando para calcular el porcentaje de individuos que también hayan desarrollado la enfermedad.
Por ejemplo, en Gran Bretaña, cerca del 5 % de los hermanos de pacientes con defectos del tubo neural también presentan el mismo defecto (comentario clínico 10-1).

Por lo tanto, el riesgo de recurrencia para los progenitores que han tenido un hijo con un defecto del tubo neural es del 5 % en Gran Bretaña.
Para trastornos como labio leporino y/o fisura palatina, que no son letales ni gravemente debilitantes, también se pueden calcular los riesgos de recurrencia para la descendencia de progenitores afectados.
Los riesgos de recurrencia empíricos son específicos para cada enfermedad multifactorial.

Al contrario que en la mayoría de enfermedades monogénicas, los riesgos de recurrencia de las enfermedades multifactoriales pueden cambiar de forma considerable de una población a otra (obsérvense las diferencias entre las poblaciones de Londres y Belfast de la tabla 10-1).
Ello se debe a que las frecuencias de gen, así como los factores ambientales, pueden diferir entre poblaciones.
Por ejemplo, el riesgo empírico de defectos del tubo neural en Estados Unidos es casi del 2 o el 3 % inferior al de la población británica. 

Los riesgos de recurrencia empíricos de las enfermedades multifactoriales se basan en estudios de grandes series de familias.
Estos riesgos son específicos de una población determinada.

En ocasiones es difícil diferenciar entre enfermedades poligénicas o multifactoriales y enfermedades monogénicas que tengan una penetrancia reducida o una expresión variable. 
Son necesarias grandes series de datos y buenos datos epidemiológicos para efectuar la diferenciación. 
Se suelen utilizar varios criterios para definir la herencia multifactorial:

El riesgo de recurrencia aumenta si existe más de un miembro de la familia afectado.
Por ejemplo, el riesgo de recurrencia en hermanos de un paciente afectado con un defecto del tabique ventricular ( VSD: ventricular septal defect ) es del 3%, pero aumenta aproximadamente hasta el 10 % si dos hermanos han padecido el YSD .
En contraste, el riesgo de recurrencia para las enfermedades monogénicas permanece idéntico con independencia del número de hermanos afectados.
Debe subrayarse que este aumento no significa que el riesgo de la familia haya cambiado en realidad.
De hecho' significa que ahora disponemos de más información acerca del verdadero riesgo de la familia:
puesto que han resultado afectados dos niños, es probable que se localicen en un nivel más elevado de la distribución de predisposición que una familia que tenga únicamente un niño afectado. 
En otras palabras, tienen más factores de riesgo (genéticos y/o ambientales) y es más probable que generen un niño afectado.

Si la expresión de la enfermedad en el probando es más grave, el riesgo de recurrencia es superior.
De nuevo, ello es congruente con el modelo de predisposición ya que una expresión más grave indica que el individuo afectado se encuentra en el extremo de la distribución de predisposición (v. fig. 10-2). 
Por lo tanto, sus parientes corren un riesgo superior de heredar genes patológicos.

Por ejemplo, la ocurrencia de labio leporino y/o fisura palatina bilateral (ambos lados) confiere un riesgo superior de recurrencia a los miembros de la familia que la ocurrencia de una fisura unilateral (un lado).

El riesgo de recurrencia es superior si el probando pertenece al género sexual menos afectado (v. el párrafo referente a la estenosis pilórica). 
Ello se debe a que un individuo afectado del sexo menos susceptible suele estar en la posición más extrema en la distribución de predisposición.

El riesgo de recurrencia de la enfermedad suele disminuir con rapidez en los parientes más lejanos (tabla 10-2).
Mientras que el riesgo de recurrencia en las enfermedades monogénicas se reducen al 50 % con cada grado de parentesco (p. ej., una enfermedad autosómica dominante corre el riesgo de recurrencia del 50 % entre hermanos, del 25 % entre tíos-sobrinos y del 12,5 % entre primos hermanos), la reducción es mucho más rápida en las enfermedades multifactoriales.
Ello se debe a que muchos genes y factores ambientales deben combinarse para producir un rasgo.
Es improbable que todos los factores de riesgo, o los necesarios, estén presentes en los miembros de la familia menos emparentados.

Si la prevalencia de la enfermedad en una población es f , entonces el riesgo para la descendencia y hermanos de los probandos es aproximadamente de vf .
Esto no es cierto para los rasgos monogénicos ya que sus riesgos de recurrencia son independientes de la prevalencia en la población.
No se trata tampoco de una norma absoluta para los rasgos multifactoriales, pero muchas de estas enfermedades tienden a adaptarse a esta predicción.
El examen de los riesgos presentados en la tabla 10-2 muestra que las primeras tres enfermedades siguen la predicción de modo razonable.
Sin embargo, el riesgo observado en hermanos en la cuarta enfermedad, autismo infantil, es notablemente superior al previsto por la vf .

Los riesgos de las enfermedades multifactoriales suelen aumentar si:
1 se afectan más miembros de la familia;

2 la enfermedad tiene una expresión más grave,

3 y el probando afectado es un miembro del sexo afectado con menos frecuencia

Los riesgos de recurrencia disminuyen con rapidez en los grados menores de parentesco.
En general, el riesgo de recurrencia entre hermanos es aproximadamente igual a la raíz cuadrada de la prevalencia de la enfermedad en la población. 

Comentario clínico 10-1.

Defectos del tubo neural

Los defectos del tubo neural (NTD: neural tube defects ), que incluyen anencefalia, espina bífida y encefalocele (así como otras formas menos comunes), representan una de las clases más importantes de defectos congénitos con una prevalencia neonatal de 1 a 3 de cada 1.000.
Existe una notable variación en la prevalencia de NTD entre las diversas poblaciones, con una tasa especialmente elevada entre algunas poblaciones británicas e irlandesas (incluso de 5 o más por cada 1.000 nacimientos). 
En Estados Unidos, los NTD son entre 2 y 3 veces más frecuentes en las regiones orientales que en las occidentales del país.
Por razones no conocidas por completo, la prevalencia de los NTD ha disminuido en muchas regiones de Estados Unidos y Europa durante las dos últimas décadas. 

Por lo general, el tubo neural se cierra alrededor de la cuarta semana de gestación.
Un defecto en el cierre o la posterior reabertura del tubo neural provoca un NTD.
La espina bífida (fig. 10-3, A ) es el NTD más común observado y consiste en una protrusión del tejido espinal por la columna vertebral (el tejido suele incluir las meninges, la médula espinal y las raíces nerviosas).
Alrededor del 75 % de los pacientes con espina bífida presentan hidrocefalia secundaria que, a su vez, provoca en ocasiones retraso mental.
A menudo se observan parálisis o debilidad muscular, pérdida de control de esfínteres y pies zambos.
Un estudio reciente efectuado en la Columbia británica ha demostrado que las tasas de supervivencia de los pacientes con espina bífida han mejorado de forma espectacular durante las últimas décadas.
Menos del 30 % de los casos nacidos entre 1952 y 1969 c hasta la edad de 10 años, mientras que el 65% de los nacidos entre 1970 y 1986 sobrevivieron hasta esa edad.

La anencefalia (fig. 10-3, B ) se caracteriza por la ausencia parcial o completa de la bóveda craneal y de los hemisferios cerebrales.
Al menos dos tercios de los casos de anencefalia nacen vivos:
los embarazos a término no sobreviven más de unas horas o días. 
El encefalocele (fig. 10-3, () consiste en una protrusión del cerebro en un saco cerrado.
Raras veces es compatible con la supervivencia.

Se cree que los NTD se originan por una combinación de factores genéticos y ambientales.
En la mayoría de poblaciones estudiadas hasta la fecha, los riesgos de recurrencia empíricos para los hermanos de casos afectados oscilan entre el 2 y el 5 %.
Congruente con un modelo multifactorial, el riesgo de recurrencia aumenta con los hermanos afectados adicionales. 
Los estudios efectuados en Gran Bretaña demuestran que el riesgo de recurrencia en hermanos es aproximadamente del 5 % cuando está afectado un hermano y del 10 % cuando se encuentran afectados dos hermanos.
Un estudio reciente realizado en Hungría demuestra que la prevalencia total de NTD es de 1 de cada 300 nacimientos y que los riesgos de recurrencia en hermanos son del 3, el 12 y el 25 % después de uno, dos y tres descendientes afectados, respectivamente.
Los riesgos de recurrencia tienden a ser algo inferiores en las poblaciones con menores tasas de prevalencia de NTD, como predice el modelo multifactorial.
Los datos de riesgo de recurrencia corroboran la idea de que las formas mayores de NTD están causadas por factores similares.
Una concepción anencefálica aumenta el riesgo de recurrencia de las subsiguientes concepciones de espina bífida y viceversa.

Los NTD se suelen diagnosticar prenatalmente a veces por ecografía y en ocasiones por una elevación de la alfa-fetoproteína (AFP) en el suero materno o el líquido amniótico (v. cap. 11).
Las lesiones de espina bífida pueden ser abiertas o cerradas (es decir, cubiertas con una capa de piel).
Es más probable que los fetos con espina bífida abierta sean diagnosticados por análisis de la AFP.

Un importante y reciente hallazgo epidemiológico consiste en que las madres que complementan su dieta con ácido fólico en el momento de la concepción tienen menor probabilidad de engendrar niños con NTD.
Este resultado se ha reproducido en varias poblaciones diferentes y, por lo tanto, parece confirmarse.
Se ha calculado que incluso el 50% de los NTD pueden evitarse Simplemente mediante complemento de ácido, fólico en la dieta (los complementos vitamínicos prenatales adicionales no tendrían demasiado efecto, ya que la administración no suele iniciarse hasta después del cierre del tubo neural).
Puesto que es probable que las madres ingieran cantidades similares de ácido fólico de un embarazo al siguiente, la deficiencia de ácido fólico justificaría al menos una parte del elevado riesgo de recurrencia entre hermanos para NTD.
Éste es un importante ejemplo de un factor no genético que contribuye a la acumulación familiar de una enfermedad.

Figura 10-3

Principales defectos del tubo neural (NTD)

A, lactante con espina bífida abierta (meningomielocele).
B, un feto con anencefalia. 
Obsérvense las anomalías de las órbitas oculares y el defecto craneal.
C, encefalocele occipital. 

Tabla 10-2

Riesgo de recurrencia (%) de parientes de primer, segundo y tercer grados

Herencia multifactorial frente a herencia

monogénica

Es importante aclarar la diferencia entre enfermedad multifactorial y enfermedad monogénica en la que existe una heterogeneidad de loci .
En el primer caso, la enfermedad es causada por la operación simultánea de múltiples factores genéticos y ambientales' cada uno de ellos con un efecto relativamente pequeño.
En contraste para una enfermedad con heterogeneidad de loci , como la osteogénesis imperfecta, tan sólo es necesaria una única mutación que la provoque.
Debido a la heterogeneidad de loci , una mutación del cromosoma 7 puede ocasionar la enfermedad en una familia, mientras que una mutación en el cromosoma 17 puede causar la enfermedad en otra familia.
Por lo tanto, existe más de un gen que puede originar la enfermedad, pero en un individuo determinado la enfermedad está causada por un único gen.

En ciertos casos, un rasgo puede estar influido por la combinación entre un único gen con grandes efectos y un «fondo» multifactorial en que los factores adicionales genéticos y ambientales tienen, individualmente, escaso efecto.
En la figura 10-4 se ilustra este concepto.
Imaginemos que la variación en la estatura, por ejemplo, se debe a un único locus (denominado gen mayor ) y a un componente multifactorial. 
Los individuos con el genotipo AA mostrarán tendencia a ser más altos, los de genotipo aa a ser más bajos y los que tengan el genotipo Aa tendrán una estatura intermedia.
Sin embargo, otros factores adicionales (el componente multifactorial) causan una variación adicional. 
Por lo tanto, los que poseen el genotipo aa tendrán una estatura variable entre 130 y 170 cm , los de genotipo Aa tendrán una estatura entre 150 y 190 cm , y los que tienen el genotipo AA oscilarán entre 170 y 210 cm .
Existe una notable solapación entre los tres genotipos principales por la influencia del fondo multifactorial.
La distribución total de la estatura "en forma de campana", se debe a la superposición de las tres distribuciones de cada genotipo.
Parece que muchas de las enfermedades estudiadas tienen tanto componentes genéticos principales como multifactoriales en las poblaciones.
Así, hay subseries de la población en que las enfermedades, como cáncer de colon, carcinoma de mama o cardiopatías, se heredan como enfermedades monogénicas (con variación individual en la susceptibilidad a la enfermedad en la que contribuyen otros factores genéticos y ambientales). 

Estas subseries suelen justificar sólo un pequeño porcentaje del número total de casos patológicos. 
Sin embargo, es importante identificar los genes mayores responsables, puesto que su función puede proporcionar información importante para la fisiopatología y el tratamiento de la enfermedad.

Las enfermedades multifactoriales pueden diferenciarse respecto a los trastornos monogénicos causadas por mutaciones en loci diferentes (heterogeneidad de locus). 
A veces, una enfermedad puede tener componentes monogénicos y multifactoriales. 

Figura 10-4

Distribución de la estatura, asumiendo la presencia de un gen mayor (genotipos xxx ) combinado con un fondo multifactorial.

El fondo multifactorial causa variación de la estatura entre individuos de cada genotipo.
Si se superponen las distribuciones de cada uno de los tres genotipos, la distribución global de la estatura es aproximadamente normal, como muestra la línea de puntos.


Micromatrices de ADN

Con unas ingeniosas herramientas de investigación llamadas micromatrices de ADN ("microarrays" si nos dejamos llevar por el anglicismo al uso), la ciencia se está adentrando en las raíces moleculares de la salud y la enfermedad, al tiempo que acelera el paso en el descubrimiento de nuevos fármacos.
Merced a esas plantillas podría también adelantarse el día en que los tratamientos uniformes de las enfermedades se sustituyan por terapias personalizadas. 

La mayoría de los pacientes que sufren linfoma difuso de células grandes tipo B responde bien a la terapia estándar. 
Pero, en más de la mitad de los casos, el cáncer no tarda en reaparecer con violencia letal.
Los médicos venían atribuyendo el hundimiento rápido de unos y la resistencia de otros a diferentes formas del tumor causadas por anomalías moleculares distintas.
Pero hasta hace dos años la ciencia carecía de medios para detectar qué pacientes sufrían la forma más virulenta y debían, por tanto, recibir un tratamiento más enérgico y arriesgado.

El problema lo resolvió una herramienta poderosa, la micromatriz de ADN ("DNA microarray").
Recurriendo a la misma, investigadores del norteamericano Instituto Nacional de la Salud, la Universidad de Stanford y otros centros pudieron distinguir entre supervivientes a corto y a largo plazo.
Basábanse para ello en las diferencias observadas en el patrón general de actividad que mostraban cientos de genes de sus células malignas en el momento del diagnóstico. 
Aquel logro debería llevar a una prueba discriminante, capaz de identificar a los pacientes expuestos a un riesgo mayor. 

Las micromatrices de ADN existen en el comercio desde 1996. 
Constituyen hoy uno de los pilares de la investigación farmacológica.
Más de 20 compañías se dedican a su distribución o a la venta de los programas informáticos asociados.
Gracias a esas plantillas asistimos a una revolución en el modo en que se estudia el funcionamiento molecular, normal y patológico, de las células.
En ellas se confía para obtener diagnósticos más rápidos y exactos de muchas enfermedades, así como para facilitar la personalización de los tratamientos clínicos, lo que implica elegir los fármacos más eficaces y con menores efectos secundarios en cada paciente.

Punteados de identificación 

Aunque hay varias clases de micromatrices, todas se ordenan a descubrir la composición del material genético en una muestra de tejido.
Todas las variedades constan de una serie de moléculas de ADN unicatenario (sondas), dispuestas en una rejilla a menudo no mayor que la huella del pulgar.
Las plantillas en cuestión se fundan en una propiedad muy útil del ADN: el emparejamiento complementario de sus bases.

De ADN están formados los más de 30.000 genes de la célula humana, las secuencias codificadoras de la composición de las proteínas.
El ADN consta de cuatro bloques de construcción, reconocidos por la primera letra de sus bases químicas: A, C, G y T.
En una cadena de ADN la base A sólo se emparejará con una T (la complementaria de A) de otra cadena; la C hará lo propio con G. Por tanto, si una molécula de ADN de una muestra de tejido se une a una sonda que tiene la secuencia ATCGGC, deduciremos que la muestra presenta la secuencia complementaria: TAGCCG.
El otro ácido nucleico fundamental, el ARN, sigue también esta estricta norma de emparejamiento de bases cuando se une al ADN; podemos, pues, deducir también la secuencia de una cadena de ARN que se empareja con ADN en una micromatriz.

Las reacciones de emparejamiento complementario de las bases han resultado cruciales para la ejecución de numerosas pruebas biológicas durante años.
Pero lo asombroso de nuestro caso es que las micromatrices de ADN pueden rastrear decenas de miles de estas reacciones a la vez en una sola tarjeta.
¿En virtud de qué?
Porque cada una de las sondas -trátese de un gen o de una secuencia de código más corta- se dispone en un lugar determinado de la rejilla, parecida a un tablero de ajedrez, y porque las moléculas de ADN o de ARN que se vierten sobre la plantilla portan un marcador fluorescente u otra etiqueta que puede detectarse con un escáner.
Una vez leída la micromatriz por el escáner, los datos se convierten en una impresión con un código de colores.

En el dominio de la investigación las micromatrices de ADN se aplican a dos usos muy distintos.
Las aplicaciones genotípicas comparan el ADN de una microplantilla con el ADN de una muestra de tejido para determinar qué genes hay en la muestra o para descifrar el orden de las letras de código en cadenas de ADN sin secuenciar.
Además de ese uso explorador de la presencia de genes en una muestra, importa otro, el de detectar la expresión, o nivel de actividad, de tales genes. Se dice que un gen se expresa cuando se transcribe en un ARN mensajero (ARNm) y se traduce en una proteína.
Las moléculas de ARN mensajero son los transcriptos móviles de los genes y sirven de molde para la síntesis de proteínas.

A la caza de genes

Se recurre al enfoque genotípico para comparar los genes de organismos diferentes (por ejemplo, para buscar claves sobre la historia de la evolución de los organismos) y para cotejar los genes de los tumores con los de tejidos normales (en busca de sutiles diferencias de composición o número de genes).
Algún día, las comparaciones de genes mediante micromatrices de ADN podrán mostrar su valor en clínica.

Sin ir más lejos, una micromatriz bien pergeñada podría establecer la causa exacta de la infección en un paciente cuyos síntomas, parecidos a los de un resfriado (dolor de cabeza, fiebre alta y dificultad para respirar), no denuncian un responsable claro.
Podría componerse una superficie con ADN que represente genes que aparecen sólo en patógenos causantes de enfermedades similares;
el laboratorio hospitalario podría extraer y marcar ADN de una muestra de tejido infectado (por ejemplo, de las fosas nasales del enfermo).
La unión del ADN del paciente a alguna secuencia de un gen del la microplantilla nos descubriría al agente infeccioso responsable.
De modo similar, las micromatrices que se están desarrollando podrían detectar si los bioterroristas han liberado cepas específicas de carbunco u otros gérmenes exóticos.

Para bien o para mal, las micromatrices de ADN podrían también determinar la predisposición genética de los individuos a numerosas enfermedades.
La mayoría de las diferencias genéticas que se dan entre las personas revisten la forma de polimorfismos de un solo nucleótido, o SNP, en los que aparece cambiada una letra.
Se podría construir una micromatriz con variantes de genes asociadas a enfermedades para detectar SNP individuales y predecir la probabilidad de que el sujeto en cuestión padezca Alzheimer, diabetes o un tipo de cáncer.
Las personas proclives podrían someterse a un control estricto, recibir un tratamiento preventivo o pasar antes por el quirófano.
Queda, empero, abierta la duda de hasta qué punto estas pruebas serán aceptadas por el público;
ese conocimiento podría alimentar la ansiedad del interesado y facilitar su posible discriminación por aseguradoras o patronos.

De esos inconvenientes queda exenta otra información valiosa que proporcionan las micromatrices de SNP.
Las variantes de genes influyen en la reacción del cuerpo a las medicinas que tomamos, lo que a su vez influye en la eficacia de los fármacos y en la intensidad de sus efectos secundarios. 
Las plantillas que pongan de relieve nuestras peculiares sensibilidades genéticas ayudarían a los clínicos a elegir las medicinas que convinieran más a cada paciente. 
Las micromatrices de SNP que revelan las mutaciones genéticas que incrementan la virulencia de los tumores permitirían que los anatomo-patólogos determinaran la malignidad escondida de un tumor en apariencia benigno.
Ambos tipos de microplantillas se están investigando ya con fines hospitalarios.

Expresión génica

No acaban ahí las apasionantes aplicaciones de la técnica.
Desde otra óptica particular, ha atraído también el interés de los investigadores en los últimos años.
Nos referimos al estudio de perfiles de expresión génica.
Para obtenerlos, se mide la cantidad de diferentes ARNm en una muestra de tejido. 
En general, cuantas más copias de ARNm produce una célula, más copias de proteína sintetizará también.
Por consiguiente, la cuantía de diversos ARNm apreciados en una muestra revelará, indirectamente, los tipos y cantidades de proteínas presentes.
Compete a las proteínas controlar y llevar a cabo la mayoría de las funciones de nuestras células y tejidos.
Se hallan en fase de desarrollo las micromatrices que miden directamente los niveles de proteína, aunque su construcción aparece rodeada de dificultades.

Con el uso del genoma como sensor para determinar cambios de actividad en genes de la célula, se obtendrán "instantáneas" detalladas del estado de alteración de las funciones celulares provocado por fármacos o enfermedades. 
A veces, resulta más útil conocer el esquema general de actividad de los genes en una muestra que saber qué genes concretos se activan en respuesta ante un estímulo. 
En esos casos, como veremos, el esquema sirve de "firma" taquigráfica que refleja el estado molecular de una muestra bajo unas condiciones particulares.

Los perfiles de expresión han demostrado su utilidad única en muchos frentes.
Los biólogos celulares los usan porque, si sabemos qué proteínas predominan después de exponer un tejido a diferentes condiciones, podemos inferir el modo en que el tejido compensa las agresiones y qué es lo que deja de funcionar cuando se desarrolla una enfermedad.

Se recurre también a las plantillas de expresión génica para descubrir las funciones de genes identificados a través de la secuenciación del ADN nuclear.
Hay otras técnicas que no utilizan micromatrices para mostrar las funciones de esos genes recientemente descubiertos (o, dicho con exactitud mayor, de las proteínas por ellos cifradas), pero tales métodos no siempre operan bien o con la presteza requerida.
Las micromatrices de expresión génica pueden subsanar tales lagunas mediante la aplicación que se ha venido a llamar "culpable por asociación", incluso sin ninguna pista previa sobre el papel de los genes en el organismo. 

El método en cuestión se funda en el hallazgo de la interrelación génica.
Los genes no son islas.
Si los de un tejido se activan e inactivan juntos en respuesta a un estímulo -un fármaco, una infección o una mutación inducida-, nos es dado suponer que esos genes que operan a coro intervienen en la misma vía reguladora; es decir, los genes trabajan juntos o en serie para inducir una respuesta celular.
Resulta, pues, razonable pensar que las funciones de un gen del grupo, en principio desconocidas, se parecen a las de los otros genes cuya responsabilidad se ha resuelto ya.

Aplicación en farmacología 

Los investigadores en el dominio de los fármacos se benefician también de las ventajas de ese método "culpable por asociación", cuando buscan proteínas cuya participación en procesos biológicos implicados en enfermedades se desconocía de antemano.
Una vez descubiertas esas proteínas, pueden engrosar la lista de blancos para desarrollar nuevas y mejores medicinas.

Peter S. Linsley, colaborador nuestro en Rosetta Inpharmatics, se propuso identificar nuevos blancos para fármacos contra enfermedades inflamatorias, en las que el sistema inmunitario pervierte su misión y lesiona partes del cuerpo.
Investigó, por tanto, qué genes de los leucocitos del sistema inmunitario aumentaban y disminuían su producción de proteínas en paralelo con el gen de la interleucina - 2 (IL-2), una proteína involucrada en los procesos inflamatorios. 

Obtuvo la respuesta tras conseguir perfiles de expresión génica de leucocitos expuestos a diversas sustancias químicas y ejecutar después un complejo programa de ajuste de pautas para detectar un conjunto de genes que se activaban o desactivaban al tiempo que se activaba el gen IL-2.
En ese conjunto de genes había uno cuya función no se había determinado todavía por otros medios.
Más o menos al mismo tiempo, otros investigadores del Instituto Pasteur de París confirmaron, por su lado, con un método diferente, que este gen intervenía en el metabolismo del IL-2.
Los hallazgos combinados sugieren que la proteína codificada por el gen en cuestión podría convertirse en óptimo blanco para los antiinflamatorios.

En los departamentos de investigación de los laboratorios farmacéuticos se utilizan los perfiles de expresión génica de una manera distinta: para detectar (y eliminar) fármacos que ejercerían efectos secundarios inaceptables. 
Quienes estudian si un determinado compuesto podría dañar el corazón pueden compilar un compendio de perfiles de expresión génica para células cardíacas expuestas a fármacos ya existentes o a sustancias químicas.
Si tratan células cardíacas con el fármaco ensayado, pueden esperar que el ordenador compare la firma resultante con el compendio de perfiles.
Una firma que se corresponda con la producida por sustancias que se sabe que dañan a las células cardíacas haría saltar la alarma.

Un compendio de perfiles de expresión génica puede también ayudar a explicar por qué un fármaco produce determinados efectos secundarios.
Apremia hoy saber por qué los inhibidores de la proteasa, que están salvando la vida de los infectados con el VIH (el virus del sida), pueden producir altos niveles de colesterol y triglicéridos en sangre, provocar extrañas distribuciones de la grasa corporal e inducir resistencia a la insulina.
Conscientes de que el hígado influye en la síntesis y degradación de los lípidos (el grupo que incluye el colesterol y los triglicéridos) y las lipoproteínas, nuestro grupo de Rosetta, en colaboración con Roger G. Ulrich y su equipo en Abbott Laboratories, decidimos comprobar si un inhibidor de la proteasa -ritonavir- inducía algunos de estos efectos secundarios interesando al hígado.

Mediante una microplantilla que representaba unos 25.000 genes de rata obtuvimos perfiles de expresión génica de tejido hepático de rata expuesto a diversos compuestos con toxicidad potencial para el hígado.
Agrupamos luego los compuestos en razón de la similaridad de sus firmas de expresión sobre unos 2400 genes que respondían intensamente a estas substancias.
A continuación, administramos ritonavir a hígados de rata y comparamos los perfiles de expresión génica resultantes con los que habíamos conseguido antes.

Descubrimos que el ritonavir activaba genes que normalmente permanecían silentes en respuesta a un agente, bien conocido, que rebaja los niveles de lípidos.
El ritonavir frena también la síntesis de proteínas que acostumbran ensamblarse en proteosomas, unas estructuras que degradan proteínas que han perdido su utilidad, entre ellas las lipoproteínas.
De tales descubrimientos se desprendía que el ritonavir incrementaba los niveles de lípidos en el hígado -y por consiguiente en sangre- en parte elevando la síntesis hepática de lípidos e inhibiendo la degradación de lipoproteínas. 
Una investigación más detenida de la interacción entre el ritonavir y las vías metabólicas de lipoproteínas y de proteosomas habrá de proporcionarnos claves para reducir sus efectos secundarios.

Ajuste de tratamientos

Disponer de un arsenal de fármacos con menos efectos secundarios constituirá un fruto valioso de los perfiles moleculares obtenidos con las plantillas de ADN.
Pero muchos médicos confían en resultados incluso mejores: sueñan con herramientas rápidas de diagnóstico que dividirían a los pacientes con síntomas similares en grupos distintos;
cada grupo recibiría un tratamiento diferente y apropiado.
Igual que quedó demostrado en el caso del linfoma, al que nos referíamos al principio de este artículo, los oncólogos necesitan imperiosamente un procedimiento para identificar a los pacientes que requieran, desde el principio, un tratamiento radical.

La investigación de nuestro grupo de Rosetta, junto con el Instituto Oncológico de Amsterdam, sobre el cáncer de mama pone de relieve la forma en que podemos servirnos de las matrices de expresión génica.
En este caso, buscábamos una prueba para determinar qué jóvenes pacientes con cáncer precoz de mama (sin metástasis en los ganglios linfáticos) necesitaban, y cuáles no, un tratamiento sistémico con quimioterapia para evitar la extensión del tumor después de la cirugía.
Aunque las normas en uso recomiendan un tratamiento sistémico para el 90% de estas mujeres, muchas de ellas probablemente evitarían metástasis incluso sin ese tratamiento.
Desgraciadamente, los métodos estándar no detectan a las mujeres con mayor riesgo.

Empezamos por generar perfiles de expresión génica para tumores en cerca de 100 mujeres de menos de 55 años de edad cuyo curso clínico posquirúrgico se había seguido durante más de cinco años.
Al principio trabajamos con una micromatriz que representaba 25.000 genes humanos.
Descubrimos, por fin, que cierta firma producida por unos 70 genes indicaba claramente que no tardarían en aparecer metástasis.
Además, la pauta opuesta revelaba un pronóstico esperanzador.
Resulta evidente que algunos tumores están programados para producir metástasis antes de alcanzar el tamaño de un guisante, mientras que otras masas mayores no están programadas para propagarse.

Nuestros resultados tendrán que ser confirmados por otros antes de que los perfiles de expresión génica se conviertan en una rutina en el tratamiento del cáncer de mama.
De aquí a dos años, es probable que muchos hospitales empiecen a ensayar los perfiles de expresión génica en su orientación diagnóstica, no sólo para el cáncer de mama, sino también para otros tipos.
Hay enfermedades que necesitan mejores herramientas de diagnóstico.
Los perfiles de expresión génica podrían ayudar a distinguir subgrupos de pacientes asmáticos, diabéticos u obesos que demandan una terapia específica.
Hablamos de aplicaciones en las que se está trabajando ya.

Antes de que las micromatrices desarrollen todo su potencial en investigación y diagnóstico, deberemos allanar algunos obstáculos.
Las plantillas, los escáneres y otros accesorios siguen siendo caros.
Podemos suponer que los costes bajarán con el tiempo.

Ahora bien, aunque bajen los precios, las técnicas pueden ser inaplicables, cuando menos al principio, para las consultas de los médicos o los laboratorios normales. 
Escasean los médicos y los técnicos que posean los conocimientos o el instrumental para preparar correctamente las muestras de tejido a utilizar con matrices.
Lo que es más, para diagnosticar, pongamos por caso, una enfermedad del hígado por los cambios de expresión génica en células hepáticas, un médico tendría que obtener tejido de ese órgano.
Y éste no es fácilmente accesible.

Tales problemas, hoy graves, se resolverán con un grano de ingenio.
A veces, los tejidos más accesibles pueden funcionar como aceptables sustitutos de los inaccesibles. 
Además, en algunos casos, no tendrán que usarse las micromatrices; podrían aportar la información necesaria para idear nuevas pruebas diagnósticas, que pueden revestir otras formas.

Conforme mejore nuestra comprensión de las células y del organismo entero, los médicos podrán diagnosticar con mayor precisión, indicar terapias más depuradas (incluidas probablemente las genéticas) y ajustar sus tratamientos a las características genéticas y al estado fisiológico real de cada paciente.
Hacia el año 2020, las organizaciones sanitarias podrían conservar, en sus ordenadores, modelos del estado molecular de sus afiliados, simulaciones virtuales que podrían actualizarse constantemente con datos de micromatrices u otros a partir de sus visitas al médico y con nueva información científica sobre biología celular.


Los anticonceptivos del futuro

Habrá claros avances en implantaciones, sustancias espermicidas y otros medios contra la fecundación. 

Con la gama de anticonceptivos disponibles,
¿se necesitan otros nuevos?
Mi opinión es tajante: sí.
Entre quienes practican el control de la natalidad (más de la mitad de las parejas del mundo) existe una gran insatisfacción con los métodos actuales, que suele desembocar en el uso inapropiado y el abandono de medidas protectoras, con el crecimiento consiguiente de embarazos no deseados y abortos.

El anticonceptivo ideal debería ser eficaz, seguro, barato, duradero y fácil de anular, exento de efectos colaterales y aplicable en algún otro momento distinto del inmediatamente precedente al coito.
También habrá de frenar la propagación de las enfermedades de transmisión sexual.
Ninguno de los productos en estudio satisface todos esos criterios, pero hay varias propuestas que cumplen la mayoría ellos y podrían estar en el mercado en el primer tercio del siglo XXI.

En el mercado de los anticonceptivos para el varón sólo se ofrece el preservativo.
Desde antiguo, los hombres se cubrían el pene con vejigas animales y fragmentos de intestinos.
El descubrimiento del látex permitió luego la fabricación uniforme de tales protectores.
Ante la opinión extendida de que reducen el placer sexual, los fabricantes comienzan a sustituirlo por otro, fino y resistente, de poliuretano; más tarde aparecerán membranas del mismo polímero o de otros.
Además, estos materiales distintos del látex serán excelentes barreras contra infecciones, no provocarán alergias y opondrán mayor resistencia al agrietamiento y la degradación por el calor, la luz y los lubricantes.

Un primer enfoque completamente innovador en la anticoncepción masculina se centra en la manipulación de las hormonas que interrumpen la producción de esperma (un reto formidable, si tenemos en cuenta que se generan al menos 1000 espermatozoides por minuto).
La producción de esperma está controlada por diversas hormonas.
El hipotálamo secreta la hormona liberadora de gonadotropina, que impulsa a la hipófisis a segregar hormona luteinizante y hormona foliculoestimulante.
La hormona luteinizante estimula, por su parte, la producción de testosterona en los testículos. 
Este esteroide, junto con la hormona foliculoestimulante, induce la división en los testículos de las células espermatogonias que formarán el esperma.

Desde un planteamiento endocrino, el anticonceptivo lo constituiría una inyección intramuscular de un andrógeno (testosterona u hormonas masculinas emparentadas) que indujera la liberación de la hormona en el torrente sanguíneo.
Esta estrategia, objeto de estudio por parte de la Organización Mundial de la Salud, se apoya en el comportamiento de los andrógenos circulantes, que ordenan al cerebro aminorar la secreción de hormona liberadora de gonadotropinas y, por tanto, de hormona luteinizante y de hormona foliculoestimulante.
Ahora bien, la presencia de niveles altos de andrógenos en sangre produce efectos colaterales indeseables:
irritabilidad, reducción de los niveles de lipoproteínas de alta densidad -el colesterol "bueno"-y aumento de acné.
Parece, sin embargo, que la adición de una progestina (forma sintética del esteroide femenino progesterona) permite tomar una dosis menor de andrógenos, innovación que debería eliminar los efectos secundarios y ser más segura que la administración del andrógeno solo.
Ese tratamiento combinado podría otorgar protección para tres meses y hallarse listo para su adquisición dentro de diez años, intervalo por lo demás mínimo para el desarrollo de cualquier técnica anticonceptiva.

Una opción alternativa sería la de bloquear, mediante moléculas sin efectos colaterales relacionados con los andrógenos, la actividad de la hormona liberadora de gonadotropinas.
Hay ya antagonistas proteínicos, aunque carentes de suficiente potencia para servir de anticonceptivos. 
Por eso se está ahondando en el diseño de inhibidores no peptídicos.
El bloqueo de la hormona liberadora de gonadotropina suprimiría la producción de testosterona, y eso significa que los varones tendrían que tomar andrógenos de sustitución para conservar la masa muscular, los caracteres sexuales masculinos y la libido.

A más largo plazo, en un cuarto de siglo, podría disponerse de agentes de acción prolongada (protección durante meses) que interrumpieran directamente la producción de esperma en los testículos o impidieran la maduración del esperma en el epidídimo:
cámara acompañante conectada a los testículos donde el esperma adquiere su movilidad.
Cierta línea de investigación sugiere que la alteración de la maduración espermática en el epidídimo podría ser la opción más factible.
Por una doble razón.
Primera: 
cualquiera que sea su forma de administración (vía oral, inyección o implante), los fármacos dirigidos contra el esperma tendrían que alcanzar los testículos o el epidídimo a través del torrente sanguíneo; sin embargo, los medicamentos transportados en la sangre suelen encontrar dificultades para abandonar la circulación y entrar en la parte de los testículos en la que se fabrica el esperma.
Además, muchos fármacos que son capaces de atorar la síntesis de esperma han demostrado ser tóxicos para las espermatogonias de los testículos e inducirían, pues, esterilidad irreversible.
Tal acción estaría bien para los animales domésticos, pero les resultaría inaceptable a la mayoría de los hombres.

EI siglo XXI traerá vacunas anticonceptivas (inmunoanticonceptivos) para varones y para mujeres cuya eficacia durará en torno a un año, probablemente.
La mayoría inducirá la síntesis inmunitaria de anticuerpos capaces de unirse (para modificar su operación) a proteínas implicadas en la reproducción.
Mediante la inyección en el sujeto de muchas copias de la proteína blanco (antígeno o inmunógeno) y otras sustancias capaces de estimular la respuesta del organismo, se activaría el sistema inmunitario.

Una de las vacunas más prometedoras que han llegado a la etapa de ensayos clínicos se propone inducir anticuerpos que inhiban la hormona liberadora de gonadotropinas en los varones. 
Puesto que esta vacuna cortaría la producción de testosterona, los varones necesitarían, una vez más, terapia de sustitución con andrógenos.

Se están ensayando también dos versiones de una vacuna femenina dirigida contra la gonadotropina coriónica, hormona segregada por la placenta en formación e imprescindible para la implantación.
En el Instituto Nacional de Inmunología de la India se ha trabajado en una versión de la misma, sujeta hoy a ensayos de eficacia en humanos.
La segunda versión aludida es la que se investiga en el seno de la OMS.

Las vacunas, masculinas o femeninas, podrían inducir respuestas inmunitarias que inmovilizarían los espermatozoides, forzarían su agrupación o impedirían la navegación hasta el ovocito para su acoplamiento con éste. 
Las vacunas femeninas podrían estimular además la síntesis de anticuerpos que se unirían a la superficie de un ovocito y formarían un escudo impenetrable para el esperma.
Los inmunoanticonceptivos tardarán en llegar, por una razón poderosa:
hay que estudiar con sumo detalle los inmunógenos que se propongan, cerciorarse de que las inoculaciones no desencadenarán respuestas contra los tejidos que no se pretendía y encontrar métodos de fabricación de vacunas en masa.
Y, añadiendo más complejidad, es probable que las vacunas deban contener diversos antígenos para compensar la posible diferente capacidad de respuesta de una persona a otra ante un mismo antígeno.

Mientras el terreno se rotura, las mujeres habrán acumulado ya experiencia con productos nuevos, muchos de ellos hormonales.
Cuentan ellas hoy con varios métodos hormonales: anticonceptivos orales, inyección mensual, inyección trimestral y sistema Norplant pentanual:
consiste éste en la colocación hipodérmica de seis varillas que contienen hormona.
La píldora, razonablemente segura y eficaz, aporta progestinas solas o en combinación con estrógeno sintético.
Los inyectables y los implantes son progestinas.
En todos los casos, se trata de bloquear la ovulación e incrementar el grosor del moco cervical, que dificulten la movilidad del esperma hacia el ovocito.

Dentro de cinco años habrá un anillo que se adaptará a la vagina, lo mismo que el diafragma, y liberará una progestina sola o con estrógeno.
Podría llevarse durante tres semanas y quitarse durante una semana (para permitir la menstruación).
Evitaría así la necesidad de tomar una píldora diaria.

También para los próximos años se espera que el fabricante de Norplant introduzca un implante de segunda generación compuesto de dos varillas, más fáciles de insertar y retirar que el prototipo primero. 
Andando el tiempo habrá implantes de una sola varilla y un sistema biodegradable que acabará disolviéndose en el organismo, pero que será extraíble y, por consiguiente, reversible durante algún tiempo.
Existe ya un dispositivo intrauterino liberador de una progestina que dura cinco años; por su parte los DIU no hormonales son eficaces en las mujeres que han estado embarazadas.

Muchos investigadores se están aplicando al desarrollo de una píldora que podría tomarse sólo una vez al mes:
en el momento en que se previera la menstruación o el último día del período (para evitar el embarazo durante el mes siguiente).
Para ciertas mujeres, esto último sería más deseable, porque, en vez de inducir un aborto precoz, evitaría la concepción. 
Pero cuesta avanzar en esa línea debido a la falta de conocimiento suficiente sobre los mecanismos que estimulan e interrumpen las hemorragias mensuales.

En algún momento de su vida es probable que una mujer tenga relaciones sexuales sin protección.
Ya se venden anticonceptivos poscoitales de urgencia:
se trata de la píldora abortiva, que puede tomarse dos o tres días después del coito.
Las píldoras anticonceptivas regulares pueden servir también como anticonceptivos de urgencia si se toman en las dosis adecuadas.
Todas ellas, sin embargo, producen efectos secundarios, por lo que se está investigando el desarrollo de versiones más suaves.
Así, se ha observado que la píldora francesa RU 486, comercializada en varios países y cuyo empleo mejor conocido es la interrupción del embarazo, causa pocas molestias cuando se toma en una dosis baja en las 72 horas posteriores al acto sexual.

Una estrategia anticonceptiva bastante diferente se basará en el desarrollo de sustancias químicas que impidan la migración del espermatozoide hasta el ovocito o la fecundación de éste.
Tales compuestos procederían de un anillo vaginal, para protección a largo plazo, o podrían introducirse en la vagina poco antes del coito, igual que se hace con los espermicidas.
Diferirían de estos últimos en varios aspectos, sin embargo.
Los espermicidas son detergentes que degradan el esperma.
Pueden acabar con la microflora que ayuda a mantener la acidez apropiada de la vagina, así como irritar la pared vaginal, facilitando la infección por virus y bacterias.
Los compuestos en estudio no serían detergentes y actuarían con mayor especificidad, es decir, entorpeciendo la normal evolución de los episodios subsiguientes a la eyaculación.

Una vez en la vagina, los espermatozoides experimentan cambios importantes, encaminados a la fecundación.
Desaparece el colesterol de la membrana que encierra la cabeza del espermatozoide, lo que aumenta la fluidez de aquélla y, por tanto, permite el cambio de posiciones de otras moléculas. 
Después, con la ayuda de la progesterona de las trompas de Falopio, se abren canales especializados en la membrana espermática, que permiten así el flujo hacia el interior de iones calcio externos.
Estos iones facilitan la reacción del "acrosoma" (saco compuesto de enzimas y situado en la cabeza del espermatozoide), que ocurre cuando el espermatozoide se encuentra con la zona pelúcida, cubierta gelatinosa que rodea la membrana externa del ovocito.
En la reacción acrosómica, la membrana plasmática del espermatozoide se funde con la del acrosoma, y se liberan enzimas al exterior para que abran paso al espermatozoide en su perforación de la cubierta del óvulo.

Entre los productos químicos que se están explorando para impedir la maduración del esperma y la reacción del acrosoma se cuentan los que obstruyen los canales de calcio o bloquean la salida del colesterol de la membrana plasmática.
(Se trabaja en la posible aplicación de estas sustancias al varón: resultarían eficaces si lograran unirse al esperma en el epidídimo y se mantuvieran así en la travesía por el tracto genital femenino.)
Otros autores se hallan empeñados en obtener una reacción acrosómica prematura, que dejaría al espermatozoide sin capacidad para fusionarse con el ovocito.

Otras líneas afines de trabajo son las que buscan inhibir las interacciones específicas entre el espermatozoide y la zona pelúcida.
En el hospital clínico de la Universidad de Duke se ensaya un compuesto que se une a una proteína espermática (la quinasa receptora de la zona) en el lugar reservado para la interacción con una proteína de la zona pelúcida.
El anclaje del compuesto bloquea la acción enzimática en la cubierta del ovocito.
Por otro lado, el esperma y el líquido seminal portan enzimas antioxidantes, que protegen la integridad de la membrana espermática; si tales antioxidantes resultaran ser exclusivos del esperma o el líquido seminal, podrían crearse fármacos que inactivaran tales enzimas, sin tener que privar a otras células de las defensas antioxidantes.
En determinados casos, anticuerpos puros generados en el laboratorio (anticuerpos monoclonales) podrían ocupar el lugar de sustancias químicas no biológicas entre los ingredientes activos de los anticonceptivos liberados en la vagina.

La creciente propagación de las enfermedades de transmisión sexual insta a buscar la combinación de los efectos anticonceptivos con los antiinfecciosos.
Las barreras pueden ser físicas (preservativos masculinos y femeninos y, en cierto grado, el diafragma) o químicas (espermicidas u otras formulaciones más selectivas).
Para evitar la infección del virus del sida y otros agentes infecciosos, la barrera química tendría que cubrir, cuando menos, la pared vaginal entera y el cuello del útero, no producir irritaciones ni ser tóxica para la flora vaginal.

Apenas han comenzado los ensayos sobre la eficacia de los espermicidas existentes para evitar la transmisión de las enfermedades venéreas.
Se sabe que frenan la infección por Chlamydia .
Es sólo un caso.
En los próximos 10 años podría haber espermicidas de diseño que preservaran la salud de la mujer puesta en peligro cuando fracasa ]a protección del hombre 0 ésta no existe.

Los métodos anticonceptivos actuales son el fruto de la investigación iniciada hace años.
De aquí al año 2000 podrían agregarse los preservativos de polímeros' los anillos vaginales y los implantes de una o dos varillas de hormona, más fáciles de extraer que las cápsulas actuales.
Unos pocos años después, llegarán los espermicidas que disminuirán la propagación de las enfermedades de transmisión sexual.
A ellos seguirán nuevos anticonceptivos de urgencia, una combinación inyectable en varones de andrógeno y progestina con una duración de acción de tres meses, implantes biodegradables e inmunoanticonceptivos. 
Puesto que no existe el anticonceptivo ideal, quienes recurran a ellos necesitan un amplio conjunto de opciones.
La investigación abierta dará su pleno rendimiento el siglo venidero.

Tratamientos de la esterilidad en el futuro

A lo largo de los diez últimos años se ha progresado bastante en las técnicas de fecundación in vitro (el procedimiento que produjo el primer bebé "probeta" en 1978).
Puede ya inducirse la ovulación de muchos óvulos en un ciclo menstrual; se extraen los ovocitos y se les inyecta esperma o ADN; tras someter los embriones formados a pruebas de detección selectiva de defectos genéticos, se implantan los sanos en el útero femenino.
Técnicas que, en los próximos años, se Irán refinando. 

La fecundación in vitro se aplica, sobre todo, en mujeres cuyas trompas de Falopio están bloqueadas o cuya incapacidad para concebir no acaba de entenderse.
Las mujeres estériles como consecuencia de la presencia de tumores fibroides benignos en el útero necesitan otras terapéuticas.
Por ahora, los fibroides molestos suelen reducirse mediante cirugía o administrando análogos de la hormona liberadora de gonadotropina. 
Cualquier cirugía comporta riesgos y el tratamiento hormonal puede ser problemático, en parte porque los análogos impiden la concepción; además, cuando se interrumpe el tratamiento, los fibroides suelen recurrir.
Si se logran identificar proteínas exclusivas de la superficie tumoral, podrían diseñarse fármacos que ocuparan el lugar de esas proteínas y curaran los tumores; así de sencillo, sin afectar a los otros tejidos o causar efectos colaterales.
Hay buenas posibilidades de que estos fármacos existan de aquí a quince años.

Habrá de emplearse terapias similares para sanar la endometriosis, crecimiento fuera del útero de células endometriales (procedentes del revestimiento uterino).
La afección produce cierta esterilidad e induce también intensos calambres menstruales.

Pero se supone que serán los varones (en especial los que no fabrican esperma) quienes saldrán más beneficiados de los avances espectaculares que se avecinan.
Se investiga la posibilidad de trasplantar células precursoras de espermatozoides en los túbulos seminíferos de los testículos: el sistema de tubos en los que de ordinario se forma el esperma.
En condiciones normales, esas células precursoras generan esperma durante toda la vida del varón; cabe esperar que las células trasplantadas hagan lo propio.

Figura 0

Las sustancias que enlazan con proteínas de la superficie del espermatocito pueden bloquear su anclaje en al cubierta del ovocito

Los agentes que bloquean la eliminación del colesterol de la membrana plasmática del espermatocito le impiden a éste perforar la cubierta del ovocito.

Los fármacos que bloquean la entrada de los iones calcio en la cabeza del espermatocito le incapacitan para penetrar en la cubierta del ovocito.

Las sustancias químicas que arruinan la acción protectora de los antioxidantes inducen la descomposición del ovocito.

Figura 1

LA CABEZA DEL ESPERMATOCITO constituye el blanco de machas estrategias anticonceptivas

Varios planteamientos ( bloques de texto a la izquierda ) se afanan en el desarrollo de sustancias químicas sintéticas ( puntos rojos en detalle de la membrana plasmática ) que alteren la cabeza del espermatocito, impidiéndole fecundar el ovocito.
Se ensayan vacunas anticonceptivas en virtud de las cuales el sistema inmunitario fabrique anticuerpos capaces de alterar la función espermática.
Hay anticuerpos que promueven la agregación de los espermatocitos, inutilizándolos ( micrografía ).

Figura 2

TESTICULOS DE RATON, antes estériles, comenzaron a producir esperma en gran cantidad después de la implantación, cerca de la periferia, de células precursoras extraídas de otro ratón

La estela de color observada en el centro es un signo de que las células trasplantadas, que estaban teñidas de azul, sobrevivieron y produjeron esperma. 


GENÉTICA DEL SISTEMA INMUNITARIO 

La organización y el control genéticos del sistema Inmunitario muestran varios fenómenos genéticos que se observan con menos claridad, o de hecho no aparecen, en otros sistemas: polimorfismo amplio, desequilibrio de enlace, una obvia relación evolutiva entre sus componentes y, en especial, un sistema único de codificación de gran diversidad en relativamente pocos loci.
Los genes del sistema inmunitario no sólo son clínicamente importantes en relación con el trasplante, la enfermedad autoinmunitaria y la respuesta a la infección, así como causa de varios trastornos monogénicos, sino que también proporcionan excelentes modelos para el análisis de la variación humana y la expresión génica. 
La aplicación de técnicas moleculares ha ampliado en gran medida el conocimiento de la respuesta inmunitaria, al mismo tiempo que ha revelado nuevos sustratos de complejidad que permanecen por explorar.

Los organismos superiores son únicos en cuanto a su capacidad para distinguir entre «lo propio» y «lo ajeno», y para generar una reacción selectiva en contra de un amplio espectro de antígenos externos. 
Esta reacción está mediada por la compleja red interactiva de células y citocinesis celular del sistema inmunitario y se denomina respuesta inmunitaria.
Los factores genéticos desempeñan un papel fundamental no sólo en la generación de la respuesta inmunitaria normal, sino también en el desarrollo de reacciones inmunitarias aberrantes y de las enfermedades inmunomediadas resultantes.
En este capítulo se describe la base genética de la respuesta inmunitaria y se señalan varios trastornos monogénicos del sistema inmunitario, así como enfermedades en las que los genes inmunorrelacionados, como los del complejo de histocompatibilidad (MHC), contribuyen a la susceptibilidad a la enfermedad, aunque no la codifican exclusivamente. 

Lo que aquí se expone no intenta proporcionar una completa revisión de inmunología o inmunogenética, para lo cual se remite al lector a la bibliografía presentada al final del capítulo.
Más bien se introducen algunos de los sistemas génicos que gobiernan la función inmunitaria y refuerzan el concepto de que, con excepción de los gemelos monocigóticos (u otros nacimientos múltiples), cada persona es única desde el punto de vista genético. 
Gran parte de la calidad única del ser humano depende de la expresión de genes del xxx y, por lo tanto, en lo que se presenta a continuación se destaca este sistema génico, así como otros dos dentro de la misma superfamilia que codifica componentes adicionales fundamentales de la respuesta inmunitaria: inmunoglobulinas ( xxx ) y receptores antigénicos de linfocitos T ( xxx ).

COMPLEJO PRINCIPAL DE HISTOCOMPATIBILIDAD 

El xxx es un locus complejo compuesto de un gran conjunto de genes que se localizan en el brazo corto del cromosoma 6 (fig. 14-1).
Según sus diferencias estructurales y funcionales, estos genes se clasifican en tres clases, cada una de las cuales es altamente compleja y polimórfica. 
Dos de éstas tres clases, la I y la II, corresponden a los genes del antígeno leucocitario humano ( xxx ), originalmente descubierto debido a su importancia en el trasplante tisular entre individuos no emparentados.

Figura 14-1

Esquema del complejo principal de histocompatibilidad en el cromosoma xxx 

TNF , factor de necrosis tumoral; Bf, factor B de properdina; xxx , componentes del complemento, xxx , 21-hidroxilasa. (Uno de los loci de xxx es un seudogén).

Los genes de la clase I ( xxx y xxx ) codifican antígenos que constituyen parte integral de la membrana plasmática de las células nucleadas.
Estos antígenos no sólo participan en el rechazo del trasplante, sino que también son fundamentales para la inmunocompetencia y están estrechamente involucrados en el reconocimiento antigénico, las interacciones linfocitarias y el desarrollo de la autotolerancia.
Un antígeno de clase I consta de dos subunidades de polipéptido, una cadena pesada codificada dentro del MHC y un polipéptido no polimórfico, la a-microglobulina, que está codificado por un gen fuera del MHC que mapea en el cromosoma 15.

El locus de la clase II está compuesto de varias subregiones que codifican los antígenos xxx , xxx y xxx .
Estas moléculas se expresan principalmente en linfocitos B, macrófagos y linfocitos T activados, pero en ciertas alteraciones pueden también expresarse en otros tipos de células.
Cada molécula de clase II es un heterodímero compuesto de subunidades a y B, ambas codificadas por el xxx .
Al igual que los antígenos de clase I, éstos son integrantes de la membrana celular y de las interacciones y función inmunitarias celulares.
Los antígenos HLA de las clases I y II desempeñan un papel crítico en el inicio de una respuesta inmunitaria y específicamente en la «presentación» del antígeno a linfocitos T, que no pueden reconocer el antígeno y responder a éste, a menos que se encuentre complementado con una molécula de xxx .

Los genes de la clase III no son genes de xxx , sino que incluyen genes para proteínas como los factores de properdina xxx y xxx , que son parte del sistema de complemento, una serie de proteínas séricas polimórficas y receptores de membrana estrechamente involucrados en la función inmunitaria.
También en esta región se encuentran genes que, cuando son defectuosos, causan enfermedades monogénicas como el gen para la 21-hidroxilasa; la deficiencia de esta última se asocia con una forma de hiperplasia suprarrenal congénita (v. cap. 10).
Por lo tanto, los genes de la clase III parecen ser distintos desde el punto de vista funcional a los de las clases I y II, y no se consideran aquí con mayor detalle.

Cierto número de otros loci genéticos dentro del xxx están genéticamente ligados a los genes xxx , pero funcionalmente no están relacionados con ellos.
Éstos incluyen los genes para el factor de necrosis tumoral y la linfotoxina, así como otros, aislados en fecha más reciente, cuya función está por definir.

Existe una notable similitud, tanto en organización como en secuencia de DNA, entre los genes xxx de las clases I y II, y entre los genes xxx y los de inmunoglobulina y los receptores de linfocitos T que se describen en las siguientes secciones.
La similitud entre estos genes y varios otros ha llevado a clasificarlos en una familia génica denominada superfamilia génica de las inmunoglobulinas (v. más adelante).
Los miembros de la familia parecen ser genes relacionados desde el punto de vista evolutivo, cuyos productos tienen una amplia variedad de funciones en el papel inmunitario de las moléculas de xxx e inmunoglobulinas. 

Polimorfismos y herencia de haplotipos xxx 

El sistema xxx es muy polimórfico.
Se han detectado numerosas variantes antigénicas distintas en xxx .

La tabla 14-l indica la complejidad y el amplio polimorfismo de este sistema.
Cada persona hereda una variante de cada uno de los subgrupos xxx de cada padre.
De acuerdo con el sistema actual de nomenclatura de xxx , no todas estas variantes antigénicas (especificidades) constituyen productos directos de genes simples; algunos, en particular las variantes de los subgrupos xxx y xxx . se localizan en subregiones de genes, en una cadena determinada o en una combinación a.

La tipificación xxx , realizada clásicamente con suero de mujeres multíparas. ha revelado una considerable variación en el perfil y la frecuencia de variantes xxx entre poblaciones diferentes.
El xxx , por ejemplo, se encuentra entre las especificidades xxx más frecuentes en todas las poblaciones, mientras que xxx se halla en la raza blanca, pero no entre negros ni asiáticos.

Los alelos xxx en un determinado cromosoma están tan estrechamente ligados que se transmiten juntos como un haplotipo. 
Los alelos son codominantes; cada progenitor posee dos haplotipos, expresa ambos y, como se muestra en la figura 14-2, transmite uno o el otro a cada hijo.

Como resultado. el progenitor y el hijo comparten sólo un haplotipo, y existe una probabilidad del 25 % de que dos hermanos hereden haplotipos HLA compatibles.
Debido a que la aceptación de tejidos trasplantados se correlaciona en gran medida con el grado de similitud entre haplotipos xxx (y grupos sanguíneos ABO) de donante y receptor, el donante predilecto para el trasplante de médula ósea o de un órgano es un pariente compatible para ABO y xxx del receptor.

Figura 14-2

La herencia de haplotipos xxx 

Generalmente, un haplotipo se transmite, como muestra esta figura, como una unidad.
En ocasiones extremadamente raras, un progenitor transmitirá un haplotipo recombinante al hijo.

Los haplotipos xxx muestran un marcado desequilibrio de enlace (v. cap. 8).
Ciertos haplotipos son mucho más frecuentes que lo esperado, mientras que otros resultan excepcionalmente raros, y la mayor parte de las 3 x 107 combinaciones fenotípicas teóricamente posibles entre individuos de raza blanca no se han observado nunca.
Algunas veces resulta imposible encontrar un adecuado donante de médula ósea aunque se efectúe una intensa búsqueda.
También existe una notable distribución étnica de haplotipos, además de la distribución étnica de marcadores HLA mencionada antes.

Tabla 14-1

Antígenos del sistema xxx 

Asociación entre xxx y enfermedad

A medida que se incrementa la delineación de alelos xxx , se ha logrado apreciar la asociación entre ciertas enfermedades y antígenos xxx o haplotipos específicos.

Como es evidente en la tabla 14-2, la mayor parte de estos trastornos son autoinmunitarios-esto es, se asocian con una respuesta inmunitaria aparentemente dirigida en contra de uno o más antígenos propios.
La asociación más intensa entre xxx y enfermedad que se conoce es la que existe entre narcolepsia y xxx ; casi el 100 % de los pacientes con narcolepsia expresan este antígeno de clase II, en comparación con sólo alrededor del 16 % en controles.

Sin embargo, únicamente un pequeño porcentaje de personas con xxx desarrollan narcolepsia, y el riesgo relativo de este trastorno en estos individuos, en comparación con los que no presentan este antígeno, es sólo de 34 (v. pie de la tabla 14-2).
También existe una asociación muy fuerte entre xxx y espondilitis anquilosante, una enfermedad inflamatoria crónica de la columna vertebral y las articulaciones sacroilíacas.
Mientras que sólo el 5 % de los individuos de raza blanca en general son B27-positivos, al menos el 90 % de los individuos blancos con espondilitis anquilosante son B27-positivos.

El riesgo de desarrollar esta enfermedad es al menos 90 veces mayor para personas que poseen el xxx que para las que no lo tienen.
De forma similar, la enfermedad de Reiter y la artritis reactiva, alteraciones muy parecidas a la espondilitis anquilosante, también se asocian intensamente con el antígeno B27.
Otro ejemplo es la asociación de la diabetes mellitus insulinodependiente con xxx en especial con heterocigosis xxx .

Tabla 14-2

Ejemplos de asociación entre antígenos xxx y enfermedad

La relevancia etiológica de la mayor parte de las asociaciones entre xxx y enfermedad aún no se conoce.
En casi todos los casos, la asociación se relaciona con la respuesta inmune y no depende de la proximidad física de un gen causante de una enfermedad con los loci xxx .
Sin embargo, hay dos excepciones: los trastornos autosómicos recesivos, hemocromatosis primaria e hiperplasia suprarrenal congénita debida a deficiencia de 21-hidroxilasa, en los que los genes defectuosos sí se encuentran dentro de xxx .
El análisis de varias mutaciones de 21-hidroxilasa que generan hiperplasia suprarrenal ha revelado que originalmente se producen mutaciones diferentes en este locus sobre sustratos de haplotipos diferentes y han permanecido en desequilibrio de ligamiento con estos marcadores específicos de haplotipo.
El gen de la hemocromatosis aún no se ha aislado, pero se halla estrechamente ligado al locus xxx .

Si bien las bases de la mayor parte de asociaciones entre xxx y enfermedad se desconocen, hasta la fecha las pruebas sugieren que los genes xxx no son los responsables únicos de la susceptibilidad a enfermedades específicas, sino que predisponen a padecer ciertas enfermedades junto con otros factores genéticos o ambientales.
Ya que las moléculas de xxx son parte integral del reconocimiento del antígeno por los linfocitos T, se especula con que su papel en la patogenia de la enfermedad puede estar relacionado con diferencias en las capacidad de estas proteínas polimórficas para interactuar con un antígeno y el receptor del linfocito T en el inicio de una respuesta inmunitaria (fig. 14-3).
Esta hipótesis, que implica un papel patogenético directo para la molécula de xxx , se apoya en el hallazgo, descrito con cierto detalle en el capítulo 15, de que la asociación entre xxx y diabetes mellitus insulinodependiente se relaciona con un cambio en un solo aminoácido en una de las áreas externas de la cadena de xxx .
Sin embargo, el mecanismo preciso por el que esta sustitución confiere susceptibilidad a la enfermedad aún se desconoce. 

INMUNOGLOBULINAS

Los anticuerpos son inmunoglobulinas que aparecen en respuesta a un estímulo de un antígeno extraño, y pueden reconocer y enlazar dicho antígeno y facilitar su eliminación.

Son mediadores de la respuesta inmunitaria en las enfermedades relacionadas con xxx que se acaban de describir, así como de la respuesta a antígenos extraños.

Cierto número de trastornos genéticos se deben a deficiencias de inmunoglobulinas (tabla 14-3).
Sin embargo, el significado primario de las inmunoglobulinas desde la perspectiva de la genética es que muestran una propiedad singular, el reordenamiento somático , por el que los genes de la línea germinal se reordenan en células somáticas para generar diversidad.

Los anticuerpos existen en dos formas: una ligada a la membrana, en los linfocitos B, y otra soluble o segregada.

Los anticuerpos secretados son producidos por células plasmáticas que derivan de linfocitos B por proliferación y maduración.
Este proceso, conocido como activación de linfocitos B, se inicia por interacción entre un antígeno específico y una molécula de anticuerpo adecuada en la membrana del linfocito B.
Esta interacción produce la expansión clonal y diferenciación del linfocito B, con formación de células plasmáticas que secretan anticuerpos específicos para el antígeno incitador, y de linfocitos B de memoria capaces de responder con rapidez y fuerza a cualquier situación posterior provocada por el mismo antígeno.

Figura 14-3

Representación esquemática de la interacción entre una molécula de xxx , una proteína extraña y el receptor de linfocitos T

Tabla 14-3

Ejemplos de trastornos monogénicos del sistema inmunitario

Estructura y diversidad de la inmunoglobulina

Uno de los puntos más complejos de la inmunogenética solía ser el mecanismo por el cual un número aparentemente infinito de anticuerpos diferentes podía codificarse en el DNA de la línea germinal de un individuo.
Se calcula que cada ser humano puede generar un repertorio de alrededor de 108 anticuerpos diferentes, aun cuando el genoma está compuesto de sólo 3 x 109 pares de bases de DNA.
Esta aparente disparidad se ha aclarado al demostrarse que los anticuerpos están codificados en la línea germinal por un relativamente pequeño número de genes que, durante el desarrollo del linfocito B, sufren un singular proceso de reordenamiento y recombinación somática que permite la generación de enorme diversidad.
El potencial para crear esta gran cifra de anticuerpos diferentes parece haber evolucionado como un mecanismo de protección contra el gran número de organismos infecciosos, agentes tóxicos y células malignas autólogas a los que una persona puede estar expuesta.

Las moléculas de Ig se componen de cuatro cadenas polipeptidicas, dos cadenas idénticas pesadas (H) y dos ligeras iguales (L), que se mantienen juntas mediante enlaces disulfuro (fig. 14-4).
Los enlaces disulfuro subdividen cada cadena en una serie de dominios homólogos.
Según sus diferencias estructurales en la porción carboxilo terminal, las cadenas H se subdividen en cinco clases o isotipos, xxx y las Ig correspondientes se denominan xxx .
Como se indica en la tabla 14-4, las clases distintas de Ig difieren funcional y estructuralmente.

Las cadenas L de las moléculas de inmunoglobulina también pueden ser de dos tipos, xxx , pero no están ambas en el mismo anticuerpo.
Por lo tanto, el anticuerpo producido por un solo linfocito B contiene un solo isotipo de cadena H y un solo subtipo de cadena L.
Además, a diferencia de otros loci autosómicos, en una célula únicamente se expresa uno del par de alelos parentales para cada cadena H: el alelo paterno o el materno, pero no ambos. 

Lo mismo es cierto para cada cadena L.
Estos fenómenos, conocidos como exclusión isotípica y exclusión alélica , respectivamente, aún no se comprenden por completo.

Cada cadena H y L de una proteína Ig consta de dos segmentos, la región constante (C) y la región variable (V).
La región constante, que determina la clase de molécula de Ig, se localiza en el extremo carboxilo y su secuencia de aminoácidos está relativamente conservada entre las Ig de la misma clase.
En contraste, la región V se sitúa en el extremo amino y su secuencia de aminoácidos muestra una amplia variación entre regiones diferentes.

Las regiones V de las cadenas H y L forman el lugar de enlace de antígenos y determinan la especificidad del anticuerpo.

Los genes que codifican las cadenas H y L se localizan en tres regiones cromosómicas no ligadas.
Los genes de la cadena L K se encuentran en el cromosoma 2 en la banda xxx , y los genes de la cadena L se hallan en el cromosoma 22 en la banda xxx ; por su parte, los genes de la cadena H, para los cinco isotipos, están en el cromosoma 14 en la banda xxx .
Múltiples segmentos muy separados en la línea germinal codifican cada cadena H y L.

Figura 14-4

Estructura básica de una molécula de inmunoglobulina que consta de dos cadenas ligeras idénticas y dos cadenas pesadas idénticas 

Cada cadena consta de una región variable (V) y una constante (C). xxx , extremo amino. xxx , extremo.

Los segmentos V y de unión (J) codifican el dominio de la región V de la cadena L, mientras que tres segmentos génicos codifican la región V de la cadena H: los segmentos xxx , y una tercera unidad, el segmento de diversidad (D) (fig. 14-5).
En total, cada conjunto de genes Ig se extiende muchos millones de pares de bases.

Durante la diferenciación del linfocito B, el DNA en los loci de la inmunoglobulina sufre reordenamiento somático. 

Para las cadenas lineras, un solo segmento V y uno solo J se yuxtaponen, con pérdida del DNA intermedio, para formar un gen completo de región variable.
Para las cadenas pesadas, un segmento D y otro J se yuxtaponen y después se combinan con un segmento V. As', como se muestra de forma esquemática en la figura 14-6, la generación de una cadena ligera K indica una recombinación que yuxtapone uno de los muchos segmentos VK a una de las regiones xxx .
Este segmento reordenado es entonces transcrito y las secuencias intermedias entre los segmentos xxx y xxx son eliminadas por medio de ensamblaje de RNA para desarrollar un mRNA maduro que generará por traducción una cadena ligera K específica.
La cadena ligera sufre un proceso de reordenamiento similar antes de la transcripción, pero presenta una organización de Imea germinal en cierto modo diferente; existen menos segmentos V, y los V y J se asocian cada uno con un segmento C" diferente (figs. 14-5 y 14-6).
Aún más diversidad puede generarse en el locus del gen de la cadena H debido al empleo adicional de uno de los múltiples segmentos D en la formación de la región variable de la cadena H (figura 14-5).

Tabla 14-4

Clases de inmunoglobulina y sus funciones

Además de las posibilidades de diversificación que proporciona la recombinación de múltiples segmentos génicos de la línea germinal, la mutación somática puntual en los genes de la región V proporciona otro importante mecanismo para la gran expansión del repertorio potencial de especificidades de anticuerpo.

Figura 14-5

Estructura de los conjuntos génicos de inmunoglobulina

Las regiones codificadoras y no codificadoras no aparecen a escala.
El número exacto de genes y el tamaño total de los conjuntos génicos se desconocen. 

Figura 14-6

Esquema del reordenamiento de los genes de las cadenas pesada y ligera de la inmunoglobulina en la formación de anticuerpo


Genética del comportamiento 

El estudio del cortejo y la cópula en la mosca de la fruta permite comprender la influencia de los genes en el despliegue de comportamientos complejos.

Las pistas iniciales sobre el funcionamiento de los mecanismos hereditarios se obtuvieron en los primeros quince años de nuestro siglo.

Fue mérito de una disciplina de reciente creación, la genética.
Los estudios sobre el color de las flores o la forma de las alas de la mosca de la fruta confirmaban las olvidadas teorías de Gregor Mendel, quien, en 1865, había propuesto que los rasgos físicos pasaban de padres a hijos mediante unidades independientes de material hereditario (o genes, según el nombre acuñado en 1911 para esas misteriosas unidades).
Como suele suceder cuando una disciplina emergente tiene sus primeros arrebatos de éxito, todo el mundo empezó a aplicar los nuevos conocimientos con criterios más generales, y a veces menos cuidadosos, para explicar otros fenómenos, sobre todo la conducta humana.
Llegó a proponerse incluso que los comportamientos complejos estaban dirigidos por un solo gen.

Sin embargo, ni siquiera los investigadores más tenaces han logrado hasta ahora poder vincular una conducta humana específica a un gen o un grupo restringido de genes.
Quizás el fracaso radique en el método.

Cuando se trata de la conducta humana, no hay forma de separar tajantemente lo debido a los genes de lo debido a la cultura y la educación.

Por otra parte, aun cuando los científicos anulasen los efectos del ambiente y se centraran exclusivamente en los aspectos genéticos de un comportamiento dado, podrían encontrarse con que las premisas de antaño eran incorrectas. 
Las investigaciones rigurosas, con organismos menos complejos, sugieren que, en la mayoría de los comportamientos, intervienen múltiples genes, algunos de los cuales actuarían de una forma muy sutil.

La cuestión sobre la heredabilidad de la conducta humana se planteó ya hace más de un siglo.
Francis Galton, pionero en la aplicación de la estadística, se ocupó del tema.
En los años ochenta, analizó diversos rasgos físicos y conductuales en padres e hijos.
Utilizando el «coeficiente de correlación» que acababa de idear, afirmó que las características del comportamiento eran heredables. 
Comparó la distribución de los rasgos en diferentes generaciones y llegó a la conclusión de que cada característica era el producto de múltiples aportaciones de material hereditario.

A principios de siglo, con el redescubrimiento de los trabajos de Mendel, ganó adeptos un punto de vista muy diferente. 
Charles B., Davenport, fundador del laboratorio de Cold Spring Harbor, y otros genéticos llegaron a atribuir a genes singulares características tan etéreas como el talento musical, el humor o la «debilidad mental». 
En 1921, por ejemplo, Davenport afirmó que «según se desprende de los abundantes árboles genealógicos analizados, la deficiencia mental de grado medio o superior se hereda a la manera de un carácter recesivo simple». 
A pesar de sus puntos de vista divergentes sobre los mecanismos hereditarios, Galton y Davenport extrajeron conclusiones similares, y arriesgadas, de sus observaciones.
Galton, creador del término «eugenesia», se convirtió en un ardiente defensor de los procesos de mejora de la raza humana mediante emparejamientos selectivos entre personas con rasgos favorables.
Davenport apoyó con vehemencia esa práctica.

En los años veinte se realizaron, con perros, algunos de los primeros experimentos diseñados para determinar la influencia de los genes en el comportamiento.
Entre otras características, se analizaban las conductas de «parada» (para mostrar la localización de una presa) y «canto» durante la caza.

Las razas caninas divergen en su comportamiento tanto como en su aspecto.
En los primeros ensayos se cruzaban perros que diferían en alguna peculiaridad del comportamiento; a continuación, se cruzaban sus descendientes entre sí. 
Si un carácter determinado del comportamiento estuviese controlado por uno o pocos genes, cabría esperar que los individuos de la última generación se agruparan en tres tipos:
un grupo cuyo comportamiento se pareciera mucho al de la madre, otro muy parecido al del padre y un tercer grupo de comportamiento intermedio.
Pero si intervinieran muchos genes, no habría en la descendencia grupos acotados, sino un amplio abanico de tipos de comportamiento muy diversos. 
Los resultados obtenidos se mostraron acordes con esta última pauta, prueba de que, tras cada carácter, se escondían varios genes.
Se obtuvieron resultados similares en experimentos realizados con ratas de laboratorio. 

Esos ensayos, aunque suministraban bastante información, tenían grandes limitaciones.
Con los experimentos de cruzamiento no podemos abordar la base genética de unas pautas de conducta que apenas si varían de un miembro a otro de la misma especie.
Para profundizar en este y otros problemas, había que identificar los genes implicados en la conducta.
Una tarea que no podía acometerse con las técnicas de entonces.

Descifrada en 1953 la estructura del ADN , en los años sesenta empezaron a caer los obstáculos técnicos que se oponían a la disección genética del comportamiento animal.
Los estudios con microorganismos revelaron que los genes cifran proteínas:
la activación de un gen da lugar a la síntesis de la proteína determinada.
Esta proteína, a su vez, desempeña alguna función necesaria para el organismo, como colaborar en la formación y operación del sistema nervioso (del cual depende, en última instancia, el comportamiento). 
Se dilucidaron también las etapas seguidas en la síntesis de proteínas y, en los años ochenta, se sentaron las bases para el desarrollo de herramientas que permiten aislar los genes uno por uno y establecer las funciones de sus correspondientes proteínas.

Seymour Benzer intervino de forma destacada en la demostración de que los genes son segmentos lineales de ADN .

A mediados de los sesenta había apostado también por no restringir los genes a los caracteres físicos. 
En una serie de ensayos con la mosca de la fruta ( Drosophila melanogaster ), comenzó a identificar genes que afectaban al comportamiento.
Esta tarea continúa en diversos centros, en particular en el laboratorio de Jeffrey C., Hall, uno de los primeros colaboradores de Benzer en este nuevo campo.
Nuestro grupo de la Universidad de Nueva York cultiva también esta línea de investigación, en la que me inicié con Hall hace unos veinte años. 

Una de las conductas que han recibido mayor atención es la que la mosca parece hacer mejor: el cortejo.
Este proceso consta de una serie de acciones, todas ellas acompañadas de un intercambio de señales visuales, acústicas y quimiosensoriales entre el macho y la hembra.
El macho, por ser el bailarín más activo en este complicado ballet, ha acaparado el interés de los estudiosos.

El ritual comienza con la fase de orientación.
El macho se coloca frente a la hembra, a unos 0,2 mm., de distancia, le golpea ligeramente el abdomen con una de las patas delanteras, y sigue tras ella si se retira.
Extiende luego un ala y la agita para ejecutar una suerte de «canto amoroso». 
Según el nivel de interés mostrado por la hembra en este punto, volverá o no a reiterar el proceso. 
Si todo marcha bien, despliega su probóscide (aparato bucal en forma de trompa) y lame los órganos genitales de la hembra.
A continuación la monta y, si está receptiva, copulan.
No hay apareamiento a menos que los machos hayan realizado el ritual completo y la hembra se haya mostrado receptiva.
La violación es poco frecuente en el mundo de las moscas de la fruta.

Como primer paso para encontrar los genes implicados en el cortejo, Hall se propuso identificar las partes del sistema nervioso central que controlan cada paso en el proceso de cortejo. 
Para ello, creó unas moscas extraordinarias, denominadas mosaicos genéticos, que portan, mezcladas, células del macho y de la hembra.

La técnica se basaba en la biología del desarrollo sexual de las moscas de la fruta.
En los embriones, el desarrollo sexual está controlado por el número de cromosomas X presente en cada célula.
Las células que tienen un solo cromosoma X determinan la formación de estructuras anatómicas masculinas y un comportamiento masculino en la mosca adulta.
Las células que portan dos cromosomas X determinan características femeninas, en su morfología y conducta.
Tales diferencias se deben a que las células con un único cromosoma X (machos) y dos cromosomas X (hembras) activan series distintas, aunque en ciertos puntos coincidentes, de genes implicados en la orientación sexual.

Hall sabía que, si las moscas formadas principalmente por células de hembra, pero con células de macho en determinados centros cerebrales, se comportaban como machos en alguna fase del cortejo, ello podría deberse a que en ese sitio del cerebro se estaba produciendo un patrón masculino de expresión génica.

Una vez constituidos los mosaicos, registraba el comportamiento de los portadores en el cortejo.
A continuación, congelaba las moscas (de 1,5 mm., de longitud) y las seccionaba en 20 finos cortes, observando en cada uno de ellos - merced a una ingeniosa técnica de tinción - la distribución de células procedentes del macho o de la hembra.
Un experimento exasperante en aquellos años setenta, si tenemos en cuenta que no había dos individuos con idéntica distribución de células del macho y de la hembra. 
Cada mosca debía sobrevivir a una batería de pruebas de comportamiento, y había que analizar las 20 secciones.

La exclusividad de cada individuo significaba que el experimentador carecía de una segunda oportunidad.

Tras examinar gran número de esos mosaicos, Hall llegó a la conclusión de que el comienzo del cortejo (orientación hacia la hembra, golpecitos en el abdomen, seguimiento y despliegue del ala) requería la presencia de células de macho en una región relativamente pequeña de la parte póstero-superior del cerebro; esa zona integra señales procedentes de diversos sistemas sensoriales de la mosca.
Es decir, las células de macho presentes en dicha zona desencadenan un mecanismo de cortejo propio de los machos, ausente en las hembras.
Las etapas posteriores del cortejo, en particular las que exigen una coordinación motora precisa, requieren la presencia de tejidos de macho en otros lugares del sistema nervioso.
Por ejemplo, para realizar un «canto» de cortejo adecuado, las moscas deben tener células masculinas en la zona de arranque antedicha y en otras partes del ganglio torácico (equivalente a la médula espinal de los vertebrados). 

Nuestro grupo ha identificado la región del cerebro implicada en la determinación de la preferencia sexual en la mosca de la fruta.
Nuestro descubrimiento se produjo de modo casual, cuando Jean François Ferveur creó estirpes de moscas con mosaico, principalmente machos, aunque portaban células de hembra en ciertas áreas del cerebro.
Antes de estudiar el comportamiento de cortejo de esos insectos, quisimos comprobar si unos machos perfectamente desarrollados tomaban por hembras a nuestros individuos con mosaico. 
No las reputaron como tales; mas, para nuestra sorpresa, algunas estirpes mostraban un comportamiento poco corriente: 
cortejaban por igual a machos y hembras.

El estudio de los cerebros de tales insectos insólitos, realizado en colaboración con Klemens F., Störtkuhl y Reinhard F., Stocker, de la Universidad de Friburgo, puso de manifiesto que la discriminación sexual se alteraba cuando poseía características femeninas alguno de los dos centros cerebrales siguientes: el lóbulo de la antena o el cuerpo pedunculado ( mush-room body ).
Ambas regiones, la segunda de las cuales está próxima a la zona de arranque del cortejo, participan en el procesamiento de las señales olfativas.
Si alguno de estos centros de análisis de los olores, o los dos, proceden de hembra, la mosca pierde su capacidad para distinguir entre machos y hembras, de modo que se muestra igualmente interesada por ambos sexos.

La implicación de esa pluralidad de regiones del sistema nervioso central del macho en el cortejo abona también la hipótesis de la participación de muchos genes en el proceso.
Se han descubierto ya más de una docena, principalmente en el laboratorio de Hall.
El gen fruitless , por ejemplo, condiciona la preferencia sexual.
Una mutación en el mismo afecta a los machos en idéntico sentido que si portasen células de hembra en el lóbulo de la antena o en el cuerpo pedunculado: 
cortejan a otros machos con tanto afán como a las hembras.
Este gen es también esencial para las últimas fases del cortejo, pues los machos que portan un gen mutado nunca intentan copular con las hembras.

El panorama que comienza a perfilarse es, por tanto, más congruente con la tesis de Galton que con la de Davenport.
Por extraño que parezca, ninguno de los genes implicados que se han identificado hasta la fecha limita su actuación al cortejo.
Existen cada vez más datos a favor de que la explicación acertada no coincide ni con la de Galton ni con la de Daveoport.
La mayoría de los genes que intervienen en el cortejo (y otras pautas de comportamiento) podrían cumplir más de una función en el organismo.
Y a su vez, machos y hembras pueden apoyarse en los mismos genes para fines diferentes.

Veamos un ejemplo.
Se conocen tres genes que intervienen en el canto de cortejo del macho.
Uno de ellos, period , lo han estudiado Hall y Charalambos P., Kyriacou, quienes decidieron examinarlo tras descubrir, en 198O, que el canto del macho tenía un ritmo característico. 
Por las investigaciones de Ronald J., Konopka, sabían ya que ese gen respondía de los ritmos circadianos de D. melanogaster :
ciclos repetitivos, como el de sueño y vigilia, que son característicos de todos los seres vivos.

¿Intervendría también period en el ritmo del canto de cortejo?
El sonido emitido al batir las alas resulta poco musical a nuestros oídos, pero sigue un patrón detectable.
Cuando el insecto sube y baja el ala una vez, el movimiento produce un sonido característico, o pulso, que puede registrarse con un aparato adecuado.
Durante unos 27 segundos, el macho va dilatando gradualmente el intervalo entre pulsos.
Luego, durante otros 27 o 28 segundos, acorta, también de forma paulatina, el intervalo. 
Y así, cuando se representan los intervalos en función del tiempo se obtiene una curva sinusoidal regular. 

Hall y Kyriacou observaron que los machos que portan un gen period normal emiten un sonido rítmico, que despierta una mayor receptividad de las hembras.
Los machos con un gen period inactivo, en cambio, generan un sonido sin rítmica, menos eficaz, por lo que se ve, a la hora de estimular a las hembras.
Si exponían hembras solas, unas a sonidos normales y otras a sonidos arrítmicos, simulados ambos por ordenador, y se las mezclaba luego con machos, las hembras sujetas a los sonidos aberrantes se mostraban menos receptivas ante las insinuaciones de aquellos.
Algunas mutaciones menos drásticas en ese gen mantienen el ritmo, pero alargan o estrechan la sinusoide, y merman también la eficacia del canto.

La finura del efecto del gen period sobre el despliegue del cortejo, y sobre el propio canto, añade crédito a la idea según la cual el cortejo, y otras pautas complejas de comportamiento, está regulado por muchos genes que actúan de forma conjunta.
Además, el hecho de que el gen period participe en la regulación de otros relojes biológicos y se exprese en diferentes partes del sistema nervioso central, apoya la tesis de que cualquier gen considerado podría condicionar más de un tipo de comportamiento.

Hall, Kyriacou y Michael Rosbash acaban de acotar la porción exacta del gen que controla el ritmo del canto.
Se trata de una pequeña región de la parte central; el resto del gen controla otros ritmos. 
Se llegó a esa división del trabajo a través, en buena medida, de otra mosca, de la especie D. simulans , que tiene el mismo ciclo de 24 horas de actividad y descanso que D. melanogaster ; aquélla, sin embargo, ejecuta un canto que difiere en los intervalos entre pulsos.
El gen period de ambas especies es similar, salvo unas pequeñas diferencias en la región central.
Mediante manipulación por ingeniería genética se han obtenido moscas que portan un gen period híbrido, resultante de reemplazar la región central del gen de D. melanogaster por el correspondiente segmento de D. simulans ; los machos híbridos cantan como los de D. simulans .

Aunque la preferencia sexual y el cortejo están programados en la mosca de la fruta, machos y hembras tienen la posibilidad de modular su actividad en respuesta a una reacción del otro; pueden, pues, aprender.
Y así como la capacidad para realizar el cortejo está dirigida por genes, también lo está la capacidad para aprender de la experiencia.
Los estudios de este fenómeno refuerzan aún más la idea de que el comportamiento está regulado por la interacción de una miríada de genes, cada uno de los cuales desempeña diversas funciones en el organismo.

Una de las cosas que el macho aprende durante el cortejo es a no perder el tiempo con una hembra ya montada y, en consecuencia, no receptiva.
Hall y Richard W., Siegel, de la Universidad de California en Los Angeles, observaron que los machos cortejan tenazmente las hembras vírgenes, pero pierden el interés por las hembras ya montadas después de 30 minutos o una hora, cuando por fin quedan impregnados por una feromona inhibidora emitida por las futuras madres.
Una vez que los machos renuncian a la persecución, dejan de interesarse por cualquier hembra, virgen o no, durante algunas horas.
Desde el punto de vista de la lógica evolutiva, ello podría atribuirse a que la presencia de una hembra montada en un grupo de hembras señale que la mayoría de ellas, si no todas, también lo han sido ya, por lo que el macho hará bien en invertir sus energías en otros gineceos.

Mis escarceos en las bases genéticas de esta respuesta comenzaron hace unos años, en colaboración con Leslie C., Griffith.
Se sabía ya que la quinasa II dependiente de calcio y calmodulina ( CaMKII ) contribuía a registrar los efectos de la experiencia en las neuronas, induciendo cambios moleculares de probable importancia decisiva para el aprendizaje. 
Por ello, decidimos comprobar si los machos de la mosca de la fruta necesitaban esa enzima y, por tanto, su correspondiente gen, para responder adecuadamente a las hembras ya montadas. 

Como primer paso, Griffith creó una estirpe de mosca cuya proteína CaMKII podía silenciarse (inactivarse) con sólo elevar la temperatura corporal.
Y observamos que, cuando la actividad de la enzima remitía aunque fuera ligeramente, los machos de esa estirpe se comportaban de una manera extraña.
Durante el cortejo, se mostraban tan ávidos de hembras vírgenes como los machos normales, y perdían el interés por las hembras montadas tras la hora acostumbrada pero parecían olvidar su rechazo casi inmediatamente. 
Si se colocaban junto a otras hembras poco después de haber estado con una ya montada, reiniciaban su persecución. 
Cuando inhibíamos aún más la proteína, los machos no aprendían nada en absoluto:
perseguían sin tregua, durante horas, a las hembras ya montadas.

Una vez demostrado que el gen CaMKII participaba, a través de su enzima correspondiente, en el aprendizaje durante el cortejo, había que averiguar el modo en que colabora la enzima en el registro de la experiencia. 
Todas las quinasas actúan fosforilando otras moléculas; les añaden grupos fosfato, que provocan la activación o inactivación de la molécula diana.

Ahora bien,
¿cuál era la diana de la quinasa en las neuronas?
¿Qué sucedía una vez fosforilada?
Esas preguntas nos llevaron finalmente a demostrar que un segundo gen expresado en las neuronas, el gen eag , opera también en el proceso de aprendizaje.

El producto proteínico del gen eag es un componente de ciertos canales de membrana que regulan el flujo de iones potasio hacia el exterior de las neuronas. 
La apertura de esos canales ayuda a controlar la excitación y la liberación de neurotransmisores, moléculas encargadas de llevar mensajes de una célula a otra.
El nombre de este gen deriva de un fenómeno observado ya en los años sesenta; cuando se anestesiaban las moscas portadoras de genes eag mutados, movían las patas; en la onda de la moda del tiempo, los descubridores bautizaron el gen con la denominación «éter a gogó» .

Por pistas que habíamos ido recogiendo, y por otras que Jing Wang y Chun-Fang Wu habían encontrado, Griffith y yo comenzamos a pensar que la enzima CaMKII podría participar en el aprendizaje modificando la proteína EAG en los canales de potasio.
A este respecto, el grupo de Eric R. Kandel había demostrado en el molusco Aplysia que, durante un proceso de aprendizaje elemental, una quinasa modificaba ciertos canales de potasio.
Por nuestro lado, hallamos que algunas formas mutantes de la proteína EAG exhibían el mismo comportamiento «embotado» de los machos durante el cortejo que se observaba cuando se impedía la acción de la enzima CaMKII .
Tales datos nos indicaban que las dos proteínas operaban en la misma cascada de interacciones moleculares y que la CaMKII actuaba sobre la EAG .

Para satisfacción nuestra, Griffith confirmó que la enzima modificaba la proteína EAG , in vitro por lo menos. 
A partir de tales hallazgos y de la información extraída de los registros eléctricos de las sinapsis de animales mutantes, nos sentimos autorizados para proponer una secuencia hipotética de reacciones moleculares que explicaría el aprendizaje de renuncia a las hembras ya montadas.

En primer lugar, la exposición de los machos a feromonas antiafrodisíacas, mientras están cortejando a hembras ya montadas, estimula unos sistemas sensoriales que llegan a la zona de arranque del cerebro.
En virtud de esa estimulación, se produce un aumento local de la concentración de calcio en las células que promueven el apetito sexual durante el cortejo.
Ese incremento activa la proteína CaMKII , que, a su vez, fosforila la proteína EAG en los canales de potasio que portan el polipéptido.
Tales modificaciones inducen la apertura del canal, que favorece el flujo de potasio hacia el exterior de las neuronas, proceso que desactiva las células y reduce su capacidad para liberar neurotransmisores.
Este apaciguamiento de las células provoca el desinterés de los machos por la cópula.

Al parecer, las moscas que portan defectos en los genes responsables de cualquiera de esas proteínas mantienen el interés por las hembras ya montadas porque los canales de potasio permanecen cerrados en las células críticas, posibilitando que las neuronas se tornen hiperactivas.

Los genes CaMKII y eag son sólo dos de los varios genes conocidos que intervienen en el aprendizaje y la memoria de D. melanogaster .
Otros participan también en el cortejo, lo que encaja con la idea de que los comportamientos dependen de la interacción de amplias redes de genes, la mayoría de los cuales participan en múltiples aspectos de la biología de un organismo.

¿Tienen alguna consecuencia para el hombre estas lecciones extraídas de los estudios genéticos con moscas?
En mi opinión, sí, dentro de ciertos límites.
Existen razones fundadas para suponer que las influencias genéticas sobre el comportamiento son, en las personas, tanto o más complicadas que en las moscas.

Podría, pues, aplicárseles la hipótesis de muchos genes multifuncionales, cada uno de los cuales aporta su minúsculo grano de arena.
Probablemente, muchos de los productos génicos que funcionan en los cerebros de las moscas revestirán parejo interés en el cerebro humano.
Ya se han descubierto los equivalentes humanos de varios genes identificados en la mosca, como el eag.
A medida que se vayan acotando otros, conoceremos mejor las interacciones moleculares en que se basa el sistema nervioso central para inducir este o aquel comportamiento.

Teorías precursoras sobre el comportamiento humano

Durante el primer cuarto del siglo XX, se enfrentaron dos puntos de vista antagónicos sobre la unicidad o multiplicidad de genes implicados en la determinación del comportamiento. 
A finales del siglo pasado, Francis Galton, uno de los padres de la estadística, afirmaba que los rasgos humanos, la manera de comportarse incluida, estaban controlados por multitud de esas unidades hereditarias que, andando el tiempo, se llamarían genes.
En cambio, Charles B., Davenport, genético de reconocido prestigio, habría de sostener más tarde que cada carácter estaba controlado por un gen. 
En nuestros días, los estudios sobre moscas de la fruta y otros animales ofrecen una visión del problema más próxima a las ideas de Galton.
La fama que conservan hoy Galton y Davenpoort obedece, sin embargo, a su vehemente defensa de la eugenesia, que en palabras del propio Galton significa controlar la «natalidad de los ineptos» y mejorar la raza humana «favoreciendo la reproducción de los más aptos».
Galtonintrodujo este término a finales del siglo XIX.
Davenport fundó un centro de investigación sobre eugenesia humana en el laboratorio de Cold Spring Harbor.

Figura 1

EL MACHO CORTEJA a una mosca hembra ejecutando una secuencia programada de actos

En las primeras etapas, se orienta hacia la hembra. 

(1) cuyo abdomen golpea con una pata delantera 
(2) a continuación, despliega un ala y la hace vibrar para producir un «canto amoroso»
(3) acto seguido, lame los órganos genitales de su pareja
(4) intenta montarla
(5) y, finalmente, copula con ella
(6) el análisis de esta secuencia sugiere que la contribución genética al comportamiento es sutil 

Figura 2

CENTROS DEL SISTEMA NERVIOSO CENTRAL que controlan las etapas del cortejo en los machos de la mosca de la fruta

Estos centros se han cartografiado utilizando moscas constituidas con mezcla de células procedentes de macho y de hembra.
Para realizar las primeras etapas del cortejo (orientación, toque y extensión del ala) y para seguir a las hembras, las moscas deben tener células de macho en una pequeña «zona de arranque», situada en la parte posterior del cerebro.
También necesitan células de macho en un sitio próximo para el acto de lamer, en determinada zona del ganglio torácico para producir sus sonidos y en muchas secciones diferentes del ganglio torácico para copular.

Figura 3

EL CENTRO DE ATRACCION por las hembras reside en dos sitios del cerebro de la mosca macho (representación esquemática): el lóbulo de la antena; el otro es el cuerpo pedunculado, situado junto a la zona de arranque responsable del comienzo del cortejo

Se descubrió la importancia de esas regiones al ver que ciertas estirpes de machos, genéticamente manipuladas para que presentaran células de hembra en cualquiera de esos sitios, cortejaban por igual a machos y hembras.

Figura 4

EL MACHO DESPLIEGA UN ALA antes de hacerla vibrar para producir un sonido o «canto» particular

En el «canto» normal, los intervalos entre pulsos aumentan gradualmente durante unos 27 segundos y después se acortan, poco a poco también.
La representación de los intervalos produce una curva sinusoidal, cuyo ritmo está controlado por el gen period .
Las moscas que portan un gen period normal emiten un canto normal; las que portan un gen ligeramente defectuoso producen ritmos anormales, y las que tienen un gen inactivo producen un canto arrítmico.


INTRODUCCIÓN

La presente es una época especial para la genética médica.
Ésta ha alcanzado un papel reconocido como disciplina central que se ocupa de la variabilidad y herencia humanas y, al mismo tiempo, ha desarrollado métodos que permiten nuevos enfoques de muchas enfermedades y promete brindar mucho más en un futuro inmediato.
Para proporcionar a los pacientes y sus familias todo el beneficio de la expansión del conocimiento genético se requiere que los médicos y sus colegas de las áreas de la salud entiendan los principales conceptos de la genética humana, y el papel de los genes y del ambiente en el desarrollo normal y anormal, así como en la enfermedad.

PAPEL DE LA GENÉTICA EN MEDICINA 

El lugar de la genética dentro de la medicina no siempre resultó tan obvio como lo es hoy.
A pesar de que su trascendencia, tanto para la base conceptual de la medicina como para la práctica clínica, ahora se reconoce por completo, no hace muchos años se creía que la genética se relacionaba con la herencia de características insignificantes, superficiales y raras, y no se había comprendido el papel fundamental del gen en los procesos básicos de la vida.
El descubrimiento de los principios de la herencia, que realizó el monje austríaco Gregor Mendel en 1865, de hecho no recibió apenas reconocimiento por parte de los biólogos ni reconocimiento alguno de otros científicos.

Por el contrario, sus informes pasaron inadvertidos en la literatura científica durante 35 años.
Charles Darwin, cuyo gran libro El origen de las especies (publicado en 1859) subrayaba la naturaleza hereditaria de la variabilidad entre los miembros de una especie como factor importante en la evolución, no sabía, sin embargo, cómo operaba la herencia.
En aquel momento se pensaba que la herencia implicaba mezcla de características de ambos padres, y aún se aceptaba la idea de Jean Baptiste Lamarck con respecto a la herencia de características adquiridas.
El trabajo de Mendel podría haber clarificado el concepto de Darwin acerca del mecanismo de la herencia de la variabilidad, pero aunque Mendel conoció las ideas de Darwin, no parece que este último conociese el significado del trabajo del austríaco o acaso ni siquiera supo de su existencia.
Francis Galton, primo de Darwin y uno de los titanes de la incipiente genética médica, también permaneció ignorante de los hallazgos de Mendel, a pesar de la relevancia de éstos en cuanto a sus propios estudios sobre «naturaleza y crianza».
El propio Mendel, quizá desalentado por los resultados de experimentos ulteriores menos favorablemente diseñados, finalmente abandonó la investigación experimental, si bien su interés por la ciencia biológica permaneció íntegro a lo largo de su vida.

Las leyes de Mendel, que constituyen la piedra angular de la ciencia genética, se derivaron de sus experimentos con guisantes de jardín.
Mendelcruzó líneas puras que diferían en una o más características bien definidas y observó la progenie de los apareamientos al menos durante dos generaciones.
Las leyes que extrajo de los resultados de sus experimentos pueden exponerse como sigue: 

Herencia unitaria .
Mendel demostró con gran claridad que no ocurre mezcla de características de los padres, sino que, aunque éstas no aparezcan en la primera generación de la progenie, los rasgos de los padres pueden reaparecer sin cambios en una generación ulterior.
La enseñanza moderna de la genética recalca poco esta ley, pero en la época de Mendel este concepto era enteramente nuevo.
El término gen para denotar la unidad de la herencia no se utilizó hasta 1909, cuando lo introdujo Wilhelm Johannsen.

Segregación.
Los dos miembros de un solo par genético, lo que hoy se conoce como alelos , no se hallan nunca en el mismo gameto, sino que siempre se separan y pasan a diferentes gametos.
En circunstancias excepcionales, cuando esta regla se rompe y los miembros de un par cromosómico no se segregan normalmente, la consecuencia característica es anormalidad grave en la progenie.

Distribución independiente .
Los miembros de diferentes pares genéticos se distribuyen en los gametos de manera independiente unos de otros.
En otras palabras, se produce una recombinación aleatoria de los cromosomas paterno y materno en los gametos. 
Como después se describirá, existe una importante excepción a esta regla, que Mendel no reconoció: 
los genes estrechamente ligados en un mismo cromosoma no se distribuyen de forma independiente, sino que tienden a permanecer juntos generación tras generación.

A comienzos del siglo XX, la mayoría de los biólogos estaban preparados para ponerse al corriente de las ideas de Mendel.
Debido a una curiosa coincidencia, tres estudiosos (Hugo De Vries en Holanda, Carl Correns en Alemania y Erick von Tschermak en Austria) redescubrieron las leyes de Mendel de modo simultáneo e independiente.

El desarrollo de la genética como ciencia no se remonta a los escritos de Mendel, sino a aquellos que comunicaron el redescubrimiento.

Pronto se reconoció la naturaleza universal de las leyes de Mendel.
Ya en 1902, Archibald Garrod, quien comparte con Francis Galton el título de fundador de la genética médica, pudo comunicar la alcaptonuria como el primer ejemplo en seres humanos de lo que hoy se conoce como herencia mendeliana.
En su articulo, Garrod reconoció generosamente su deuda con el biólogo William Bateson, quien observó el significado genético de la consanguinidad entre los padres de algunos enfermos que sufrían lo que Garrod llamó «errores congénitos del metabolismo».
De igual manera, Bateson identificó que, si más de un miembro de la familia estaba afectado, los otros sujetos enfermos eran casi siempre hermanos del paciente estudiado, no sus padres, hijos ni otro tipo de parientes.
Batesonreconoció este patrón, como podría esperarse si la alcaptonuria se heredara como un carácter recesivo de acuerdo con las leyes de Mendel.
Ésta es la primera evidencia clara en la investigación del modo en que interactúan los genetistas médicos y los no médicos.
Tal interacción ha continuado hasta el presente y ha contribuido en gran medida a la rápida expansión de este campo.
El hallazgo de Bateson precedió a la introducción del término genética para denominar la nueva ciencia, término también acuñado por Bateson en 1906.

El entendimiento creciente de la naturaleza universal de la estructura y función biológicas de los organismos vivos llevó de modo inevitable al reconocimiento del papel fundamental de los genes en los procesos vitales.
El concepto, que se insinuó en los trabajos de Garrod, fue formulado de manera clara por Beadle y Tatum, en 1941, como la hipótesis de «un gen-una enzima», la cual ahora se conoce más comúnmente como «un gen-una proteína». 
En la actualidad, la genética de la bioquímica humana constituye uno de los temas dominantes dentro de la genética médica.

A principios de los años 40 se produjeron rápidos progresos en el análisis molecular del material genético, comenzando con el descubrimiento de que los genes se componían de ácido desoxirribonucleico (DNA).
En 1953, James Watson y Francis Crick describieron la estructura molecular del DNA, por lo cual obtuvieron el Premio Nobel en 1962.
Otros descubrimientos destacados de los primeros días de la genética molecular incluyen el reconocimiento de que el DNA de los genes se transcribe en ácido ribonucleico (RNA), el cual a su vez se traduce en proteína, y la dilucidación del código genético por el cual la secuencia de aminoácidos de las proteínas está registrada en el DNA.

A finales de los años 50 se desarrollaron técnicas para el estudio científico de los cromosomas humanos y los investigadores comenzaron a explorar el papel de éstos en el desarrollo sexual, así como el papel de las anomalías cromosómicas como causa de alteración en el desarrollo físico y mental de problemas reproductivos.
Aunque la citogenética del ser humano, el estudio de los cromosomas humanos, ha sido un área de intensa investigación durante muchos años, todavía existen muchos problemas desafiantes.

Continúan comunicándose nuevos hallazgos significativos acerca de la estructura cromosómica normal y anormal, especialmente ahora que el análisis de cromosomas avanza en estrecha relación con el análisis molecular en el estudio del genoma humano.

Desde mediados de los años 70, con la ayuda de nuevas y poderosas tecnologías para la manipulación y análisis del DNA, el campo de la genética médica se ha transformado.

Hoy en día, los científicos son capaces de localizar e identificar genes que generan proteínas humanas esenciales, caracterizar sus mutaciones y captar la naturaleza de sus productos proteicas, alcanzando así un entendimiento más profundo de muchas causas de enfermedad, las cuales se comprendían poco o se ignoraban en el pasado.
Debido a que el enfoque molecular resultó muy diferente con respecto a otros previos, y a su enorme potencial, se denominó «la nueva genética».
En fecha más reciente.
Los términos genética inversa y clonación posicional se han empleado para denominar la clonación de genes de función desconocida mediante técnicas de mapeo génico.

Algunos de los muchos ejemplos del estudio mediante genética molecular de las enfermedades humanas se describen en capítulos ulteriores.

Durante muchos años, si bien había gran interés en el mapeo de genes humanos para su localización cromosómica, el progreso fue relativamente lento.
La dificultad más importante era la inconveniencia de usar seres humanos para el mapeo génico, debido al pequeño tamaño de las familias. el largo tiempo de generación y la imposibilidad de realizar las uniones planeadas que sí era posible emplear con organismos experimentales.
Sin embargo, por medio del análisis citogenético y de genética molecular, junto con estudios familiares, el mapeo génico humano se ha convertido en un área de vanguardia en la investigación genética del ser humano.
El Proyecto Genoma Humano, un proyecto internacional que se propone llevar a cabo el mapeo del genoma humano completo antes del año 2005, con seguridad constituye uno de los mayores programas de investigación genética y se espera que proporcione importantes beneficios médicos (Watson, 1990).

La expansión y aplicación del conocimiento genético ya ha generado fructíferas consecuencias para la medicina clínica.

Hoy en día, por lo menos una tercera parte de los pacientes en hospitales pediátricos se encuentran allí debido a trastornos genéticos.
Ello constituye un gran cambio con respecto a los primeros años de este siglo e incluso al período previo a la era de los antibióticos. 
Antes de la época de la inmunización, la mejora de la nutrición y los antibióticos, la mayoría de los niños hospitalizados presentaba enfermedades infecciosas o trastornos nutricionales como raquitismo.
En la actualidad se sabe que algunos de los niños con infecciones tienen defectos genéticos que deterioran su resistencia y, al menos en países desarrollados, la mayor parte de los casos de raquitismo no se deben a desnutrición, sino a genes deletéreos.
Los avances de la medicina clínica y quirúrgica incrementan la probabilidad de supervivencia y, por lo tanto, tienden a elevar la prevalencia de defectos genéticos.

Disciplinas de la genética médica y humana

La genética constituye una materia diversa que se ocupa de la variación y la herencia de todos los organismos vivos.
Dentro de este amplio campo, la genética humana es la ciencia de la variación y la herencia en los seres humanos, y la genética médica estudia la variación genética humana de significado médico. 
Las dos áreas se superponen de manera extensa y, de hecho, para muchos genetistas son la misma.

Dentro de la genética médica y humana existen muchos campos de interés, como indican las diversas direcciones en que se ha desarrollado la genética.
Las áreas de especialización más importantes son el estudio de cromosomas (citogenética), el estudio de la estructura y función de genes individuales (genética molecular y bioquímica), y la aplicación al diagnóstico y cuidado del enfermo (genética clínica).
El significado literal del término clínica es junto a la cama (del griego klinikos ), y un genetista clínico es un médico genetista apropiadamente cualificado y directamente implicado en el diagnóstico de enfermedades genéticas y en el cuidado de los individuos que las sufren.

Otros campos de la genética humana, como la genética poblacional, la epidemiología genética, la genética del desarrollo y la inmunogenética, también tienen relevancia médica, en especial en relación con la comprensión y prevención de la enfermedad humana. 

Además de emplearse en la práctica clínica, la genética médica se aplica al cuidado de enfermos en las áreas de consejo genético, detección poblacional para identificar a individuos con riesgo de desarrollar o transmitir un trastorno genético, y diagnóstico prenatal.
El consejo genético, que combina suministro de información de riesgo con función de apoyo, se está convirtiendo en una nueva profesión sanitaria. 
La detección poblacional de enfermedades genéticas ha llegado a ser una importante iniciativa de salud pública. 
El diagnóstico prenatal, que utiliza muchas especialidades clínicas y de laboratorio, además de la genética, es hoy día probablemente el área principal en la que la genética se aplica al cuidado del paciente.

Aunque la genética médica se ha asociado de modo muy estrecho con la pediatría, también resulta relevante para muchas otras especialidades médicas. 
En obstetricia, el diagnóstico prenatal de ciertos defectos genéticos se ha convertido en un componente estándar de la atención prenatal.
En la medicina de adultos se reconoce que muchos trastornos frecuentes, como la enfermedad de las arterias coronarias, la hipertensión y la diabetes mellitus, poseen importantes componentes genéticos; asimismo se considera que la medicina preventiva puede ser mucho más eficaz si se dirige hacia grupos de alto riesgo genéticamente definidos, más que hacia la población en general. 
Las aplicaciones en muchas otras áreas, en particular neurología, hematología y oncología, se revisan en capítulos ulteriores.

Si bien gran parte de la genética médica se halla dentro de la rama principal de la medicina, la genética se diferencia de otras especialidades médicas en que a menudo se orienta a la prevención, así como al tratamiento, y en que su atención con frecuencia se dirige no sólo al paciente, sino a toda la familia.

Principios éticos en genética médica

Los principios de la ética médica se aplican tanto a la genética como a otras disciplinas médicas, pero algunos de los dilemas éticos que pueden surgir en la genética médica rara vez se plantean, si es que aparecen, en otras ramas de la práctica clínica. 
Se crea un tipo de problema por la habilidad creciente de los médicos para realizar pruebas presintomáticas-esto es, identificar un gen que provoca enfermedad en un sujeto antes de que el trastorno se evidencie clínicamente-.
Ahora, los médicos también son capaces de reconocer ciertas predisposiciones genéticas que se asocian con riesgo adicional de desarrollar una enfermedad particular; por ejemplo, saben que individuos con deficiencia hereditaria de A1-antitripsina (tabla 1-1)presentan un riesgo incrementado de enfermedad pulmonar, lo cual se acentúa especialmente en ambientes con humo. 

La enfermedad de Huntington (EH), que también aparece en la tabla 1-1, es un trastorno neuropsiquiátrico que se inicia en la mitad de la vida y se hereda de un progenitor afectado con carácter autosómico dominante.
En muchos casos, los genetistas son capaces de determinar, mucho antes de que la entidad se manifieste, si el familiar de un sujeto con EH ha heredado el gen.
Como se describe en los capítulos 4 y 8, la prueba requiere análisis de marcadores de DNA cercanos al gen de HD, en muestras de sangre de miembros de la familia.
Sin embargo, si ha de realizarse la prueba presintomática, el paciente debe proporcionar su consentimiento para que se informe a sus parientes acerca del diagnóstico y de las implicaciones para ellos y sus familias; asimismo, los familiares han de estar de acuerdo en brindar muestras de sangre para análisis. 
Cada miembro tiene derecho a decidir de manera informada si participa o no en el estudio, sin coerción y después de que el médico o consejero genético le informe de modo completo acerca de las implicaciones de su decisión. 
Los participantes tienen derecho a no saber si se hallan en riesgo y, en ciertas circunstancias, esto entra en conflicto con el derecho de otros miembros de la familia a poseer tal información. 
La confidencialidad, principio cardinal de la ética médica, puede resultar difícil de mantener, en especial cuando se trata de pruebas presintomáticas en personas emparentadas.

A los miembros de la familia que han acordado participar en la entrega de muestras de sangre quizá les resulte difícil entender por qué no tienen derecho a conocer los resultados de la prueba de los otros.
Más aún, es posible que las pruebas proporcionen información inesperada; por ejemplo, pueden revelar casos de paternidad falsa.
La identificación de paternidad falsa implica la pregunta adicional de si el sujeto que brindó la muestra quiere recibir dicha información, lo cual subraya el hecho de que una eventualidad de este tipo tenga que resolverse de antemano y se considere en el documento de consentimiento que cada participante debe firmar.
Estos breves ejemplos ilustran algunas de las dificultades que se encuentran en las pruebas de predicción de enfermedad genética (Huggins y cols., 1990).

CLASIFICACIÓN DE LOS TRASTORNOS GENÉTICOS

En la práctica clínica, la significación principal de la genética se relaciona con su papel en la identificación de la causalidad de un gran número de trastornos.
Prácticamente, cualquier enfermedad es el resultado de la acción combinada de genes y ambiente, pero el papel relativo del componente genético puede ser grande o pequeño.

Entre los trastornos que se deben total o parcialmente a factores genéticos se reconocen tres tipos principales: 

1. Trastornos monogénicos.
2. Trastornos cromosómicos.
3. Trastornos multifactoriales.

Los trastornos monogénicos son provocados por genes mutantes.
La mutación puede estar presente en un solo cromosoma de un par (con un alelo normal en el cromosoma homólogo) o en ambos cromosomas del par.
En cada caso, la causa es un error único en la información genética.

Por lo regular, dichos trastornos muestran patrones genealógicos obvios y característicos.
La mayor parte de tales defectos son raros, con una frecuencia que puede ser de hasta I en 500, aunque comúnmente es menor (tabla 1-1).
No obstante, existen muchas clases diferentes.
Se han descrito cerca de 4.000 fenotipos monogénicos, de los cuales 3.000 constituyen trastornos genéticos (McKusick,1990), y su impacto conjunto es significativo.
En centenares de estas enfermedades se ha reconocido el defecto bioquímico básico y, en muchas, se ha aislado y clonado el gen responsable (Beaudet y cols., 1989).

Hoy se reconoce el papel de los genes mitocondriales, como el de los nucleares, como causa de enfermedades, y en muchos trastornos se ha identificado un defecto en un gen mitocondrial, incluyendo la neuropatía óptica hereditaria de Leber.
Por consiguiente, las entidades transmitidas mitocondrialmente deben reconocerse como un tipo especial de trastorno monogénico. 

Tabla 1-1

Ejemplos seleccionados de trastornos genéticos destacados

Transtorno.
Características.
Incidencia.
Capítulos.


Trastornos monogénicos.


Deficiencia de adenosindesaminasa.
Defecto enzimático autosómico recesivo que provoca inmunodeficiencia; primera alteración aprobada para terapia genética humana.


Deficiencia de A1-antitripsina.
Deficiencia autosómica recesiva de un inhibidor de la proteasa plasmática, que provoca una enfermedad pulmonar obstructiva en homocigotos (más grave en fumadores) y enfermedad hepática en algunos niños afectados.


Fibrosis quística.
Trastorno autosómico recesivo frecuente que afecta la regulación del transporte de iones a través de la membrana en células exocrinas; primer gen identificado sin conocimiento previo del producto génico o de la alteración cromosómica estructural.


Distrofia muscular de Duchenne.
Frecuente trastorno muscular recesivo ligado al X; el gen se identificó por la alteración cromosómica estructural sin conocimiento previo del producto proteico; el gen es muy grande y, por ello, resulta una diana frecuente de la mutación.


Hipercolesterolemia familiar.
Defecto autosómico dominante del receptor de lipoproteína plasmática de baja densidad (LDL); se asocia con una frecuencia incrementada de enfermedad de las arterias coronarias.


Síndrome del X frágil.
Forma común de retraso mental ligado al X; se relaciona con un lugar frágil en el cromosoma X; características genéticas singulares; un tercio de las mujeres portadoras presentan retraso moderado.


Deficiencia de glucosa 6-fosfato deshidrogenasa.
Trastorno farmacogenético ligado al X; posee muchas variantes, pero dos son frecuentes: una en el Mediterráneo y otra en África; los heterocigotos muestran, como ventaja, resistencia a la malaria.


Hemofilia A.
Frecuente defecto de coagulación ligado al X.


Enfermedad de Huntington.
Enfermedad neurodegenerativa autosómica dominante, de comienzo tardío; puede pronosticarse mediante pruebas moleculares en la etapa presintomática; primer gen que se localizó en un cromosoma por medio de análisis de ligamiento de DNA.


Distrofia miotónica.
Trastorno autosómico dominante de inicio y gravedad variables; efecto materno: los pacientes con patología congénita tienen siempre madres afectadas.


Neurofibromatosis tipo 1.
Trastorno autosómico dominante común con expresión variable, tasa elevada de mutación, riesgo de neoplasia, alta proporción de nuevas mutaciones (50 %); gen muy grande.


Osteogénesis imperfecta.
Grupo heterogéneo de defectos moleculares del colágeno; son los prototipos de defectos en proteínas estructurales.


Fenilcetonuria .
Deficiencia autosómica recesiva de la hidroxilasa de la fenilalanina, que puede causar retraso mental; es prototipo de las enfermedades que se detectan por cribado neonatal; el retraso se previene con tratamiento nutricional inmediato.


Retinoblastoma.
Neoplasia embrionaria de la retina, que se asocia con mutación o deleción de un par de alelos en el cromosoma 13; existen formas tanto hereditarias como esporádicas.


Drepanocitemia .
Defecto en un gen de la cadena p-globina de la hemoglobina; fue la primera enfermedad molecular que se identificó; es común en África; tiene una frecuencia elevada debido a que los heterocigotos presentan resistencia a la malaria .


Enfermedad de Tay-Sachs.
Deficiencia autosómica recesiva de hexosaminidasa A; muy común en la población de judíos ashkenazis [1] .


Talasemia.
Numerosos defectos génicos de la cadena de a-globina y B-globina de la hemoglobina, lo que produce desequilibrio en las cantidades relativas de las cadenas; muy frecuente en el Mediterráneo y en el sur de Asia.


Tumor de Wilms.
Tumor renal de la infancia que se relaciona con una delación en la región 11p13; quizá sea parte de un síndrome de genes contiguos con aniridia, trastornos genitourinarios y retraso mental (síndrome WAGR);un alelo normal participa en el desarrollo del aparato urogenital.


Trastornos citogenéticos .


Síndrome de Down.
Trisomia 21, la causa más frecuente de retraso mental moderado; el riesgo se relaciona con edad avanzada de la madre .


Trisomía 18.
Autosoma extra con fenotipo característico de malformaciones congénitas múltiples y graves en niños nacidos vivos.


Trisomía 13.
Autosoma extra con fenotipo característico de malformaciones congénitas múltiples y graves en niños nacidos vivos.


Síndrome de Klinefelter.
Fenotipo masculino y anomalías características, generalmente asociados con cariotipo 47, XXY.


Síndrome de Turner.
Fenotipo femenino y anomalías características, generalmente asociados con cariotipo 45,X.


Síndrome XXX.
Cariotipo anormal frecuente; puede estar asociado con dificultades para el aprendizaje e infertilidad; a menudo fenotípicamente normal.


Síndrome XYY.
Cariotipo anormal frecuente; puede estar asociado con trastornos de la conducta; a menudo fenotípicamente normal.


Síndrome de Prader-Willi .
Síndrome dismórfico con obesidad y deterioro de la cognición; ejemplo de impresión genómica ( genomic imprinting ) a menudo debida a delación citogenética en la región xxx en el cromosoma heredado del padre.


Trastornos multifactoriales .
Malformaciones congénitas .


Labio leporino con o sin paladar hendido .
Labio leporino con o sin paladar hendido, unilateral o bilateral.


Enfermedades congénitas del corazón.
Grupo de cardiopatías congénitas, algunas en parte genéticas.


Defectos del tubo neural.
Anencefalia, espina bífida y otras formas menos comunes.


Enfermedades de la edad adulta.
Algunos tipos de cáncer .
El papel de la herencia en muchos tipos de cáncer permanece sin comprenderse del todo, pero a menudo es de gran importancia.


Enfermedad de las arterias coronarias.
Trastorno con causas múltiples, incluyendo predisposición genética.


Diabetes mellitus.
Insuficiencia en la producción de insulina por las células B del páncreas; ciertos genes de antígenos leucocitarios humanos constituyen importantes determinantes de riesgo.


Trastornos mitocondriales .


Neuropatía óptica hereditaria de Leber.
La primera enfermedad que se demostró que se debía a un defecto en el genoma mitocondrial; se hereda por la vía materna.


[1] Judíos de Europa central y oriental no sefarditas.

En los trastornos cromosómicos , el defecto no se debe a un error único en el original genético, sino a exceso o deficiencia de los genes contenidos en cromosomas enteros o sus segmentos.
Por ejemplo, la presencia de una copia adicional de un cromosoma, el número 21, produce un trastorno específico, el síndrome de Down (tabla 1-1), aunque ningún gen individual en el cromosoma es anormal.
Como grupo, los trastornos cromosómicos son muy comunes, afectan aproximadamente a 7 de cada 1.000 niños nacidos vivos y provocan cerca de la mitad de los abortos espontáneos en el primer trimestre del embarazo.

La herencia multifactorial genera varios Trastornos del desarrollo que causan malformaciones congénitas y muchos trastornos frecuentes de la edad adulta.
De nuevo, parece que no existe un error único en la información genética. sino más bien una combinación de pequeñas variaciones que en conjunto pueden producir un defecto grave o predisponer a éste.
Quizá también participen los factores ambientales.
Los trastornos multifactoriales tienden a repetirse en familias, pero no muestran los patrones genealógicos característicos de los rasgos monogénicos.

¿Cómo puede reconocer un médico que un trastorno es genético?

Una de las características irónicas de la enfermedad genética es que las personas afectadas casi nunca buscan a un genetista, sino más bien a un médico general.
Debido a que los pacientes no se presentan con quejas como «¡mis genes me están matando!» o «me duelen los genes», el médico general debe reconocer que los síntomas tienen o quizá tengan una explicación genética, y ha de enviar al individuo a recibir los cuidados de un genetista.
Así, es responsabilidad de los médicos generales reconocer los primeros signos clínicos de una entidad genética, así como preguntarse a sí mismos, de manera rutinaria, si la genética desempeña un papel en el problema. 
Esto es lo que se entiende por enfoque genético de la medicina o «medicina genética».

Si un trastorno aparece más de una vez en una familia, es posible que éste tenga una causa genética, y uno de los propósitos de este libro es describir las maneras de identificar diversos patrones de enfermedad genética en familias.
Sin embargo, no todos los trastornos que afectan a más de un miembro en una familia son de origen genético, y no todas las entidades genéticas se presentan en más de un miembro de una familia.
¿Cómo puede el médico reconocer que un trastorno en cierto paciente es de naturaleza genética, con todo lo que dicho reconocimiento implica para el paciente y los miembros de su familia?

No existe una respuesta simple para esa pregunta.
Una clave importante es, por supuesto, el diagnóstico. 
Si bien muchas alteraciones genéticas requieren pruebas diagnósticas específicas, hoy en día la tarea del diagnóstico se facilita por las abundantes y excelentes descripciones de entidades genéticas en la literatura médica.
Los sistemas informatizados también están contribuyendo al diagnóstico genético, en particular al diagnóstico de síndromes dismórficos.

Con objeto de ilustrar la variedad de problemas genéticos que un médico tiene que afrontar, el recuadro anexo contiene cuatro breves historias de pacientes con trastornos genéticos relativamente comunes.
Cada uno de los cuadros se expone con mayor detalle en otro lugar del presente libro.

PATRONES DE HERENCIA MONOGÉNICA 

En el capitulo 1 se denominaron y caracterizaron brevemente las tres principales categorías de trastornos genéticos monogénicos, cromosómicos y multifactoriales.
En este capítulo se exponen con mayor detalle los patrones clásicos de transmisión de los trastornos de un solo gen y se describen algunas excepciones notables a los patrones usuales.

Los rasgos monogénicos a menudo se denominan mendelianos porque, como las características de los guisantes de jardín que estudió Gregor Mendel. se segregan en las familias y, por término medio, ocurren en proporciones fijas entre la descendencia de tipos específicos de uniones.
Los fenotipos monogénicos descritos hasta la fecha se enumeran en la obra clásica de Victor A. McKusick, Mendelian inheritance in man (9., edición., 1990), que ha resultado indispensable para los genetistas médicos durante muchos años. 
La edición de 1990 de este libro incluye más de 4.500 loci y en muchos de éstos existe al menos una forma de mutación conocida que se asocia con un trastorno clínicamente significativo.
Así del total estimado de 50.000 a 100.000 genes humanos, aproximadamente de un 5 a un 10% ya se han identificado.
El ritmo de los nuevos descubrimientos es rápido y parece seguro que se acelerará debido al interés y a la actividad internacionales en la realización del mapeo e identificación de la secuencia del genoma humano completo.

La variación hereditaria del genoma constituye la piedra angular de la genética humana y médica. 
Las formas alternativas de información genética en un locas determinado se denominan alelos .
Muchos genes tienen sólo una versión normal, denominada alelo «salvaje».
Otros loci génicos exhiben polimorfismos (literalmente «muchas formas»), lo cual significa que en la población existen al menos dos alelos normales relativamente comunes en el locus (cap., 6).
Además del alelo o los alelos normales, la mayor parte de loci identificados también tienen uno o más alelos raros.
De hecho, muchos loci humanos se identificaron originalmente a través de los trastornos clínicamente significativos provocados por un raro alelo mutante.

El genotipo de un individuo es su constitución genética, tanto de manera colectiva en todos los loci o, de manera más característica, en un solo locus. 
El fenotipo constituye la expresión observable de un genotipo en forma de rasgos morfológicos, bioquímicos o moleculares.
Por supuesto, un fenotipo puede ser normal o anormal en un cierto sujeto, pero en este libro, en el que se subrayan los trastornos de significado médico. 
La atención se centra en fenotipos anormales, esto es, trastornos genéticos.

Un trastorno monogénico es aquel que está determinado por un alelo específico en un solo locus en uno o ambos miembros de un par cromosómico.
El alelo variante, que surgió por mutación en algún momento en el pasado reciente o lejano y casi siempre es relativamente raro, reemplaza el alelo normal de tipo «salvaje» en uno o ambos cromosomas.
Cuando un individuo posee un par de alelos idénticos, se dice que es homocigótico (un homocigoto ); cuando los alelos son diferentes, el individuo es heterocigótico (un heterocigoto o portador).
El término compuesto o heterocigoto compuesto se utiliza para describir un genotipo en el cual dos alelos mutantes distintos se hallan presentes, en lugar de uno normal y uno mutante.
Dichos términos (homocigótico, heterocigótico y compuesto) pueden aplicarse a una persona o a un genotipo.
El término mutación se utiliza en genética médica en dos sentidos: algunas veces indica un nuevo cambio genético que no se ha observado previamente en ningún pariente y en otras ocasiones señala un alelo anormal.

- Varón
- Mujer
- Sexo no especificado
- Número de niños con indicación de su sexo
- Afectados
- Heterocigotos para rasgo autosómico
- Portador de rasgo recesivo ligado al X
- Propositus
- Individuo muerto
- Muerte prenatal
- Aborto
- Individuo adoptado perteneciente a la misma familia
- Individuo adoptado no perteneciente a la familia
- Matrimonio
- Unión extramarital
- Divorcio
- Unión consanguínea
- Gemelos monocigóticos
- Gemelos dicigóticos
- Gemelos de cigosidad desconocida
- Individuos numerados en la genealogía
- El propositus es II-2
- Sin descendencia

Figura 4-1

Símbolos comúnmente utilizados en las genealogías

Los trastornos monogénicos se caracterizan por sus patrones de transmisión en familias.
Para establecer dicho patrón, un primer paso habitual consiste en obtener información acerca de los antecedentes familiares del paciente y resumir los detalles en forma de una genealogía , utilizando símbolos estándar (fig., 4-1).
Los patrones que muestran los trastornos monogénicos en las genealogías dependen principalmente de dos factores: 1) la localización cromosómica del locus génico, que puede ser autosómico (ubicado en un autosoma) o ligado al X (situado en el cromosoma X), y 2) la clase de fenotipo, que puede resultar dominante (que se expresa incluso cuando sólo un cromosoma de un par porta el alelo variante) o recesivo (que se expresa únicamente cuando ambos cromosomas de un par portan un alelo variante). 
Así, existen sólo cuatro patrones básicos de herencia monogénica.

Herencia autosómica y ligada al X

La distinción entre herencia autosómica y ligada a X resulta obvia en tanto que depende sólo de la localización cromosómica del gen.
Sin embargo, la expresión clínica de un gen anormal también depende de que éste sea autosómico o ligado a X. Existen dos consideraciones, que se exponen con más detalle posteriormente: 1) Los varones poseen únicamente un X y es por ello que se les denomina hemicigóticos con respecto a los genes ligados al X, en vez de homocigóticos o heterocigóticos; los varones 46, XY nunca son heterocigóticos para los rasgos ligados al X. 2) Para compensar el complemento doble de genes ligados al X en las mujeres, opuesto al complemento único en los varones, en las células de las mujeres únicamente se expresan los alelos de uno de los dos cromosomas X, mientras que ambos alelos de cualquier locus autosómico son activos.

Herencia dominante y recesiva

Por definición, un fenotipo que se expresa de la misma manera tanto en el homocigoto como en el heterocigoto es dominante , y un fenotipo que se expresa sólo en homocigotos (o, para rasgos ligados al X, hemicigotos) es recesivo .
En genética médica, sin embargo, esta definición no es estrictamente cierta, y cualquier fenotipo que se expresa en heterocigotos se clasifica como dominante.
Los trastornos autosómicos dominantes son típicamente más graves en homocigotos que en heterocigotos y, hasta ahora, sólo uno de tales trastornos, la enfermedad de Huntington, se sabe que es igualmente grave en ambos genotipos.

Cuando la expresión del genotipo heterocigótico resulta diferente de los dos tipos homocigóticos e intermedio entre éstos, el fenotipo puede describirse con mayor precisión como incompletamente dominante o incompletamente recesivo.

Si la expresión de cada alelo puede detectarse incluso en presencia del otro, los dos alelos se denominan codominantes .

Cada uno de los trastornos genéticos que se mencionan en este libro como ejemplo de un principio genético se describen brevemente desde el punto de vista clínico y genético en un lugar apropiado en este texto, a menudo en un recuadro.
Véase la tabla 1-1 y el índice para mayor información acerca de dónde hallar muchas de estas descripciones clínicas.

Si se es estricto, debe aclararse que es el fenotipo, y no el alelo, el que resulta dominante o recesivo; sin embargo, los genes se clasifican como dominantes o recesivos sobre la base de su expresión fenotípica, y los términos «gen dominante» y «gen recesivo» son utilizados ampliamente, aunque de manera imprecisa.

La distinción entre herencia dominante y recesiva no es absoluta; por el contrario, constituye una designación arbitraria que se basa en fenotipos clínicos que pueden no tener significación a nivel de acción génica. 
Para loci autosómicos, cada alelo se expresa como un producto génico; en los heterocigotos, uno de los alelos puede suprimirse, o bien su producto puede resultar defectuoso y, en última instancia, no funcional, pero donde quiera que esto sea biológicamente posible, ambos alelos se expresan y el fenotipo resulta la consecuencia de la expresión combinada de ambos.
Si bien un fenotipo recesivo se define como clínicamente indetectable en heterocigotos, machos rasgos que se clasifican como recesivos poseen de hecho manifestaciones heterocigóticas cuando se examinan a nivel celular, bioquímico o molecular.

Por ejemplo, el trastorno bien conocido de la hemoglobina, la enfermedad (o anemia) drepanocítica , se hereda como una enfermedad autosómica recesiva (v., cap., II para una exposición más detallada). 
Los pacientes con la enfermedad son homocigóticos para un alelo defectuoso en el locus de la, B-globina y, consecuentemente, producen hemoglobina anormal S (HbS) y no hemoglobina normal del adulto (HbA) en sus eritrocitos, que adquieren forma de hoz en condiciones de baja tensión de oxígeno.
Los heterocigotos producen tanto HbA como HbS; una proporción de sus eritrocitos muestran el fenómeno drepanocítico y los sujetos padecen anemia leve.
Así, a nivel de la síntesis de hemoglobina, el alelo normal de D-globina y el alelo defectuoso se expresan como alelos codominantes; a nivel de la función fisiológica, el alelo normal es incompletamente dominante (y el alelo anormal, recesivo de manera incompleta) y, por último, a nivel clínico, la enfermedad drepanocítica se comporta como un rasgo recesivo. 

En los trastornos autosómicos dominantes, el producto del alelo normal es típicamente aquel que se requiere en cantidades normales.
Un gen mutante que no produce proteína o genera una cantidad reducida de proteína o proteína en forma anormal origina un fenotipo anormal. 

Desde un punto de vista amplio. el producto del gen normal a menudo es una proteína estructural no enzimática, como el colágeno, o bien un componente proteico de una membrana o un receptor.
Sin embargo, hasta la fecha en la mayor parte de los trastornos autosómicos dominantes. 
La naturaleza de la proteína defectuosa o perdida es aún completamente desconocida.
Por el contrario, muchos de los trastornos autosómicos recesivos descritos hasta la fecha son defectos enzimáticos en los que parece existir un margen de seguridad lo bastante amplio como para permitir la función normal en heterocigotos, aunque sólo uno del par de alelos sea completamente funcional y el otro (anormal), defectuoso o no funcional.

Estas consideraciones se exponen con detalle en el capítulo 19.

En resumen.
La distinción entre un alelo mutante dominante y uno recesivo es realmente simple: en el heterocigoto, con un alelo mutante y otro normal,
¿es suficiente la cantidad residual de producto génico para realizar su función designada?
Si la respuesta es sí, el alelo mutante (y su trastorno asociado) es recesivo.
Si la respuesta es no, el alelo mutante (y su trastorno) es dominante.

Otros factores que afectan los patrones genealógicos

Si bien como regla general las genealogías de los trastornos monogénicos pueden clasificarse fácilmente como autosómicas o ligadas al X y como dominantes o recesivas, el patrón de herencia de una genealogía individual también está determinado por otros varios factores que hacen difícil la interpretación del modo de herencia.
Especialmente con las familias de pequeño tamaño típicas de los actuales países más desarrollados, el paciente quizá resulte el único miembro afectado de la familia. y la naturaleza genética del trastorno tal vez no sea inmediatamente aparente.
Otros aspectos que deben considerarse son la segregación al azar de genes de los padres a los hijos a través de los gametos: las mutaciones nuevas que no resultan infrecuentes en algunos tipos de enfermedad genética: las dificultades para el diagnóstico debidas a la expresión ausente o variable del gen involucrado; los posibles efectos de otros genes y del ambiente en la expresión génica: la incapacidad de los sujetos con ciertos genotipos para sobrevivir hasta el momento del nacimiento, y la falta de información precisa con respecto a la presencia del trastorno en parientes o acerca de relaciones familiares.

Tal como se describe en la sección «Patrones no clásicos de expresión monogénica», varios mecanismos biológicos raros e inusuales pueden generar patrones genealógicos no clásicos.
Ejemplos de algunas de dichas situaciones que pueden confundir los intentos para proporcionar un consejo genético preciso aparecen en los problemas al final de este capítulo.

Algunas veces, un patrón genealógico simula un patrón monogénico, aunque el trastorno no posea una base monogénica.
Es fácil confundirse en este sentido por condiciones que exhiben herencia multifactorial, por ciertos tipos de trastornos cromosómicos familiares o por efectos teratogénicos.
Los trastornos hereditarios monogénicos pueden distinguirse de otros tipos de trastornos familiares por sus proporciones de segregación típicamente mendeliana entre parientes, así como mediante otros métodos, entre los cuales el más directo es la demostración de un defecto puntual a nivel del producto génico o a nivel del DNA que se corresponde con el fenotipo.

Muchos pacientes con trastornos genéticos no tienen parientes con una afectación similar, pero incluso si un trastorno genético ocurre sólo en un miembro de un grupo familiar, es posible reconocer su origen genético. 
Debido a la gran semejanza del fenotipo entre distintas familias con el mismo defecto, a menudo pueden utilizarse patrones de herencia bien establecidos en otras familias con el mismo trastorno como base para el diagnóstico y el consejo genético (McKusick, 1990).

Heterogeneidad

Cuando un trastorno genético que parece ser una entidad única se analiza meticulosamente, es frecuente hallar que resulta genéticamente heterogéneo; esto es, el trastorno incluye varios fenotipos que son similares, pero en realidad están determinados por diferentes genotipos (v., tabla 4-1 para los ejemplos).
La heterogeneidad quizá sea el resultado de diferentes mutaciones en el mismo locus ( heterogeneidad alélica ), de mutaciones en diferentes loci ( heterogeneidad no alélica o de locus ) o de ambas.
El reconocimiento de la heterogeneidad genética constituye un aspecto importante del diagnóstico clínico y el consejo genético.

Para muchos fenotipos, el análisis genealógico por sí solo resulta suficiente para demostrar heterogeneidad genética.

Por ejemplo, se sabe hace tiempo que la retinitis pigmentosa , una causa común de trastorno visual debida a degeneración retiniana que se asocia con distribución anormal del pigmento en la retina, ocurre en formas autosómicas dominantes, autosómicas recesivas y ligadas al X.
En años recientes, se ha mostrado que la heterogeneidad resulta incluso más extensa; el análisis de DNA ha señalado que existen cuando menos dos formas ligadas al X y tres autosómicas dominantes.
El síndrome de Ehlers-Danlos, en el que existen numerosas manifestaciones de un defecto subyacente de la estructura del colágeno, puede también presentar herencia autosómica dominante, autosómica recesiva o ligada al X, y el análisis a nivel clínico y molecular ha mostrado que hay al menos 10 tipos distintos de este trastorno.

Tabla 4-1

Heterogeneidad en trastornos monogénicos seleccionados

Transtorno.
Tipo de heterogeneidad.
Comentarios.


Enfermedad de Charcot-Marie-Tooth .
De locus (no alélica).
Formas AD y XR.


Fibrosis quística.
Alélica.
Insuficiencia pancreática vs. suficiencia.


Sordera recesiva congénita .
Alélica y de locus.
Numerosos tipos descritos.


Distrofia muscular de Duchenne y de Becker.
Alélica.
Clínicamente distintas.


Síndrome de Ehiers-Danlos.
Alélica y de locus.
Muchas variantes distintas.


Homocistinuria .
Alélica y de locus.
Sensible a la vitamina vs. no sensible.


Mucopolisacaridosis .
Alélica y de locus.
Gran heterogeneidad .


Miotonía congénita.
De locus.
Formas AD y AR.


Retinitis pigmentaria.
De locus.
Formas AD, AR y XR.


Enfermedad de Tay-Sachs.
Alélica.
Varias formas.


B-Talasemia.
Alélica.
Numerosas diferentes mutaciones en el gen de la B-globina producen el fenotipo .


La heterogeneidad alélica es una importante causa de variación clínica.
Muchos loci poseen más de un alelo mutante; de hecho, en un locus determinado puede haber varias o muchas mutaciones, que producen trastornos clínicamente indistinguibles o muy similares.
En otros casos, diferentes alelos mutantes en el mismo locas generan muy distintas presentaciones clínicas. 
Un ejemplo clásico de heterogeneidad alélica como base de heterogeneidad clínica es el trastorno autosómico recesivo denominado síndrome de Hurler , una mucopolisacaridosis que se genera por deficiencia del enzima a-L-iduronidasa .
Un trastorno alélico más leve, el síndrome de Scheie , se origina por una mutación diferente en el mismo locus, y un fenotipo de gravedad intermedia, el síndrome de Hurler-Scheie , se observa en pacientes con genotipo compuesto (v., también tabla 12-4).

A menos que tengan padres consanguíneos, es probable que muchos pacientes con trastornos autosómicos recesivos tengan genotipos compuestos más que genotipos verdaderamente homocigóticos, si bien en la mayoría de las ocasiones estos individuos se identifican como homocigotos si poseen un par de alelos anormales.
Debido a que las diferentes combinaciones alélicas pueden tener consecuencias clínicas en cierto modo diferentes, los clínicos necesitan conocer la heterogeneidad alélica como una posible explicación de la variabilidad entre pacientes con la misma enfermedad.

Magnitud de los trastornos mendelianos 

Las enfermedades mendelianas conocidas exigen un alto coste en cuanto a fallos reproductivos, muerte prematura e incapacidades. 
Estos trastornos son primariamente, aunque no de manera exclusiva, enfermedades de la edad pediátrica; menos del 10 % se desarrollan después de la pubertad, y sólo el I % ocurren después del periodo reproductivo (Costa y cols., 1985).
Aunque individualmente son raros, como grupo generan una proporción importante de las enfermedades y muertes infantiles.
En un estudio poblacional de más de un millón de neonatos vivos, la incidencia de trastornos monogénicos graves se estimó en 0,36 % (Baird y cols., 1988).
En otro estudio de neonatos vivos y mortinatos, uno de cada 45 presentó algún tipo de anomalía congénita; de las anomalías, alrededor de una de cada 30 pareció ser un trastorno monogénico y alrededor de uno de cada cuatro de éstos se pensó que constituía una nueva mutación (Nelson y Holmes, 1989).
Entre los niños hospitalizados, de un 6 a un 8 % probablemente tengan trastornos monogénicos (Beaudet y cols., 1989). 

Terminología

Aunque los principios de la genética médica son relativamente fáciles de entender, su poco conocida terminología quizás haga parecer la materia como inaccesible en un primer momento.

Para ayudar a resolver este problema de lenguaje, algunos términos que antes no se han definido se introducen ahora y se ilustran en la genealogía de la figura 4-2.

El miembro a través del cual una familia con un trastorno genético es identificada por primera vez es el propositus , o, si está afectado, también puede llamarse caso índice .
El propositus es usualmente un paciente, pero puede ser algún otro miembro de la familia. 
Una familia puede tener más de un propositus , si es detectada mediante más de una fuente.
Un conjunto de hermanos y hermanas se denomina fratría .
La primera generación de la descendencia de una pareja constituye la F1 (primera generación filial).
Los parientes se clasifican como de primer grado (padres, hermanos y descendientes del propositus ), segundo grado (abuelos y nietos, tíos y tías, sobrinos y sobrinas, medios hermanos), tercer grado (p., ej., primos hermanos) y así sucesivamente, según el número de pasos (en otras palabras, el número de meiosis) en la genealogía entre dos familiares. 
La descendencia de los primos hermanos son primos en segundo grado, y un niño es el sobrino en segundo grado del primo hermano de sus padres.
Los miembros de las parejas relacionados por descendencia son consanguíneos .

Si existe sólo un miembro afectado en una familia, éste se denomina caso «aislado», o si se determina que el trastorno se debe a una nueva mutación, el individuo se denomina caso «esporádico» (fig., 4-2). 

Figura 4-2

Relaciones dentro de un grupo familiar

La flecha indica la propositus, 111-5 que representa un caso aislado de trastorno genético. 
Ella tiene cuatro hermanos, xxx .
Su pareja (esposo) es 111-6, y tienen tres hijos (su progenie F1).
La propositus tiene nueve parientes de primer grado (sus padres, hermanos y descendientes), nueve parientes de segundo grado (abuelos, tíos/tías, sobrinos/sobrinas, nietos), dos de tercer grado (primos hermanos) y cuatro de cuarto grado (sobrinos segundos).
xxx son primos segundos de xxx .
xxx , cuyos padres son consanguíneos están doblemente emparentados con la propositus: son parientes de segundo grado a través de su padre y de cuarto grado su madre.

HERRAMIENTAS DE LA GENÉTICA MOLECULAR HUMANA

Uno de los objetivos principales de la moderna genética médica y humana es comprender a nivel molecular la base de las mutaciones que generan enfermedades genéticas y utilizar dicha información para mejorar los métodos de diagnóstico y tratamiento potencial de estos trastornos. 

Estos estudios se basan en los conceptos de la genética molecular, que constituyen una extensión lógica de conceptos firmemente enraizados en la citogenética, la genética clínica y la genética bioquímica. 
Como la mayor parte de los avances en el conocimiento. los de la genética molecular han recibido una considerable ayuda del desarrollo de metodologías y tecnologías que difícilmente podrían haberse imaginado en fecha tan cercana como mediados de los años 70.
Los métodos, si no los conceptos, de la genética molecular han sido revolucionarios.
En los últimos 15 años se han desarrollado y utilizado varias técnicas que permiten el análisis detallado de genes normales y anormales.
La aplicación de estas técnicas ha permitido incrementar el conocimiento de los procesos moleculares a todos los niveles, del gen al organismo completo, y ha proporcionado las bases para que continúe ampliándose el acervo de procedimientos de laboratorio para la detección y diagnóstico de enfermedades genéticas.

Este capítulo no intenta ser un «libro de cocina» de recetas de experimentos genéticos o métodos diagnósticos de laboratorio.

Más bien sirve sólo como introducción a las técnicas que han generado y continúan produciendo los avances en la investigación de genética básica y aplicada.
El contenido de este capitulo completa el material básico que se presentó en los capítulos 2, 3 y 4, y proporciona las bases para el entendimiento de gran parte de la información molecular contenida en los capítulos siguientes.
Los lectores que han seguido cursos de genética molecular humana o que poseen experiencia de laboratorio en esa misma área pueden revisar superficialmente o incluso omitir la lectura de este capítulo, lo cual no interferirá con la continuidad del texto.
Quienes consideren que el material de este capítulo es demasiado breve hallarán revisiones mucho más detalladas de las técnicas modernas, junto con referencias completas, en la bibliografía que se indica al final del capítulo.

PRINCIPIOS DE LA CLONACIÓN MOLECULAR

El proceso de la clonación molecular consiste en el aislamiento de una secuencia de DNA de interés y la obtención de múltiples copias de ésta en un organismo, usualmente una bacteria, que puede crecer durante largos períodos de tiempo.
Así pueden aislarse grandes cantidades de molécula de DNA en forma pura para análisis molecular detallado (fig., 5-1).
La capacidad de generar copias casi infinitas (clones) de una secuencia particular es la base de la tecnología de DNA recombinante y de su aplicación a la genética humana y médica.
La denominación «DNA recombinante» se refiere a combinaciones nuevas de DNA creadas entre secuencias de DNA humanas (o de otra clase) de interés y moléculas de DNA bacteriana (o de otra clase) capaces de duplicarse indefinidamente en el laboratorio.
Como otros muchos avances tecnológicos, éste surgió con su propia jerga, cuyo dominio puede parecer más dificultoso que los conceptos involucrados (v., recuadro).

Figura 5-1

El fundamento de la clonación molecular para aislar cantidades infinitas de una secuencia deseada de DNA particular en forma pura

Enzimas de restricción 

Uno de los avances clave en el desarrollo de la clonación molecular fue el descubrimiento, a principios de los años 70, de las endonucleasas de restricción bacterianas, enzimas que reconocen secuencias de doble cadena específicas (generalmente cortas) en el DNA y cortan éste en el lugar de reconocimiento o cerca de éste.
Por ejemplo, la enzima de restricción EcoRI reconoce la secuencia específica de seis pares de bases xxx siempre que aparece en una molécula de DNA de doble cadena y corta el DNA en ese lugar produciendo una muesca en cada cadena entre la G y la A adyacente.
Esto genera dos fragmentos, cada uno con un extremo de cuatro bases y una sola cadena (fig., 5-2).
Estos extremos «pegajosos» son útiles para las subsecuentes reacciones de unión en la construcción de moléculas de DNA recombinante. 
Otras enzimas de restricción reconocen diferentes secuencias de nucleótidos que resultan específicas para cada enzima en particular.
Varios centenares de dichas enzimas se conocen hoy en día; algunas de las que se emplean más frecuentemente se indican en la tabla 5-1.
La mayor parte de las enzimas de restricción poseen lugares de reconocimiento que se componen de cuatro o seis pares de bases, aunque unas cuantas poseen lugares de mayor tamaño.
Generalmente, las secuencias son palindromos ; esto es, se leen igual 5' a 3' en ambas cadenas.

La rotura de una molécula de DNA con una enzima de restricción particular «digiere» el DNA hasta formar una colección reproducible y característica de fragmentos que refleja la frecuencia y la localización de los lugares de corte específicos.
Esta propiedad de las enzimas de restricción posee dos importantes implicaciones, fundamentales para el papel que desempeñan en la tecnología del DNA recombinante y en su aplicación a la genética médica.
En primer lugar, la digestión de muestras de DNA genómico con, por ejemplo, la enzima EcoRI genera una colección de aproximadamente un millón de fragmento EcoRs I , cada uno con una localización particular en el genoma.
Debido a que EcoRI corta el DNA de doble cadena de manera específica en todas y cada una de las xxx que encuentra, y debido a que incluso una modificación de una sola base en un lugar potencial de corte impide el reconocimiento y corte que realiza la enzima, dicha digestión permite examinar, en efecto, esta secuencia particular de seis nucleótidos en aproximadamente un millón de localizaciones en el genoma. 
En promedio, una enzima con un lugar de reconocimiento de seis pares de bases debe cortar el DNA humano cada 46 pares de bases, o una vez cada 4 kb .
Sin embargo, en realidad tales lugares no están localizados de manera aleatoria (lo que refleja las particulares composiciones y secuencias de bases de diferentes regiones del genoma) y se observan fragmentos EcoRI que presentan tamaños de unos cuantos a más de un millón de pares de bases.

El lenguaje de la clonación molecular

cDNA (DNA complementario): un DNA sintético copiado del RNA mensajero (mRNA) por la enzima transcriptasa inversa. 
Se emplea para referirse tanto a una copia de una sola cadena como a su derivado de dos cadenas.
Expresiones: «un clon de cDNA», «una biblioteca de cDNA», «aislar un cDNA».

Clon/clonación: una molécula de DNA recombinante que contiene un gen u otra secuencia de DNA de interés. 
El acto de generar dicha molécula.
Expresiones: «aislar un clon» o «clonar un gen».

Huésped: el organismo empleado para aislar y producir una molécula de DNA recombinante.
Generalmente, una cepa de la bacteria Escherichia coli o de la levadura Saccharomyces cerevisiae.
Expresiones:
¿qué huésped se utilizó?

Hibridación: el acto de dos moléculas de ácido nucleico complementarias de una sola cadena que forman enlaces y se convierten en una molécula de doble cadena.
Expresiones: «la sonda se hibrida con un gen».

Inserto: un fragmento de DNA humano clonado en un vector particular.
Expresiones: «se purificó el inserto». 

Biblioteca: una colección de clones recombinantes de una secuencia que sabemos que contiene el gen, el cDNA u otras secuencias de DNA de interés.
En principio, una biblioteca puede contener todas las secuencias de DNA representadas en la célula. tejido o cromosoma originales.
Expresiones: «una biblioteca de cDNA muscular», «una biblioteca del genoma humano».

Ligamiento: el acto de formar enlaces de fosfodiéster para unir dos moléculas de DNA de doble cadena con la enzima DNA-ligasa.
El ligamiento constituye el paso esencial en la creación de moléculas de DNA recombinante. 
Expresiones: «los fragmentos se ligaron». 

Sonda/sondar: una molécula de DNA o RNA clonada, marcada con trazador radiactivo o de otro tipo y utilizada para identificar sus secuencias complementarias por medio de hibridación molecular.
El acto de emplear dicha molécula.
Expresiones: «la sonda de B-globina,«sondar el DNA de un paciente».

Endonucleasas de restricción (enzimas de restricción): enzimas que reconocen secuencias específicas de DNA de doble cadena y cortan el DNA en el lugar de reconocimiento o cerca de éste.
Expresiones: «una digestión con enzima de restricción» (o solamente «una digestión de restricción») o «la enzima de restricción EcoRI» .

Mancha de Southern: un filtro al que se ha transferido el DNA, a menudo después de la digestión con enzimas de restricción y electroforesis en gel para separar las moléculas de DNA por tamaño (el nombre proviene de Ed Southern, que desarrolló la técnica).
El acto de generar dicho filtro e hibridarlo con una sonda específica. 
Expresiones: «sondar una mancha de Souther», «realizar un southern».

Vector: la molécula de DNA en la que se ha clonado el gen u otro fragmento de DNA de interés.
Puede replicarse en un huésped particular.
Algunos ejemplos incluyen plásmidos, bacteriófagos lambda, cósmidos y cromosomas artificiales de levadura.
Expresiones: «un vector de clonación»,«el vector cósmido». 

En segundo lugar, todas las moléculas de DNA digeridas con EcoRI , independientemente de su origen, poseen extremos pegajosos idénticos de una sola cadena, sea cual fuere la naturaleza de las secuencias de DNA que flanquean un lugar EcoRI particular.
Así, cualesquiera de las dos moléculas de DNA que se han generado mediante digestión con EcoRI pueden unirse por interacción de sus extremos complementarios de cuatro bases, tras lo cual se forman los enlaces de fosfodiéster en cada fibra mediante una enzima denominada DNA-ligasa.
Este paso de ligamiento crea una molécula de DNA «recombinante», en la que un extremo proviene de una fuente de DNA y el otro, de una fuente diferente (fig., 5-2).
Muchas enzimas de restricción, como EcoRI , generan extremos cortos; otras, sin embargo, cortan ambas cadenas en el mismo lugar, lo cual genera extremos romos.
La DNA-ligasa puede unir incluso estas moléculas.

Vectores

Un vector es una molécula de DNA que puede replicarse de manera autónoma en un huésped, como las celarlas bacterianas o de levadura a partir de las cuales es posible aislarlo en forma pura para el análisis.
La clonación de fragmentos de DNA humano en un vector por medio de enzimas de restricción y DNA-ligasa (como acabamos de describir) permite la propagación del fragmento clonado con la molécula vector.

Debido a que los vectores de replicación pueden generar un gran número de copias por célula, ya que los huéspedes bacterianos o de levadura pueden crecer indefinidamente en el laboratorio, es posible obtener grandes cantidades de secuencias de DNA de interés.
Cierto número de vectores se utilizan de modo común para este propósito, cada uno con sus ventajas y limitaciones (tabla 5-2).

Figura 5-2

Proceso de clonación de un segmento de DNA humano (entre dos lugares EcoRI ) en un vector de clonación plásmido

La abreviatura «ori», señala un origen de replicación de DNA para el plásmido replicante en células bacterianas.
Las abreviaturas « amp » y « tetr », señalan genes bacterianos que confieren resistencia a ampicilina y tetraciclina. 
Se muestra el crecimiento de bacterias en placas que poseen antibióticos seleccionados para aquellas células que contienen copias del plásmido, con su inserción de DNA humano clonado.

Plásmidos

Los plásmidos son moléculas circulares de DNA de doble cadena que se replican de modo extracromosómico en las bacterias o, menos comúnmente, en levaduras.
Debido a que puede mantenerse un número moderado de copias de dichos plásmidos (fig., 5-1), resultan ideales para generar grandes cantidades de una secuencia de DNA clonada.
Los plásmidos diseñados especialmente para clonación molecular son generalmente pequeños (con un tamaño de varias kilobases) y contienen un origen de replicación (para replicación en Escherichia coli o en levadura), uno o más marcadores elegidos (con frecuencia un gen que confiere resistencia a antibióticos) y uno o más lugares de restricción para la clonación de moléculas de DNA foráneo.
En la figura 5-2 aparece un dibujo esquemático de los pasos más importantes en la clonación de DNA foráneo en el lugar EcoRI de un plásmido.
La identificación de colonias que contienen el plásmido recombinante deseado. seguida por el crecimiento y el aislamiento del DNA del plásmido puro, permite el aislamiento de grandes cantidades del inserto clonado.
La clonación en plásmidos constituye un procedimiento estándar para el análisis de moléculas pequeñas de DNA (tabla 5-2).

Tabla 5-1

Ejemplos de enzimas de restricción y sus secuencias de Reconocimiento

Enzima de restricción.
Fuente.
Secuencia de reconocimiento.


xxx .
Bacillus amyloliquefaciens H .
xxx .


EcoR I .
Escherichia coli xxx .
xxx .


HaeIII .
Haemophilus aegyptius .
xxx .


xxx .
Haemophilus influenzae Rd .
xxx .


xxx .
Nocardia otitidis-cavarium .
xxx .


xxx .
Staphylococcus aureus 3A .
xxx .


xxx .
streptomyces stanford .
xxx .


Tabla 5-2

Ejemplos de vectores utilizados comúnmente en clonación molecular

Vector.
Huésped.
Tipo de clonación.
Margen del tamaño inserto .


Plásmidos.
Bacteriano y de levadura.
Genómica, cDNA.
Usualmente xxx .


Bacteriófago lambda.
Bacteriano.
Genómica, cDNA .
Hasta xxx .


Cósmidos .
Bacteriano.
Genómica.
Hasta xxx .


«Cromosomas artificiales» .
De levadura.
Genómica.
De xxx a xxx .


Bacteriófago lambda

Otro vector a menudo utilizado es el bacteriófago lambda, un virus bacteriano con una molécula de DNA de doble cadena relativamente larga (alrededor de 45 kb ).
Durante su crecimiento en Escherichia coli , el bacteriófago lambda se replica para producir un enorme número de virus infecciosos, lo cual al final mata las células bacterianas infectadas de esta manera y libera alrededor de un millón de bacteriófagos. 

Durante esta fase infecciosa de crecimiento, aproximadamente una tercera parte del genoma de los bacteriófagos no es esencial («fragmentos internos» en fig., 5-3) y puede reemplazarse por otras secuencias de DNA; así, resulta muy apropiado para la clonación de fragmentos bastante grandes (de hasta 20 kb ) de DNA humano.

Cósmidos

Fragmentos aún más grandes de DNA foráneo (hasta de 50 kb ) pueden clonarse en vectores cósmidos. 
Los cósmidos son básicamente plásmidos que emplean la habilidad de las partículas infecciosas del bacteriófago lambda para «empaquetar» de manera eficiente grandes piezas lineales de DNA e introducirlas en las células bacterianas.

Después de la infección de las bacterias de manera similar a la de los virus lambda, el cósmido se vuelve a hacer circular y se replica como un plásmido grande.

«Cromosomas artificiales» de levadura

Para muchos procedimientos de la clonación de genes y del mapeo génico, ilustrados en el capítulo 8, resulta ventajoso aislar la porción más grande posible del DNA cromosómico humano.
Hasta mediados de los años 8O, los vectores cósmidos representaron el vehículo más grande de clonación posible. 
Sin embargo, en 1987, Olson y cols., desarrollaron una técnica para clonar piezas mayores de DNA en vectores que se replicaban y segregaban en el huésped Saccharomyces cerevisiae (la levadura común utilizada en panadería), así como en cromosomas de levadura lineales normales (Burke y cols., 1987).
Estos «cromosomas artificiales» de levadura (YAC) poseen centrómeros y telómeros, como los cromosomas de levadura normales, y permiten la clonación y el aislamiento de fragmentos de DNA de hasta 1.000 kb de longitud, mucho más pequeños que un cromosoma humano normal, pero aproximadamente del mismo tamaño que un cromosoma normal de levadura. 

Construcción de bibliotecas 

El propósito de la clonación molecular es, por supuesto, aislar un gen particular u otra secuencia de DNA en grandes cantidades para su posterior estudio.
Un procedimiento común para lograr esto es construir un conjunto de clones de DNA recombinante a partir de una fuente (DNA genómico o mRNA) que contenga el gen o la secuencia de interés. 
Dicha colección de clones se denomina biblioteca , la cual, al menos teóricamente, contiene todas las secuencias que se encuentran en la fuente original. 
A continuación debe identificarse el clon o los clones de interés de la biblioteca por medio de métodos sensitivos de valoración que permitan hallar, en algunos casos, hasta una sola copia del clon en cuestión en un conjunto de hasta 10 millones de clones.

Bibliotecas genómicas

Un procedimiento para construir una biblioteca de DNA genómico se muestra en la figura 5-3.
En esta técnica, aplicada por vez primera a finales de los años 70, el DNA genómico humano es parcialmente digerido con una enzima de restricción como Sau 3A, de manera que en algunos de los lugares se produce la rotura y en otros no.
De este modo, asumiendo la rotura aleatoria de tales lugares, es posible obtener una colección de fragmentos superpuestos de tamaño adecuado para la clonación y ligarse en los «brazos» del bacteriófago lambda preparado, de modo que los extremos Sau 3A de los fragmentos de DNA humano pueden ligarse en el vector (fig., 5-3).
Después del empaquetamiento del cromosoma recombinante lambda en partículas del bacteriófago infeccioso, la biblioteca, que contiene un millón de fragmentos de DNA genómico o más, puede almacenarse para aislamiento futuro de muchos genes.
Una de las primeras bibliotecas de genoma humano, construida por Maniatis y cols. (Lawn y cols., 1978), continúa utilizándose hoy en día en centenares de laboratorios de todo el mundo para el aislamiento de genes humanos.

Es posible facilitar en gran medida el proceso de selección de la secuencia de interés comenzando con una biblioteca que ya se ha enriquecido para dicha secuencia, lo cual reduce de manera considerable el número total de clones recombinantes que deben evaluarse.
Para el DNA genómico, una manera particularmente eficaz de realizar esto consiste en emplear una biblioteca que contenga sólo clones de un cromosoma humano específico.
Así, si se quiere aislar un gen de, por ejemplo, el cromosoma 7, puede obtenerse una biblioteca específica de cromosoma que contiene clones únicamente del cromosoma 7.
Dichas bibliotecas hoy están disponibles para todos los cromosomas individuales humanos.
Estas bibliotecas se generan mediante clonación de DNA de cromosomas que se separan por su tamaño de todos los demás cromosomas mediante una técnica llamada clasificación cromosómica activada con fluorescencia .
La aplicación de esta técnica particular se explica con mayor detalle en el capítulo 8.

CITOGENÉTICA CLÍNICA: PRINCIPIOS GENERALES Y ANOMALÍAS AUTOSÓMICAS

En 1959 se inició una nueva era en la genética médica con dos descubrimientos casi simultáneos: el hallazgo de Lejeune y cols. , de que los enfants mongoliens (niños con Mongolismo, hoy conocido como síndrome de Down o trisomía 21 ) tienen 47 cromosomas en lugar de los 46 habituales, en sus células somáticas y las primeras observaciones de Ford y cols. , así como de Jacobs y Strong, de anomalías de cromosomas sexuales en pacientes con trastornos del desarrollo sexual.
Hoy se sabe que las anomalías cromosómicas constituyen una categoría principal de la enfermedad genética, que genera una gran proporción de todas las pérdidas reproductivas, malformaciones congénitas y retraso mental, así como desempeña un papel importante en la patogenia de la malignidad.
Las anomalías específicas de los cromosomas generan 60 o más síndromes identificables que en conjunto son más frecuentes que todos los trastornos mendelianos monogénicos juntos.
Se hallan en aproximadamente el 0,7 % de los recién nacidos vivos, en cerca del 2 % de todos los embarazos en mujeres con más de 35 años de edad y en el 50 % de los abortos espontáneos del primer trimestre.

En este capítulo se exponen las anomalías numéricas y estructurales que se observan en los cariotipos humanos y se describen algunas de las anomalías mejor conocidas de los autosomas.
Los cromosomas sexuales y sus anomalías se presentan en el capítulo siguiente. 

Principios generales de la citogenética clínica

Las anomalías de los cromosomas pueden ser numéricas o estructurales, y pueden afectar uno o más autosomas, cromosomas sexuales o ambos de manera simultánea.
Con creces, el tipo más frecuente de anomalía cromosómica clínicamente significativo es la aneuploidía , un número anormal de cromosomas debido a un cromosoma ausente o adicional, que siempre se asocia con desarrollo anómalo físico o mental, o bien ambos.
Las translocaciones recíprocas (intercambio de segmentos entre cromosomas no homólogos) también son relativamente frecuentes, pero generalmente no tienen ningún efecto fenotípico, si bien, como se explica más adelante, pueden constituir un riesgo incrementado asociado de descendencia anormal.
En el recuadro se presentan las frecuencias relativas de anomalías numéricas y estructurales que se observan en abortos espontáneos, en fetos analizados con amniocentesis de madres mayores de 35 años de edad y en recién nacidos vivos.

- Cariotipo anormal
- Incidencia total
- Porcentaje de anomalías
- Anomalías numéricas
- (aneuploidía, poliploidía)
- Anomalías estructurales
- Equilibradas
- No equilibradas

En la tabla 9-1 se indican algunas abreviaturas que se utilizan frecuentemente en las descripciones de cromosomas y sus anomalías, así como ejemplos de cariotipos anómalos.

ANOMALÍAS DEL NÚMERO DE CROMOSOMAS

Un complemento cromosómico es heteroploide cuando presenta cualquier número de cromosomas diferente al normal.
Un múltiplo exacto del número haploide de cromosomas se denomina euploide , y cualquier otro número cromosómico, aneuploide .

Triploidía y tetraploidía 

Además del número diploide característico de las células somáticas normales, ocasionalmente se comunican otros dos complementos cromosómicos euploides, triploide y tetraploide .
Tanto la triploidía como la tetraploidía se han observado en fetos, y unos pocos niños triploides han nacido vivos, aunque su supervivencia ha resultado breve. 
Es probable que la triploidía ocurra por un fallo en una de las divisiones de maduración en el óvulo o, generalmente, en el espermatozoide.
La expresión fenotípica de un cariotipo triploide depende de la fuente del conjunto cromosómico adicional.
Los triploides con un conjunto adicional de cromosomas paternos presentan típicamente una placenta anormal y se clasifican como molas hidatiformes parciales, pero aquellos que muestran un conjunto adicional de cromosomas maternos no se clasifican de esta manera y abortan de forma espontánea en un período temprano del embarazo.
Los tetraploides son siempre xxx o xxx lo que sugiere que la tetraploidía es causada por una incompleta división de segmentación temprana del cigoto.

Aneuploidía

La aneuploidía es el tipo más frecuente y clínicamente significativo de trastornos cromosómicos humanos y ocurre en al menos 3 a 4 % de todos los embarazos clínicamente reconocidos.
Aunque por definición una persona es aneuploide si posee menos o más cromosomas que un múltiplo exacto del conjunto haploide, la mayoría de los pacientes aneuploides presentan trisomía (tres en lugar del par normal de un determinado cromosoma) o, con menos frecuencia, monosomía (un solo representante de un determinado cromosoma).

Tanto la trisomía como la monosomía pueden tener consecuencias fenotípicas graves.

La trisomía puede existir en cualquier cromosoma del conjunto, pero la trisomía de un cromosoma completo es raramente compatible con la vida.
Con creces, el tipo más común de trisomía en recién nacidos vivos es la trisomía 21 (cariotipo 47, xxx o xxx ), la constitución cromosómica que se observa en el 95 % de los individuos con síndrome de Down.
La monosomía de un cromosoma entero casi siempre es letal (una excepción importante es la monosomía para el cromosoma X que se describe en el cap. , 10).

Las causas de aneuploidía no se conocen bien, pero se sabe que, cualquiera que sea el mecanismo molecular subyacente, el mecanismo cromosómico más común es la no disyunción meiótica, el fallo de la separación normal de un par de cromosomas durante una de las dos divisiones meióticas, generalmente durante la meiosis I.
Las consecuencias de la no disyunción durante las meiosis I y II son diferentes.
Si el error ocurre durante la meiosis I, el gameto con 24 cromosomas contiene los miembros paterno y materno del par.
Si sucede durante la meiosis II, el gameto con el cromosoma adicional contiene ambas copias del cromosoma paterno o materno. (Sin embargo, es casi seguro que haya ocurrido la recombinación en la meiosis I precedente, lo que genera algunas diferencias genéticas entre las cromátides y, de este modo, en los cromosomas hijos correspondientes).

Tabla 9-1

Algunas abreviaturas utilizadas para la descripción de cromosomas y sus anomalías y ejemplos representativos

Abreviatura.
cen .
del.
der .
dic .
dup .
fra .
i.
ins .
inv .
mar .
mat .
p.
pat .
q.
r.
rop .
rob .
t.
ter .


significado.
Centrómetro .
Deleción.
cromosoma derivado .
Cromosoma dicéntrico.
Duplicación .
Sitio frágil.
Isocromosoma.
Inserción.
Inversión.
Cromosoma marcador.
Origen materno.
Brazo corto del cromosoma.
Origen paterno.
Brazo largo del cromosoma.
Cromosoma en anillo.
Translocación recíproca.
Translocación robertsoniana .
Translocación.
Extremo.
Ganancia de.
Pérdida de.
Rotura y unión.
Mosaicismo.


Cuadro.
Cariotipo de mujer normal.
Cariotipo de varón normal.
Mujer con síndrome del maullido de gato debido a delación de parte del brazo corto de un cromosoma 5.
Cromosoma de translocación derivado del cromosoma 1 y que contiene el centrómero de este mismo cromosoma.
Cromosoma de translocación que posee centrómeros de los cromosomas X e Y.
Varón con cromosoma X frágil.
Mujer con isocromosoma para el brazo largo del cromosoma X.
Inversión pericéntrica del cromosoma 3.
Mujer con un cromosoma adicional no identificado.
Varón con cromosoma de translocación der adicional heredado de la madre.
Mujer con cromosoma X en anillo .
Mujer con translocación equilibrada entre los cromosomas 2 y 8, con roturas en xxx y xxx .
Mujer con delación parcial del brazo largo desde Xq21 a Xqter (la nomenclatura muestra la porción del cromosoma que está presente).
Mujer con trisomía 21.
Mujer normal portadora de translocación robertsoniana entre los brazos largos de los cromosomas 14 y 21; el cariotipo carece de un cromosoma 1 4 y uno 21 normales xxx .
Cromosoma 4 con una porción delecionada del brazo corto.
Cromosoma 5 delecionado en un paciente con síndrome del maullido de gato, con un punto de rotura de delación en la banda xxx .
Descripción de la porción dar xxx de xxx .
Mujer con dos poblaciones celulares, una con cariotipo normal y otra con trisomia 8.


Figura 9-1

Placa metafásica de un paciente con trisomía 21

Figura 9-2

Las diferentes consecuencias de la no disyunción en la meiosis I (izquierda) y la meiosis II(derecha)

Si el error ocurre en la meiosis I, los gametos contienen una forma representativa de ambos miembros del par cromosómico 21 o carecen por completo de cromosoma 21. 
Si la no disyunción sucede en la meiosis II, los gametos anormales poseen dos copias de un cromosoma 21 parental (y ninguna copia del otro) o carecen de un cromosoma 21.

Para determinar el origen parental del error y la etapa meiótica en la cual ocurrió, así como para saber si existe asociación entre la no disyunción y la recombinación, se han utilizado tanto sondas de DNA polimórfico que cubren el brazo largo del cromosoma 21 como heteromorfismos citogenéticos.
Como se muestra en la figura 9-3, en muchos casos, los análisis de marcadores en los cromosomas de los padres y del paciente aneuploide revelan la etapa y el origen parental del error.

Se han comunicado formas más complicadas de aneuploidía múltiple.
Ocasionalmente, un gameto tiene un representante adicional de más de un cromosoma.
La no disyunción puede ocurrir en dos divisiones meióticas sucesivas o, por azar, en ambos gametos masculino y femenino de manera simultánea, lo que produce cigotos con números cromosómicos extraños que son extremadamente raros, excepto en el caso de cromosomas sexuales.
La no disyunción también puede ocurrir en una división mitótica después de la formación del cigoto.

Si esto ocurre en una división por segmentación temprana, puede producirse un mosaicismo clínicamente significativo.
En algunas líneas celulares malignas y algunos cultivos celulares, la no disyunción mitótica puede provocar cariotipos muy anómalos.

Figura 9-3

Determinación del origen de la no disyunción en la trisomía 21

En esta familia, los dos cromosomas 21 de la madre pueden distinguirse por medio de un heteromorfismo cromosómico .
Su hija, que presenta síndrome de Down, ha heredado ambos heteromorfismos, lo que demuestra que el origen de su cromosoma 21 adicional es materno.

La madre y su hija son heterocigóticas para el heteromorfismo, así como para los dos polimorfismos de DNA del cromosoma 21 (loci A y B), lo que demuestra que la no disyunción ocurrió en la meiosis I materna.

Algunos reordenamientos son estables y pueden pasar sin alterarse a través de la división celular, mientras que otros resultan inestables.
Para que un reordenamiento cromosómico sea estable, debe tener elementos estructurales normales, incluyendo un solo centrómero funcional y dos telómeros.

Algunos de los tipos de reordenamientos estructurales que se observan en los cromosomas humanos se ilustran en la figura 9-4.

ANOMALÍAS DE LA ESTRUCTURA CROMOSÓMICA 

Los reordenamientos estructurales se originan por rotura cromosómica seguida de reconstitución en una combinación anormal.
Los reordenamientos pueden ocurrir de muchas maneras, todas las cuales son más raras que la aneuploidía. 

El tipo más frecuente. una translocación equilibrada (recíproca o robertsoniana, que se abordan más adelante), está presente en cerca de I de cada 500 neonatos.
El intercambio cromosómico ocurre espontáneamente con poca frecuencia y también puede ser inducido por agentes que generan roturas cromosómicas (clastógenos), como la radiación ionizante, algunas infecciones virales y muchas sustancias químicas.
Al igual que las anomalías numéricas, los reordenamientos estructurales pueden estar presentes en todas las células de una persona o en forma de mosaico.

Los reordenamientos estructurales se definen como equilibrados ,si los conjuntos cromosómicos tienen el complemento normal de información genética, o bien desequilibrados , si existe información adicional o perdida.

Reordenamientos desequilibrados

En los reordenamientos desequilibrados, es probable que el fenotipo resulte anormal debido a delación, duplicación o (en algunos casos) ambas.
La duplicación de parte de un cromosoma es comparable con una trisomía parcial y la delación genera una monosomía parcial.
Cualquier cambio que altere el equilibrio normal de genes funcionales puede provocar desarrollo anormal.

Deleción

La delación es una pérdida de un segmento cromosómico que genera desequilibrio cromosómico .
Un portador de una delación cromosómica (con un homólogo normal y otro delecionado) es hemicigótico con respecto a la información genética existente en el segmento correspondiente del homólogo normal.
Las consecuencias clínicas dependen del tamaño del segmento delecionado y del número y función de los genes que contiene.

Una deleción puede ser terminal o intersticial. 
Es posible que las delaciones se originen simplemente por rotura cromosómica y pérdida del segmento acéntrico. 
En algunos casos también puede generarse una delación por recombinación desigual entre cromosomas homólogos o cromátides hermanas mal alineados (fig., 9-4 B).
Por último, las delaciones también aparecen por segregación anormal de una translocación o una inversión equilibrada, como se describe más adelante. 

Figura 9-4

Reordenamientos estructurales de cromosomas, descritos en el texto

A) Deleción terminal e intersticial; cada una genera un fragmento acéntrico
B) Recombinación desigual entre segmentos de cromosomas homólogos o entre cromátides hermanas (el segmento duplicado o delecionado se indica con los corchetes) 
C) Cromosoma en anillo con dos fragmentos acéntricos
D) Generación de un isocromosoma para el brazo largo de un cromosoma
E) Translocación robertsoniana entre dos cromosomas acrocéntricos
F) Inserción de un segmento de un cromosoma en otro no homólogo

Figura 9-5

Demostración de delaciones cromosómicas en «síndromes de genes contiguos», mediante bandeo de alta resolución 

En cada caso, el homólogo a la derecha está delecionado.
Arriba: supresión de xxx en una copia del cromosoma 15 en el síndrome de Prader-Willi .
Abajo: supresión del brazo corto terminal de una copia del cromosoma 17 en el síndrome de Miller-Dieker.
Se muestran los cromosomas de dos células. 
En ambos síndromes, la pérdida de material de un homólogo genera aneusomía segmentaria, un desequilibrio parcial de material genético.

Las técnicas de bandeo de alta resolución pueden revelar delaciones que son muy pequeñas para apreciarse en placas metafásicas ordinarias.
Para ser identificable citogenéticamente mediante bandeo de alta resolución, una delación debe comprender al menos de 2.000 a 3.000 kb , pero se han detectado algunas delaciones indetectables cariotípicamente con consecuencias fenotípicas por medio de técnicas moleculares (Ledbetter y Cavence, 1989).
Las pequeñas delaciones dentro de genes o aquellas que se extienden a varios genes contiguos constituyen mecanismos reconocidos de mutación.

Se han identificado numerosas delaciones en la investigación de pacientes dismórficos y en el diagnóstico prenatal, pero el conocimiento de los genes funcionales perdidos en los segmentos delecionados y su relación con las consecuencias fenotípicas está en extremo limitado actualmente. 
Varios síndromes dismórficos se asocian con delaciones citogenéticamente visibles. 
El ejemplo de los síndromes de Prader-Willi y Angelman sugiere que la impresión genómica ( genomic imprinting ), un proceso que marca los cromosomas materno y paterno de modo diferente, puede generar diferencias en la expresión fenotípica en pacientes con deleciones que parecen idénticas en extensión, pero resultan diferentes en cuanto a su origen parental.

Tabla 9-2

Ejemplos seleccionados de aneusomía segmentaria

Transtorno.
Deleción.
Descripción.


Síndrome de Langer-Giedion.
Retraso mental, microcefalia, dismorfismo, anomalías óseas.


Secuencia de DiGeorge.
Ausencia de timo y paratiroides.


Síndrome WAGR.
Tumor de Wilms con aniridia, gonodoblastoma y retraso mental.


Retinoblastoma.
Neoplasia embrionaria de las células retinianas, cáncer en la infancia.


Síndrome de Prader-Willi.
Síndrome dismórfico.


Síndrome de Angelman.
Síndrome dismórfico .


Síndrome de Miller-Dieker .
Síndrome dismórfico grave con lisencefalia y otras malformaciones.


Duplicaciones

Las duplicaciones, como las delaciones, pueden originarse por recombinación desigual o por segregación anormal meiótica en un portador de una translocación o una inversión.
En general, la duplicación parece ser mucho menos nociva que la delación.
Sin embargo, como la duplicación en un gameto genera desequilibrio cromosómico y la rotura cromosómica que provoca puede alterar los genes, a menudo la duplicación produce alguna anomalía fenotípica.

Si bien se han comunicado muchas duplicaciones, muy pocas se han estudiado hasta ahora y las generalizaciones con respecto a los fenotipos asociados resultan prematuras.

No obstante, ciertos fenotipos parecen relacionarse con duplicaciones de regiones cromosómicas determinadas.

Cromosomas en anillo

Los cromosomas en anillo se forman cuando se producen dos roturas en un cromosoma y los extremos rotos de éste se reúnen en una estructura anular.

Si el centrómero está dentro del anillo, al carecer de aquél los dos fragmentos dístales se pierden.
Los cromosomas en anillo son muy raros, pero se han detectado para cada cromosoma humano.

Los anillos pueden generar dificultades en la mitosis cuando las dos cromátides hermanas del cromosoma en anillo intenten separarse en la anafase.
Es posible que se produzca rotura del anillo seguida de fusión, generándose anillos más grandes o más pequeños.
Debido a esta inestabilidad mitótica, es frecuente encontrar cromosomas en anillo sólo en cierta proporción de células.

CROMOSOMAS SEXUALES Y SUS ANOMALÍAS 

Los cromosomas X e Y han atraído la atención y el interés durante mucho tiempo porque difieren entre los sexos, tienen sus propios patrones específicos de herencia y están implicados en la determinación primaria del sexo.
Estructuralmente son muy distintos v están sujetos a formas diferentes de regulación genética, aunque se aparean en la meiosis de los varones. 
Por todas estas razones requieren especial atención. 
En el capítulo anterior se consideraron los principios generales de la citogenética clínica, así como las características específicas de los principales cuadros de aneuploidía autosómica.
En este capítulo se revisan las anomalías más frecuentes de cromosomas sexuales y sus consecuencias clínicas, el estado actual del conocimiento con respecto al control de la determinación sexual y otras anomalías mendelianas de la diferenciación sexual.

BASES CROMOSÓMICAS DE LA DETERMINACIÓN DEL SEXO

Durante decenios se ha sabido que las células humanas de varones y mujeres tienen cromosomas sexuales diferentes y la diferencia es visible en la interfase y en la mitosis.

Aunque el descubrimiento de Painter de los cromosomas sexuales humanos no pudo explotarse clínicamente en esa Época porque las técnicas citogenéticas eran inadecuadas, tras el descubrimiento de las masas de cromatina sexual ( corpúsculos de Barr ) en las células en interfase de las mujeres, pero no en las de los varones, pronto se desarrolló una técnica simple que permitió estudiar los corpúsculos de Barr en frotis bucales.
Como resultado, muy pronto se reconoció que, aunque la mayoría de las mujeres eran «cromatín-positivas» y casi todos los varones eran «cromatín-negativos», existían algunas excepciones.
Especialmente notable fue el hecho de que muchas mujeres infértiles de corta estatura con un trastorno conocido como síndrome de Turner no tenían corpúsculos de Barr, mientras que ciertos varones altos e infértiles con una alteración conocida como síndrome de Klinefelter sí poseían dichos corpúsculos.

Poco tiempo después de ser factible el análisis citogenético, la base cromosómica de estas discrepancias se hizo evidente.
Debido a que los hallazgos anómalos de cromatina sexual sugirieron que los síndromes de Turner y Klinefelter estaban caracterizados por constituciones de cromosomas sexuales inusuales, estos síndromes fueron dos de los primeros trastornos para los que se realizaron estudios cromosómicos. 
Se observó que los pacientes con síndrome de Klinefelter poseían 47 cromosomas, con dos cromosomas X y uno Y, en tanto que la mayoría de los individuos con síndrome de Turner tenían sólo 45 cromosomas, con un solo cromosoma X (cariotipo 45, X).
Pronto esos hallazgos establecieron el papel crucial del cromosoma Y en el desarrollo normal del varón.

El siguiente paso en la comprensión de los cromosomas sexuales humanos fue la explicación de la cromatina sexual en términos de inactivación del X .
A medida que se iban identificando nuevas anomalías de los cromosomas sexuales, se observó que el número de corpúsculos de Barr que se apreciaban en las células en interfase era siempre uno menos que la cantidad total de cromosomas X por célula:

Tabla

Fenotipo sexual.
Cariotipo.
Corpúsculos de Barr.


Varón .
xxx .
0.


xxx .
1.


xxx .
2.


xxx .
3.


Mujer.
xxx .
0.


xxx .
1.


xxx .
2.


xxx .
3.


xxx .
4.


Figura 10-1

Cromatina sexual (corpúsculos de Barr) en células epiteliales de mucosa bucal humana

Las flechas indican la cromatina sexual cerca de la membrana nuclear en células femeninas.
Una célula masculina (derecha) no tiene cromatina sexual. 

La teoría de la inactivación del X ( hipótesis de Lyon ) asegura que un cromosoma X se inactiva en las células somáticas de mujeres normales (pero no en varones normales), con lo que se iguala la expresión de genes ligados al X en ambos sexos.
El corpúsculo de Barr representa el cromosoma X inactivo con replicación tardía.

La asincronía de replicación entre cromosomas X activos (replicación temprana) e inactivos (replicación tardía) puede reconocerse citogenéticamente mediante procedimientos de bandeo especializados, que se denominan «bandeo de replicación» (fig., 10-2).
En pacientes con cromosomas X adicionales, cualquier cromosoma X en número superior a uno se inactiva y forma un corpúsculo de Barr (v., cuadro anterior).
Así, todas las células somáticas diploides en mujeres y varones poseen un solo cromosoma X activo, independientemente del número total de cromosomas X o Y presentes.
La inactivación del X y sus consecuencias se abordaron en relación con trastornos ligados al X en el capítulo 4.

Aunque se han definido muchas anomalías de los cromosomas sexuales y se han descrito en detalle sus consecuencias clínicas, todavía es un misterio el papel preciso de los cromosomas sexuales en la diferenciación sexual.
Existen excepciones, no entendidas por completo, a la regla de que las mujeres son siempre XX y los varones, siempre XY.
Estas excepciones, que incluyen varones XX, mujeres XY y hermafroditas verdaderos XX, sugieren que el cromosoma Y completo no constituye el único determinante del sexo fenotípico.
Actualmente se está empleando el análisis molecular para encontrar una explicación a estas combinaciones inusuales cariotipo/fenotipo, así como para identificar el gen o los genes en el cromosoma Y que determinan el sexo.
Las discrepancias entre sexo cromosómico y sexo gonadal o fenotípico también pueden deberse a mutaciones monogénicas, algunas ligadas al X y otras autosómicas.

EMBRIOLOGÍA DEL APARATO REPRODUCTOR 

La embriología de los aparatos reproductores masculino y femenino se resume en la figura 10-3.
Alrededor de la sexta semana de desarrollo en ambos sexos, las células germinales primordiales han migrado desde su localización extraembrionaria temprana hasta las crestas gonadales, en donde las rodean los cordones sexuales para formar un par de gónadas primitivas.
Hasta este momento, la gónada en desarrollo, cromosómicamente XX o XY, es bipotencial.

El concepto actual es que el desarrollo de un ovario o un testículo está determinado por la acción coordinada de una secuencia de genes que llevan a la formación del ovario, cuando no existe ningún cromosoma Y presente, o bien al desarrollo testicular, si se encuentra dicho cromosoma. 

La vía de desarrollo ovárico se sigue a menos que un gen del brazo corto del cromosoma Y, designado como TDF (factor determinante del testículo), actúe como un interruptor y desvíe el desarrollo a la vía masculina.
La búsqueda del gen principal que determina el testículo es uno de los actuales problemas de vanguardia en la genética médica, y volveremos a abordarlo más adelante.

Figura 10-2

Bandeo de replicación y replicación tardía del cromosoma X inactivo en linfocitos de sangre periférica de una mujer normal 46,XX (izquierda) y una mujer 47,XXX (derecha)

Las regiones cromosómicas brillantes teñidas son las que se replican en etapa tardía de la fase S.
Los cromosomas X activos (flecha pequeña) se replican sincrónicamente con el resto del cariotipo.
Los cromosomas X inactivos (flechas grandes) se replican tardíamente. 

En presencia de un cromosoma Y, el tejido medular forma testículos típicos con túbulos seminíferos y células de Leydig, que, con el estímulo de la gonadotropina coriónica humana de la placenta. pueden secretar andrógenos.
Los espermatogonios, que derivan de las células germinales primordiales por medio de 200 o más mitosis sucesivas, forman las paredes de los túbulos seminíferos junto con las células de soporte de Sertoli.

Si no se encuentra presente ningún cromosoma Y, en su ausencia la gónada forma un ovario.
La corteza se desarrolla, la médula retrocede y los oogonios comienzan a desarrollarse dentro de los folículos.
Los oogonios derivan de las células germinales primitivas por una serie de alrededor de 30 mitosis. muchas menos que la cantidad requerida para la espermatogénesis.
Al final del tercer mes los oogonios comienzan la meiosis I, pero este proceso se detiene en una etapa llamada dictioteno , en la que la célula permanece hasta que se produce la ovulación muchos años después.
La mayor parte de los oogonios degeneran después del nacimiento, y sólo alrededor de 400 maduran hasta óvulos durante los más o menos 30 años de madurez sexual de la mujer.

Figura 10-3

Esquema de las gónadas y los conductos genitales, como se describen en el texto

Mientras las células germinales primordiales están migrando a las crestas genitales, el engrosamiento de estas crestas indica el desarrollo de los conductos genitales, el mesonéfrico (anteriormente llamado de Wolff) y el paramesonéfrico (antes denominado de Müller).
En el varón, las células de Leydig de los testículos fetales producen andrógeno, que estimula los conductos mesonéfricos para que formen los conductos genitales masculinos, y las células de Sertoli producen una hormona que suprime la formación de los conductos paramesonéfricos.
En la mujer (o en un embrión sin gónadas), los conductos mesonéfricos se atrofien y el conducto paramesonéfrico se desarrolla hasta constituir el sistema de conductos femeninos.

En un embrión en etapa temprana, los genitales externos consisten en un tubérculo genital, protuberancias labioscrotales pareadas y pliegues uretrales pareados.

Desde este estado no diferenciado, los genitales externos masculinos se desarrollan bajo la influencia de andrógenos, o, en ausencia de un testículo, se forman genitales externos femeninos, independientemente de si se encuentra presente un ovario o no.

CROMOSOMA Y

La estructura del cromosoma Y y su papel en el desarrollo sexual se han analizado a nivel molecular.
En la meiosis del varón, los cromosomas X e Y normalmente se aparean por segmentos en los extremos de sus brazos cortos y ocurre recombinación en esta región .
El segmento apareado incluye la región seudoautosómica de los cromosomas X e Y, llamada así porque las copias ligadas a X e Y de esta zona son homólogas entre sí, igual que en los pares de autosomas.

El principal gen determinante del testículo 

Como resultado de la recombinación en la meiosis I, las secuencias en la región seudoautosómica de los cromosomas X e Y se intercambien en condiciones normales .

En raras ocasiones, sin embargo, la recombinación genética ocurre entre los brazos cortos de X e Y fuera de la región seudoautosómica.
Este mecanismo de intercambio aberrante puede producir dos anomalías raras: varones XX y mujeres XY .
Los varones XX son varones fenotípicos con un cariotipo 46, XX, que generalmente poseen algunas secuencias cromosómicas del Y translocadas al brazo corto del X.
Estas secuencias no son visibles citogenéticamente, pero el análisis molecular las ha revelado.
De forma similar, las mujeres fenotípicas con un cariotipo 46,XY han perdido la región determinante de los testículos del cromosoma Y.
Cada uno de estos trastornos de inversión sexual sucede con una frecuencia de aproximadamente I por cada 20.000 nacimientos.

Figura 10-4

Esquema de la región que determina el sexo en el cromosoma Y

Los reordenamientos cromosómicos en varones XX y en una mujer con una t(Y;22) han sido claves para la asignación de la región que determina el sexo en una localización cercana al extremo de la región seudoautosómica. 
Mutaciones puntuales en el gen SRY en mujeres 46,XY proporcionan evidencia directa de que este gen puede ser el mismo que TDF.

Figura 10-5

Etiología del fenotipo de varón XX por intercambio aberrante entre secuencias ligadas a X e Y

Los cromosomas X e Y en condiciones normales se recombinan dentro del segmento seudoautosómico en la meiosis masculina.
Si la recombinación ocurre por debajo del limite seudoautosómico, entre las porciones específicas X e Y, las secuencias que generan la diferenciación sexual del varón pueden translocarse de Y a X.
La fecundación con un espermatozoide que contiene uno de estos cromosomas X genera un varón XX.

Se han utilizado diferentes delaciones de la región seudoautosómica y de la zona sexual específica del cromosoma Y en un esfuerzo para mapear la localización precisa de la región primaria determinante del testículo en el brazo corto del cromosoma Y.
Examinando a un varón XX con el segmento más pequeño identificado de material cromosómico del Y y a una mujer portadora de una translocación xxx que carecía de una pequeña región Yp, xxx reconocieron una región de aproximadamente 140 kb (equivalente a sólo el 0,2 % del cromosoma Y completo) que probablemente codifique al menos parte del gen TDF.
Las secuencias de DNA contenidas en esta región se conservan en gran medida en otros mamíferos, y se ha identificado un segmento homólogo en el cromosoma X.
La secuencia ligada al Y se denominó ZFY porque potencialmente codifica una proteína denominada transportadora de cinc que, por analogía con otras proteínas de este tipo, se piensa que se enlaza con ácidos nucleicos en secuencias específicas y, por lo tanto, potencialmente regula la expresión génica.

Inicialmente se aceptó la posibilidad de que la secuencia ZFY fuera el principal gen determinante del testículo, largo tiempo buscado, el TDF.
Sin embargo, estudios más recientes han planteado serias dudas a esta interpretación. 

La presencia de un gen homólogo ligado al X (conocido como ZFX), la ausencia de secuencias similares a ZFY en los cromosomas sexuales de los marsupiales, informes de varones XX con secuencias del cromosoma Y muy cercanas al límite de la región seudoautosómica, pero sin secuencias ZFY y la ausencia de la expresión del homólogo de ZFY de ratón en testículos embrionarios de ratones indican que ZFY y TDF son distintos y ZFY no puede inducir por sí solo la diferenciación sexual en el varón.
Otro locus candidato, denominado «región determinante del sexo en Y» (SRY), fue identificado por xxx.

Este gen se encuentra entre el límite de la región seudoautosómica y ZFY en el brazo corto del cromosoma Y, y está presente en varios varones XX o hermafroditas XX (que se describen más adelante en este capítulo) que no tienen secuencias ZFY .

La delación del gen SRY (así como del gen ZFY) en mujeres con la translocación xxx antes mencionada deja abierta la pregunta de cuál (o cuáles) de estos dos genes genera masculinización.
En fecha más reciente, sin embargo, se han encontrado tres mujeres xxx que no tenían alteraciones citológicas del cromosoma Y, pero presentaban mutaciones en la secuencia codificadora de SRY, lo que implica en gran medida a SRY en la determinación sexual del varón.
No obstante, no es posible descartar un papel secundario para ZFY.
De este modo, la identidad y localización precisa del verdadero gen o genes determinantes de testículos están, por establecer.
Mientras tanto, lo hasta aquí señalado sirve como una excelente ilustración de la importancia de examinar unos cuantos pacientes inusuales, o incluso uno solo, para establecer principios científicos de extraordinario significado teórico y práctico.


Computación con ADN

La manipulación de ADN orientada a la resolución de problemas matemáticos obliga a redefinir el término «computación». 

Ordenador.
La palabra evoca imágenes de teclados y monitores.
Nos vienen a la mente palabras como «ROM» , «RAM» , «gigabyte» y «megahertz». 
Nos hemos hecho a la idea de que la computación se efectúa por medio de componentes electrónicos construidos sobre un sustrato de silicio.

Pero,
¿ha de ser así?
La computadora de que nos valemos para leer estas palabras se parece muy poco a un ordenador personal.
Tal vez nuestra forma de concebir la computación sea demasiado restringida.
¿Y si las máquinas de cómputo fueran ubicuas adoptasen multitud de formas?
¿No podría existir una computadora en fase líquida, que efectuase sus cálculos por interacción entre moléculas en disolución? 
Podría.
Esta es la historia de la computación con ADN .

Mi participación en la misma arranca de 1993.
Hasta entonces no había puesto los pies en un laboratorio de biología molecular.
Aunque soy matemático e informático, había investigado un poquito sobre el sida, investigación no carente de importancia.
Por desdicha, tuve muy poco éxito en comunicar mis ideas a los expertos en el tema.
Queriendo, pues, ser más persuasivo, decidí empezar por conocer a fondo la biología el VIH .
De ahí lo del laboratorio de biología molecular.
En él, guiado por Nickolas Chelyapov (que es ahora jefe científico de mi propio laboratorio), comencé a estudiar los métodos de la biología moderna.

Era fascinante.
Estaba creando con mis propias manos moléculas de ADN que no existían en la naturaleza.
Y las estaba introduciendo en bacterias, donde impartían instrucciones de montaje para la producción de proteínas capaces de alterar la naturaleza misma del organismo.

Durante este período de aprendizaje intenso cayó en mis manos Biología Molecular del Gen , uno de cuyos autores es James D., Watson, que compartió el Nobel con Francis Crick.
El concepto que yo tenía de la biología cambió de repente.
La biología había dejado de ser la ciencia de cosas de un olor indescifrable que se guardaban en frigoríficos (la idea que conservaba desde mis días de estudiante en la Universidad de California en Berkeley, allá por los años sesenta).
Esta ciencia estaba experimentando una revolución, adquiriendo rápidamente la profundidad y la potencia reservadas hasta entonces a la física.
La biología consistía ahora en el estudio de la información almacenada en ADN - ristras de cuatro letras, A , T , G y C , símbolos de las bases adenina, timina, guanina y citosina - y de las transformaciones que esa información experimenta en el interior de la célula. 
¡Aquí había matemáticas!

Cierta noche, leyendo el texto de Watson, llegué a la descripción de la ADN polimerasa.
Es la reina de las enzimas, la creadora de vida.
En condiciones apropiadas, dada una hebra de ADN , la ADN polimerasa produce una segunda hebra complementaria - su complemento Watson-Crick- en la cual cada C es reemplazada por una G , cada G por una C , cada A por una T , y cada T por una A .
Por ejemplo, si una molécula posee la secuencia CATGTC , la ADN polimerasa producir una nueva molécula con la secuencia GTACAG .
La polimerasa permite la reproducción de ADN , lo que a su vez posibilita la reproducción de las células y, en última instancia, la reproducción de uno mismo.
Para un reduccionista estricto la vida es la reproducción de ADN por medio de la ADN polimerasa.

La ADN polimerasa constituye una nanomáquina asombrosa, una molécula excepcional que «salta a horcajadas» sobre una hebra de ADN y por ella se desliza mientras lee cada base y escribe su complementaria sobre una hebra nueva, en crecimiento, de ADN .
Reflexionando sobre esta enzima maravillosa, descubrí sorprendido su semejanza con un trabajo de Alan M., Turing, de 1936.
Este famoso matemático británico- e independientemente de él, Kurt Godel, Alonzo Church y S., C., Kleen- habían emprendido un estudio riguroso de la noción de «computabilidad».
Un trabajo teorético que precedió en unos diez años al advenimiento de los ordenadores reales y fructificó en algunos de los principales resultados matemáticos del siglo XX.

Para tal estudio, Turing había inventado una computadora «de juguete», la «máquina de Turing». 
No tenía ninguna pretensión de que fuera un dispositivo real; se trataba, por el contrario, de un artefacto mental, idóneo para la investigación matemática. 

Por eso debía ser sumamente simple; Turing lo logró con brillantez.
Una de las versiones de su máquina consistía en un par de cintas y un mecanismo de «control finito», que iba haciendo avanzar la cinta «de entrada», leyendo datos de ella, al mismo tiempo que movía la cinta «de salida», leyendo y escribiendo otros datos.
El control finito se podía programar mediante instrucciones sencillas; además podía escribirse un programa que leyera en la cinta de entrada una ristra de signos A , T , C y G , y fuera escribiendo su ristra Watson-Crick complementaria en la cinta de salida. 
La semejanza con la ADN polimerasa era clamorosa.

Pero había otro dato importante que confería a esta semejanza un carácter asombroso, a saber, la universalidad de la computadora de Turing.
Pese a su sencillez, cabe programarla para que compute todo cuanto sea computable. 
(Esta proposición constituye, en esencia, la famosa «tesis de Church»).
Dicho de otro modo, cabía programar una máquina de Turing para que produjese ristras Watson-Crick complementarias, para que descompusiera números enteros en sus factores primos, para que jugase al ajedrez, y así por menudo.
Mas,
¿cómo aplicar el ADN a la resolución de problemas?

Mi primera idea fue construir una computadora de ADN a imagen de la máquina de Turing, en la que una enzima reemplazara al control finito.
Una idea muy parecida había sido propuesta unos diez años antes por Charles H., Bennet y Rolf Landauer, de IBM .
Pero, aunque se conocía una enzima (la ADN polimerasa) capaz de efectuar la complementación Watson-Crick, se suponía imposible la existencia de otras enzimas capaces de realizar funciones de interés matemático, como la descomposición factorial de números.

Lo cual nos lleva a una confesión.
Los biotecnólogos somos una comunidad de ladrones.
Robamos de la célula. 
Estamos muy lejos de poder crear ex novo máquinas moleculares milagrosas, como la ADN polimerasa.

Por fortuna, tres o cuatro mil millones de años de evolución han producido células repletas de máquinas diminutas y maravillosas.
Son estas máquinas, incautadas a la célula, las que hacen posible la biotecnología moderna.
Pero, que se sepa, la evolución no ha creado una máquina molecular que juegue al ajedrez.
Por tanto, si pretendiera construir una computadora de ADN capaz de algo que encerrara valor en computación, tendría que recurrir a los instrumentos disponibles.
Este instrumental consta, en esencia, de lo siguiente:

Emparejamiento Watson-Crick

Cada hebra de ADN tiene su complemento Watson-Crick.
Ocurre que si, en una disolución, una molécula de ADN tropieza con su complementaria, las dos hebras se imbrican, vale decir, las dos hebras se enroscan una alrededor de la otra en la célebre doble hélice.
Las hebras no están ligadas por enlaces covalentes, sino por puentes de hidrógeno, unas fuerzas débiles.
Cuando una molécula de ADN de la disolución se topa con una molécula de ADN que no es complementaria suya (ni posee largas secuencias de complementariedad), las moléculas no se imbrican.

Polimerasas

Las polimerasas copian información de una molécula en otra.
Por ejemplo, la ADN polimerasa construir una hebra de ADN, complementaria Watson-Crick, a partir de un molde de ADN .
La enzima requiere, cierto, una «señal de arranque» que le diga dónde ha de empezar a realizar la copia complementaria. 
Un cebador aporta la señal:
iniciador que consiste en un fragmento (posiblemente corto) de ADN enroscado al molde por complementariedad Watson-Crick. 
Cada vez que se encuentra una pareja cebador- molde. la polimerasa empieza a añadir bases al cebador, y va creando una copia complementaria del molde.

Ligasas

Las ligasas unen moléculas entre sí.
Por ejemplo, la ADN ligasa toma dos hebras de ADN que se encuentran próximas y las une mediante enlaces covalentes, formando una sola hebra. 
Con la ADN ligasa la célula restaña las hebras rotas por culpa, entre otros motivos, de una exposición a luz ultravioleta.

Nucleasas

Las nucleasas seccionan los ácidos nucleicos.
En ese orden, las endonucleasas de restricción «exploran» las hebras de ADN en busca de una secuencia determinada de bases; hallada, seccionan la molécula por ese punto, dividiéndola en dos piezas.
Una enzima de restricción es EcoRI (de Escherischia coli ), que secciona la molécula tras la G de la secuencia GAATTC ; casi nunca tajar una hebra de ADN por otro sitio.
Se ha sugerido que las enzimas de restricción evolucionaron para defender de los virus a las bacterias.
Así E. coli cuenta con un medio (la metilación) de proteger a su propio ADN de la EcoRI , pero un virus invasor que contuviera la secuencia letal GAATTC quedaría hecho trizas.
Aunque mi computadora de ADN no utilizó enzimas de restricción, éstas sí se emplearon en experimentos posteriores.

Electroforesis en gel

No le hemos quitado a la célula este ingenio.
En un extremo de una lámina gruesa de gel se dispone una solución de moléculas heterogéneas de ADN ; se aplica una corriente eléctrica. 
Las moléculas de ADN , dotadas de carga negativa, se mueven hacia el polo positivo, o ánodo: 
las hebras cortas cursan más rápidas que las largas.
El proceso separa, pues, el ADN de acuerdo con su longitud.
Para observar el patrón de bandas formado en el gel con las moléculas de ADN de diversa longitud nos valemos de reactivos químicos especiales y luz ultravioleta.

Síntesis de ADN

Podemos ya escribir una secuencia de ADN en un papel, enviar la nota a un servicio de síntesis y recibir días más tarde un tubo de ensayo que contiene aproximadamente 1018 moléculas de ADN , todas las cuales (o la mayoría, cuando menos) tienen la secuencia descrita.
Podemos ahora manipular secuencias de longitud aproximada xxx .
Las moléculas se entregan deshidratadas, en un tubito, y ofrecen el aspecto de un montoncito blanco y amorfo.

No parecía probable que ninguno de esos ingenios sirviera para jugar al ajedrez.

Pero no se debe olvidar otro hecho importante que los lógicos de los años treinta nos han enseñado:
la computación es sencilla.
Para construir una computadora bastan un método para almacenar información y unas cuantas operaciones simples para actuar sobre ella.
La máquina de Turing almacena información en forma de secuencias de letras escritas sobre una cinta y manipula esa información mediante las instrucciones contenidas en el control finito.
Un ordenador electrónico almacena información en forma de secuencias de ceros y unos, y manipula esa información mediante las operaciones disponibles en los microcircuitos de su procesador.
Merece destacarse que prácticamente cualquier método de almacenar información y cualquier conjunto de operaciones para actuar sobre tal información resultan adecuados.

¿Adecuados para qué?
Para la computación universal, para el cómputo de cualquier cosa susceptible de computarse.
Para hacer que nuestro ordenador realice complementos Watson-Crick, o juegue al ajedrez, sólo se necesita comenzar con la información de entrada correcta, y aplicar la secuencia debida de operaciones,es decir, la ejecución de un programa.
El ADN constituye una forma magnífica de almacenar información.
De hecho, la célula ha estado utilizando este método para almacenar las «instrucciones de la vida» durante miles de millones de años.
Además, las enzimas, como las polimerasas y las ligasas, han servido para operar sobre esta información.
¿Había elementos suficientes para construir una computadora universal?
A la vista de las lecciones impartidas en los años treinta, estaba seguro de que la respuesta era afirmativa.

La tarea siguiente consistía en seleccionar un problema que resolver.
El problema no debería parecer preparado a la medida de la máquina. para que ésta pudiera resolverlo; sí debería, en cambio, poner de manifiesto las posibilidades que abre el nuevo método de computación. 
Opté por el problema del camino hamiltoniano. 

William Rowan Hamilton fue astrónomo real de Irlanda a mediados del siglo XIX.

El problema que hoy lleva su nombre lo hemos ilustrado en un recuadro particular, al que nos referiremos de continuo. 
Representemos por flechas (lados orientados) los vuelos directos entre las ciudades (vértices) del mapa (grafo). 

Por ejemplo, se puede volar sin escalas de Barcelona a Madrid, pero no de Madrid a Barcelona.
La tarea (el problema del camino hamiltoniano) consiste en determinar si existe una secuencia de vuelos consecutivos (un camino) que parta de Valencia (vértice inicial), acabe en Zaragoza (el vértice final) y visite cada una de las demás ciudades ( Barcelona y Madrid) exactamente una Vez.

Un itinerario así se denomina camino hamiltoniano. 
En el ejemplo presentado en el recuadro resulta fácil ver que existe sólo un camino hamiltoniano, la ruta que recorre las ciudades en este orden: Valencia, Barcelona, Madrid, Zaragoza.

Si cambiamos el orden, y exigimos que sea Zaragoza el punto de partida, y Valencia el final de trayecto, es evidente que no existirá ningún camino hamiltoniano.

Con mayor generalidad, dado un grafo con lados orientados y vértices inicial y final especificados, se dice que existe un camino hamiltoniano si y solamente si existe un camino que parte del vértice inicialmente fijado, concluye en el vértice final y visita cada vértice una sola vez.
El problema del camino hamiltoniano consiste en decidir si, para un grafo cualquiera, con vértices inicial y final especificados, existe o no un camino hamiltoniano.

El problema del camino hamiltoniano ha sido objeto de profundo estudio en informática.
No se ha descubierto ningún algoritmo eficiente (es decir, rápido) para resolverlo. 
Antes bien, parece verosímil la existencia de grafos de menos de xxx vértices, para los cuales la determinación de la existencia o inexistencia de caminos hamiltonianos exigiría cientos de años, incluso utilizando los mejores ordenadores y algoritmos disponibles. 

En los primeros años setenta se demostró que el problema del camino hamiltoniano era «NP - completo». 
Sin entrar en la teoría de NP - completitud, baste decir que este descubrimiento convenció a la mayoría de los teóricos de que no es posible en absoluto ningún algoritmo eficiente para el problema (aunque ]a demostración de este hecho sigue siendo el problema abierto más importante de las ciencias de cómputo, el problema denominado « NP = P ».
No se está diciendo que no existan algoritmos para el problema del camino hamiltoniano, sino sólo que no los hay eficientes.
Consideremos el algoritmo siguiente:

Dado un gráfico con n vértices, 
1. Genérese un conjunto de caminos aleatorios que atraviesen el grafo
2. Para cada camino del conjunto:
a. Compruébese si el camino parte del vértice inicial y acaba en el vértice final; Si no es así elimínese el camino del conjunto
b. Compruébese si el camino visita exactamente n vértices; Si no es así, elimínese el camino del conjunto
c. Para cada vértice, compruébese si ese camino pasa por el vértice; De no ser así elimínese el camino del conjunto

3. Si el conjunto no es vacío, se informa que existe un camino hamiltoniano; Si el conjunto es vacío, se informa que no existe camino hamiltoniano


No se trata de un algoritmo perfecto.
Pero si la generación de caminos es lo suficientemente aleatoria, y el conjunto generado bastante grande, el algoritmo presenta grandes probabilidades de suministrar la respuesta correcta. 
Este es el algoritmo que he implantado en el primer cómputo por ADN .

Busqué para mi experimento un problema de camino hamiltoniano pequeño, que admitiera una solución rápida en el laboratorio, aunque lo bastante grande para proporcionar una clara «demostración de posibilidades» de la computación con ADN.
Seleccioné el mapa de 7 ciudades y 14 vuelos del inserto del recuadro.

Un estudio no científico ha puesto de manifiesto que, por término medio, hacen falta unos 54 segundos para hallar el único camino hamiltoniano que admite este grato. (Los lectores pueden empezar ya).

Para simplificar aquí la exposición, nos fijaremos en el mapa del recuadro, que contiene sólo cuatro ciudades.
Valencia, Barcelona, Madrid y Zaragoza, conectadas por seis vuelos.
El problema consiste en determinar la existencia de un camino hamiltoniano que parta de Valencia y concluya en Zaragoza.

Empecé asignando a cada ciudad una secuencia arbitraria de ADN .
En nuestro ejemplo, Valencia se convierte en ACTTGCAG , Barcelona en TCGGACTG , y así las demás.
Resulta conveniente imaginar que la primera mitad de la secuencia de ADN es el nombre de pila de la ciudad, y la segunda, el apellido.
Así pues, el apellido de Valencia es GCAG , mientras que el nombre de Barcelona es TCGG .
A continuación, le asigné a cada vuelo directo un «número de vuelo» en ADN , obtenido por concatenación del apellido de la ciudad de origen con el nombre de la ciudad de destino.
En el ejemplo del recuadro ilustrado, el número del vuelo de Valencia a Barcelona se convierte en GCAGTCGG .

Recordemos que cada hebra de ADN tiene su complemento Watson-Crick.
Así pues, cada ciudad tiene su nombre ADN complementario.
El nombre complementario de Valencia, por ejemplo, es TGAACGTC .

Tras elaborar estas codificaciones, tuve sintetizados los nombres complementarios en ADN de las ciudades y los de los números de vuelo.(Se vio más adelante que sobraban los nombres en ADN de las ciudades). 
Tomé una pizca (unas xxx moléculas) de cada una de las distintas secuencias; las introduje en un mismo tubo de ensayo.
Para empezar el cómputo, me limité a añadir agua, así como ligasa, sal y otros pocos ingredientes, con que remedar las condiciones del interior de una célula.
En total, la disolución utilizada sería del orden de una cincuentava parte de una cucharadita.
En cosa de un segundo, tenía en mi mano la solución al problema del camino hamiltoniano. 

Para comprobarlo, consideremos lo que transpira en el tubo. 
Por ejemplo, el vuelo de Valencia a Barcelona, que tiene un «número ADN » ( GCAGTCGC ),y el nombre complementario de Barcelona ( AGCCTGAC )podrían encontrarse por azar.
Por diseño, la primera secuencia acaba con TCGG , mientras que la segunda empieza por AGCC .

Dado que estas dos secuencias son complementarias, se adherirán una a otra.
Si el complejo resultante se encuentra ahora con el número de vuelo de Barcelona a Madrid ( ACTGGGCT ), éste, también, se unirá al complejo, porque el final del primero ( TGAC ) es complementario del comienzo del segundo ( ACTG ).
Los complejos, de este modo, Irán aumentando de longitud, siendo sucesivamente «entablillados»los números de vuelo por los nombres en ADN , complementarios, de las ciudades. 
Después, la ligasa de la mezcla engarzará de forma permanente las cadenas de los números de vuelo en ADN .
Por tanto, el tubo de ensayo contiene moléculas que codifican sendas aleatorias que visitan las distintas ciudades (como exigía el primer paso del algoritmo).

Puesto que empecé con tan gran número de moléculas de ADN , y dado que el problema solamente contenía un puñado de ciudades, podía tener la seguridad de que al menos una de las moléculas formadas sería la codificación del camino hamiltoniano. 
Estremecía el pensar que la solución de un problema matemático pudiera quedar almacenada en una molécula.

Fijémonos también en que todos los caminos fueron creados de una vez, por interacciones simultáneas de cientos de billones de moléculas.
Esta reacción bioquímica equivale a un enorme procesamiento en paralelo. 

En el caso del mapa del recuadro ilustrado no hay más que un camino hamiltoniano, el que pasa por Valencia, Barcelona, Madrid y Zaragoza, en ese orden. 
Por tanto, la molécula que codifique la solución tendrá la secuencia GCAGTCGGACTGGGCTATGTCCGA .

Por desgracia, aunque tenía la solución en la mano, había también otros xxx billones de moléculas que codificaban caminos no hamiltonianos. 
Era preciso eliminarlas.
Para expurgar las moléculas que no empezaban en la ciudad de partida ni concluían en el destino final, me basé en la reacción en cadena de la polimerasa ( RCP ).
Esta técnica exige muchas copias de dos breves tramos de ADN que sirvan de cebadores, que indiquen a la ADN polimerasa dónde ha de empezar su replicación Watson-Crick.
Los cebadores utilizados fueron el apellido de la ciudad de partida (GCAG para Valencia) y el complemento Watson-Crick del primer nombre de la ciudad final ( GGCT para Zaragoza).
Los dos cebadores actuaban concertadamente.
El primero alertaba a la ADN polimerasa para que copiase complementos de secuencia que tuvieran la ciudad de partida correcta:
el segundo iniciaba la duplicación de moléculas que codificaran la ciudad final correcta.

La RCP cursa mediante ciclos térmicos, consistentes en repetidas elevaciones y descensos de la temperatura de la mezcla contenida en el tubo de ensayo.
Las temperaturas moderadas promueven que la ADN polimerasa empiece la replicación:
las temperaturas más elevadas provocan que las hebras encestadas resultantes se escindan de su estructura en doble hélice. lo que permite la replicación subsiguiente de las piezas individuales.

Con ello las moléculas que tuvieran las ciudades iniciales y finales correctas se reprodujeron a una tasa exponencial. 
En comparación, las moléculas que tenían ciudad inicial correcta, pero una ciudad terminal incorrecta, o viceversa, se duplicaron con una tasa lineal, mucho más lenta.
Las secuencias de ADN sin principio ni final correctos permanecieron sin replicarse.
Así pues, una vez completa la RCP , tomando una pequeña cantidad de la mezcla, se obtenía una disolución donde había muchas copias de las moléculas con las ciudades inicial y final correctas, y pocas, o ninguna, que no cumplieran este criterio.
Quedaba así efectuado el paso xxx del algoritmo.

Recurrí luego a la electroforesis en gel para identificar las moléculas que alcanzaban la longitud correcta (en el ejemplo del recuadro ilustrado, una longitud de 24).
Se desecharon las demás moléculas.
Con ello se concluía el paso xxx del algoritmo.

Para comprobar si los itinerarios de las secuencias restantes pasaban todas por las ciudades intermedias, me apoyé en la separación por afinidad, proceso que utiliza múltiples copias de una «sonda» de ADN que codifica el nombre complementario de una ciudad (por ejemplo, Barcelona).
Estas sondas van prendidas de bolitas microscópicas de hierro, de un micrómetro de diámetro.

Suspendí las bolas en el tubo que contenía las moléculas restantes en condiciones que favorecían el emparejamiento Watson-Crick.
Sólo las moléculas que incluyeran el nombre de la ciudad deseada (Barcelona) se trabarían con las sondas.
Coloqué después un imán al costado del tubo de ensayo, para atraer y mantener las bolitas férricas adheridas a la pared del tubo, mientras vertía fuera la fase líquida cuyas moléculas no contenían el nombre de la ciudad deseada.

Añadí después nuevo disolvente y retiré el imán, para que las esferitas de hierro volvieran a quedar en suspensión.
Elevando la temperatura de la mezcla, las moléculas se desprendían de las sondas, para disolverse de nuevo en el líquido.
Volví a aplicar el imán para atraer las esferitas hacia las paredes del tubo, pero ahora las esferitas no llevaban adheridas moléculas.
El líquido, que contenía las hebras de ADN deseadas (en el ejemplo, las que determinan rutas que pasan por Barcelona), podía ser vertido en un nuevo tubo para ulterior separación. 
El proceso fue repetido para las ciudades intermedias restantes (Madrid, en este caso).
Este proceso iterativo, que me llevó todo un día cuando lo realicé en mi laboratorio, fue la parte más tediosa del experimento. 

Con las separaciones de afinidad terminó el paso xxx del algoritmo.
Sabía yo que las moléculas que quedasen en el tubo sanan precisamente las que codificasen los caminos de Hamilton.
Por tanto, si el tubo contenía todavía algo de ADN , se podía concluir que existía en el grato un camino hamiltoniano; la inexistencia de ADN indicaría que no existía dicho camino.
Por suerte, para realizar esta determinación podía aplicar un paso de RCP más, seguida por otra operación de electroforesis en gel.
Los análisis finales revelaron que las moléculas subsistentes realmente codificaban el camino de Hamilton deseado.
Tras siete días de laboratorio, se había llevado a cabo el primer cómputo realizado con ADN .

¿Qué ocurrirá en el futuro?
Las computadoras moleculares encierran muchas propiedades atractivas. 
Proporcionan un almacenamiento de información de una densidad excepcional.
Por ejemplo, un gramo de ADN , que ocupa, en seco, alrededor de un centímetro cúbico, puede almacenar aproximadamente la información de un billón de discos compactos. 
Aportan un enorme paralelismo.
Incluso en el diminuto experimento realizado en un cincuentavo de cucharadita de disolución, alrededor de unos xxx números de vuelo, codificados en ADN , quedaron simultáneamente concatenados en cosa de un segundo.
No está claro si el más veloz de los superordenadores disponibles podría realizar tan rápidamente una tarea semejante.

Las computadoras moleculares ofrecen, además, la posibilidad de alcanzar el máximo rendimiento desde el punto de vista energético.
En principio, bastaría un joule para realizar unas xxx operaciones de ligadura. 
Se trata de un resultado muy notable, habida cuenta de que la segunda ley de la termodinámica dicta un máximo de xxx operaciones (irreversibles) por joule (a temperatura ambiente).
Las supercomputadoras existentes son, en punto a energía, mucho menos eficientes, pues ejecutan a lo sumo unas xxx operaciones por joule.

En muchos departamentos y laboratorios se trabaja para sacar partido de estas propiedades.
¿Tendrán éxito en la creación de computadoras moleculares capaces de competir con los ordenadores electrónicos? 
Es cosa por ver.
Durante medio siglo, enormes inversiones financieras e intelectuales han convertido a los ordenadores electrónicos en las maravillas de nuestra era.
Serán difíciles de vencer.

Pero seríamos muy miopes si considerásemos esta investigación sólo desde el punto de vista práctico.
Mi experimento puede verse como una manifestación de un rea nueva de la ciencia, que es posible gracias a nuestra capacidad, rápidamente creciente, para controlar el mundo molecular.
Podemos hallar en muchos lugares pruebas de esta nueva «ciencia molecular».
Sin ir más lejos, Gerald F., Joyce, del Instituto Scripps de Investigación en La Jolla, «cría» billones de moléculas de ARN , generación tras generación, hasta que evolucionan moléculas «campeonas» que poseen las propiedades catalíticas que él busca. 
Julius Rebek, Jr., del Instituto de Tecnología de Massachusetts, crea moléculas capaces de reproducirse, lo que nos informa de cómo pudo haber surgido la vida en la Tierra.
Estimulado por la investigación en computación con ADN , Erik Winfree, del Instituto de Tecnología de California, sintetiza complejos moleculares «inteligentes» que se pueden programar para que se ensamblen a sí mismos según estructuras predeterminadas y de complejidad arbitraria.
Hay muchos otros ejemplos.
Debemos centrar nuestra atención en la enormes posibilidades que este campo ofrece y en cultivarlas. 

Por lo que a mí concierne, me basta saber que la computación con ADN es posible.
A lo largo del último medio siglo han florecido la biología y las ciencias de cómputo, las cuales, no cabe duda, ocuparán un lugar central en nuestro progreso científico y económico en el nuevo milenio.
Pero la biología y las ciencias de cómputo - la vida y la informática - están relacionadas.
Tengo fe en que en su interfase esperan grandes hallazgos a quienes allí acudan.

Figura 1

LAS MOLÉCULAS DE ADN , con sus secuencias de adenina, timina, citosina y guanina (representadas por las letras A , T , C y G ), pueden servir para almacenar información y calcular

La molécula aquí mostrada en color, GCAGTCGGACTGGGCTATGTCCGA , determina la solución del caso particular del problema del camino hamiltoniano, que se expone en el recuadro ilustrado del artículo.

Figura 2

EMPAREJAMIENTO WATSON-CRICK , en el que las C forman pares con las G y las A con las T 

Producir hebras de ADN con números de vuelo (aquí se muestran:
de Valencia a Barcelona, de Barcelona a Madrid, y de Madrid a Zaragoza) mantenidas extremo contra extremo mediante hebras que codifican los nombres complementarios de las ciudades (aquí vemos: Barcelona y Madrid).

Figura 3

LAS LIGASAS vuelven a empalmar moléculas sueltas

Cada vez que la proteína descubre la proximidad de dos hebras de ADN , establece entre ellas un enlace covalente, empalmándolas y formando una sola hebra.

Figura 4

LA REACCIÓN EN CADENA DE LA POLIMERASA , abreviada RCP , se utiliza para la replicación de las moléculas de ADN que comienzan por la ciudad de partida (Valencia) y concluyen en la ciudad final (Zaragoza)

En este ejemplo, un «cebador» - la ristra GGCT , representativa del complemento del nombre de Zaragoza - se engarza en el extremo derecho de una hebra de ADN .
El cebador indica a la polimerasa que ha de empezar a sintetizar una secuencia complementaria de la hebra.
Una vez que la polimerasa ha terminado, se escinde la doble hélice en dos hebras, para construir secuencias complementarias de cada mitad.
Este proceso se repite muchas veces, con el fin de obtener un gran número de copias de moléculas que contengan las ciudades inicial y final correctas.
A continuación, mediante electroforesis en gel, se seleccionan de entre éstas las moléculas que tengan secuencias de longitud correcta, que es xxx .

Figura 5

LAS MOLÉCULAS DE SONDEO sirven para localizar hebras de ADN que codifiquen caminos que pasen por las ciudades intermedias (Barcelona y Madrid)

A tal fin, se ligan moléculas sonda que contengan el complemento del nombre en ADN de Barcelona ( AGCCTGAC ) a una microesfera de hierro en suspensión dentro del líquido. 
Por la afinidad Watson-Crick, las sondas capturan hebras de ADN que contienen el nombre de Barcelona ( TCGGACTC ).
Acto seguido, se procede a eliminar las hebras de ADN que no contengan el nombre de Barcelona.
El proceso se repite con moléculas sonda que codifiquen el complementario del nombre en ADN de Madrid.
Una vez efectuados todos los pasos computacionales, las hebras restantes serán los que codifiquen la solución GCAGTCGGACTGGGCTATGTCCGA .

El problema del camino hamiltoniano 

Imaginemos un mapa de ciudades conectadas por vuelo directo (a la derecha, arriba).
En el caso ilustrado se va directamente desde Barcelona hasta Zaragoza, pero no en sentido contrario. 
Se trata de determinar si existe un camino que parta de una ciudad inicial (Valencia), concluya en la ciudad final (Zaragoza) y visite exactamente una vez cada una de las ciudades restantes.
En la computación con ADN , a cada ciudad se le asigna una secuencia de ADN ( ACTTGCAG , en el caso de Valencia), que se puede imaginar compuesta por un nombre ( ACTT ) seguido de un apellido ( GCAG ).
Los números de vuelo en ADN pueden ser definidos después por concatenación del apellido de la ciudad de origen con el nombre de la ciudad de destino (a la derecha, abajo). 
Los nombres complementarios en ADN de las ciudades son la secuencia complementaria de sus correspondientes nombres en ADN , en los cuales cada C es reemplazada por una G , cada G por una C , cada A por una T , y cada T por una A .
(Para simplificar la exposición en nuestro caso, se han suprimido los detalles de las terminaciones xxx y xxx de las moléculas de ADN ).
En el caso de este problema concreto, existe sólo un camino hamiltoniano, que pasa por Valencia, Barcelona, Madrid y Zaragoza, en ese orden.
En la computación, el camino está representado por GCAGTCGGACTGGGCTATGTCCGA , una secuencia de ADN de longitud xxx .
Podemos ver a la izquierda el mapa con siete ciudades y xxx vuelos directos utilizados en el experimento real.


Sustitución dirigida de genes 

La biología de los mamíferos está experimentando una auténtica revolución, impulsada por una nueva técnica que permite crear ratones portadores de mutaciones controlados de cualquier gen conocido. 

Las células de nuestro organismo portan en el núcleo un manual de instrucciones donde se especifican sus distintas funciones.
Aunque ese manual es el mismo para todas, cada estirpe celular-hepáticas, epidérmicas y otras- echa mano de un capítulo diferente para cumplir su propia misión.
El manual contiene, asimismo, la información en cuya virtud un embrión unicelular, el óvulo fecundado, se convierte, primero, en feto y, luego, en bebé. 
Y además sigue suministrando información mientras el niño madura física e intelectualmente. 
Cada uno de nosotros es irrepetible;
en efecto, el manual difiere ligeramente de un sujeto a otro, de suerte que en él están especificadas casi todas las características físicas y muchas de las claves del comportamiento que nos individualizan.

Manual tan extraordinario se llama genoma.
Está escrito en forma de nucleótidos, con un alfabeto de cuatro letras: adenilato ( A ), citidilato ( C ), guanilato ( G ) y timidilato ( T ).
Igual que la secuencia de letras de una palabra determina su significado, así la secuencia de nucleótidos del ADN encierra la información.
En cada división celular se replica el manual entero: una copia por cada célula resultante.
En humanos y ratones, el genoma viene a constar de unos tres mil millones de nucleótidos.

Con mi grupo de la Universidad de Utah he desarrollado la técnica que permite cambiar una letra una frase o varios párrafos del manual de instrucciones de las células de un ratón.
Reescribiendo partes del texto y evaluando las consecuencias de esa alteración de las instrucciones sobre el desarrollo del múrido, o sobre su operación posterior, podemos desentrañar las peculiaridades del programa que gobierna dichos procesos.

Las unidades funcionales del manual son los genes.
Escogemos uno y cambiamos su secuencia específica de nucleótidos:
alteraremos su función.

Para mayor claridad:
si sospecháramos la participación de cierto gen en el desarrollo cerebral, podríamos preparar embriones de ratón en los que el gen normal estuviera «fuera de combate», anulado del todo.
Si por culpa de esa inactivación nacieran ratones con malformaciones cerebelares, sabríamos que el gen en cuestión era decisivo para la formación del cerebelo.
El proceso mediante el cual se introducen cambios específicos en la secuencia de nucleótidos de un gen se denomina sustitución dirigida de genes (« gene targeting »).

La información que se obtenga con los experimentos de sustitución génica en los ratones debiera sernos útil a los humanos, pues el 99 por ciento, y algún pico más, de los genes coinciden en ambas especies y cumplen similares cometidos.
Esa línea de investigación en el ratón arroja luz, no sólo sobre las etapas del desarrollo embrionario humano, sino también sobre la constitución de nuestro sistema inmunitario y su intervención en la lucha contra las infecciones.
Es de presumir que esa técnica de sustitución nos conduzca más lejos, hasta el funcionamiento del cerebro humano y la forma en que ciertos defectos génicos se traducen en enfermedad. 

A propósito de esto último, se está aplicando la técnica para provocar en ratones modelos de enfermedades humanas: fibrosis quística, cáncer y aterosclerosis, por citar algunas.

El entusiasmo despertado por la técnica de sustitución se alimenta con otra esperanza, la de su promesa de ahondar en el conocimiento generado por el proyecto genoma.
Aspira esta magna empresa a establecer la secuencia nucleotídica de todos los genes que componen el genoma humano y el del ratón (unos 200.000 genes en cada caso).
Se ha identificado sólo la función de un pequeñísimo porcentaje de genes de cada una de esas especies.
La secuencia de nucleótidos de un gen determina los aminoácidos que deben ensartarse para formar una proteína dada. 
(Las proteínas llevan a cabo la mayoría de las actividades celulares.)

La propia secuencia de aminoácidos de una proteína suministra, a su vez, importantes pistas sobre el papel que ésta desempeña en las células: actividad enzimática, componente estructural o molécula transmisora de señales. 
Pero la secuencia - no basta, por sí misma, para revelar las tareas realizadas por la proteína durante la vida del animal.

La sustitución génica dirigida sí puede proporcionar esa información y ampliar nuestro conocimiento sobre las funciones de los genes y sus proteínas.

La sustitución génica nos ofrece una nueva manera de hacer genética de mamíferos:
la posibilidad de ver cómo intervienen los genes en los procesos biológicos.
Los métodos clásicos de la disciplina, muy fructíferos a la hora de abordar esos procesos en organismos elementales, no acababan de adaptarse al estudio de seres de la complejidad de los mamíferos. 

Para saber cómo replican su ADN bacterias y levaduras, unicelulares, basta con exponer mil millones o más a un agente químico lesivo para el ADN (un mutágeno). 
Eligiendo la dosis adecuada del mutágeno, aseguraremos que cada individuo porte una mutación en uno o varios genes.
A partir de esta población de bacterias o levaduras que han experimentado mutagénesis, identificaremos individuos incapaces de replicar su ADN.
En tamaña población resulta probable hallar individuos distintos con mutaciones en cada uno de los genes necesarios para la replicación del ADN. (En la replicación del genoma bacteriano o de levaduras participan un centenar largo de genes.) 
Una vez identificados los genes, podemos determinar su papel específico en la replicación:
qué genes controlan la decisión de copiar el ADN y cuáles la precisión y tasa de duplicación.

Planteamientos similares se han aplicado a los organismos pluricelulares.
Los dos favoritos de los genéticos son Caenorhabditis elegans , un gusano, y Drosophila melanogaster , la mosca del vinagre.
No obstante, incluso en estas formas sencillas de organismos pluricelulares, la identificación de todos los genes implicados en un proceso biológico es dificilísima.

Varios factores explican semejante dificultad.
Uno es el tamaño del genoma.
El de la bacteria Escherichia coli contiene sólo 3000 genes, mientras que el de D. melanogaster tiene al menos 20.000 y el de ratón, 10 veces esa cifra.
Cuanto mayor es el número de genes tanto mayor es la complejidad, porque aquellos forman redes interactivas más intrincadas.
Averiguar el efecto de cualquiera de esos genes en una red tan enrevesada constituye una tarea hercúlea.

Además, el mayor tamaño de los organismos pluricelulares pone límites prácticos al número de individuos que puede abarcarse en un experimento de mutagénesis. 
No cuesta demasiado buscar tipos específicos de mutantes entre más de mil millones de bacterias o levaduras sometidas a mutagénesis.
Pero detectarlos aunque sólo sea en 100.000 moscas constituiría un experimento de una magnitud inabordable.
A modo de comparación, en los ratones se alcanzaría el límite práctico de detección de una mutación dada en torno a los mil animales.

Al ser en su mayoría diploides los organismos pluricelulares, las dificultades logísticas para identificar y estudiar los genes se redoblan;
ser diploides significa que sus células poseen dos copias de los genes, una heredada de la madre y otra del padre.
A efectos de supervivencia, importa mucho tener dos copias de la mayoría de los genes. 
Normalmente, si una copia sufre una mutación perjudicial, la otra la compensa, y así no se producen graves consecuencias. 

Esta redundancia significa también que una mutación provocará defectos fisiológicos o anatómicos en el organismo sólo si las dos copias del gen están dañadas.
Se logra este tipo de individuos cruzando parentales que tengan cada uno la mutación en una copia del gen.
Aproximadamente una cuarta parte de la descendencia de esos cruzamientos será portadora de las dos copias defectuosas del gen.

Pese a las dificultades, la identificación de mutaciones seleccionadas en animales vivos sigue siendo el mejor medio para empezar a aclarar y ordenar las etapas de todo proceso biológico. 
Además, si queremos entender procesos que sólo ocurren en los organismos complejos, como el desencadenamiento de la respuesta inmunitaria, habrá que realizar en éstos los análisis.

Esa es la razón por la que los genéticos interesados en el desarrollo, las funciones neuronales, la respuesta inmunitaria, la fisiología y las enfermedades de los mamíferos hayan empezado a trabajar con el ratón.
Desde un punto de vista genético, este animal es un mamífero ideal.
Pequeño y prolífico, constituye, además, un excelente referente para la mayoría de los procesos biológicos humanos.

Con todo, las manipulaciones genéticas que pueden llevarse a cabo en los ratones son, comparadas con las operaciones permitidas en los organismos simples, muy limitadas.
Para identificar ratones que han experimentado mutagénesis y son portadores de defectos en los genes que intervienen en determinados procesos, tendrían que controlarse de 10.000 a 100.000 individuos, a un costo prohibitivo.

Por ello, los estudiosos se han venido ciñendo a los animales mutantes que surgían de modo espontáneo en sus colonias.
Fruto de la tenacidad de los genéticos, se dispone hoy de una nutrida colección de ratones mutantes.

Pero ni siquiera ese grupo de ratones mutantes se halla libre de inconvenientes:
no representan una muestra aleatoria de las posibles mutaciones del genoma del ratón, sino que contiene un número desproporcionado de mutaciones que se traducen en anomalías fisiológicas 0 de comportamiento fácilmente perceptibles.
Así, abundan las mutaciones que afectan al color del pelaje, mientras que hay pocas que influyan en las primeras etapas del desarrollo (pues suelen determinar la muerte inesperada del embrión).

Por no hablar de lo costoso que resulta aislar los genes responsables de los defectos manifiestos en los ratones mutantes; tarea ésta que exige a menudo años de esfuerzo concertado.
Podemos deducir muchas de las etapas involucradas en los fenómenos biológicos sin descubrir los genes implicados; pero sin aislarlos no hay progreso molecular:
no se puede determinar la naturaleza de las proteínas cifradas por los genes mutados, ni identificar las células en las que dichos genes son activos.

La sustitución génica dirigida permite superar tales dificultades al dejar en nuestras manos qué gen queremos alterar.
También permite ejercer el control sobre el tipo de modificaciones introducidas en el gen, de suerte tal que la mutación puede diseñarse a medida para abordar cuestiones específicas sobre la función del gen.
Los criterios para seleccionar el gen a matar pueden basarse en el conocimiento obtenido de la investigación en ratones y otras especies.
Ahora resulta bastante fácil aislar una serie de genes que son activos durante la formación del corazón del ratón;
la sustitución de genes permitiría, por tanto, determinar el papel de cada uno de esos genes en el desarrollo cardíaco.
Por vía alternativa, podemos averiguar si existe en el ratón una serie de genes cuya función en D. melanogaster consiste en guiar el desarrollo neuronal y si cumplen una función similar.

De entrada se pretende anular el gen a estudiar, para conocer las consecuencias que arrastra la ausencia del producto que dicho gen determina.
Probablemente serán complejas y afectarán a múltiples vías metabólicas. 
Puede comprenderse aún mejor la función del gen introduciendo mutaciones definidas, más sutiles, que incidan sólo en una de sus múltiples funciones. 
Muy pronto, podrán ponerse los genes bajo el control de un conmutador, elemento que permitirá conectar y desconectar un gen durante el desarrollo embrionario o posnatal del ratón. 
Supongamos, por ejemplo, un gen, hipotético, responsable de la formación y el funcionamiento de una serie de neuronas. 
La anulación del gen determinaría la ausencia de esas neuronas durante la formación del cerebro e impediría valorar la actividad génica en el adulto.
Si el gen estuviera bajo el control de un conmutador, éste podría dejarse conectada durante el desarrollo, y las neuronas se formarían normalmente.
En el adulto, el conmutador se desconectaría; 
podríamos evaluar entonces la función del gen sobre las neuronas.

En los últimos 15 años la técnica de la sustitución génica dirigida ha avanzado mucho. 
Cuando yo empecé, a finales de los años setenta, utilizaba agujas de cristal muy pequeñas para inyectar ADN directamente en el núcleo de las células de mamífero.
Las agujas se controlaban mediante micromanipuladores hidráulicos y las dirigíamos hacia el núcleo con la ayuda de microscopios de alta resolución. 
El procedimiento resultó de una gran eficacia. 
Una de cada tres o cinco células recibía ADN funcional y comenzaba a dividirse y a transmitirse establemente a las células hijas.

Cuando investigué el sino de esas moléculas de ADN en el interior celular, un sorprendente fenómeno llamó mi atención.
Aunque las moléculas de ADN recién introducidas se insertaban al azar en uno de los cromosomas de la célula receptora, a veces se introducía más de una molécula en el mismo sitio y todas tenían la misma orientación.

Igual que en los idiomas existe una dirección de lectura (de izquierda a derecha en español), así también en las moléculas de ADN.
Según parecía, antes de que las células llevaran a cabo la inserción aleatoria, había algún mecanismo nuclear en virtud del cual las moléculas de ADN introducidas se engarzaban en la misma orientación.

Demostramos que las células utilizaban un proceso de recombinación homóloga para conseguir tales engarces.
La recombinación homóloga se produce sólo entre moléculas de ADN que tienen la misma secuencia de nucleótidos:
se alinean una al lado de la otra, ambas sufren cortes y se juntan por lo extremos cortados. 
Los empalmes se realizan con tal precisión, que no se altera la secuencia nucleotídica del sitio donde se producen dichos engarces.

Esta inesperada observación revelaba que todas las células de ratón, y presumiblemente todas las de mamífero, poseían la maquinaria necesaria para llevar a cabo la recombinación homóloga.
En aquellas fechas no había razones para sospechar que contaran con ella las células somáticas, que no están implicadas en la reproducción sexual.

Comprobamos la eficacia sobresaliente del mecanismo, pues logramos microinyectar más de 100 moléculas de ADN de idéntica secuencia, que las células engarzaron en la misma orientación.
Inmediatamente comprendí que, si aprovechábamos esta maquinaria para provocar la recombinación homóloga entre una molécula recién introducida de ADN de nuestra elección y la misma secuencia del cromosoma de una célula, podríamos reescribir a voluntad su manual de instrucciones.

Entusiasmado ante esta perspectiva, en 1980 solicité una subvención para investigar la viabilidad de la técnica de sustitución génica dirigida.
Pero el comité de expertos que revisó el proyecto lo rechazó. 
En su opinión, la probabilidad siquiera de que la secuencia de ADN recién introducida encontrase su secuencia homóloga en los 1000 volúmenes del manual de instrucciones genéticas parecía mínima.

Pese a la denegación, decidí aprovechar los fondos que recibía de otro proyecto.
Era una empresa arriesgada.
Si fracasaba, no dispondría de datos suficientes para solicitar la renovación de la subvención.
La fortuna vino en mi ayuda.
En 1984, cuando volvimos a solicitar fondos para proseguir la investigación, teníamos pruebas de la viabilidad de la sustitución génica en las células. 
Los expertos que habían rechazado la primera propuesta demostraron ahora su buen sentido del humor.
El informe de valoración del nuevo proyecto empezaba con la siguiente frase:
«Nos alegramos de que no siguiese nuestros consejos».

¿Cómo se efectúa la sustitución génica dirigida en las células?
Se empieza por clonar el gen que nos interesa y propagarlo en bacterias, para así lograr una fuente pura de ADN que contiene el gen.
A continuación, en un tubo de ensayo, se cambia la secuencia de nucleótidos del gen en función de los fines del experimento.
Al gen alterado se le denomina vector director (« targeting vector »).

Se introduce el vector en células vivas.
Dentro del núcleo celular, forma complejo con las proteínas que constituyen la maquinaria celular de recombinación homóloga.
Ayudado por esas proteínas, el vector busca su secuencia homóloga (el objetivo) por todo el genoma.
Si la encuentra, se alinea junto a ella y la sustituye.

Por desgracia, esta sustitución dirigida ocurre sólo en una pequeña fracción de las células tratadas.
Lo habitual es que el vector se inserte al azar en lugares no homólogos o que ni siquiera se integre. 
Debemos, pues, ser capaces de identificar las células en las que la sustitución ha resultado satisfactoria. 
Aproximadamente una de cada millón de células tratadas tiene la sustitución deseada.

Para simplificar la búsqueda de esas células, utilizamos dos «marcadores seleccionables», que se introducen en el vector director desde el principio.
La inclusión de un marcador seleccionable «positivo» promueve la supervivencia y el crecimiento de las células que han incorporado el vector, ya sea donde se pretendía o en una localización aleatoria dentro del genoma.
La inclusión del marcador seleccionable «negativo» ayuda a eliminar la mayoría de las células que han incorporado el vector en una localización aleatoria. 

El marcador positivo, normalmente un gen de resistencia a neomicina ( neor ), se coloca de tal forma que quede flanqueado por secuencias de ADN también presentes en el gen diana homólogo que se desea reemplazar. 
El marcador negativo, por lo general el gen de la quinasa de timidina ( tk., ) de un virus herpes, se fija a un extremo del vector.
Cuando se produce recombinación homóloga, los segmentos inalterados del gen clonado, junto con el gen neo r intercalado entre ellos, sustituyen a la secuencia diana presente en el cromosoma. 
Pero el gen tk., , situado fuera de la zona de secuencias alineadas no se integra en el cromosoma y es degradado por la célula.
Por el contrario, cuando las células insertan aleatoriamente el vector director, lo engarzan todo, incluido el gen tk., , en el ADN.
Si no hay integración, se pierden el vector y sus marcadores.

No necesitamos examinar directamente el ADN para identificar cuál de esas posibilidades ha ocurrido.
Nos basta con cultivar las células en un medio que contenga dos drogas: un análogo de la neomicina llamado G418 y el antiherpético ganciclovir.
El G418 mata a las células que carecen del gen neo r protector en sus cromosomas, esto es, las que no han integrado el ADN vector.
Pero permite sobrevivir y crecer a las células portadoras de las inserciones, ya sean aleatorias o dirigidas.
A su vez el ganciclovir mata todas las células que tienen el gen tk., herpético, es decir, las que albergan una inserción aleatoria.
Al final, las únicas células supervivientes vienen a ser las portadoras de la inserción dirigida (las que poseen el gen «seleccionable positivo» neo r y carecen del gen «seleccionable negativo» tk., ).

En 1984 habíamos demostrado ya que era posible sustituir genes específicos en células de ratón cultivadas. 
Estábamos preparados para ampliar la técnica al genoma de ratones vivos.
Nos servimos de células especiales desarrolladas en 1981 por Matthews H. Kaufman y Martin J. Evans.
Se trata de las células madre embrionarias (ME), es decir, células pluripotentes obtenidas de las primeras fases de un embrión de ratón, que pueden cultivarse indefinidamente en placas de Petri y están capacitadas para originar cualquier línea de células. 

En resumen, mediante el procedimiento antes descrito, produjimos células ME con una mutación dirigida en una de las dos copias del gen elegido.
Luego introdujimos las células ME en embriones muy jóvenes de ratón y los dejamos desarrollarse a término.
Algunos de los ratones resultantes, cuando maduraron, produjeron esperma derivado de las células ME.
Cruzando estos ratones con otros normales, conseguimos descendientes heterocigotos para la mutación, esto es, ratones que la portan en todas las células en una de las dos copias del gen.

En casi todos los casos, esos heterocigotos estarán sanos, porque la segunda copia del gen, intacta, seguirá funcionando correctamente.
Pero el cruzamiento de estos heterocigotos con hermanos o hermanas portadores de la misma mutación produce homocigotos: animales con la mutación dirigida en las dos copias del gen.
Estos animales exhibirán anomalías que revelarán las funciones normales del gen sustituido en todos sus tejidos.

Empezamos inyectando nuestras células ME modificadas en embriones en estado de blastocisto, que aún no se han implantado en el útero materno.

Como dependíamos del color del pelaje para saber si el procedimiento marchaba de acuerdo con el plan, elegimos blastocistos que en un desarrollo normal se transformarían en crías de color diferente del encontrado en las producidas por la estirpe de ratón de la que obtuvimos las células ME.

Las células madre se aislaron de un ratón marrón que portaba dos copias del gen agoutí .
Este gen, aún cuando esté presente en una sola copia, produce colorido marrón debido a la presencia, en cada uno de los pelos, de una banda de pigmento amarillo sobre la pigmentación negra normal (la producción de los pigmentos propiamente dichos está controlada por otros genes).
Por tanto, seleccionamos blastocistos que, de no ser tratados, se convertirían en ratones negros (los ratones son de color negro cuando el gen agoutí heredado de ambos padres es defectuoso).
Esperamos, por fin, a que el embrión, que contenía las células ME modificadas, llegara a término en una «madre de alquiler».

Si todo va bien, las células ME alteradas se reproducen repetidamente durante este tiempo, transmitiendo copias completas de todos sus genes a sus células hijas;
éstas se mezclan con las del embrión y contribuyen a la formación de la mayoría de los tejidos del ratón.
Por consiguiente, los recién nacidos son quimeras:
ratones compuestos por células derivadas de las ME extrañas y de las células del embrión original.
Las quimeras son fáciles de identificar por los amplios parches de color marrón que presentan en un pelaje negro.
Si los animales no portaran células derivadas de las ME, serían completamente negros porque carecerían de genes agoutí funcionales.

Ahora bien, con sólo mirar las quimeras no podemos determinar si las células ME dieron origen a células germinales, instrumento por el que se transmite la mutación a las generaciones futuras.
Y esto lo averiguamos sólo al pasar a la etapa siguiente:
la de producción de ratones heterocigotos portadores de una copia de la mutación en todas sus células.
Para generar esos animales, cruzamos ratones quiméricos machos con hembras negras, defectivas para el gen agoutí .
Los descendientes serán marrones si el esperma que fecundó el óvulo procede de las células ME (ya que todo ese esperma lleva el gen agoutí ).

Si el esperma deriva de las células del blastocisto original (sin genes agoutí funcionales), los descendientes serán negros.

Así pues, cuando vemos crías marrones, sabemos que los genes transportados por las células ME han ido a parar a esos descendientes, y ya podemos iniciar los cruzamientos entre hermanos heterocigotos para producir ratones con dos copias defectuosas del gen objetivo.
Antes, sin embargo, debemos saber qué crías marrones son las portadoras de una copia del gen mutado.
Esto lo conseguimos buscando la mutación por examen directo de su ADN.
Cuando se cruzan hermanos heterocigotos, uno de cada cuatro descendientes tendrá las dos copias defectuosas del gen.
Identificamos los homocigotos también por análisis directo de su ADN, esta vez buscando dos copias de la mutación.
Luego, se examinan los animales para comprobar la existencia de anomalías anatómicas, fisiológicas 0 de comportamiento que puedan dar pistas sobre la función del gen alterado.

Todo el procedimiento, desde la clonación de un gen hasta la producción de ratones con una mutación dirigida en ese gen, viene a durar alrededor de un año. 

Laboratorios de todo el mundo aplican ahora la técnica de la sustitución génica dirigida en ratones para estudiar una amplia variedad de problemas biológicos. 
Desde 1989 se han producido más de 250 estirpes portadoras de defectos genéticos seleccionados.
Unos pocos ejemplos de los descubrimientos obtenidos permitirán ilustrar el tipo de información que estos animales pueden suministrar.

En mi propio laboratorio hemos estado explorando las funciones de los genes homeóticos, o genes Hox .
Estos genes son una suerte de conmutadores maestros encargados de que las diferentes partes del organismo, como las extremidades, los órganos y las partes de la cabeza, se formen en los lugares adecuados y adapten su morfología correcta.
Los estudios sobre los genes homeóticos de Drosophila han suministrado valiosa información sobre sus actividades.
Sin embargo, quedan por resolver muchas cuestiones.
Por ejemplo, D. melanogaster tiene sólo ocho genes Hox , mientras que el ratón y el hombre tienen 38 cada uno.

Probablemente, la expansión de la familia Hox desempeñó un papel crucial en la progresión evolutiva de invertebrados a vertebrados, aportando la maquinaria adicional necesaria para formar un cuerpo más complejo. 
¿Qué función precisa cumplen esos 38 genes?

Antes de que dispusiésemos de la técnica de sustitución génica dirigida, no había forma de responder a esa pregunta, ya que nadie había encontrado ratones o seres humanos con mutaciones en ninguno de los 38 genes Hox .
Mis colegas y yo mismo estamos embarcados ahora en un proyecto cuyo objetivo es establecer la función de cada uno de esos genes.

Más tarde intentaremos identificar cómo establecen una red interactiva para dirigir la formación de nuestro cuerpo.

Dentro de ese programa, hemos descubierto que la alteración dirigida del gen Hox A-3 origina múltiples defectos.
Los ratones que portan dos copias matadas del gen mueren al nacer por disfunción cardiovascular debida al desarrollo incompleto del corazón y los principales vasos sanguíneos que arrancan del mismo.
Esos ratones nacen también con aberraciones en otros muchos tejidos, entre ellos el timo y el paratiroides (que faltan), la glándula tiroidea, el hueso y los cartílagos de la parte inferior de la cabeza, el tejido conectivo, el músculo y el cartílago de la faringe.

Esas anomalías, aunque dispares, comparten un rasgo común:
todos los tejidos afectados descienden de células que estaban agrupadas en un principio en una estrecha región de la parte superior del embrión en desarrollo. 
Los rudimentos del corazón, por ejemplo, se alojan en esa región antes de alcanzar su ubicación más retrasada en el tórax.
Parece, por tanto, que la función del gen Hox A-3 consiste en controlar la construcción de muchos de los tejidos y órganos que se originan en esa estrecha región.

Inesperadamente comprobamos que el desorden producido por la desactivación del gen Hox A-3 del ratón semejaba el observado en el síndrome de Di George, una enfermedad hereditaria humana.
El análisis cromosómico de los pacientes con ese síndrome muestra que el gen HoxA-3 humano no es el culpable; 
las víctimas presentan daños genéticos en un cromosoma distinto del que alberga dicho gen.

Ahora sabemos, sin embargo, que el gen responsable del síndrome actúa dificultando la activación del Hox A-3 o los episodios desencadenados por ese gen.
Además, contamos ya con un modelo de la enfermedad en el ratón que acabará proporcionando pistas para su tratamiento.
Este imprevisto beneficio resalta el valor de la investigación básica: los descubrimientos nacidos de la curiosidad suelen llevar a aplicaciones muy prácticas.

Los inmunólogos se han beneficiado también de la técnica de sustitución génica.

La están aplicando para acotar la responsabilidad de cada uno de los más de 50 genes que influyen en el desarrollo y el funcionamiento de las dos clases principales de células defensoras del organismo: linfocitos B y linfocitos T .

Los oncólogas acuden también a ella.
Con frecuencia se confirma que uno o más tipos de tumores comparten mutaciones en un gen concreto, cuyo papel normal, sin embargo, se desconoce.
Su descubrimiento mediante el empleo de nuestra técnica de anulación puede ayudar a revelar cómo contribuye la forma mutante del gen al desarrollo de la neoplasia.

El gen supresor tumoral p53 ofrece un ejemplo que hace al caso.
Se entiende por genes supresores de tumores aquellos cuya inactivación contribuye al desarrollo del cáncer.
Es posible que hasta en el 80 por ciento de todos los cánceres humanos el gen p53 esté mutado, pero hasta hace muy poco se desconocía su función normal.
El análisis de ratones homocigotos para una mutación dirigida e n p53 indicó que probablemente este gen actúe a modo de cancerbero que impida la división de las células sanas hasta no haber reparado cualquier daño que haya podido sufrir su ADN.
Daños infligidos a menudo a las células como consecuencia de las frecuentes agresiones ambientales a las que están sometidas.
La pérdida de los genes p53 funcionales elimina esa salvaguarda, lo que facilita la transmisión del gen lesionado a las células hijas, donde participa en el desarrollo cancerígeno.

No sólo el cáncer.
Más de 5000 enfermedades humanas se han atribuido a defectos genéticos. 
La identificación de los genes y las mutaciones responsables de esas enfermedades permitirá conseguir las mismas mutaciones en ratones.
Los modelos en ratones harán posible, a su vez, la identificación detallada de los procesos que median entre el funcionamiento anómalo de un gen y la manifestación de la enfermedad.

Con un mejor conocimiento de la patología molecular de la enfermedad diseñaremos terapias más eficaces. 
Entre los modelos que se están creando ahora destacan los ratones con diferentes tipos de mutaciones en el gen de la fibrosis quística.

Esta técnica genética se está empezando a emplear también en el estudio de la aterosclerosis, responsable último de accidentes cerebrovasculares y cardiopatías. 
A diferencia de la fibrosis quística, la aterosclerosis no está causada por mutaciones en un solo gen.
Defectos en varios genes se combinan con los factores ambientales para promover la acumulación de placas en las arterias. 
Pero se han conseguido prometedores modelos en ratones alterando genes cuya participación en el procesamiento de los triglicéridos y el colesterol se conoce.
Preveo también que pronto se desarrollarán modelos para la hipertensión, otra de las culpables de las enfermedades cardíacas y las apoplejías, ahora que se están identificando los genes a los que se atribuye participación en su desarrollo.

Cuanto más ahondamos en la genética de las enfermedades, mayor es nuestro deseo de aplicar la terapia génica para corregir los defectos.
De momento, las técnicas utilizadas en dicha terapia se basan en la inserción aleatoria de genes sanos en los cromosomas para compensar las versiones dañadas.

Mas los genes así insertados no suelen funcionar con la eficacia de los que ocupan su lugar correcto en el cromosoma. 
En principio, la sustitución génica dirigida puede ofrecer una solución a ese problema.
Sin embargo, antes de que la técnica pueda utilizarse para corregir un gen defectuoso en el tejido de un paciente, habrán de obtenerse cultivos de células capaces de participar en la formación de esos tejidos en el adulto.
Dichas células, que, como las ME de nuestros estudios, se denominan células madre, están presentes en médula ósea, hígado, pulmones, piel, intestinos y otros tejidos.
Pero la investigación sobre cómo aislar y cultivar esas células está todavía en mantillas.

Antes de vencer los obstáculos técnicos que impiden la aplicación general de nuestros métodos a la terapia génica, la sustitución génica encontrará un uso común en el estudio de la neurobiología de los mamíferos.
Ya se han preparado ratones con mutaciones dirigidas que alteran su capacidad de aprendizaje. 
La identificación de nuevos genes neuronales acelerará el desarrollo de esta línea de trabajo.

Sustitución dirigida de genes en cultivos celulares

1. Se alteran las copias de un gen ( barra de la izquierda ) en el tubo de ensayo y se prepara un vector director ( barra alargada ).
El gen mostrado se ha inactivado por inserción del gen neo r ( verde ) en la región que cifra la proteína ( azul ).
El gen neo r servirá de marcador para indicar que el ADN vector se ha instalado en un cromosoma.
En un extremo del vector se ha introducido un segundo marcador: 
el gen tk., de herpes ( rojo ).

2. Vector y sus dos marcadores se introducen en células ( gris ) aisladas de un embrión de ratón.

3. Se produce recombinación homóloga ( arriba ):
el vector se alinea con el gen normal (el objetivo), ubicado en un cromosoma;
a continuación, las regiones idénticas alineadas del vector (junto con cualquier ADN que haya intercalado entre ellas) ocupan el lugar del gen original y se produce la exclusión del marcador situado en el extremo ( rojo ).
En muchas células, sin embargo, se produce inserción aleatoria de todo el vector (también el marcador extra) en algún cromosoma ( centro ) y en otras la integración fracasa ( abajo ).

4. Para aislar las células portadoras de una mutación dirigida, se cultivan todas en un medio que contenga ciertas drogas: un análogo de la neomicina ( G418 ) y el ganciclovir.
El G418 es letal para las células, a menos que lleven un gen neo r funcional- elimina, pues, las células en las que no se haya integrado el vector ADN ( gris ).
El ganciclovir mata las células que tienen un gen tk., ; es decir, elimina las células portadoras de un vector integrado al azar.
Las únicas células que sobreviven y proliferan son las que portan la inserción dirigida ( verde ).

Sustitución génica dirigida en ratones

1. Se aíslan células madre embrionarias (ME) ( verde, a la izquierda ) de una estirpe de ratón de color marrón y se introduce una mutación dirigida en un cromosoma ( inserto ).
Las células ME se colocan a continuación en embriones jóvenes.
El color del pelaje de las futuras crías sirve de guía para comprobar si las células ME han sobrevivido en el embrión.
En general se introducen las células ME en embriones que, sin ellas, originarían ratones totalmente negros. 
Estos embriones se obtienen a partir de una estirpe de color negro ( abajo ), carentes del gen agoutí , que produce pelo marrón, aunque esté presente en una sola copia.

2. Los embriones que contienen las ME se dejan crecer a término en «madres de alquiler».
Se examina el pelo de los recién nacidos.
La presencia de parches marrones sobre una fondo negro manifiesta que las células ME han sobrevivido y proliferado en el animal.
(Se llaman quimeras porque contienen células procedentes de dos estirpes de ratón.)
Un color totalmente negro significaría que las células ME han perecido.

3. Se cruzan ratones quiméricos macho con hembras negras (no agoutí ).
Se analiza la descendencia en busca de indicadores de la mutación dirigida ( inserto verde ) en el gen deseado. 
Los ratones negros se apartan;
si los animales han nacido a partir de esperma procedente de células ME - y con probabilidad de portar la mutación - serían marrones.
El examen directo de los genes de los ratones marrones revela cuál de esos animales ( recuadro ) ha heredado la mutación dirigida.

4. Se cruzan machos y hembras portadores de la mutación para obtener ratones cuyas células presenten la mutación en las dos copias del gen ( inserto ), careciendo por tanto del gen funcional.
La identificación final de esos animales ( recuadro )se realiza por análisis directo de su ADN.
Por último, se comprueba si tienen algún defecto físico o de comportamiento.

Figura 1

MUTACION DIRIGIDA de un gen celular

El procedimiento consiste en introducir copias mutadas del gen ( moléculas verdes y amarillas de la izquierda ) en las células y dejando que una copia ocupe el lugar del gen original ( molécula amarilla de la derecha ) en el cromosoma correspondiente. 
Estas células alteradas permiten producir ratones con mutaciones genéticas específicas.
La aparición de una cola curvada y problemas de equilibrio y audición en uno de los ratones ( arriba ) llevó al descubrimiento de que el gen afectado, el int-2 , participa en el desarrollo de la cola y el oído interno.

Figura 2

Ratón recién nacido con una mutación dirigida en las dos copias del gen Hox A-3 

Su cuerpo está más curvado que el de un ratón normal.
Muestras de tejidos de ratones mutantes (izquierda) y normales (derecha) revelan que los mutantes también carecen de timo y tienen una glándula tiroidea exageradamente empequeñecida.
Estos y otros defectos indican que el gen Hox A- 3 es necesario para el desarrollo de los tejidos y los órganos que se originan a partir de una estrecha banda de células presentes en los embriones jóvenes.


Mecanismo molecular del control génico 

Las actividades de nuestros genes están rigurosamente reguladas por elaborados complejos de proteínas que se ensamblan en el ADN .
Cualquier perturbación en el proceso de ensamblado puede ser causa de enfermedades.

El asma, el cáncer, las cardiopatías, los trastornos inmunitarios y las infecciones víricas son enfermedades que, a primera vista, tienen poco en común. 

Sólo a primera vista.
En última instancia todas se producen por defecto o por exceso de una o varias proteínas, que son las moléculas que llevan a cabo la mayoría de las reacciones que se producen en el cuerpo.
Esta circunstancia ha dado un empuje definitivo a las investigaciones que tienen como objetivo el conocimiento y la manipulación de la maquinaria que regula un paso esencial en la síntesis de proteínas: la transcripción de los genes.
Para sintetizar una proteína, el gen que especifica su composición debe transcribirse, copiarse, de ADN en moléculas de ARN mensajero, que posteriormente sirven de molde para la síntesis proteica. 

Mucho antes de que la terapia se convirtiera en objetivo, la transcripción había cautivado el interés de los científicos que buscaban desentrañar los mecanismos que regulan este proceso, en la esperanza de que aclarase algunos puntos del misterio de la vida.
Todas las células del cuerpo portan el mismo genoma, esto es, los aproximadamente xxx genes que constituyen el patrimonio genético de un ser humano.
¿Cómo a partir de una célula, el huevo fecundado, se origina esa miríada de tipos celulares, cada uno de los cuales utiliza una porción diferente del total de genes para producir mezclas diferentes de proteínas? 
Y una vez formado el cuerpo,
¿cómo consiguen las células aumentar y disminuir las cantidades de proteínas que fabrican, en respuesta a sus propias necesidades y las del organismo en general?

Para responder a estas preguntas y diseñar drogas capaces de modular la transcripción, había que conocer el funcionamiento de la maquinaria que controla la lectura del código genético en la célula humana. 
Tras 25 años de investigaciones, la estructura global de este aparato empieza a vislumbrarse con nitidez.
Los trabajos de mi laboratorio, en la Universidad de California en Berkeley, y de otras instituciones, han revelado que una parte de la maquinaria que dirige la transcripción de la mayoría, si no todos, los genes humanos, consta de unas 50 proteínas distintas.
Esas proteínas deben ensamblarse en un apretado complejo con el ADN , antes de que una enzima especial, la polimerasa de ARN, pueda empezar a copiar el ADN en ARN mensajero. 
Todos esos componentes se pueden ya mezclar en el tubo de ensayo para conseguir una maquinaria transcripcional operativa. 
Existen, además, otras proteínas que se encajan en determinados sitios de esta compleja maquinaria, y la «programan», indicando qué genes hay que transcribir y con qué celeridad.
Los puntos cruciales de tales instrucciones también empiezan a revelársenos. 

A finales de los setenta, empezamos, en Berkeley, a trabajar con genes humanos.

En esa época se sabia muy poco sobre la maquinaria transcripcional de nuestras células.
Pero ya se tenia una visión bastante clara de la transcripción en procariotas (bacterias y otros organismos unicelulares primitivos que carecen de núcleo definido).
Estos conocimientos adquiridos facilitaron bastante los estudios con células humanas y otros eucariotas (nucleados), y ayudaron a definir características de la transcripción extrapolables a la mayoría de los organismos.

Las investigaciones con bacterias demostraron que los genes están divididos en dos regiones principales, de función distinta.
Una de ellas, la región cifrada, determina el orden que deben seguir los aminoácidos para formar una proteína.

Ese orden viene dictado por la secuencia de nucleótidos presente en una de las cadenas de la doble hélice de ADN .
Los nucleótidos se distinguen unos de otros por la base nitrogenada que portan: adenina ( A ), timina ( T ), citosina ( C )o guanina ( G ).
La otra región del gen cumple una función reguladora:
controla el ritmo con el que la polimerasa de ARN debe transcribir la región cifrada en ARN mensajero.

En bacterias, como en la mayoría de los procariotas, la región reguladora, denominada promotor, reside en un tramo de nucleótidos localizados a corta distancia (a menudo sólo 10 nucleótidos) curso arriba del sitio donde comienza la región cifrada.
Para que la transcripción se produzca de forma adecuada y eficaz, la polimerasa de ARN debe unirse al promotor.
Una vez allí, se desliza hacia el inicio de la región cifrada, desde donde viaja sobre el ADN , como un tren sobre la vía, construyendo una réplica en ARN de la secuencia cifrada.

Salvo en los genes muy largos, el número de moléculas de ARN que se fabrican en un momento dado depende principalmente del ritmo con el que la polimerasa de ARN se une al promotor e inicia la transcripción. 

Pero la polimerasa de ARN es una molécula muy promiscua, incapaz de distinguir entre un promotor y otra secuencia de ADN .
Para que la enzima se dirija hacia los promotores de genes específicos, las bacterias producen ciertas proteínas, los factores sigma, que se unen a la polimerasa de ARN .
Los complejos resultantes reconocen secuencias de nucleótidos especificas en el promotor, y se unen a ellas.
De ese modo, los factores sigma programan la polimerasa de ARN y consiguen que ésta se engarce sólo en secuencias promotoras. 

Los factores sigma desempeñan, por tanto, un papel decisivo en la activación diferencial de los genes bacterianos. 
Sabedores de ello, cuando nos decidimos a investigar el aparato transcripcional humano, empezamos por buscar moléculas similares a las sigma en las células humanas.
Pero no valoramos suficientemente el nivel de complejidad hacia el que había evolucionado la maquinaria encargada de extraer la información genética de nuestros genomas.
Pronto fuimos conscientes de la posible ausencia de factores sigma humanos, o de que, si existían, no tenían por qué tener la misma forma que los bacterianos.

Si no existen factores sigma simples en eucariotas,
¿cómo aseguran estas células que la polimerasa de ARN transcriba los genes correctos, en el momento adecuado y con el ritmo preciso?
La respuesta empezó a perfilarse una vez que conocimos el insólito diseño de los genes eucariotas.

En 1983, se sabía ya que todos los eucariotas, desde las unicelulares levaduras hasta los más complejos organismos pluricelulares, portan tres tipos de secuencias discretas de nucleótidos, necesarias para que la polimerasa de ARN inicie la transcripción.
Una de esas secuencias, normalmente muy próxima a la región cifrada, opera como un promotor bacteriano.

Es el centro del promotor, sitio a partir del cual la polimerasa comienza su tarea.
Muchos genes tienen centros promotores de ese tipo.

Walter Schaffner, Steven Lanier McKnight y otros identificaron, además, unos elementos reguladores nuevos, los intensificadores ( enhancers ), que estimulan la transcripción.
Estas secuencias pueden encontrarse a miles de nucleótidos curso arriba o abajo del centro promotor.
Estudios posteriores revelaron la existencia de silenciadores, que son secuencias que inhiben la transcripción y que, de nuevo, pueden encontrarse a grandes distancias del centro promotor.

Para entendernos, si el centro promotor fuese el encendido de un coche, los intensificadores serían el acelerador y los silenciadores los frenos.
Los genes eucariotas pueden contar con varios intensificadores y silenciadores; dos genes distintos pueden compartir idénticos elementos intensificadores o silenciadores.

Pero no existen dos genes que posean la misma combinación de intensificadores y silenciadores.
Así, las células controlan la transcripción de cada gen individualmente. 

El descubrimiento de esos elementos llevó a dos conclusiones relacionadas y sorprendentes.
Era evidente que los intensificadores y silenciadores no podían, por sí solos, controlar la actividad de la polimerasa de ARN .

Más bien parecían ser los sitios de «anclaje» de una familia de proteínas.
Las proteínas que se uniesen a intensificadores y silenciadores, que ahora se denominan activadores y represores, respectivamente, enviarían a la polimerasa de ARN mensajes de estimulación o represión, directa o indirectamente (es decir, pisarían el acelerador o el freno).
En buena lógica, el ritmo de transcripción de un gen dependería, pues, de la actividad conjunta de todas las proteínas - o factores de transcripción- unidas a los diversos elementos reguladores. 

Se trataba de explicar por qué las proteínas que se unen a secuencias de ADN situadas muy lejos del centro promotor de un gen podían influir en la transcripción de dicho gen.
Lo mismo que otros laboratorios, abordamos el problema intentando aislar factores de transcripción humanos; hasta entonces no se había aislado ninguno (salvo la propia polimerasa de ARN ).
Suponíamos que una vez que hubiésemos purificado algunos de esos factores, podríamos adentrarnos en las claves de su funcionamiento.

Puesto que muchas proteínas que se unen al ADN no intervienen en la lectura de los genes, el de buscar factores de transcripción por el criterio de pescar proteínas nucleares capaces de unirse al ADN tampoco era el procedimiento más adecuado.
Mi grupo adoptó una estrategia más selectiva, buscando proteínas que in vitro se uniesen al ADN y estimulasen la transcripción. 

En 1982, William S., Dynan determinó que un componente de cierta mezcla de proteínas nucleares cumplía todos los requisitos de un factor transcripcional.

Se unía a un elemento regulador común a ciertos genes, denominado bloque GC por su abundancia en esos nucleótidos.
Lo más importante, no obstante, era que, cuando se añadía a una preparación de proteínas nucleares, entre ellas la polimerasa de ARN , la sustancia sólo estimulaba la transcripción de genes que portaban bloques GC .
Habíamos, pues, identificado el primer factor transcripcional humano capaz de reconocer una secuencia reguladora específica.

Lo llamamos SP1 ( specific protein 1 ).

Nos pusimos a purificar la molécula.
Había que vencer una dificultad característica:
los factores de transcripción se dan en pequeñísimas cantidades dentro de la célula; por lo común, un factor particular puede representar la cienmilésima parte del total de proteínas de una célula humana.

En 1985, James T., Kadonaga encontró una forma de salvar esa dificultad técnica, introduciendo una poderosa herramienta, que se utiliza desde entonces para purificar incontables factores de transcripción y otras proteínas de unión a ADN , que normalmente se hallan en cantidades ínfimas.

Como Sp1 reconocía bloques GC selectivamente, Kadonaga sintetizó moléculas de ADN que sólo contenían secuencias de ese tipo, y las «pegó» por procedimientos químicos a unas bolitas sólidas.
A continuación, pasó una mezcla compleja de proteínas nucleares humanas por el ADN , sabedor de que sólo la Sp1 quedaría unida a dicho ADN .
Y de esa forma, separando las proteínas que se habían quedado pegadas al ADN sintético, purificó la proteína Sp1 .

Los estudios realizados por el grupo de Mark Ptashne nos han mostrado que los reguladores transcripcionales bacterianos son proteínas modulares, en las que regiones diferenciales realizan tareas distintas.
En cuanto conocimos la secuencia de aminoácidos de Sp1 , nos aprestamos a buscar esos módulos; observamos que al menos había dos interesantes.

Un extremo de la molécula contenía una región capaz de plegarse formando tres dedos de cinc, que son estructuras en las que cierta parte de la proteína se pliega alrededor de un tomo de cinc.
Esos dedos constituyen los «garfios» que utilizan muchas proteínas activadoras para unirse al ADN .
Sp1 era la segunda proteína en la que se encontraban esos motivos.
El equipo de Aaron Klug había descubierto, muy poco tiempo antes, dedos de cinc en un factor de transcripción de rana.

El otro extremo de Sp1 contenía un dominio formado por dos segmentos discretos, ricos en el aminoácido glutamina.

Sospechábamos, fundados en un importante descubrimiento, que esa región intervenía en la transcripción. 
En experimentos in vitro , las moléculas Sp1 mutantes que carecían de ese dominio se trababan perfectamente con el ADN , pero no estimulaban la transcripción génica.
Tal hallazgo indicaba que Sp1 no afectaba a la transcripción en virtud de la simple unión al ADN ; el segmento rico en glutaminas, ahora denominado activador, interaccionaba con alguna otra parte de la maquinaria transcripcional.
Pero, ¿qué parte?

En 1988, cuando iniciamos la búsqueda de la diana de Sp1 , ya sospechábamos dónde podría encontrarse.
Íbamos conociendo mejor el complejo transcripcional basal, una parte del cual parecía el objetivo verosímil.

A mediados de los ochenta, el grupo de Robert G., Roeder demostró que la polimerasa de ARN no podía transcribir genes eucariotas si previamente no se habían ensamblado en el centro promotor otros factores de transcripción, ahora llamados factores basales.
Durante ese decenio, el laboratorio de Roeder y otros habían identificado al menos seis de esos factores esenciales: xxx .

En el tubo de ensayo, esta colección de factores permitía a la polimerasa transcribir el gen correspondiente a un ritmo basal, bajo e invariante, pero sin posibilidad de modulación.
Cuando mi grupo mezcló los componentes del complejo (incluyendo la polimerasa de ARN ) con un gen que tenía un bloque GC , obteníamos un nivel de transcripción bajo y constante.
Sólo si añadíamos Sp1 a la mezcla observábamos un notable incremento de la transcripción.

A finales de los ochenta ya se sabía que las células humanas alojaban al menos dos tipos de factores de transcripción. 
Los factores basales se requieren para la iniciación de la transcripción en todos los genes; otras proteínas - activadores y represores- dictan el ritmo de inicio de la transcripción por el complejo basal.
A genes diferentes les corresponden controles por combinaciones distintas de activadores y represores. 
Nos parece, sin embargo, que, in vivo , es muy raro que el complejo basal se ensamble espontáneamente.
La mayoría de las veces las células dependen de activadores para iniciar su ensamblado. 

Los resultados sugerían que el dominio de Sp1 rico en glutaminas estimulaba la transcripción interaccionando con un factor basal.
En particular, sospechábamos que Sp1 entraba en contacto con el factor D , y facilitaba su unión al promotor.
Nos centramos en esa subunidad porque Phillip A., Sharp y Stephen Buratowski habían demostrado que dicho factor llegaba al centro promotor antes que los demás factores basales y permitía el ensamblado de la maquinaria basal.
En realidad, el factor D es el único componente basal capaz de reconocer ADN .
Se une selectivamente a una secuencia denominada bloque TATA , que hay en los centros promotores de muchos genes eucariotas.

Para seguir nuestra hipótesis, necesitábamos ahondar en la composición del factor D , que creíamos se trataba de una proteína solitaria.
No éramos los únicos interesados en desentrañar su estructura.

Comenzó la carrera hacia la purificación del factor.
Su aislamiento a partir de células humanas resultó más difícil de lo previsto. 
Muchos grupos probaron suerte con las levaduras.
Por fin, en 1989, varios laboratorios, independientemente, consiguieron aislar una proteína de levaduras con las propiedades esperadas para el factor D. 
La proteína, conocida como TBP (de TATA binding protein ) reconocía al bloque TATA , se unía a él selectivamente y determinaba un nivel bajo de transcripción cuando se unía al centro promotor mediante la polimerasa de ARN y otros componentes de la maquinaria basal.

Convencidos de que la proteína TBP era el propio factor D , nos dispusimos a comprobarlo.
Confirmado ello con los experimentos que realizamos, había que determinar exactamente qué regiones de la TBP interaccionaban con Sp1 y otros reguladores.
Muy pronto nos hallaríamos a medio camino entre la frustración y la gloria.

B., Franklin Pugh utilizó moléculas purificadas de proteína TBP , en vez de las preparaciones de factor D sin purificar.
Comprobó que la sustitución no distorsionaba la transcripción basal.
Sin embargo, para nuestra sorpresa y consternación, también comprobó que Sp1 dejaba de activar la maquinaria basal.
Había que concluir, por tanto, que el factor D y la proteína TBP no eran equivalentes; el factor D constaba de proteína TBP y otras subunidades. (Hoy sabemos que muchos factores de transcripción contienen más de una proteína).
Según todos los indicios, esas subunidades no se necesitaban para las operaciones que realizaba la maquinaria basal, pero eran esenciales para que los activadores regulasen tales mecanismos.

En otras palabras, esos componentes adicionales no eran activadores, pues no se unían a secuencias especificas de ADN .
Ni tampoco eran factores basales, ya que en su ausencia se alcanzaban niveles bajos y no regulados de transcripción.
Parecían constituir una tercera clase de factor transcripcional, denominados hoy coactivadores.
Más tarde, propusimos que los coactivadores, y no la proteína TBP , eran las dianas de los dominios de unión a proteínas de los activadores.
Sospechábamos que los activadores tendrían que unirse a coactivadores específicos para acelerar el ritmo de activación de la polimerasa de ARN por el complejo basal.

Pensábamos que debía ser así porque no cabía imaginar que una sola proteína ( TBP ) tuviera suficientes sitios de unión como para acomodar a todos los activadores fabricados por las células humanas.
Pero si los coactivadores ligados a la TBP presentasen muchos dominios de unión, sumados todos ellos podrían proporcionar los sitios de anclaje necesarios para transmitir los mensajes de los cientos o miles de activadores a la maquinaria de transcripción.

A Pugh se le ocurrió la idea de que los coactivadores podían funcionar como tales moléculas adaptadoras. 
Sus datos me convencieron de la verosimilitud de su hipótesis, aunque no todos en nuestro laboratorio pensaban lo mismo.
Nuestras reuniones semanales de principios de los noventa fueron testigos habituales de acaloradas discusiones.
No era raro que, cuando se presentaba a otros investigadores del campo el concepto de coactivador, también se mostrasen escépticos. 
Esta reacción a un resultado inesperado y complicado quedaba probablemente justificada, ya que nuestros datos no eran incuestionables.

Ni siquiera hablamos conseguido todavía aislar un solo coactivador.

Para terminar de convencernos de que estábamos en lo cierto, debíamos diseñar un procedimiento experimental que estableciese sin ambigüedades que los coactivadores existían y operaban como los transmisores que habíamos imaginado.

Los dos años siguientes a la propuesta de la hipótesis del coactivador por Pugh los dedicamos a purificar un complejo intacto y funcional que contuviese la TBP y todos los demás constituyentes asociados al factor D.
Atravesamos momentos amargos, en los que parecía que la más que impopular hipótesis del coactivador podía basarse en errores de nuestra propia investigación.

Pero el descubrimiento llegó.
En 1991, Brian D., Dynlacht, Timothy Hoey, Naoko Tanese y Robert Weinzierl, de nuestro laboratorio, encontraron una forma ingeniosa de aislar copias puras del factor D .
Posteriores análisis bioquímicos revelaron que, además de la TBP , la unidad completa incluía 8 proteínas hasta entonces desconocidas. 
Puesto que carecíamos todavía de pruebas de que esas proteínas operasen como coactivadores, las denominamos genéricamente TAF , o factores asociados a la TBP .

Nos convencimos de que los TAF enviaban señales moleculares desde los activadores hasta el aparato transcripcional basal después de separar las proteínas del TBP y hacer otros experimentos.
En ese contexto, demostramos que, en mezclas de activador Sp1 , factores basales y polimerasa de ARN, sólo se estimulaba la producción de ARN mensajero de un gen que contenía un bloque GC cuando también se añadían los TAF .
Posteriormente, Jin-Long Chen mezcló en un tubo de ensayo TBP purificada, los ocho TAF aislados, un gen humano y el resto de la maquinaria basal de transcripción.
El conjunto de proteínas se ensamblaba en el gen y respondía ante diversos tipos de proteínas activadores.
Al poco tiempo, demostramos que esos activadores actuaban uniéndose directamente a ciertos TAF .
En conjunto, los coactivadores del factor D vienen a constituir una suerte de unidad central de procesamiento que integra señales reguladoras emitidas por activadores que están unidos al ADN .

Los complejos formados por activadores, coactivadores y maquinaria basal representan el equivalente humano de los factores sigma.
Además, atraen a la polimerasa hacia genes específicos y con un ritmo muy concreto.
En cierto modo, los complejos pueden considerarse factores sigma formados por muchas subunidades.
Pruebas recientes abonan la posibilidad de que nos hallemos ante una forma universal de regulación génica en eucariotas.
Esos estudios confirman que en levaduras también existen coactivadores y que el factor D consta de muchas subunidades en hongos, igual que en el hombre.

Sin embargo, aunque los resultados sean satisfactorios, no acaban de explicar de qué manera la unión de los activadores a los intensificadores y a los coactivadores influye en el ritmo de transcripción de los genes in vivo por parte de la polimerasa de ARN .
Puede que la unión de los activadores a los intensificadores haga que el ADN se pliegue de forma que acerque a los intensificadores entre sí y a éstos con el centro promotor.
Esta disposición puede facilitar que los activadores (solos o en concierto con otros) se acoplen con los coactivadores y determinen así el anclaje del factor D en el promotor. 
Ello posibilitaría, a su vez, el ensamblado de todo el complejo basal.
La formación de este complejo podría cambiar la configuración del ADN subyacente y hacer que la polimerasa de ARN avance hacia la región cifrada que debe transcribir. 

Sobre el funcionamiento de los represores sabemos menos. 
No obstante, muchos pensamos que los represores podrían también unirse a veces a los coactivadores.

Esta unión inhibiría la transcripción, impidiendo que los activadores se engarcen en los sitios habituales sobre los coactivadores.
En otras ocasiones, los represores podrían «puentear» la maquinaria basal, bloqueando la transcripción al impedir que los activadores conecten con los intensificadores.

Pese a las lagunas en el razonamiento, podemos ya esbozar una respuesta a la pregunta de por qué células distintas fabrican mezclas diferentes de proteínas durante el desarrollo embrionario y en los organismos maduros.
Para que un gen se transcriba a un ritmo mensurable se necesita la presencia de los activadores adecuados y que éstos eludan el efecto inhibidor de los represores.
Las células fabrican proteínas diferentes porque portan mezclas distintas de activadores y represores.
La pregunta a plantear ahora es cómo decide la célula qué factores transcripcionales debe producir en primer lugar.
Pero ésa es otra historia en la que también se están ya consiguiendo algunos progresos.

¿Cómo podemos aprovechar estos nuevos conocimientos sobre la regulación génica para desarrollar medicamentos que combatan enfermedades graves motivadas por la transcripción excesiva o inadecuada de un gen?
En teoría, impidiendo que los activadores correspondientes se unan a los intensificadores o coactivadores se debería poder disminuir un ritmo de transcripción excesivo.
Por su parte, estabilizando la maquinaria transcripcional de un gen deberíamos poder contrarrestar una indeseada transcripción débil. 

El bloqueo podría conseguirse colocando un «tapón» molecular en un activador, para impedir que interaccione con un coactivador, o atrayendo al activador hacia un señuelo que imite al coactivador.
La estabilización de un complejo podría conseguirse desplegando moléculas que refuercen las interacciones entre activadores y ADN , o entre activadores y coactivadores.
Se trata de estrategias todavía lejanas, pero quizá resulte interesante considerar una muestra de sus posible aplicaciones. 

Tomemos, por ejemplo, el virus de la inmunodeficiencia humana ( VIH ), la causa del sida.
Para reproducirse en células humanas, el VIH necesita un factor de transcripción propio, denominado TAT , que estimule la transcripción de los genes víricos.
Si hubiese algún agente inhibidor que reconociese el TAT , e ignorase a los factores de transcripción humanos, se podría detener la replicación del virus sin afectar la producción de proteínas del paciente. 

A la inversa, el tratamiento de algunas enfermedades, por ejemplo la hipercolesterolemia, podría basarse en la estimulación de la transcripción de ciertos genes.
La hipercolesterolemia incrementa el riesgo de cardiopatías.

El colesterol se acumula en la sangre hasta niveles destructivos cuando la proteína de baja densidad ( LDL ), el colesterol malo, no se elimina bien.
En teoría, la enfermedad podría corregirse activando la transcripción del gen para el receptor de la LDL en células hepáticas.
Este receptor ayuda a limpiar la sangre de LDL.

Esperamos que muy pronto se compruebe la viabilidad de la idea:
Michael S., Brown y Joseph L., Goldstein están estudiando a fondo los componentes moleculares de la transcripción del gen del receptor.

Hasta hace muy poco, nadie había dedicado mucho esfuerzo a buscar moléculas pequeñas, productos naturales o cualquier otro compuesto capaz de modular la transcripción. 
A pesar de ello, existen en el mercado varios medicamentos, encontrados por azar, que actúan alterando la actividad transcripcional.
Uno de ellos, el RU486 (la píldora abortiva), reprime la función de ciertos receptores de esteroides, una clase de activadores que dirigen el desarrollo embrionario.
De forma similar, el inmunosupresor ciclosporina y la xxx bloquean la transcripción de un gen cuyo producto proteínico es necesario para ciertas células del sistema inmune.
Estas drogas, no obstante, actúan indirectamente.
Activan una enzima que impide el funcionamiento de un factor de transcripción sobre ese gen.

Con el tiempo se Irán identificando las combinaciones exactas de factores transcripcionales que regulan la expresión de genes concretos.
Y los diseñadores de medicamentos utilizarán probablemente esa información para desarrollar sofisticados compuestos capaces de combatir distintas enfermedades. 

Figura 1

MAQUINARIA MOLECULAR que regula la actividad de los genes que cifran proteínas

Consta de una docena larga de factores de transcripción ( formas en color ).
Estas subunidades se presentan en las diferentes etapas ( numeradas ) del ensamblado sobre un gen.
Cada subunidad puede, a su vez, estar formada por muchas proteínas.
El complejo final (4) controla el ritmo con el que la polimerasa de ARN incoa una de las etapas cruciales de la síntesis de proteínas (5): la transcripción, o copiado, del ADN en ARN mensajero ( rojo ).

Figura 2

DOS MOLÉCULAS de proteína activadora Sp1 (par de esferas punteadas, unidas a unas secuencias intensificadoras, los bloques GC (regiones de color púrpura) 

La unión se lleva a cabo mediante dedos de cinc; los puntos de contacto con el ADN están resaltados por semiesferas de color naranja.
Una vez unida al ADN , Sp1 Utiliza una región rica en glutamina ( color marrón, en el esquema ) para transmitir señales activadores de la transcripción a un coactivador específico ( figura verde ).

Figura 3

MUTUA COMUNICACIÓN ENTRE LOS ACTIVADORES, amén de la señalada con el ADN o con los coactivadores; se demostró en un experimento con Sp1

En la micrografía se muestran copias de la proteína ( manchas oscuras ) unidas al ADN por bloques GC situados en los extremos de la molécula.
Una vez allí, entrarán en mutuo contacto y darán origen a un bucle de ADN ( abajo ).

Figura 4

LA PROTEÍNA que se une a TATA ( azul ) es una molécula en forma de silla de montar, casi simétrica

Su cara inferior monta sobre el ADN ( amarillo ) y parece que lo dobla. 
Este pliegue puede facilitar el ensamblado del complejo que inicia la transcripción.
Se admite que los coactivadores, no ilustrados aquí, se unen a la cara superior. 

Figura 5

CÉLULAS CULTIVADAS ( izquierda ), que detienen su proliferación y mueren ( derecha ) cuando se bloquea la síntesis de un coactivador

La muerte se debe a que no pueden producir proteínas necesarias para su supervivencia.
Este y otros descubrimientos similares indican que los coactivadores son esenciales para la transcripción de la mayoría de los genes, o tal vez de todos.

Figura 6

UNA DE LAS MUJERES MÁS ALTAS DEL MUNDO mide casi xxx metros

La raíz de la alteración reside en su hipófisis, que produce un exceso de hormona de crecimiento. 
Los investigadores esperan tratar ésta y otras muchas enfermedades incrementando o disminuyendo la transcripción de genes específicos.

Anatomía del mecanismo de transcripción 

El aparato molecular que controla la transcripción en las células humanas consta de cuatro tipos de componentes. 
Los factores basales ( figuras azules, abajo ), designados con una sola letra, son esenciales para la transcripción, pero no pueden incrementar o disminuir su ritmo, tarea que está encomendada a moléculas reguladoras:
activadores ( rojo ) o represores ( gris ), que pueden variar de un gen a otro.
Los activadores, y posiblemente también los represores, se comunican con los factores basales a través de los coactivadores ( verde ), proteínas que forman un apretado complejo con las proteínas de unión a TATA ( TBP ).
La TBP es el primer factor basal que se asienta en el centro promotor, zona de la región reguladora de los genes.
Los coactivadores se designan de acuerdo con sus pesos moleculares (en kilodalton).

ACTIVADORES

Estas proteínas se unen a los genes en unos sitos denominados intensificadores.

Los activadores participan en la decisión sobre los genes a expresar y estimulan el ritmo de transcripción. 

REPRESORES

Estas proteínas se unen a grupos específicos de genes en sitios denominados silenciadores.
Impiden el funcionamiento de los activadores, disminuyendo el ritmo de la transcripción.

COACTIVADORES

Estas moléculas «adaptadoras» integran señales de los activadores, quizá también de los represores, y transmiten los mensajes a los factores basales. 

FACTORES BASALES

En respuesta a órdenes recibidas de los activadores, estos factores colocan la polimerasa de ARN en el comienzo de la región cifrada de un gen y envían la enzima a realizar su trabajo.


Clonación y terapia génica 

La recién estrenada técnica de la clonación suele identificarse con la creación de copias genéticas de individuos adultos.
Pero no es ésa su única aplicación posible: la clonación podría combinarse con otros medios empleados en biotecnología para conseguir nuevas metas o simplemente mejorar métodos ya disponibles.
Aunque la técnica está todavía en su infancia, y necesita mayor profundización y desarrollo, se ha empezado a hablar de ella por lo que pudiera aportar a los intereses de la terapia génica.
En particular, a la terapia génica de línea germinal: introducción de modificaciones en el código hereditario destinadas a corregir defectos que se transmitirían a las generaciones subsiguientes.

La terapia de línea germinal, que todavía no es objeto de investigación en humanos, podría, en principio, prevenir enfermedades mortales o debilitantes. 
Pensemos, por ejemplo, en la fibrosis quística o la anemia falciforme, patologías que son transmitidas sigilosamente de generación en generación por personas portadoras de una copia de un gen defectuoso;
la enfermedad se manifiesta cuando dos portadores tienen un hijo que hereda dos copias defectuosas.

Las pruebas genéticas prenatales pueden revelar si un feto o embrión presenta alguna de estas afecciones. 
Algunos padres podrían recurrir al aborto y tirar de nuevo los dados genéticos con otro embarazo.
Pero esa opción fracasa si ambos son portadores, si ambos presentan, por ejemplo, anemia falciforme.
La terapia genética, ayudada por la clonación, podría, en teoría, corregir las afecciones de esos niños, de sus hijos y de los hijos de sus hijos, es decir, de la progenie subsiguiente.

Se partiría, en el laboratorio, de un óvulo fecundado que se desarrollara dentro de una masa de tejido embrionario temprano.
Luego, mediante virus atenuados u otros vectores, se insertaría en las células del tejido embrionario un gen funcional, por ejemplo el que cifrara la betaglobina, proteína transportadora de oxígeno en la sangre, que aparece mutada en la anemia falciforme. (Una secuencia señal insertada junto con el gen ayudaría a identificar las células que incorporaron el gen correctamente).
El ADN de una de esas células modificadas podría entonces implantarse dentro de un óvulo de la madre, comenzando el embarazo de nuevo.
En efecto, este último paso reemplazaría el embrión original por un clon más sano de sí mismo.

La terapia de línea germinal no necesita, de suyo, el rodeo de la clonación.
Con ésta, sin embargo, podría hacerse mucho más fácil. 
Las células embrionarias en estadio muy temprano mantienen la capacidad de originar un embrión completo si se separan; así se forman los gemelos, trillizos y cuatrillizos idénticos.
Se trataría, pues, de alterar el ADN de las células embrionarias y devolver una a la madre para la gestación.
Pero las células embrionarias pierden su "totipotencia" muy pronto, tras contadas divisiones celulares, lo que obligaría a trabajar con muy pocas células.
Con las técnicas de manipulación al uso se fracasaría en numerosos intentos; con la clonación, empero, la edad y número de células disponibles para la manipulación son ilimitados.

En teoría, la clonación permitiría la terapia sobre células de un embarazo más avanzado (aunque esto plantearía problemas éticos para muchos padres).
En una variación sobre este tema, la terapia génica podría aplicarse sobre células de uno de los progenitores.
Un niño clonado a partir de estas células alteradas estaría libre del defecto genético, pero sería un duplicado genético de su padre donante.

La clonación podría eliminar algunas de las barreras prácticas a las que se enfrenta la terapia génica de línea germinal, pero no modifica las éticas. 
Muchos investigadores no esconden su preocupación de que se pudiera recurrir a las técnicas de línea germinal para fines eugenésicos, con un trasfondo autoritario o incluso genocida.

De la clonación, constituida en herramienta de la investigación básica, podría beneficiarse también la terapia génica de células somáticas. 
Al facilitar la producción de un número ingente de células genéticamente idénticas, debería ayudar a dilucidar los mecanismos en cuya virtud las células embrionarias se convierten en un tipo particular de célula.
Este proceso de determinación implica la inactivación de otros genes.
El hecho de poder ahora revertir dicho proceso habrá de ayudarnos a entender enfermedades que, como el cáncer, resultan de errores cometidos en dicha determinación.
La clonación podría, por tanto, permitirnos acotar los genes responsables de la enfermedad en cuestión y corregirlos.
Si a ello se llegara, con la clonación tendríamos no tanto una población determinada cuanto una población más sana.


Progresos en la detección del cáncer

Los ensayos para detectar la presencia de un tumor antes de que aparezcan los síntomas pueden salvar más vidas que los nuevos fármacos.

Una mujer acude a la consulta del médico tras haberse notado un bulto en un pecho.

El médico palpa el bulto.
Comienza una historia harto repetida.
La biopsia solicitada confirma el diagnóstico: cáncer de mama.
Se prescribe la cirugía y quizá radiaciones o quimioterapia.

Este panorama suele terminar con pobres resultados, por la sencilla razón de que el tumor se ha descubierto sólo después de la aparición de los síntomas. 

Muchas personas se han enterado de los síntomas precursores del cáncer a través de las normas de autoexamen que han editado distintas instituciones sanitarias. 

Más, para cuando aparecen los síntomas- masa o bulto apreciable, dolor o hemorragia en un órgano-, muchos tumores han adquirido ya un tamaño significativo. 
A pesar de una cirugía a fondo para extirpar el tumor, muchos cánceres en estado avanzado son recurrentes o cursan con metástasis y pueden acarrear la muerte del paciente.
Por el contrario, los tumores pequeños son menos capaces de extenderse y pueden erradicarse con mayor probabilidad.

Una reciente revolución en la biología molecular y nuestra comprensión de la genética del cáncer han contribuido al desarrollo de una serie de ensayos prometedores, tanto para evaluar el riesgo que se corre de contraer cáncer como para descubrir los tumores mientras su todavía exiguo desarrollo permite una extirpación quirúrgica eficaz. 
Además, otros ensayos pueden determinar la mejor forma de quimioterapia para un paciente dado o la probabilidad de que un cáncer pueda reaparecer tras su extirpación. 
En lugar de utilizar sondas invasoras, las pruebas pueden efectuarse con una muestra de orina o una gota de sangre.
A pesar del persistente énfasis actual sobre los nuevos tratamientos para el cáncer por terapia génica, somos muchos los que creemos que una detección precoz y un mejor seguimiento salvarán a la mayoría de los pacientes de cáncer en los años venideros, al permitir que las terapias ya existentes se apliquen en el momento en que puedan resultar más eficaces.

Una herencia genética

Como ocurre en muchas otras enfermedades, la tendencia a contraer cierto cáncer puede ser hereditaria.
Ciertas mutaciones en genes específicos, transmitidos de padres a hijos, determinan una susceptibilidad a varios tipos de cáncer de mama y colon, melanomas y otros tipos de tumores menos frecuentes.
En la actualidad se están poniendo a punto pruebas sencillas en sangre para cazar mutaciones en el ADN en los dos tipos conocidos de genes con susceptibilidad en el cáncer de mama ( BRCA1 y BRCA2 ).
Estas pruebas ayudarán a evaluar el riesgo de un desarrollo precoz del cáncer de mama. 
Si una mujer porta esta mutación presenta una alta probabilidad, aunque no una certeza, de desarrollar cáncer de mama, ordinariamente antes de los 40 años.

E inversamente, si una mujer no es portadora de la mutación, su riesgo de desarrollar cáncer de mama no es mayor que el de la población general (aproximadamente una de cada 8 mujeres contraerá la enfermedad a lo largo de su vida). 
Los nuevos ensayos permitirán a los médicos seguir de cerca a los miembros de las familias genéticamente susceptibles.
Mediante la mamografía y otras técnicas de examen tradicionales se pueden detectar tumores precoces. 
Ahora bien, como se cree que sólo una pequeña proporción de cánceres son hereditarios- alrededor del 10 por ciento de todos los casos -, estos ensayos sólo pueden tener valor en las familias de alto riesgo.
Además de para el cáncer de mama, podrán emplearse otros ensayos genéticos que determinen la propensión hacia otros tipos de cáncer; de colon, por ejemplo.

La capacidad de determinar el riesgo de cáncer de una persona, décadas antes de la manifestación de la enfermedad, suscita un conjunto de cuestiones sociales e incluso psicológicas.
Los legisladores han comenzado ya a dictar leyes para impedir que las compañías de seguros discriminen a los portadores de ciertas mutaciones génicas.
El conocimiento de la herencia genética puede también suponer para algunas personas un peso psicológico terrible que puede extenderse a toda su familia.
Incluso los miembros de la familia que no son portadores de la mutación deben enfrentarse a sentimientos de culpabilidad relacionados con la situación.

Además de los problemas sociales, deben sopesarse diversos obstáculos técnicos antes de que estos ensayos se apliquen de forma generalizada.

A pesar de los importantes avances conseguidos en las técnicas genéticas, la capacidad de diseñar pruebas fiables que detecten mutaciones relacionadas con el cáncer sigue constituyendo un reto.
Pueden escaparse ciertos casos si en una prueba no se descubren todas las mutaciones que pueden desembocar en una tumoración maligna.

No sólo ha de ser precisa la prueba; debe contribuir a mejorar las tasas de supervivencia.
Algunos críticos de las pruebas de susceptibilidad sostienen que el seguimiento intensivo que se hace tras una prueba positiva- una batería rutinaria de mamogramas, por ejemplo- puede impedir que los tumores se detecten con la suficiente precocidad como para mejorar las posibilidades de recuperación de la paciente.
Sin embargo, ciertos datos de algunos estudios de familias con alto riesgo de contraer cáncer de colon sugieren que una vigilancia estrecha, una medicación con agentes químicos que impiden el cáncer y, en algunos casos, la extirpación del colon reducen espectacularmente la mortalidad.

La opción preferente de extirpar el colon o la mama quizá no consiga los resultados preventivos que e buscan. 
Por ejemplo, tras una mastectomía, puede quedar en la paciente tejido mamario canceroso, aunque haya disminuido el riesgo de que pueda surgir un tumor.
Las limitaciones inherentes a los ensayos de susceptibilidad subrayan la necesidad de desarrollar mejores estrategias de detección precoz: 
descubrimiento de los tumores cuando son muy pequeños o cuando comienzan a tornarse malignos.
Una mejor detección deber ayudar no sólo a las familias con una susceptibilidad heredada, sino también a la población en general. 

Bien sea que los cambios genéticos se hereden, como en los síndromes de cáncer familiar, bien que se adquieran a lo largo de la vida de las personas, el cáncer procede de alteraciones en el ADN, nuestro código genético. 
Para que las células se conviertan en agresivamente malignas-proliferando sin control, introduciéndose en otros tejidos y sufriendo metástasis-, deben dañarse diversos genes relacionados con el cáncer.
Sabemos ya que las pequeñas agrupaciones de células precancerosas (consideradas aún benignas, pero a punto de empezar a convertirse en cancerosas) y las primeras células cancerosas albergan con frecuencia cambios genéticos detectables, un hallazgo que permite nuevas aproximaciones a su comprobación. 

Figura 1

LA SUSCEPTIBILIDAD AL CÁNCER puede a veces rastrearse examinando ciertas mutaciones genéticas 

El equipo del autor del artículo está buscando en los genes de la familia Lueder, de Nebraska, una mutación ligada al síndrome de un cáncer de colon, implicada también en tumores del tracto urinario. 
Los genes involucrados residen en ciertas secciones de los tres cromosomas (en gris en el diagrama).

Sondas moleculares

Los actuales análisis citológicos - examen al microscopio de las células procedentes de un frotis de Pap, por ejemplo- resultan a menudo insuficientes para identificar unas pocas células anormales atendiendo sólo a su forma y tamaño.
Por análisis del ADN, sin embargo, se pueden detectar pequeños grupos de células matadas que descarga un órgano de cancerización reciente en los líquidos corporales: desde la orina hasta los esputos o incluso fluidos excretados por los pezones.
La reacción en cadena de la polimerasa ( RCP )permite fabricar más de un millón de copias a partir de una hebra sencilla de ADN presente en una célula precancerosa o cancerosa.
Gracias a esta técnica de reproducción molecular podemos acometer ensayos en muestras clínicas mínimas, como una gota de fluido.

El ADN que se copia mediante la RCP puede, a continuación, hibridarse.
Las dos hebras de la «escala» familiar de ADN se separan, y luego se exponen a sondas genéticas consistentes en una sola hebra de ADN que contiene una mutación específica comúnmente encontrada en una célula cancerosa.
Cualquier ADN en una muestra de fluido que tenga la misma mutación se unirá a la sonda, que puede estar marcada con un colorante fluorescente o con un material radiactivo.

Gran parte del trabajo sobre análisis de ADN para detección del cáncer se ha realizado en mi laboratorio de la Universidad Johns Hopkins.

Utilizando estos métodos moleculares mis colegas y yo hemos encontrado mutaciones génicas reveladoras de cáncer - en los esputos para el cáncer de pulmón, en la orina para el cáncer de vejiga y en las heces para el cáncer de colon.
Hace varios años nuestro equipo demostró que podían detectarse ciertas mutaciones en el oncogén ras examinando las heces de pacientes con pólipos, que son excrescencias en el colon precursoras de cáncer de colon.
Las mutaciones aparecían también en los pacientes en los que se había desarrollado cáncer de colon.

Estos resultados han conducido a ensayos con mayor número de pacientes para determinar si la identificación de las mutaciones del gen ras en heces puede convertirse en una estrategia de rutina.
Con este ensayo se pueden descubrir los pólipos antes de que éstos se detecten por colonoscopia (inspección del colon con un colonoscopio).
La simple extirpación de un pólipo disminuye en gran medida los riesgos de que un paciente contraiga cáncer.

El ensayo sobre mutaciones ras podría devenir rutinario en los laboratorios médicos dentro de pocos años. 
Pero este tipo de ensayo genético puede resultar demasiado largo y costoso cuando se buscan las múltiples mutaciones que se pueden encontrar en algunos genes.
Para descubrir cada mutación se ha de emplear una sonda de ADN distinta.
En una estrategia alternativa se recurre a los microsatélites para localizar los cánceres; los microsatélites son fragmentos cortos de ADN repetitivo.
Puesto que estas unidades de repetición no contienen una información útil para la célula, a veces se les denomina ADN chatarra.
Pero los microsatélites sí encierran una gran cantidad de información para el diagnosticador de cánceres y también para los forenses, quienes lo utilizan como uno de los métodos de huella dactilar de ADN .

Repartidos por todo el ADN en cada cromosoma, los microsatélites se han revelado muy importantes en la diagnosis del cáncer.
La ausencia de una agrupación de estas unidades repetitivas indica la deleción de una región de un cromosoma.
Y un cambio en el tamaño de los microsatélites constituye, a su vez, la confirmación de una alteración genética.

En la Johns Hopkins acometimos un pequeño ensayo de búsqueda de microsatélites anormales en la orina de pacientes que presentaban síntomas de cáncer de vejiga.
Nos encontramos con ciertos cambios en microsatélites al comparar el ADN de la orina con el de la sangre.
La vejiga expele células cancerosas por la orina, pero no contamina la sangre.
La sangre actúa como una muestra control frente a la cual se ensaya la orina. 

En 19 de 20 de estos pacientes, encontramos cambios en el ADN microsatélite que señalaban la ausencia de una región entera de un cromosoma.
Las mismas alteraciones se documentaron a continuación en biopsias de tumores de estos pacientes.
En pacientes sin cáncer no encontramos microsatélites anormales. 
Aunque el ensayo no funcionó en un paciente, el 95 por ciento de detección puede compararse favorablemente con los datos de otras técnicas de diagnóstico menos complejas, como la de los frotis de Pap utilizados para detectar el cáncer de cuello de útero.

La sencillez y bajo coste del ensayo de los microsatélites le confieren una ventaja respecto a la detección de mutaciones genéticas específicas, como la de ras.
De hecho, la técnica puede automatizarse en su totalidad: 
un técnico de laboratorio necesita sólo una gota de orina y una de sangre.

Apretando un botón, un aparato que realiza la RCP hará copias de ADN de una muestra de orina para identificar un patrón de microsatélite que confirme la presencia de cáncer de vejiga.
Se han iniciado ya estudios con mayor número de sujetos para validar nuestros resultados preliminares.
Queda aún por determinar si esta estrategia funcionará con todos los cánceres.

Otras estrategias para la detección precoz se han centrado en seguir las cantidades de proteínas que o bien son el producto de un gen mutado o bien aparecen como consecuencia de la bioquímica singular de un cáncer particular. 
Un ejemplo es el antígeno específico de próstata (AEP) en la sangre de pacientes con cáncer de próstata.
El ensayo del AEP tiene un papel establecido en el seguimiento del progreso de estos enfermos:
grandes cantidades de proteína significa una recurrencia del cáncer. 
Pero el ensayo puede convertirse en una herramienta fiable para la detección precoz.
Muchos médicos han comenzado ya a usarlo de forma rutinaria para detectar tumores de próstata.

Tabla 1

Algunos síndromes de cánceres familiares

Síndrome [1] .
Melanoma familiar.
Cáncer hereditario de mama o de ovario.
Cáncer hereditario de mama.
Cáncer hereditario de colon no formador de pólipos .
Síndrome de Li-Fraumeni.
Neoplasia endocrina múltiple.


Cáncer .
Melanoma, de páncreas.
De mama, de ovarios, otros.
De mama, otros.
De colon, de útero, otros.
De cerebro, sarcomas, otros .
De la médula, del tiroides, otros.


Gen.
MTS1/p16 (gen supresor de tumor).
BRCA1 (gen supresor de tumor).
BRCA2 (gen supresor de tumor).
MSH2, MLH1, PMS1, PMS2 (genes supresores de tumor).
p53 (gen supresor de tumor).
RET (oncogén).


Marcadores enzimáticos

Un ensayo sencillo de proteína que ha resultado prometedor tanto para la detección como para el seguimiento del cáncer utiliza una telomerasa, una enzima que opera cuando aparece el cáncer.
La enzima afecta a los telómeros - segmentos de los extremos de los cromosomas que se desarrollan progresivamente más cortos en cada división celular.
Cuando los telómeros se acortan hasta cierta longitud, transmiten instrucciones a la célula para su autodestrucción; suministran así un mecanismo para que el cuerpo se desembarace de las células envejecidas.
En la mayoría de las células normales falta la telomerasa, pero en el cáncer es activa y bloquea el acortamiento de los telómeros.
En consecuencia, las células cancerosas no mueren.

Puesto que la enzima raras veces está presente en las células normales, puede servir de marcador que revele la presencia precoz de células cancerosas.
En teoría, el escrutinio de la telomerasa tiene el potencial de suministrar una estrategia general para la detección de cáncer en los fluidos y tejidos corporales.
La empresa Geron ha empezado a desarrollar un ensayo de la actividad de la telomerasa que se basa en la investigación realizada por Jerry W., Shay, de la Universidad de Texas, y Carol Greider, de Cold Spring Harbor.

La investigación en ensayos con proteínas es anterior al advenimiento de los ensayos con marcadores genéticos. 
Muchos de los ensayos, sin embargo, no han respondido a las expectativas, ya que producen demasiados resultados falsos. 
Por eso hoy se trabaja por encauzar la investigación hacia rutas genéticas.

Además de la detección precoz, los clínicos deben determinar el grado de facilidad de diseminación o crecimiento de un tumor.
Esta evaluación o proceso de estadificación (neologismo que traduce el término inglés staging :
determinación del estadio particular que ha alcanzado una condición o enfermedad progresiva) se convierte en un componente crítico para establecer el tratamiento adicional que va a recibir el paciente después de la cirugía, la radiación o la quimioterapia.
En la estadificación, los médicos examinan piezas de un tejido para corroborar la total extirpación del tumor.
Pero las células tumorales pueden también pasar a los nódulos linfáticos cercanos.
El número de nódulos afectados después de la extirpación quirúrgica es importante para establecer la prognosis.

Los médicos saben desde hace tiempo que la aproximación estándar a la estadificación- mediante identificación de las células anormales en el microscopio óptico- no suele servir para detectar poblaciones de células cancerosas muy pequeñas.
Nuestro equipo de la Johns Hopkins acaba de aplicar técnicas moleculares para detectar células malignas, escondidas, en pacientes con cáncer de laringe y otros cánceres de cabeza y cuello.
A pesar de una cirugía radical, estos tumores acostumbran reaparecer en la misma zona.
En un estudio piloto, examinamos pacientes cuyos tumores sabíamos que albergaban mutaciones en el gen p53 .
Es éste un gen supresor de tumores encargado de impedir el crecimiento celular incontrolado; cuando se inactiva, las células suelen evolucionar en cancerosas.

Preparamos sondas moleculares para el p53 .
Las utilizamos para ensayar los nódulos linfáticos y el tejido vecino que queda después de una extirpación radical del tumor.
En más de la mitad de los casos, se dio al menos una zona circundante del cáncer que, aunque negativa al microscopio óptico, contenía células con las mismas mutaciones en el p53 que el tumor.
Estas células cancerosas se habían introducido en el tejido que rodeaba los nódulos linfáticos y quedaron allí después de la operación quirúrgica.

En los pacientes que dieron positivo en el ensayo, el cáncer recurría con frecuencia; el sitio de su reaparición solía ser la misma área donde habíamos detectado originalmente la presencia de células malignas.
Por contra, los pacientes que dieron negativo en el ensayo tras la intervención quirúrgica tienen aún que experimentar otro episodio de la enfermedad.
Se han identificado también estas mutaciones en los nódulos linfáticos de pacientes con cáncer de colon.

Los marcadores moleculares del tipo del gen p53 pueden también ayudar a prever la posible respuesta de los pacientes ante las diversas formas de quimioterapia.
La función normal del p53 consiste en detectar el daño genético y luego dirigir la célula hacia su propia muerte, es decir, hacia el desencadenamiento de eventos celulares que conforman la apoptosis.
Muchos tipos de quimioterapia, en su acción, provocan daños genéticos a las células, lo que excita al gen p53 que inicia entonces la apoptosis.
Pero los tumores en los que se ha eliminado o inactivado el gen p53 quizá no respondan ante determinados tipos de la quimioterapia acostumbrada.
Por eso, y a propósito del cáncer de mama de pacientes con p53 mutante, se están considerando terapias alternativas, como el taxol, que tal vez no dependan de dicho gen para iniciar la apoptosis.

Para satisfacer el potencial de detección y seguimiento genéticos, no bastará con descubrir la presencia de un gen mutado.
Habrá que conocer la ubicación precisa de un pequeño grupo de células malignas, para así eliminarlas.
Los avances registrados en la técnica de formación de imágenes - resonancia magnética o tomografía computarizada- ayudarán a detectar estas lesiones.
Estudios que avanzarán con la representación «biológica»: 
ingiriendo compuestos de baja radiactividad o usando técnicas fluorescentes cuya radiación indica las trayectorias seguidas por un tumor.

A pesar de los beneficios de la detección molecular, la mayoría de los estudios mencionados en este artículo se encuentran en fases iniciales y hay que esperar una validación de los mismos en ensayos clínicos con gran número de pacientes.
Sin embargo, pienso con mucho optimismo que, de aquí a cinco años, la detección molecular- y las estrategias subsiguientes para la estadificación y las aproximaciones para los correspondientes tratamientos específicos- serán una parte del chequeo médico rutinario.

Probablemente nunca existirá un ensayo único con el que puedan descubrirse todas las clases de tumores.
Cada tipo de cáncer tiene su propia firma molecular y, por tanto, requerir su propio ensayo.
Aun así, los cambios genéticos que conducen al cáncer pueden constituir también el punto débil final de la enfermedad. 
Podemos contemplar el tiempo en que una minúscula muestra de sangre, de tejidos o de diversos fluidos corporales permitir descubrir la presencia de un tumor nuevo o metastásico- en pulmón, en mama, colon u otro órgano- con el tiempo suficiente para erradicarlo.
La sensibilidad de estos ensayos puede cambiar la idea de cáncer.

¿Es prematuro el ensayo genético?

La posibilidad de localizar con precisión las mutaciones genéticas que predisponen al cáncer ha generado fuertes controversias en el seno de la comunidad médica. 
En los años ochenta se identificó el primer marcador de la susceptibilidad al cáncer: la mutación genética causante del retinoblastoma, una tumoración ocular.
Pero fue a mediados de los noventa cuando se produjo el descubrimiento de genes implicados en el cáncer de mama- y el desarrollo subsiguiente de ensayos que permitían evaluar la susceptibilidad a la enfermedad-, lo que trajo el tema a primera línea del debate público.
La importancia de encontrar los genes del cáncer de mama trasciende la propia enfermedad, dado que los genes pueden predisponer tanto a hombres como a mujeres a otros tumores, desde el cáncer de ovario hasta, probablemente, el de próstata.

El dilema para moralistas y médicos gira en torno al significado aún nebuloso de los resultados de los ensayos. 
Si un ensayo apoya la presencia de una mutación genética, una mujer con una historia familiar de cáncer de mama se enfrenta a un riesgo del 85% -no a una certeza- de contraer la enfermedad.
Pero los riesgos aún son desconocidos para una mujer que no tiene ningún familiar que haya contraído la enfermedad.

Incluso con los resultados de los ensayos en la mano, una mujer se enfrentará a decisiones difíciles acerca de qué hacer con lo que conoce.
Un ensayo negativo de un defecto genético hereditario puede darle un sentimiento infundado de complacencia, ya que el 85% de los cánceres no son hereditarios y aún sigue corriendo el riesgo de contraer un cáncer de tipo no hereditario.
Puede también haber heredado mutaciones que conducen a la enfermedad y que los expertos no han hallado todavía.

Tampoco el resultado positivo de una prueba resuelve la cuestión.
Un rastreo más insistente puede demostrarse inadecuado:
la mamografía puede pasar por alto un tumor.
Una extirpación preventiva de ambos pechos no garantiza que los tejidos que quedan tras la cirugía vayan a estar libres de cáncer.

A los críticos de las pruebas les preocupa el posible abuso de esta información por las compañías de seguros y empresarios a la hora de contratar.
En algunas partes se han dictado leyes para impedir la utilización de tales pruebas genéticas para cualquier tipo de discriminación en los contratos.

Pese a todo, hay quien recurre a tales resultados fuera de los intereses investigadores propiamente dichos.
Una clínica- el Instituto de Genética y Fertilización in vitro de Fairfax, Virginia - se ofrece para pruebas sobre una mutación en mujeres judías ashkenazis.
Dos compañías- Myriad Genetics y OncorMed - han desarrollado ensayos más amplios que buscan una gama más extensa de mutaciones en los dos genes conocidos de cáncer de mama, el BRCA1 y BRCA2 .
Se espera que tales pruebas se incorporen dentro de pocos años en la rutina clínica.

La oposición unánime de la comunidad médica a las pruebas clínicas fuera del contexto de un estudio de investigación da muestras de cuartearse.
En el número de mayo del Journal of Clinical Oncology , la Sociedad Estadounidense de Oncología Clínica rompió filas respecto a otros grupos al recomendar que se permitieran los ensayos a quien tuviera una historia familiar de cáncer de mama. 
Los partidarios de las pruebas creen que ignorar la información genética asequible puede poner en peligro a una paciente. 
Las ambigüedades y ansiedades que acompañan a las pruebas, dicen, pueden controlarse mediante un adecuado asesoramiento.
David Sidransky, autor del articulo que acompaña, piensa así.
Sidransky, asesor de OncorMed, indica que incluso sin hacer pruebas para la susceptibilidad genética la estricta vigilancia de los pacientes con alto riesgo de cáncer de colon ha conducido a una disminución espectacular de la mortalidad.

Sidransky sugiere que las mujeres con una mutación génica de cáncer de mama podrían someterse a un régimen de vigilancia intensiva y podrían ser candidatas para ensayos clínicos de nuevos tipos de compuestos quimioterapéuticos.
Saber que se alberga una mutación puede causar estrés al paciente y a su familia,reconoce Sidransky.
«Estos problemas -añade- no se pueden comparar, sin embargo, con sufrir una metástasis de cáncer de mama y morir de la enfermedad».
Otros observadores titubean.
Francis S. Collins, quien dirige el Centro de Investigación del Genoma Humano, respondía así a una encuesta sobre el tema en el Journal of Clinical Oncology .
«Nos preocupa que la capacidad de ensayar la susceptibilidad hereditaria preceda a la capacidad de informar a los individuos de mejores opciones médicas, de suministrar asesoramiento y criterios que ayuden a los individuos y a las familias a tomar decisiones que afectan la calidad de vida, y a proteger a las familias frente a la múltiples formas de discriminación».

Diagnóstico de Hubert H., Humphrey 27 años después

El poder de las nuevas herramientas de diagnóstico molecular quedó patente en 1994, cuando nuestro equipo de investigación de la facultad de medicina de la Universidad Johns Hopkins diagnosticó el cáncer de vejiga de Hubert H., Humphrey en una muestra de orina de hacia 27 años. 
Humphrey es un caso clásico que subraya la necesidad de una detección precoz.

En 1967, siendo vicepresidente con Lyndon B., Johnson, descubrió sangre en su orina.
Sus médicos hicieron pruebas en busca de células anormales.
No pudieron, sin embargo, hallar una prueba definitiva de cáncer, y así se postergó cualquier tratamiento enérgico. 
A los pocos años se le diagnosticó correctamente, y en 1976 Humphrey fue sometido a radioterapia y cirugía radical.
Murió cuando la enfermedad volvió a aparecer.

En el experimento (diagrama), los investigadores- Ralph H., Hruban, Per van der Riet, Yener S., Erozan y David Sidransky - fuimos autorizados por la viuda de Humphrey, Muriel Humphrey Brown, para trabajar con las muestras de orina tomadas en 1967 y una muestra del tumor extirpada años después. 

Sabemos hoy que ciertas mutaciones en el gen p53 denuncian cáncer de vejiga.
Pero queríamos averiguar si una modificación de ese género sería detectable en la orina de 1967.
Para encontrarlo, confirmamos primero que el tumor portaba una mutación en el p53 (parte baja del diagrama).
Extrajimos e hicimos copias del ADN y a continuación secuenciamos (identificamos cada nucleótido o bloque de construcción del ADN ) en una porción del gen p53 .
La secuenciación reveló una mutación en un punto:
un nucleótido (timina) habría reemplazado a otro (adenina).
A continuación sintetizamos una sonda consistente en una hebra sencilla de ADN que reconocería, o se aparearía con, ADN portador de la misma mutación.
Se agregó un marcador radiactivo a las hebras de ADN para seguir la trayectoria de la sonda.

Por separado hicimos copias del ADN a partir del gen p53 de la muestra de orina(parte superior del diagrama), sirviéndonos de la técnica de la reacción en cadena de la polimerasa ( RCP ).
A continuación insertamos el ADN en bacterias que crecieron formando colonias que se depositaron sobre una membrana de nylon.
En las colonias, se disociaron las hebras de ADN (al objeto de que pudieran aparearse con la sonda correcta).
Cuando las sondas se situaron en la membrana, se aparearon con el ADN bacteriano que contenía la mutación, lo que indicaba que la mutación había, en efecto, estado presente en la orina de Humphrey ya en 1967.

Se han utilizado métodos similares para diseñar las pruebas que se están empleando en la práctica clínica de detección del cáncer.
La orina o la sangre se tratan con una sonda que localiza con precisión una mutación indicadora de cáncer. 

El caso de Humphrey subraya el potencial de las nuevas técnicas moleculares para un diagnóstico positivo cuando otros métodos producen resultados equívocos. 
Un diagnóstico precoz podría haber determinado un tratamiento quirúrgico salvador años antes y podría haber cambiado el curso de la historia política. 
Humphrey podría incluso haber reconsiderado su decisión de presentarse a la elección presidencial frente a Richard M., Nixon en las elecciones de 1968.

[1] Los síndromes pueden abarcar varios tipos de cáncer.


Crecimiento microbiano aeróbico sobre sustratos de un átomo de carbono

Definición, objetivo y fundamento 

La definición de sustrato de un átomo de carbono puede parecer obvia.
Sin embargo, en términos metabólicos, bajo esa denominación se incluyen no sólo sustancias que contienen nada más que un átomo de carbono, como dióxido de carbono, metano y metilamina, sino también otras con más de un átomo de carbono pero carentes de enlaces carbono-carbono, como: xxx .

En el presente capitulo se discutirán inicialmente las rutas metabólicas que rinden energía halladas en organismos capaces de utilizar aeróbicamente sustratos de un carbono y, en segundo lugar, se estudiarán las rutas biosintéticas por las que los sustratos se transforman en los precursores de tres y cuatro átomos de carbonos necesarios para el crecimiento.
Se ha adoptado un selectivo tratamiento dirigido a aquellos microorganismos, sustratos y rutas que tienen interés biotecnológico.
En particular, se está acentuando el interés en la utilización de metano, metanol y monóxido de carbono mientras que al dióxido de carbono se le está dando una cobertura más selectiva.
Esto no significa que los organismos capaces de utilizar dióxido de carbono para su crecimiento (autótrofos) no sean importantes en Biotecnología.
Realmente, las plantas, algas y cianobacterias que utilizan la energía de la luz para generar ATP y tienen la facultad de fijar el dióxido de carbono ( fotoautótrofos ) destacan como organismos importantes en los desarrollo biotecnológicos. 
Sin embargo, los libros de Bioquímica(Lehninger, 1982) estudian detalladamente los procesos fotosintéticos de transporte de electrones y la fotofosforilación así como las rutas subsiguientes de fijación autotrófica de dióxido de carbono.
Por el contrario, los mecanismos de generación de ATP en los organismos capaces de crecer aeróbicamente a partir de metano y metanol ( metilotrofos ) y de monóxido de carbono ( carboxidotrofos ) están menos documentados.
Igualmente, las rutas de conversión de sustratos de un átomo de carbono, excepto dióxido de carbono, a los precursores de tres y cuatro carbono que se requieren para la biosíntesis raramente se discuten en los textos generales. 

No obstante, estos microorganismos y su explotación tienen un gran interés.
Por ejemplo, los metilotrofos se están utilizando ya para la producción de biomasa (cabe citar el caso de Metaylophilus methylotrophus para producir biomasa a partir de metanol mediante el proceso denominado Prutreen ICI).
Además, ciertos enzimas que desempeñan un papel eleve en la oxidación de estos sustratos, especialmente la metano monooxigenasa, se están explotando en procesos de conversiones químicas ( bioconversores ) y en biosensores .
En relación con los carboxidotrofos, cabe decir que su interés se está actualmente demostrando en relación a su posible papel en los biofiltros para eliminar el no deseado monóxido de carbono; como células biofuel destinadas a la conversión de la energía química liberada durante la oxidación de monóxido de carbono en electricidad y en biosensores para la detección y cuantificación de monóxido de carbono en diversos entorno s, como minas, apareamientos subterráneos y tunales.
Evidentemente, es importante un conocimiento pleno de la bioquímica de todos estos microorganismos para estudiar sus aplicaciones biotecnológicas. 

Los compuestos

El metano ( xxx ) es el compuesto de un átomo de carbono más abundante.

Se genera en ambientes anaeróbicos como en las profundidades del lodo, arrozales y en el estómago de los rumiantes en las fases finales de la degradación anaeróbica de sustrato orgánicos.
El proceso de producción de metano por las bacterias metanogénicas (se estudia en el apéndice de este capítulo) juega un papel muy importante en el ciclo del carbono en el que más del 50 % del carbono anual que ingresa en los hábitats acuáticos se regenera en forma de metano.
La metanogénesis da lugar a la producción anual de unos 800 millones de toneladas de metano.
Las bacterias anaerobios generan también cantidades importantes de metanol ( xxx ) pero no llega a los niveles de metano.
Sin embargo, se produce en la atmósfera a partir de metano durante procesos fotooxidativos.

Entre otros sustratos de un átomo de carbono, sólo se produce monóxido de carbono y dimetilsulfuro en cantidades significativas durante procesos degradativos naturales.
También se producen grandes cantidades de monóxido de carbono, que se libera la atmósfera, en los escapes de los automóviles y altos hornos En diversos proceso industriales, se generan también subproductos entre los que existen compuestos de un átomo de carbono.
Por ejemplo, de la industria del curtido derivan formaldehído( xxx ), formato ( xxx ), metilamina ( xxx ), dimetilamina ( xxx ) y trimetilamina ( xxx ); durante el tratamiento del caucho se produce formato; en la industria del galvanizado y en la de extracción de metales se forma cianuro (CN) y durante el tratamiento de la pulpa de madera se generan dimetil sulfuro ( xxx ), dimetilsulfóxido ( xxx ) y dimetilsulfona ( xxx ).

En todos estos casos los subproductos presentan un serio problema de afluentes.

El conocimiento de los microorganismos que puedan utilizar tales sustratos será de gran importancia para el desarrollo de biosensores que indiquen, tanto cualitativa como cuantitativamente, la presencia de aquellas sustancias e igualmente para el desarrollo de algún medio económico que degrade los efluentes a otros menos tóxicos.

Metilotrofos

Antes de la década de los años 60 se habían aislado y descrito sólo unos pocos microorganismos, aparte de los autótrofos, capaces de crecer aeróbicamente a partir de compuestos de un átomo de carbono.
Todos los microorganismos que tienen la facultad de crecer a partir de compuestos orgánicos de un átomo de carbono (es decir, las sustancias listadas en el párrafo anterior excepto el cianuro y el monóxido de carbono) se conocen como metilotrofos.
Por otra parte, los que pueden crecer utilizando monóxido de carbono como sustrato se les denominan carboxidotrofos y los que utilizan cianuro cianotrofos .

Las bacterias metilotrofas se han dividido en dos tipos. 
Microorganismos del Tipo I : son especies obligadas que pueden crecer sólo sobre metano y/o metanol. 
Tienen distribuidas, de forma uniforme, inclusiones membranosas en forma de disco que se desarrollan particularmente cuando crecen sobre metano; utilizan la ruta de la ribulosa fosfato ( xxx ) para la asimilación del sustrato (véase Sección 3.10) y poseen un ciclo del TCA incompleto.
En contraste, las bacterias del Tipo II son, con frecuencia, microorganismos facultativos que no sólo pueden utilizar la lista completa de compuestos de un átomo de carbono (habitualmente con la excepción del metano) sino también una amplia variedad de otros sustratos con más de un átomo de carbono.
Sus sistemas de membrana son diferentes; se asocian en parejas y se localizan en la periferia de la célula; utilizan una ruta asimilatoria diferente, la ruta de la L-serina (véase Sección 3.11) y poseen un ciclo del TCA completo.
Estas diferencias tan marcadas entre las bacterias metilotrofas de los Tipos I y II indican, probablemente, dos caminos evolutivos diferentes que han convergido en la utilización de sustratos similares. 

Las bacterias de los Tipos I y II se dividen, a su vez, en los subgrupos I y II y en diversos géneros diferentes, de acuerdo con los criterios taxonómicos en los que se incluyen el contenido en guanina y citosina del DNA, la morfología y la temperatura óptima de crecimiento. 
Los microorganismos del Tipo I se encuadran dentro de tres géneros: Methylomonas, Methylobacter y Methylococcus .
Methylococcus capsulatus ha estado sometido a considerables controversias debido a que posee características de las especies de los Tipos I y II.
En particular, no utiliza obligatoriamente metano o metanol, pudiendo, por tanto, usar como sustratos otros compuestos de un átomo de carbono de una forma autotrófica (véase Sección 3.9).
Este ejemplo ilustra las dificultades taxonómicas de un grupo de microorganismos insuficientemente clasificados en un momento en que nuevas cepas se están aislando regularmente.
Los microorganismos del Tipo II comprende los géneros obligados Methylosinus y Methylocystis y los facultativos son especies de los géneros Methylobacterium, Methylophilus, Thiobacillus, Hyphomicrobium, Paracoccus y numerosas pseudomonadáceas.

Además de bacterias, se han aislado diversas levaduras de cultivos enriquecidos capaces de crecer a partir de metano y metanol; se incluyen especies de los géneros Rhodotorula, Candida, Kloeckera, Hanensula y Sporobolomyces .
Hasta el momento, se sabe mucho menos acerca de la estructura de membranas y de aspectos generales del metabolismo de estas levaduras.

Los sistemas de membrana de las bacterias metilotrofas ha despertado un considerable interés porque su desarrollo parece variar dependiendo de las condiciones de crecimiento. 
Parecen estar bien desarrolladas cuando crecen a partir de metano pero lo están menos y más insuficientemente organizadas a medida que envejece el cultivo e ingresan en la fase estacionaria de crecimiento.
Esta circunstancia también ocurre en las especies facultativas cuando se utilizan otros sustratos incluso aunque las membranas estén mucho menos desarrolladas inicialmente.
Variaciones similares acontecen como respuestas a los niveles de oxigeno y a las diferencias de temperatura; niveles bajos de oxigeno y temperaturas bajes implican un mayor desarrollo de la membrana.
Hasta el momento no se han ofrecido aún razones convincentes que expliquen este comportamiento.

Rutas oxidativas para la generación de ATP en bacterias que crecen a partir de metano, metanol, fomaldehído y formato

No existe un sitio de ingreso en el ciclo del xxx para la oxidación de los sustratos de un átomo de carbono (véase Sección 21).
Por ello, los microorganismos que crecen a partir de tales sustratos deben utilizar una ruta oxidativa diferente destinada a la generación de aceptores de electrones reducidos que puedan ingresar en el sistema de transporte de electrones para la producción de xxx Los primeros estudios, realizados fundamentalmente por Quayle y sus colaboradores utilizando diversos inhibidores metabólicos en microorganismos que crecían a partir de metano, mostraron que en presencia de iodoacetato se acumulaba metanol mientras que en presencia de sulfito se acumulaba formaldehído y el metabolito que se acumulaba en el medio de cultivo de células en reposo era formato.
Estas observaciones condujeron a la proposición de la ruta oxidativa mostrada en la Figura 3.1 en la que el dióxido de carbono es el producto final. 
La estequiometría global puede resumirse como sigue: xxx .

En estudios posteriores se desarrolló esta ruta lineal para explicar la oxidación no sólo del metano sino también la del metanol, formaldehído y formato como sustratos energéticos; dichos sustratos ingresarían en la ruta en el punto adecuado.
Cada reacción de esta ruta se considera a continuación con más detalle.

Metano mono-oxigenasa

La reacción catalizada por la metano mono-oxigenasa es la más interesante de la ruta y fue la mas difícil de demostrar.
Inicialmente se consideraron varias posibilidades para la oxidación del metano a metanol: xxx .

Figura 3.1

Ruta oxidativa para la generación de xxx en los metilotrofos

X = aceptor de electrones; xxx aceptor de electrones reducido; (1) metano mono-oxigenasa, (2) metanol deshidrogenasa; (3) formaldehído deshidrogenasa; (4) formato deshidrogenasa El uso de xxx demostró que el oxigeno del metanol derivaba del xxx y no del xxx , con lo que se excluyó la alternativa (c).

Después se hipotetizó que la reacción (b) era más probable que la (a) debido a que la energía disponible para la generación de xxx sería menor con la reacción (a) dado que un nucleótido reducido ( xxx ) se consumía en la reacción con lo que disminuiría la producción neta de xxx y, a su vez, seria menor el potencial de producción de xxx .
En consecuencia, se argumentó que si se utilizaba la ruta (a) el rendimiento del crecimiento a partir de metano seria más bajo que el que se obtendría a partir de metanol.
De hecho, el rendimiento del crecimiento era mayor en los microorganismos que crecieron a partir de metano. 
No obstante, muchos investigadores opinaron que la reacción (b) era termodinámicamente improbable.
La solución del problema sobre la reacción que estaba implicada en el proceso tuvo que aguardar hasta el desarrollo de un sistema in vitro capaz de convertir el metano en metanol.
Dicho sistema se desarrolló para Methylococcus capsulatus .

Rápidamente se demostró después que el enzima que existía era la metano mono-oxigenasa de la reacción (a).

Estudios posteriores sobre este sistema enzimático continuaron con la controversia y actualmente se admite que existen dos formas del enzima, uno ligado a la membrana y otro soluble. 
El sistema soluble se demostró primero en Methylococcus capsulatus y el particulado en Methylosinus trichosporium pero actualmente se cree que los dos enzimas pueden existir en ambos microorganismos; la presencia de uno u otro dependería de las condiciones de crecimiento y, en particular, de la naturaleza del factor de crecimiento limitante ( xxx ).
Más recientemente, se ha mostrado en Methylococcus capsulatus , en el que previamente se asumía que sólo poseía el enzima soluble, que la disponibilidad de cobre parece influir en la localización celular y tipo de la metano mono-oxigenasa. 
Cuando crece en presencia de bajos niveles de cobre el enzima que prevalece es el soluble pero se transforma en el particulado si crece en presencia de niveles elevados de cobre.
Además, cambia la composición proteica del enzima.

El enzima soluble de Methylococcus capsulatus consiste de tres proteínas: A, B y C.
La proteína A contiene hierro no heme y azufre, mientras que la proteína B posee un grupo prostético desconocido y la proteína C es una flavoproteína que tiene también hierro y azufre.
La secuencia de la reacción que se ha postulado para la oxidación del metano por este enzima es: donde FPC es la flavoproteína C.
El enzima particulado está más insuficientemente caracterizado pero se sabe que la de Methylosinus trichosporium se compone también de tres proteínas. 
Cabe destacar que el enzima purificado no utiliza xxx como dador de electrones; en su lugar una de las tres proteínas componentes, el citocromo c reducido, desempeña esta función.

Se requieren obviamente nuevos estudios en ambos tipos de metano mono-oxidasa para aclarar totalmente los mecanismos de oxidación del metano a metanol y para resolver la cuestión de la localización celular y el tipo del enzima.
En particular, seria interesante establecer si la facultad de cambiar de un tipo a otro es un fenómeno extendido, lo que llevaría a clarificar el significado de tales cambios y a examinar el mecanismo regulador implicado.
Igualmente, se requieren realizar más estudios sobre el enzima aislado a partir de un amplio espectro de metilotrofos dado que, por ejemplo, en la levadura Candida el citocromo P450 parece sustituir a la proteína B en el esquema apuntado anteriormente mientras que en el caso de Pseudomonas putida una proteína distinta, rubredoxina, parece reemplazar a las proteínas A y B.

Es indudable, sin embargo, que la aclaración de todas esas cuestiones llegará pronto debido al intenso interés que hay en la explotación de la actividad metano mono-oxidasa en una gran variedad de procesos biotecnológicos. 
Este interés radica en la observación tan importante y poco habitual de que el enzima puede actuar sobre una amplia variedad de sustratos pudiendo utilizar alcanos de cadena corta y larga, alquenos y sustancias aromáticas y heterocíclicas, como ciclohexano, benceno, tolueno y piridina.
Se prevé que el enzima pueda utilizarse para catalizar reacciones que, por los procesos químicos normales, son difíciles de conseguir o proporcionan rendimientos bajos Por ello, el enzima podría tener un enorme valor en 1a industria química.
Además, ciertas moléculas «rebeldes» que no son fácilmente biodegradables sino que persisten en el entorno originando problemas de contaminación, podrían transformarse, mediante el uso de este enzima, en productos más fácilmente biodegradables con lo que se conseguiría aliviar algunos de los efectos ambientales no deseables.

La amplia afinidad de sustratos de la metano mono-oxidasa permite que los metilotrofos obligados que posean este enzima puedan oxidar y presumiblemente aprovechar, al menos en términos de producción de ATP, otros sustratos sobre los que no pueden crecer.
Higgins, Best y Hammond (1980) han discutido este fenómeno y han rechazado el uso del término metabolismo fortuito con el que se le ha denominado, argumentando que algún beneficio debe proporcionar al microorganismo. 
Se considera preferible el término de co-metabolismo .

Metanol deshidrogenasa

La metanol deshidrogenasa de las bacterias es menos variable que la metano mono- oxidasa, diferenciándose entre unas y otras especies sólo en la afinidad por el sustrato y en el peso molecular.
En todos los casos el enzima oxida a los alcoholes primarios con una longitud de cadena de hasta al menos xxx .

En algunas especies, el enzima exhibe también actividad formaldehído deshidrogenasa Una de las características de las metanol deshidrogenasas es que todas contienen un grupo prostético basado en una quinona denominado metoxatina o pirrolo quinolina quinona (PQQ) y, de aquí, que se mencione como una de las pocas quinoproteínas que se ha aislado hasta la fecha.
Este grupo prostético parece funcionar como el aceptor de electrones primario en la oxidación de metanol a formaldehído y, a su vez, transfiere electrones al citocromo c.
Este último se vuelve a oxidar por una oxidasa terminal que es un citocromo xxx o un citocromo o .

Formaldehído deshidrogenasa 

La formaldehído deshidrogenasa es un enzima común encontrado en la mayoría de los organismos dado que el formaldehído es un metabolito implicado en muchas reacciones celulares.
Como se ha indicado anteriormente, la metanol deshidrogenasa puede exhibir actividad formaldehído deshidrogenasa. 
En la mayoría de las especies, sin embargo, se ha caracterizado una formaldehído deshidrogenasa NAD dependiente distinta.
El subsiguiente sistema de transporte de electrones no se conoce bien pero se cree que está implicado el citocromo c, actuando como oxidasa terminal el citocromo xxx o el xxx .

Algunas bacterias del tipo I que utilizan la ruta RMP para la asimilación del formal debido (véase Sección 3.10) tienen actividades formal debido deshidrogenasa muy baja al igual que la del enzima siguiente en la secuencia oxidativa, la formato deshidrogenasa.
Se ha demostrado que estos microorganismos pueden oxidar al formaldehído por la ruta RMP, actuando la 6-fosfoclicerato deshidrogenasa en una versión oxidativa especializada de la ruta.
La discusión del uso de esta ruta oxidativa alternativa es más oportuna cuando se estudie la ruta RMP (véase Sección 3.10). 

Formato deshidrogenasa

La formato deshidrogenasa es también un enzima muy difundido.
Se ha demostrado en la mayoría de los que es NAD dependiente ligado al citocromo c y después a una oxidasa terminal.

De todo lo expuesto hay que deducir que hay una considerable variación individual entre las bacterias en relación con el mecanismo preciso de las reacciones responsables de la oxidación del metano a dióxido de carbono.
En particular, lo que nosotros hoy sabemos de los aceptores/dadores de electrones, de las cadenas de transporte de electrones implicadas y de su acoplamiento a la fosforilación oxidativa para rendir xxx es poco y queda aún mucho por aclarar Indudablemente, los rendimientos de xxx en cada uno de los pasos que catalizan las tres deshidrogenasas son elevados liberando la suficiente energía para la generación de 3 ATP resultando un total de, al menos, 9 xxx por molécula de metanol oxidada a xxx .
Este proceso contrasta con la generación de 6 xxx por cada uno de los átomos de carbono de glucosa oxidada totalmente a xxx en la ruta glicolítica y ciclo del xxx .
Por lo tanto, el rendimiento de energía a partir de la oxidación de metano y metanol es extremadamente grande y explica el gran rendimiento de crecimiento en la mayoría de las bacterias que utilizan estos sustratos en comparación con sustratos más oxidados.

Rutas oxidativas en levaduras que crecen sobre metanol

Las levaduras metilotrofas sólo crecen muy lentamente sobre metano y la mayoría de las investigaciones se han realizado sobre la facultad de oxidar el metanol aunque es posible que estos microorganismos puedan utilizar también otros compuestos de un átomo de carbono.
Uno puede preguntarse ¿por qué la ruta oxidativa de las levaduras se trata separadamente a la de las bacterias?
La respuesta es que los detalles de la ruta es muy diferente en la primera reacción, la conversión de metanol en formaldehído.
Esta reacción está catalizada no por una deshidrogenasa sino por la metanol (alcohol) oxidasa .
Este enzima, que se localiza en peroxisomas, es una flavoproteína ligada al FAD que consta de ocho subunidades idénticas y capaz de oxidar numerosos alcoholes además del metanol. 
La reacción global para la oxidación del metanol es: xxx .

La toxicidad del peróxido de hidrógeno liberado se neutraliza inmediatamente por una catalasa peroxisomal y, por ello, la reacción global es totalmente irreversible. 
Debe notarse que esta reacción no conlleva la reducción de un aceptor de electrones el que podría reoxidarse mediante el mecanismo de transporte de electrones para rendir xxx .
Por lo tanto, los rendimientos de xxx son más bajos en las levaduras que en las bacterias, lo que probablemente explica las velocidades de crecimiento más lentas y los rendimientos de crecimiento más bajos que caracterizan a las levaduras que utilizan metanol como sustrato.

El formaldehído, la sustancia resultante de la reacción catalizada por la metanol oxidasa, se oxida, como en las bacterias, a formato.
Igualmente, esta reacción difiere de la de las bacterias.
La formaldehído deshidrogenasa de las levaduras es NAD y glutatión (GSH)-dependiente y la reacción comienza con la formación de S-hidroximetilylutation rindiendo formilglutatión: xxx .

En Candida boidinii el tiol-éster se hidroliza después merced a una esterasa regenerándose GSH y liberándose formato: xxx .

La formato deshidrogenasa oxida finalmente el formato a dióxido de carbono en una reacción que, al parecer, es idéntica a la que se produce en las bacterias.

Se ha demostrado que tanto la actividad formaldehído deshidrogenasa como la formato deshidrogenasa de las levaduras están localizadas exclusivamente en el citoplasma.
Por ello, el formaldehído resultante de la actuación de la metanol oxidasa en el peroxisoma debe abandonar dicho orgánulo para la subsiguiente oxidación y el xxx generado por las dos deshidrogenasas debe alcanzar la mitocondria para oxidarse de nuevo mediante el sistema de transporte de electrones. 
Los experimentos realizados sugieren que el sistema de transporte de electrones implica el uso del sistema xxx oxidasa de las mitocondrias que genera no tres sino sólo dos xxx por cada par de electrones transportados.
De nuevo, este hecho apoyaría el rendimiento de crecimiento más bajo que, en comparación con las bacterias, se observa en las levaduras que crecen a partir de metanol.

Crecimiento microbiano aeróbico sobre sustratos de dos átomos de carbono

Objetivo y fundamento

Existe una mayor variedad de componentes de dos átomos de carbono que de uno; en consecuencia una mayor diversidad de microorganismos que pueden utilizar dichos sustratos, lo que realizan usando una gran variedad de rutas tanto degradativas (rutas oxidativas para la obtención de energía) como asimilatorias (rutas biosintéticas).
Sin embargo, el examen de los sustratos y de las rutas que utilizan para metabolizarlos permite hacer una serie de generalizaciones basadas en el estado de oxidación/reducción del sustrato.
Dicho estado puede agruparse en tres categorías: sustratos con una relación xxx de xxx o más (por ejemplo, etanol ( xxx ) y acetato ( xxx ); sustratos con una relación xxx inferior a xxx pero superior a xxx (por ejemplo, glicolato ( xxx )y glioxilato ( xxx ) y sustratos muy oxidados con relaciones xxx por debajo de xxx (por ejemplo, oxalato ( xxx ).
En general, el crecimiento microbiano a partir de sustratos de dos átomos de carbono ha despertado menos interés que sobre sustratos de un átomo de carbono.
Esto no quiere decir que este bajo interés actual continúe al mismo nivel dado que muchos compuestos de dos átomos de carbono se generan en ecosistemas naturales y en forma de subproductos en algunas industrias y pueden ser sustratos potencialmente útiles para la producción de biomasa.

Rutas oxidativas para la generación de ATP

La mayoría de los sustratos de dos átomos de carbono que se han utilizado para estudiar el metabolismo oxidativo y energético son los que tienen la relación xxx de xxx o más.
En general, se oxidan rindiendo acetil CoA, el que después se oxida también vía el ciclo del xxx .
Por ejemplo: xxx .

Por lo tanto, el metabolismo oxidativo de tales sustratos es esencialmente igual que en el resto de los organismos (véase Sección 2.1) pero, como se estudiará, no es así en el caso de las rutas anapleróticas (véase Sección 4.3).

Los sustratos más oxidados, con relaciones xxx menores de xxx pero superiores a xxx , se oxidan a través de dos rutas conocidas.

Primero, ellos pueden reducirse hasta el nivel de acetato que se oxida después vía el ciclo del TCA (es decir, a través de una ruta de reducción/oxidación). 

Esta ruta excepcional se da en Paracoccus denitrificans y es muy interesante (véase Figura 4.1); se descubrió porque la facultad de este microorganismo de oxidar el glicolato o glioxilato se bloqueaba por los clásicos inhibidores del ciclo del xxx : monofluoracetato, malonato y arsenito.

La ruta que se muestra en la Figura 4.1 es, en algunos aspectos, relativamente única.
La reacción clave es la condensación de glicina (2C) y glioxilato (2C) para formar hidroxiaspartato (4C) en una reacción catalizada por la hidroxipiruvato sintetasa .
El aminoácido se desamina después a su cetoácido equivalente que, al descarboxilarse, se transforma en fosfoenolpiruvato; éste se transforma en piruvato que sufre una segunda descarboxilación rindiendo acetato que se oxida vía el ciclo del TCA.

Obviamente, la ruta genera aceptores de electrones reducidos (por propio derecho, como en los pasos oxidativos del ciclo del TCA) que se reoxidan a través de los procesos normales de transporte de electrones generándose ATP.
La característica excepcional de esta vía metabólica que, como parte de la ruta en la que se genera acetato, se produce una reacción de condensación xxx que origina un compuesto de cuatro átomos de carbono.
De aquí que la ruta se comporte no sólo como una vía oxidativa sino que desempeña también un; función anaplerótica. 

Globalmente, utilizando como ejemplo el glioxilato y asumiendo que acontecen lo mecanismos normales de transporte de electrones, esta ruta rinde, durante la oxidación completa de dos moléculas de glioxilato, 15 ATP (es decir, una relación xxx de xxx mientras que en la oxidación total de una molécula de acetato se generan 12 ATP (e decir, una relación xxx de 6).

El menor rendimiento energético refleja el estado más oxidado del sustrato.

La segunda ruta, más normal, utilizada para la oxidación de sustratos como glicolato y glioxilato (y la glicina derivada del glioxilato por aminación) es la conocida como ciclo del ácido dicarboxílico (DCA) (véase Figure 4.2).
Se ha observado en coliforme pseudomonadáceas y muchos otros grupos de bacterias que utilizan estos sustratos. descubrimiento de la misma derivó de la observación de que, en coliformes, el monofluoracetato inhibía acusadamente la oxidación del glioxilato mientras que tal inhibición no se producía en el ciclo clásico del TCA.
Esta paradoja se resolvió al observar que el malato era el producto inicial de la oxidación del glioxilato y su formación dependía de la inducción, previa al crecimiento sobre glioxilato, del enzima malato sintetasa. Posteriormente se demostró que el monofluoracetato inhibía a este enzima.

Después de su formación, el malato se oxida a oxalacetato el que se ve afectado por dos reacciones descarboxilativas, formándose primero piruvato y luego acetil CoA Como se muestra en la Figura 4.2, el acetil CoA actúa como aceptor inicial de glioxilato y se regenera en la última reacción del ciclo.
Por tanto, en el ciclo del xxx , una molécula de glioxilato se oxida totalmente a dióxido de carbono y se generan dos aceptores de electrones reducidos y una molécula de xxx .
Asumiendo que se dan los procesos normales de transporte de electrones, la oxidación del glioxilato rinde, por tanto, xxx (es decir, una relación xxx ).

Figura 4.1

Oxidación de glicolato y glioxilato en Paracaccus denitrificans 

1 glicolato deshidrogenasa
2 glioxilato: glutamato transaminasa
3 hidroxiaspartato sintetasa
4 oxaloacetato:glutamato transaminasa
5 PEP-carboxiquinasa
6 piruvato quinasa
7 piruvato deshidrogenasa

Figura 4.2

Ciclo del ácido dicarboxílico

1 glicolato deshidrogenasa
2 malato sintetasa
3 malato deshidrogenasa
4 piruvato carboxilasa
5 piruvato deshidrogenasa

Es interesante comparar los ciclos del TCA y DCA. 
El primero da cuenta de la oxidación completa de acetil CoA con la participación y regeneración obligadas de un cetoácido mientras que por el ciclo del DCA se oxida totalmente el glioxilato con la participación y regeneración obligadas de acetil CoA Sin embargo, el papel de ambas rutas es idéntico: la oxidación de sustratos de dos átomos de carbono como parte del proceso de generación de ATP y el suministro de precursores biosintéticos (acetil CoA, piruvato, malato y oxalacetato) Por ello, ambas rutas son anfibólicas y para reponer los precursores biosintéticos se necesita una ruta anaplerótica (véase Sección 4.3).
Debe apuntarse que los microorganismos que utilizan el ciclo del DCA para oxidar sustratos, como el glioxilato, tienen que recurrir también, al menos, a un ciclo parcial del TCA. 
Es así porque necesitan producir oxoglutarato (cinco átomos de carbono) como precursor de glutámico que juega un papel clave y central en el metabolismo de los aminoácidos (véase Figura 2.1 y Sección 2.2).

Se han realizado relativamente poco estos estudios acerca de las rutas oxidativas que se utilizan para generar ATP a partir de compuestos de dos átomos de carbono muy oxidados, como oxalato.
Este compuesto se utiliza extensamente en la industria del automóvil para eliminar la herrumbre de las chapas antes de proceder a la pintura de las mismas; presenta, a veces, problemas residuales debido a que es bastante tóxico. 
En consecuencia, ha despertado gran interés el uso de microorganismos con fines.

Es superfluo decir que un compuesto tan oxidado no es un buen sustrato para el crecimiento; su potencial de generación de ATP es pequeño.
Sin embargo, algunas pseudomonadáceas, como Pseudomonas oxalaticus pueden utilizar oxalato aunque de una forma lenta y poco eficaz.

La ruta para la utilización del oxalato representa un punto intermedio entre las rutas cíclicas normales para la oxidación de compuestos de dos átomos de carbono (ciclos TCA y DCA) y la ruta oxidativa lineal del metabolismo de los compuestos de un átomo de carbono (véase Sección 3.4) La ruta oxidativa del oxalato es lineal y tiene, por tanto más semejanza con la oxidación de compuestos de un átomo de carbono que con la de dos. 
Por otra parte, como se discute en la Sección 4.3, las rutas biosintéticas son más típicas que las del crecimiento a partir de sustratos de un átomo de carbono.

La ruta oxidativa lineal es simple (véase Figura 4.3).
El enzima clave es formato deshidrogenasa que participa también en la oxidación de compuestos de un átomo de carbono en los metilotrofos. 
Cuando el xxx reducido que se forma durante esta reacción se oxida, se generan 3 ATP, resultando una relación xxx de xxx , un rendimiento muy bajo que refleja la naturaleza de un sustrato muy oxidado.

Nótese que en esta ruta, al contrario de lo que ocurre en la oxidación de otros compuestos de dos átomos de carbono, no participan precursores biosintéticos de tres y cuatro átomos de carbono.
Por lo tanto, estrictamente hablando, no se requiere una ruta anaplerótica pero se necesita un mecanismo para generar los precursores de tres y cuatro átomos de carbono.

Rutas biosintéticas y anapleróticas de los microorganismos que crecen a partir de sustratos de dos átomos de carbono

Como se ha indicado anteriormente (véase Sección 4.2), la mayoría de los compuestos de dos átomos de carbono se oxidan vía el ciclo del TCA.
Ello necesita una ruta anaplerótica para reponer los niveles de intermediarios del ciclo de TCA que se requieren como precursores biosintéticos.
Como existen pocos microorganismos que han desarrollado mecanismos para carboxilar los compuestos de dos átomos de carbono a piruvato o fosfoenolpiruvato que posteriormente puedan ser carboxilados a oxalacetato o malato, se requiere otro mecanismo anaplerótico para la conversión de los sustratos de dos átomos de carbono en los precursores de cuatro átomos de carbono necesarios.

Ya se ha indicado que la excepcional ruta de reducción/oxidación de Paracoccus denitrificans cumple este cometido (véase Sección 4.2) pero las rutas oxidativas más normales que se encuentran en otros microorganismos (es decir, los ciclos del TCA y DCA)requieren una ruta anapletórica separada.
La naturaleza de esta ruta refleja, de nuevo, las relaciones xxx del sustrato y la ruta oxidativa. 
Para compuestos con una relación xxx de xxx o más se usa el ciclo del glioxilato para reponer los niveles de intermediarios del ciclo del TCA mientras que para compuestos con una relación más baja de xxx que se oxidan por el ciclo del xxx se utiliza la ruta del glicerato .
En ésta se incluye el muy oxidado ácido oxálico, que como se recordará, se utiliza biosintéticamente como otros compuestos de dos átomos de carbono incluso aunque su oxidación se efectúe como si fuese un compuesto de un átomo de carbono.

Figura 4.3

Ruta oxidativa lineal para la oxidación del ácido oxálico

El ciclo del glioxilato (véase Figure 4.4) utiliza la ruta normal de ingreso en el ciclo del TCA.
Por lo tanto, el acetil CoA se condensa con oxalacetato para formar citrato que se metaboliza posteriormente mediante las reacciones del ciclo del TCA rindiendo isocitrato.
En este punto, se suprimen las dos reacciones sucesivas de descarboxilación oxidativa del ciclo del TCA que conducen a la formación primero de oxoglutarato y luego succinato.

El isocitrato (6C) se degrada por la isocitrato liasa formándose succinato (4C) y glioxilato (2C). 
El glioxilato se condensa después con una segunda molécula de acetil CoA (2C) produciéndose malato (4C) en una reacción catalizada por la xxx .
El malato así formado se oxida para regenerar el oxalacetato original.
La ruta origina, por tanto, una producción neta de un compuesto de cuatro átomos de carbono (succinato), con lo que, de esta forma, se cubre la función anaplerótica: xxx .

La ruta del ácido glicérico (véase Figura 4.5) conduce a la producción de oxalacetato para reponer el intermediario en los ciclos del TCA y DCA pero este objetivo se logra de una forma bastante diferente a como se consigue mediante el ciclo del glioxilato.
En vez de la condensación xxx del ciclo del glioxilato, dos moléculas de glioxilato participan en la reacción que rinde dióxido de carbono y el semialdehído ácido tartrónico (TAS) de tres átomos de carbono.
Esta reacción está catalizada por un enzima especifico de esta ruta, glioxilato carboligasa .
El metabolismo posterior del TAS se efectúa vía su reducción a ácido glicérico por el enzima, también característico de esta ruta, TAS reductasa .
El ácido glicérico se fosforita y metaboliza, siguiendo las reacciones glicolíticas normales, a fosfoenolpiruvato o piruvato que posteriormente pueden sufrir una carboxilación rindiendo oxalacetato.
Que la ruta siga uno u otro camino depende del enzima carboxilante que exista en el microorganismo, la piruvato carboxilasa o la PEP carboquinasa .
La secuencia global de los sucesos puede resumirse así xxx o de otra forma xxx .

Como se ha mencionado anteriormente, el ácido oxálico se asimila también vía esta ruta.
Como Pseudomonas oxalaticus asimila el formato por la vía autotrófica de la ruta RBP después de oxidar los sustratos a formato, se podría esperar que el oxalato se oxidara a formato y fijarse después como dióxido de carbono.
Sin embargo, los experimentos con sustancias mareadas utilizando células cultivadas en xxx y xxx mostraron un modelo muy diferente a aquel.
Los primeros intermediarios mareados detectados a partir de C-oxalato fueron el semialdehído ácido tartrónico y ácido glicérico, lo que indicaba que se utilizaba la ruta del ácido glicérico.
Posteriormente se demostró que el oxalil CoA se reducía para formar glioxilato: xxx que ingresa e la ruta del glicerato. 

Figura 4.4

Ciclo del glioxilato, reacciones comunes a las del ciclo del TCA

1 enzima de condensación del citrato 
2 aconitasa
3 isocitrato liasa
4 malato sintetasa, reacciones especificas del ciclo del TCA
5 malato deshidrogenasa, reacciones específicas del ciclo del glioxilato
6 isocitrato deshidrogenasa
7 oxoglutarato deshidrogenasa
8 succínico deshidrogenasa
9 fumarato fumarato deshidrogenasa

Resumen

El crecimiento microbiano a partir de sustratos de dos átomos de carbono se efectúa mediante una variedad de rutas oxidativas y anapleróticas de acuerdo con el estado inicial de oxidación/reducción del sustrato. 
La mayoría de los compuestos de dos átomos de carbono se metabolizan para rendir acetato que después se oxida por el ciclo del TC, Como esta ruta es anfibólica en la que se suministran precursores biosintéticos que sirven también para la ruta oxidativa terminal, se requiere una ruta anaplerótica para reponer los precursores.
Esta operación se realiza normalmente por el ciclo del glioxilato.
En el caso de sustratos más oxidados, la ruta oxidativa es la del ciclo del DCA y la anaplerótica es la del glicerato.
El muy oxidado ácido oxálico se oxida vía una ruta lineal bastante semejante a la que se utiliza para la oxidación de sustratos de un átomo c carbono pero se asimila por una ruta de compuestos de dos átomos de carbono, la del glicerato.

Figura 4.5

Ruta del glicerato

1 glioxilato carboligasa
2 semialdehído ácido
3 tartrónico reductasa
4 glicerato quinasa
5 enolasa
6 piruvato quinasa
7 piruvato carboxilasa
8 fosfoenolpiruvato carboquinasa

Se han realizado un número mayor de investigaciones sobre los mecanismos de control implicados en el crecimiento de microorganismos que utilizan compuestos de dos átomos de carbono que en los que usan compuestos de un átomo de carbono.
En general, estos estudios indican que los enzimas requeridos para convertir un determinado sustrato en acetil CoA o glioxilato para su ingreso en los ciclos del TCA o del DC son inducidos por la presencia del sustrato como única fuente de carbono, como lo demuestra los enzimas específicos del ciclo anaplerótico del glioxilato y de la ruta d glicerato.

Se ha prestado un especial interés al papel de los enzimas isocitrato liasa y malato sintetasa en el metabolismo de los compuestos de uno y dos átomos de carbono.
Al crecer sobre la mayoría de sustratos de dos átomos de carbono, el enzima es esencial en el ciclo del glioxilato. 
Es interesante resaltar que el papel del enzima, incluso aunque catalice una reacción idéntica en cada caso, es bastante diferente.
En la ruta de la L-serina, forma parte de una ruta biosintética y es esencial para el mecanismo de regeneración de una unidad de aceptor (glicina) de un derivado del THF de un átomo de carbono.
Por otra parte, en el ciclo del glioxilato tiene un papel anaplerótico esencial.

El enzima malato sintetasa desempeña, en el crecimiento a partir de compuestos de dos átomos de carbono, dos o más funciones que contrastan notablemente.
En el crecimiento a partir de glioxilato, actúa en la ruta oxidativa del ciclo del DCA mientras que en el crecimiento a partir de acetato, o compuestos que se transforman en acetato, el enzima tiene una función anaplerótica en el ciclo del glioxilato.
La doble función de enzimas de este tipo plantea una interrogante acerca de cómo se controla su inducción.
Esta pregunta no se ha contestado todavía satisfactoriamente pero está claro, que al menos en el caso de la malato sintetasa, los dos enzimas implicados son productos resultantes de genes diferentes; uno de ellos se induce por sustratos oxidados vía el ciclo del DCA mientras que el otro se induce por sustratos que se oxidaron por el ciclo del TCA y asimilados por el ciclo del glioxilato.

El control de las rutas metabólicas de Pseudomonas oxalaticus es también de considerable interés, sobre todo la cuestión de por qué y cómo se induce la ribulosa xxx difosfato carboxilasa en el crecimiento a partir de formato, el que se oxida vía formato deshidrogenasa a dióxido de carbono y se fija vía la carboxilasa, mientras que el oxalato, que se oxida también vía formato deshidrogenasa a dióxido de carbono ocasiona la represión de la ribulosa xxx difosfato carboxilasa e induce la síntesis de glicólico deshidrogenasa, glioxilato carboligasa y TAS reductasa.
Lo que está claro es que la doble función de enzimas en el metabolismo de compuestos de uno y dos átomos de carbono y, en el caso del oxalato, las fascinantes alternativas de la ruta metabólica permiten deducir la versatilidad de los microorganismos para crecer a partir de varios sustratos. 

Crecimiento microbiano aeróbico sobre algunos sustratos de más de dos átomos de carbono

Objetivo y fundamento

Ya se ha indicado en el Capitulo 2, Secciones 2.1 y 2.3 que todos los compuestos con tres o más átomos de carbono se metabolizan por rutas que se encuentran en la mayoría de todos los organismos vivos, por lo que dichas rutas no se van a considerar aquí.
Esencialmente, tales sustratos se convierten, habitualmente por vías bastante directas, en intermediarios de la glicolisis antes de su oxidación vía el ciclo del TCA.
La anaplerosis se efectúa mediante la carboxilación de compuestos de tres átomos de carbono.
Sin embargo, existen dos tipos de compuestos que se utilizan como sustrato del crecimiento microbiano que tienen un interés biotecnológico especial.
Son los hidrocarburos distintos al metano y compuestos aromáticos. 
En el presente Capitulo se van a considerar algunos aspectos específicos de su utilización.

Oxidación y asimilación de hidrocarburos alifáticos

Se sabe desde hace tiempo que los microorganismos tienen la facultad de degradar hidrocarburos y ya en 1950 se hizo un primera revisión Zobell (1950) sobre el tema.
Los estudios iniciales se abandonaron debido a la imposibilidad de disponer de sustratos puros.
Los estudios realizados con esos sustratos impuros dieron lugar a unos modelos de crecimiento y respiración que se demostró se debían a que los microorganismos oxidaban, y presumiblemente utilizaban, alguna impureza y seguía una disminución del crecimiento y un gasto de oxigeno mientras se inducía un nuevo grupo de enzimas para utilizar una segunda impureza, y así sucesivamente hasta que se hablan utilizado todas las impurezas.
Después era cuando se utilizaba el hidrocarburo como sustrato pero en ese momento, en cultivos controlados, la presencia de otros nutrientes ya era limitada.

Actualmente, existe un gran interés en la explotación de los microorganismos que degradan los hidrocarburos debido a diversas razones.
Primera, esos microorganismos pueden emplearse para la producción de biomasa utilizando como sustratos subproductos de la industria petrolífera u otros productos residuales que contienen hidrocarburos.
Segunda, están siendo estudiados exhaustivamente para utilizarlos como una forma de solucionar los problemas de contaminación que ocasionan los petróleos, en especial en la dispersión de derramamientos de estos productos.
Tercera, se está investigando el potencial de dichos microorganismos para liberar aceites pesados de sustratos que los contengan dado que mediante técnicas convencionales es generalmente imposible recuperar más del 70 o del petróleo de un yacimiento petrolífero. 
A la vista de estas aplicaciones biotecnológicas, es interesante estudiar el grupo de microorganismos capaces de utilizar los hidrocarburos e, igualmente, como realizan esta función.
Al margen de estas características potencialmente útiles de los microorganismos que degradan los hidrocarburos pueden, por otra parte, ocasionar problemas debido a su capacidad de crecer en las interfases aceite/agua y en la emulsiones aceite/agua, causando problemas de biodeterioro en los sistemas mecánicos a consecuencia de la degradación del aceite.

Muchas bacterias, levaduras y mohos filamentosos pueden oxidar y asimilar los hidrocarburos alifáticos.
Cabe citar entre ellos a especies de los géneros Corynebacterium Pseudomonas y Candida .
En general, los hidrocarburos de cadena corta, aunque son oxidados, no se utilizan como único sustrato para el crecimiento pero los de cadena larga, de ocho carbonos (por ejemplo n-octano) o más, si que proporcionan un buen crecimiento. 

Se puede decir, en términos generales, que los hidrocarburos saturados se utilizan más fácilmente que los insaturados; que se prefieren los hidrocarburos de ceden lineal a los de cadena ramificada y los de cadena par a los que poseen un número impar de átomos de carbono.

La ruta oxidativa (véase Figura 5.1) sigue el modelo de la del crecimiento sobre metano: xxx .
El ácido graso resultante se metaboliza después, habitualmente por la ruta de la B-oxidación , rindiendo acetil CoA que se oxida finalmente vía el ciclo del TCA. 
El sistema mono-oxigenasa responsable de la oxidación del hidrocarburo al alcohol correspondiente y los del alcohol y la aldehído deshidrogenasas son similares a los que se utilizan c la conversión de metano en metanol, formaldehído y formato.
Se han descrito dos mecanismos diferentes del transporte de electrones asociado para la mono-oxigenasa; uno dependiente del citocromo P450 en Corynebacterium sp y el otro, detectado en Candida sp que depende de rubredoxina.
Igualmente, se ha demostrado que las deshidrogena están ligadas al NAD.

En algunas ocasiones puede presentarse una oxidación diterminal (véase Figura. 5).
Conlleva la formación de ácidos grasos dicarboxílicos que después se oxidan a acetil CoA mediante la vía x-w oxidación en la que se forma, como producto final adición. ácido oxálico. 
No está aun aclarado cual es el destino del ácido oxálico.
Algunas especies pueden realizar una oxidación subterminal en la que el sitio de ataque de la mono-oxigenasa es un carbono distinto a los átomos de carbono finales. 
El producto final también el acetil CoA que se oxida vía el ciclo del TCA.
Se sabe bastante menos de como acontece la oxidación de los alquenos (hidrocarburos insaturados) e hidrocarburos de cadena ramificada pero sí está claro que son reacciones catalizadas por monooxigenasas. 
Como se ha indicado, el producto final de la ruta oxidativa de los hidrocarburos es el acetil CoA que se oxida posteriormente vía el ciclo del TCA, por lo que se producirá mucho ATP, no debido a la B o a-w oxidación sino a los nucleótidos reducidos que se generan.

Figura 5-1

Rutas oxidativas para la degradación de hidrocarburos de cadena larga ejemplarizadas con el n-decano. 

A la izquierda se muestra la oxidación monoterminal y a la derecha la diterminal.
Cuando se trató de la ruta de la producción de precursores para la biosíntesis de tres y cuatro átomos de carbono, se podía considerar que los microorganismos afectados crecían a partir de acetato dado que este compuesto era el producto final del paso inicial de la oxidación.
No puede sorprender, por tanto, que los microorganismos que utilizan hidrocarburos alifáticos contengan isocitrato liasa y malato sintetasa .
En consecuencia, utilizan el ciclo del glioxilato como ruta anaplerótica.

Un aspecto fascinante del crecimiento microbiano a partir de hidrocarburos alifáticos es el ingreso real de estas sustancias insolubles.
Las experiencias indican que los microorganismos segregan agentes de superficie específicos que emulsifican los hidrocarburos en una forma que ayuda a atravesar la membrana celular.
Se sabe también que cuando los microorganismos crecen sobre hidrocarburos tienen un contenido mucho mayor de lípidos en sus membranas.
Puede que las emulsiones de hidrocarburos al entrar en contacto con membranas con un elevado contenido de lípidos sea suficiente para que se produzca el ingreso del sustrato.
No obstante, es posible también que mecanismos de transporte especiales estén aún por descubrir.

Oxidación y asimilación de sustancias aromáticas

La capacidad de los microorganismos de degradar las sustancias aromáticas se ha estudiado en una gran variedad de microorganismos utilizando una enorme cantidad de compuestos.
El interés biotecnológico de estas degradaciones es de gran importancia desde el punto de vista del uso de enzimas y células inmovilizados en bioconversiones específicas, sobre todo para catalizar reacciones que por medios químicos son difíciles de realizar o proporcionan rendimientos bajos (por ejemplo, la síntesis de medicamentos u otros compuestos deseables). 
Una segunda área potencial es la biodegradación, y de aquí, la posibilidad de eliminar afluentes potencialmente tóxicos que contienen sustancias aromáticas.
De forma similar, se podían degradar pesticidas, con lo que se lograría una detoxificación por medios basados en el conocimiento de la degradación microbiana de sustancias aromáticas.

La variedad de sustancias aromáticas utilizadas por uno u otro microorganismo es extensa pero sólo se pretende realizar una breve descripción de los principios más importantes.
Se remite al lector a la revisión de Dagley (1978), donde puede encontrarse un estudio más detallado.
El metabolismo de diversos compuestos aromáticos se efectúa mediante una amplia variedad de mecanismos que implican inicialmente reacciones de oxigenación e hidroxilación.
Como cabría esperar, la oxigenación se efectúa, con frecuencia, mediante reacciones con actividad mono-oxigenasa.
Es imposible aquí proporcionar detalles de las mismas dada las grandes variaciones que existen. 
Sin embargo, sí conviene apuntar que la reacción clave de estas degradaciones es la apertura del anillo bencénico para lo que existen sólo dos rutas principales.
La mayoría de las sustancias aromáticas que pueden oxidarse totalmente se convierten inicialmente en catecol o sustancias afines a través de una meta u orto fusión.
El metabolismo posterior de los productos resultantes de estas reacciones conduce a la generación de intermediarios del ciclo del TCA, habitualmente succinil CoA y acetil CoA.
El último se oxida vía el ciclo del TCA para generar ATP mientras que el primero puede actuar como una sustancia anaplerótica del ciclo.
Por lo tanto, la ruta implicada en la utilización de sustratos aromáticos rinde productos finales que satisfacen los dos requisitos del crecimiento.

Resumen

Solamente se ha ofrecido una descripción breve y general del metabolismo aeróbico de compuestos que contienen tres o más átomos de carbono.
Se ha hecho así debido a la gran variedad de compuestos de este tipo existentes y debido también al hecho de que tras una o dos reacciones iniciales la gran mayoría de los mismos se oxidan vía la glicolisis y el ciclo del TCA (véase Fig. 2.1).
Esta ruta tiene, como intermediarios, compuestos de tres átomos de carbono que pueden carboxilarse para dar intermediarios de cuatro átomos de carbono del ciclo del TCA y satisfacer así la provisión de los precursores biosintéticos necesarios para el crecimiento.
Por lo tanto, cuanto se ha descrito en primer lugar como rutas «normales» del metabolismo que representa el enclave que unifica la bioquímica puede operar en el caso del metabolismo de compuestos de tres átomos de carbono o más.
Incluso los hidrocarburos y las sustancias aromáticas pueden metabolizarse de acuerdo con los mismos principios, es decir, convirtiéndose en intermediarios de un pequeño número de rutas que ingresan en las rutas metabólicas centrales.


ESTRUCTURA Y FUNCIÓN DE LOS CROMOSOMAS Y LOS GENES

En los años 80 se realizaron progresos sobresalientes en cuanto al conocimiento de la estructura y función de cromosomas y genes a nivel molecular.
Esto se debió principalmente al descubrimiento y la amplia aplicación de la tecnología del DNA (ácido desoxirribonucleico) recombinante, que ha proporcionado las herramientas para un nuevo enfoque de la genética médica.
Estas herramientas han demostrado su utilidad en muchas situaciones clínicas. 
En este capítulo se presenta una revisión de los aspectos de la genética molecular que se requieren para la comprensión del enfoque genético de la medicina.
Este capítulo no intenta proporcionar una descripción extensa de la oleada de información nueva con respecto a la estructura y regulación génicas. 
Para ampliar la información que se presenta en este capítulo, el capítulo 5 describe muchos procedimientos experimentales de la moderna genética molecular que se están convirtiendo en cruciales para la práctica y entendimiento de la genética médica y humana. 

ORGANIZACIÓN DEL GENOMA HUMANO 

El genoma humano en su forma diploide se compone aproximadamente de 6 a 7.000 millones de pares de bases (6 a 7 millones de pares de kilobases [kb]) de DNA organizado de manera lineal en 23 pares de cromosomas.
El genoma contiene, según cálculos actuales, de 50.000 a 100.000 genes (codificando, por lo tanto, un número igual de proteínas) que controlan todos los aspectos de la embriogénesis, el desarrollo, el crecimiento, la reproducción y el metabolismo -esencialmente, todo lo relacionado con aquello que hace al ser humano un organismo funcional-. 
Así, la influencia de los genes y la genética en los estados de salud y enfermedad es amplia, y sus raíces son la información codificada en el DNA que se halla en el genoma humano.
Se entiende el papel de sólo un pequeño porcentaje del número total de genes. 
No obstante, el armazón molecular y estructural general del genoma, de sus cromosomas y de sus genes es cada vez más evidente.
El análisis de la organización del genoma humano constituye un área de considerable interés en la genética médica y humana actual: se anticipa que mucha información genética, si no toda, del genoma se identificará en un próximo futuro y se examinará a nivel molecular como parte de lo que se ha llamado el Proyecto Genoma Humano, un esfuerzo internacional para mapear y secuenciar todo el genoma humano.

La caracterización y entendimiento de los genes y su organización en el genoma tienen un enorme impacto en la comprensión de los procesos fisiológicos del organismo humano tanto de la salud como de la enfermedad y consecuentemente, en la práctica de la medicina en general. 
Como señaló Paul Berg, ganador del Premio Nobel: Así como nuestro conocimiento y práctica actuales de la medicina recaen en un conocimiento refinado de la anatomía, fisiología y bioquímica humanas. de la misma forma tratar la enfermedad en el futuro exigirá una comprensión detallada de la anatomía, fisiología y bioquímica moleculares del genoma humano...
Requeriremos un conocimiento más detallado de la manera en que los genes humanos se organizan y del modo en que funcionan y se regulan. 
También se necesitarán médicos versados en la anatomía y fisiología molecular de cromosomas y genes, de la misma manera que un cirujano cardíaco está informado de la estructura y funcionamiento del corazón (Berg. 1981. pág. 302).

Estructura del DNA: una breve revisión 

El DNA es una macromolécula polimérica de ácido nucleico que se compone de tres tipos de unidades: un azúcar de cinco carbonos (la desoxirribosa), una base que contiene nitrógeno y un grupo fosfato (figura 3-1). 
Las bases son de dos tipos: purinas y pirimidinas.
En el DNA existen dos bases purínicas: adenina (A) y guanina (G), y dos pirimidínicas: timina (T) y citosina (C).
Los nucleótidos, cada uno compuesto de una base, un grupo fosfato y un azúcar, se polimerizan en largas cadenas de polinucleótidos mediante uniones 5'-3' fosfodiéster que se forman entre las unidades adyacentes de desoxirribosa (figura 3-2).
En el caso de cromosomas humanos intactos, estas cadenas de polinucleótidos (en su forma de doble hélice) pueden extenderse cientos de millones de nucleótidos.

La estructura anatómica del DNA transporta la información química que permite la transmisión exacta de la información genética de una célula a sus células hijas y de una generación a la siguiente. 
Al mismo tiempo, la estructura primaria del DNA especifica la secuencia de aminoácidos de las cadenas de polipéptidos de las proteínas como se describirá después en este mismo capítulo.
El DNA posee características especiales que le brindan estas propiedades.
El estado natural del DNA, como lo descubrieron James Watson y Francis Crick en 1953, es una doble hélice (figura 3-3).
La estructura helicoidal semeja una escalera de caracol dextrógira, en la que las dos cadenas de polinucleótidos avanzan en direcciones opuestas, mantenidas juntas por puentes de hidrógeno entre pares de bases: A de una cadena emparejada con T de la otra, y G con C (figura 3-3).
Por lo tanto, el conocimiento de la secuencia de las bases de nucleótidos de una cadena permite de manera automática determinar la secuencia de las bases de la otra.
Así, una molécula de DNA de doble cadena puede replicarse de modo preciso por la separación de las dos cadenas y la síntesis de las dos nuevas cadenas complementarias, en concordancia con la secuencia de la cadena modelo original (figura 3-4).
De modo similar, cuando es necesario el carácter complementario de las bases permite la reparación eficaz y correcta de las moléculas dañadas de DNA.

Figura 3.1

Cuatro bases del DNA y estructura general de un nucleótido en el DNA

Cada una de las cuatro bases se enlaza con desoxirribosa y un grupo fosfato para formar los nucleótidos correspondientes. 

Figura 3.2

Porción de una cadena polinucleótida de DNA, que muestra los enlaces de fosfodiéster 3'-5' que unen nucleótidos adyacentes

Figura 3.3

Estructura del DNA 

A la izquierda aparece una representación bidimensional de las dos cadenas complementarias de DNA, donde se aprecian los pares de bases AT y GC.
Nótese que la orientación de las dos cadenas es antiparalela.
A la derecha se halla el modelo de doble hélice de DNA, tal como lo propusieron Watson y Crick .
Las "barras" horizontales representan las bases pareadas. 
Se dice que la hélice es dextrógira, porque la cadena que va del extremo inferior izquierdo al extremo superior derecho cruza sobre la cadena opuesta.

Estructura de los cromosomas humanos

Se cree que cada cromosoma humano consta de una única doble hélice de DNA continua: esto es, cada cromosoma es una molécula de DNA larga, lineal y de doble cadena. (La única excepción a esto lo constituye el pequeño cromosoma circular mitocondrial, que se trata más adelante en esta sección).
En el genoma humano, el tamaño de estas moléculas lineales va de aproximadamente 50 millones de pares de bases (para el cromosoma más pequeño, el número 21) a 250 millones de pares de bases (para el cromosoma mayor, el número 1).
Sin embargo, los cromosomas no son únicamente dobles hélices de DNA.
La molécula de DNA de un cromosoma coexiste con una serie de proteínas cromosómicas básicas llamadas histonas y con un grupo heterogéneo de proteínas ácidas no histónicas cuya caracterización no está bien definida.
Este complejo de DNA y proteína se denomina cromatina .
Existen cinco tipos principales de histonas, que se conocen como H1, H2A, H2B, H3 y H4.
Éstas desempeñan un papel fundamental en el adecuado empaquetamiento de la fibra de cromatina, como lo evidencia la notable conservación de las secuencias de aminoácidos de H2A, H2B, H3 y H4 a través de la evolución, incluso entre especies tan distintas como pollos v seres humanos.

Dos copias de cada una de estas cuatro histonas constituyen un octámero, alrededor del cual se enrolla un segmento de DNA de doble hélice como hilo en un carrete (figura 3-5). 
Aproximadamente, 140 pares de bases de DNA se asocian con cada núcleo de histonas, lo cual hace que den casi dos vueltas alrededor del octámero.
Después de un corto segmento «espaciador» de DNA (de 20 a 60 pares de bases), se toma el siguiente complejo nuclear de DNA y así sucesivamente, lo cual brinda a la cromatina la apariencia de cuentas en un collar.
Cada complejo de DNA con histonas nucleares se denomina nucleosoma , la unidad básica estructural de la cromatina.
La histona H1, cuya secuencia de aminoácidos varía más entre especies que las de las histonas nucleares, parece que se enlaza al DNA en el extremo de cada nucleosoma, en la región espaciadora internucleosómica.
La cantidad de DNA que se asocia con cada unidad de cromatina, incluidos el núcleo del nucleosoma y la región espaciadora, es de alrededor de 200 pares de bases.

Durante el ciclo celular, como se describió brevemente en el capítulo 2, los cromosomas atraviesan de manera ordenada las etapas de condensación y descondensación (figura 3-6).
En el núcleo en interfase, los cromosomas y la cromatina se encuentran bastante descondesados en relación con el estado altamente condensado de la cromatina en la metafase. 
Sin embargo, incluso en los cromosomas en interfase, el DNA en la cromatina se halla sustancialmente más condensado de lo que estaría como una doble hélice simple libre de proteínas.
La mayor parte del DNA, si no todo, en el núcleo interactúa con las histonas para formar nucleosomas.
En esta etapa, el DNA se condensa en cerca de una décima parte de su extensión natural. 
Así, en términos más concretos, el DNA del cromosoma 1 humano, que mediría alrededor de 15 cM de longitud como doble hélice simple, mide sólo alrededor de 1.5 cM cuando se asocia con proteínas cromosómicas. 

Las largas cadenas de nucleosomas se compactan más por sí solas en una estructura cromática secundaria helicoidal denominada solenoide (figura 3-5), que aparece en el microscopio electrónico como una gruesa fibra de 30 nm de diámetro (alrededor de tres veces más gruesa que la cadena nucleosómica).
Esta fibra parece ser la unidad fundamental de organización de la cromatina.
Cada giro del solenoide contiene cerca de seis nucleosomas.
Así, en este nivel de compactación en un núcleo en interfase, la cromatina que conforma el cromosoma 1 mide cerca de 0.3 cM de longitud.
Los solenoides se empaquetan por sí solos en asas o parcelas unidos en intervalos de 10 a 100 kb a un armazón o matriz de proteína no histónica. 
Se ha especulado que, de hecho, las asas constituyen unidades funciónales de replicación de DNA o de transcripción génica, o de ambas, y los puntos de anclaje de cada asa son fijos a lo largo del DNA cromosómico.
Por lo tanto, un nivel de control de la expresión génica quizá dependa de la manera en que el DNA y los genes se empaquetan en los cromosomas y de la forma en que se asocian con proteínas cromosómicas en dicho proceso de empaquetamiento.
A diferencia de los cromosomas que se observan en preparaciones teñidas bajo el microscopio o en fotografías, los cromosomas de las células vivas aparecen como estructuras fluidas y dinámicas.
Los diversos niveles jerárquicos de empaquetamiento que se observan en un cromosoma en interfase se ilustran de modo esquemático en la figura 3-5.

Las asas pueden constituir el inicio de los engrosamientos semejantes a nudos (llamados cromómeros ) que se observan al microscopio en cromosomas de la profase temprana, cuando comienza la mitosis (figura 3-6).
Cuando los cromosomas se condensan más, los cromómeros adyacentes se fusionan en otros mayores.
Estos grupos de cromómeros finalmente se convierten en las bandas G oscuras teñidas de los cromosomas en profase o metafase. 
En la profase, los cromosomas se aprecian fácilmente con el microscopio.
Como se señaló en el capítulo 2, en esta etapa pueden reconocerse 1.000 o más bandas en preparaciones de cromosomas teñidos (bandeo de alta resolución), y cada banda contiene varios millones de pares de bases de DNA (y, como se describirá más adelante, quizá de 50 a 100 genes).
En la profase, el cromosoma 1 se ha condensado hasta tener una longitud total de cerca de 50 xxx (condensado hasta alrededor de 1/3.000 de la longitud de una doble hélice simple).
Cuando se ha condensado al máximo, en la metafase, el DNA en los cromosomas mide alrededor de 1/10.000 de su longitud natural. 
Por lo tanto, cada banda que se reconoce citogenéticamente en preparaciones de metafase contiene, por término medio, cerca de 10 a 20 millones de pares de bases de DNA.
Después de la metafase, en tanto que las células completan la mitosis o la meiosis, los cromosomas abandonan su estado de condensación y vuelven a su estado extendido como cromatina en el núcleo en interfase, listos para comenzar el ciclo de nuevo (figura 3-6). 

El comportamiento preciso de los cromosomas individuales durante el ciclo celular está determinado por cierto número de elementos funciónales que resultan imprescindibles para la correcta expresión, duplicación y segregación de los cromosomas.
Entre dichos elementos, los más caracterizados son los orígenes de replicación de DNA, telómeros y centrómeros .
De estos dos últimos se habló en el capítulo 2.
Antes se explicó brevemente la replicación de una doble hélice de DNA (figura 3-4).
La considerable longitud de un cromosoma humano (como todos los cromosomas eucarióticos) requiere que la replicación comience en varios centenares de lugares para completar la síntesis de la molécula entera dentro de la fase S del ciclo celular.
Los puntos de inicio de la síntesis de DNA, llamados orígenes , pueden ser secuencias específicas de DNA que se ubican en intervalos de varios centenares de kilobases a lo largo de la extensión del cromosoma.

La enorme cantidad de DNA empaquetado en un cromosoma en metafase puede apreciarse cuando los cromosomas se tratan con objeto de desprender la mayor parte de las proteínas de la cromatina para observar el armazón proteico (figura 3-7).
Cuando el DNA se libera de los cromosomas tratados de dicha manera, pueden observarse largas asas de DNA y que el armazón residual reproduce el contorno de un cromosoma típico en metafase.

Figura 3.4

Replicación de una doble hélice de DNA, que genera dos moléculas hijas idénticas, cada una compuesta de una cadena original (en negro) y otra recién sintetizada (en rojo)

Figura 3.5

Niveles jerárquicos de empaquetamiento de cromatina en un cromosoma humano

Figura 3.6

Ciclo de condensación y descondensación de un cromosoma durante el ciclo celular 

Cromosoma mitocondrial

Si bien la mayor parte de los genes se localizan en el núcleo, un grupo pequeño pero importante reside en el citoplasma, en la mitocondria.
Los genes mitocondriales se transmiten exclusivamente a través de la madre (cap. 4).
Todas las células humanas poseen centenares de mitocondrias, y cada una contiene cierto número de copias de una pequeña molécula circular, el cromosoma mitocondrial.
La molécula de DNA mitocondrial tiene un tamaño de sólo 16 kb de longitud (menos del 0,03% de la extensión del más pequeño cromosoma nuclear) y codifica 13 genes estructurales clave, así como otros varios genes estructurales de ácido ribonucleico (RNA).
Se han demostrado mutaciones en genes mitocondriales en varios trastornos neuromusculares, incluida la neuropatía óptica hereditaria de Leber, que se trasmite por vía materna (cap. 4).

Tabla 3.1

El DNA en el genoma humano

Clases de DNA en el genoma humano

La organización del DNA en el genoma humano es mucho más compleja de lo que se creyó aun en fecha tan reciente como 1980.
De hecho, menos del 10% del DNA en el genoma codifica genes.
Sólo alrededor de tres cuartas partes de la longitud lineal total del genoma está formado por el denominado DNA de copia simple o única , es decir, DNA cuya secuencia de nucleótidos está representada sólo una vez (o como mucho unas pocas veces) por genoma haploide.
El resto del genoma consiste en varias clases de DNA repetitivo e incluye DNA cuya secuencia de nucleótidos se repite, idénticamente o con alguna variación, de centenares a millones de veces en el genoma.
Mientras que la mayor parte (pero no todos) de los 50.000 a 100.000 genes del genoma están representados en DNA de copia simple, la fracción de DNA repetitivo se cree que participa en el mantenimiento de la estructura cromosómica o bien quizá no posea ningún papel esencial (tabla 3-1).

LOS GENES EN LAS POBLACIONES

La genética es una especialidad singular entre las diversas disciplinas de la medicina porque centra su atención en una familia, en lugar de hacerlo en un paciente individual. 
La genética médica se ocupa no sólo de la realización de un diagnóstico correcto en un caso particular, sino de la determinación de genotipos de otros miembros de la familia y del cálculo de los riesgos de recurrencia para los padres de un sujeto afectado y para sus hermanos, así como para familiares más lejanos. 
Además, ya que en dichos riesgos generalmente no sólo influyen los genotipos de los familiares directos, sino también los de las personas de la población general que entran en la familia por matrimonio, el consejo genético debe tener en cuenta la probabilidad de que se presenten genotipos específicos en poblaciones diferentes.
Así, tanto para realizar un diagnóstico clínico genéticamente correcto como para determinar los riesgos de recurrencia como parte de un consejo genético eficaz, con frecuencia importa, por ejemplo, si una familia tiene sus orígenes en las islas Británicas o en el Mediterráneo, o bien en Finlandia, así como si existen antecedentes familiares de consanguinidad o si la familia es de raza blanca, negra o asiática.
En genética, más que en cualquier otra especialidad médica, el paciente es un reflejo de la población a la que pertenece.

La genética poblacional es el estudio de la distribución de los genes en las poblaciones y de la manera en que las frecuencias de genes y genotipos se mantienen o cambian.
El tema de este capítulo es que el conocimiento de los genes de diferentes enfermedades que son frecuentes en poblaciones distintas y su efecto o falta de efecto sobre la ventaja selectiva reproductiva puede resultar muy valioso en el diagnóstico clínico y el consejo genético.
La genética poblacional tiene mucho en común con la epidemiología, el estudio de las interrelaciones de varios factores genéticos y ambientales que determinan la frecuencia y distribución de trastornos en comunidades humanas.
Las dos áreas se fusionan en el campo de la epidemiología genética, que se ocupa principalmente de enfermedades que poseen complejos patrones de herencia o son causados por una combinación de factores hereditarios y ambientales.
Como se describe con más detalle en el capítulo 15, este enfoque ha mejorado de hecho la comprensión de la genética de muchas enfermedades, en especial de los trastornos habituales de la edad adulta.

En este capítulo se describe el principio básico de la genética poblacional, el equilibrio de Hardy-Weinberg. 
Consideramos las suposiciones de dicho principio y los factores que pueden provocar desviación real o aparente del equilibrio en poblaciones reales en comparación con poblaciones ideales.
Asimismo, se considera la manera para determinar las frecuencias y las tasas de mutación de ciertos genes de importancia clínica, tanto autosómicos como ligados al X.

POBLACIONES HUMANAS

La especie humana, con cerca de 5.000 millones de miembros, se divide en muchas subpoblaciones distinguibles, las más grandes de las cuales se denominan comúnmente razas .
Éstas se definen como grupos de poblaciones principales cuyos acervos genéticos difieren entre sí. 
Existen tres divisiones raciales principales: caucásicos, negros y asiáticos, cada una de las cuales tiene numerosos subgrupos genéticamente diferentes.
Aunque los cromosomas humanos y los loci que contienen son idénticos en todos los miembros de la especie, las frecuencias alélicas en muchos loci varían ampliamente entre grupos poblacionales. 
Como se señaló en el capítulo anterior. 
Algunas variantes están casi restringidas a miembros de un solo grupo, si bien no necesariamente se hallan en todos los miembros de dicho grupo.
Más a menudo, sin embargo, la situación más frecuente es que distintos alelos tengan frecuencias diferentes en poblaciones distintas.
Dentro de cada población existe mucha variación, siendo ésta de mayor magnitud, por término medio, que las diferencias medias entre grupos.

La mutación es la base de las diferencias genéticas entre las razas y sus subpoblaciones.
La selección de mutaciones favorables como respuesta a condiciones ambientales o la probabilidad de supervivencia de mutaciones neutrales específicas o incluso nocivas, junto con un grado de aislamiento reproductivo entre los grupos, permiten que se establezcan las diferencias genéticas entre grupos poblacionales.
En el periodo glacial más reciente, hace 10.000 años aproximadamente, la masa terrestre euroasiática, en la que se habían desarrollado asentamientos humanos pequeños y esparcidos. se dividió en tres regiones distintas separadas por áreas glaciales montañosas que constituyeron barreras físicas para la comunicación y el apareamiento.
Se cree que en estas regiones, geográfica, y por tanto, genéticamente aisladas unas de otras, se desarrollaron los tres grupos raciales principales.
Después cada grupo se subdividió cada vez más en numerosas subpoblaciones distintas, a menudo denominadas grupos étnicos , con sus propios conjuntos característicos de frecuencias génicas.
Muchas veces hay diferencias marcadas en las frecuencias alélicas entre grupos poblacionales, tanto para alelos que generan enfermedad genética como para marcadores genéticos neutrales presumiblemente selectivos, como grupos sanguíneos, polimorfismos proteicos y algunos de DNA (tablas 7-1 y 7-2).
Aunque los primeros son muy importantes para determinar los riesgos de recurrencia de enfermedades genéticas en grupos poblacionales específicos, los últimos resultan importantes como marcadores de la evolución humana reciente.

Tabla 7.1

Ejemplos seleccionados de alelos de enfermedades con frecuencias diferentes en poblaciones distintas 

Alelo xxx del gen de la xxx -globina (anemia drepanocítica).
Alta en África, menos común en cualquier otra parte.


Alelo xxx del gen de la xxx - globina.
Alta en África occidental específicamente.


Fibrosis quística.
Alta en poblaciones de raza blanca europeas y de Estados Unidos; baja en poblaciones asiáticas y africanas; baja en Finlandia.


Enfermedad de Tay- Sachs.
Frecuencia alta de varios alelos entre los judíos ashkenazi y reducida en la mayor parte de las poblaciones no ashkenazi.


Coroideremia .
Común en Finlandia; muy rara en cualquier otra parte.


Distrofia miotónica .
Alrededor de 1 por 10.000 20.000 en la mayor parte de las poblaciones, pero ,1 por 1.000 en ciertas partes de Quebec .


Tabla 7.2

Ejemplos de loci polimórficos con diferentes frecuencias alélicas en poblaciones distintas

Sistema del grupo sanguíneo ABO.
Variación amplia; p. ej., alelo B común en asiáticos, pero ausente en las poblaciones aborígenes americanas.


Sistemas de otros grupos sanguíneos.
Amplia variación de la frecuencia de alelos frecuentes; algunos alelos raros muestran distribución restringida (el alelo R xxx del Rh se encontró sólo en África .


xxx -antitripsina.
Las frecuencias de los tres principales alelos M varían entre poblaciones (p. ej., M1 de 0,51 a 0,98; M2 de 0,00 a 0,20) .


Alcohol-deshidrogenasa.
Tres loci: ADH1 , ADH2, ADH3; la variante ADH2 es mucho más común en japoneses (90 %) que en europeos (15 %).


Aldehído-deshidrogenasa.
Deficiencia de ALDH1 en el 50 % de los asiáticos y en xxx 5% de los indios norteamericanos.


Sistema HLA.
Numerosos alelos en cada sublocus, variación amplia de la frecuencia.


Actividad lactasa.
Dos alelos principales para actividad alta y baja; Actividad baja después de la primera infancia común en africanos y asiáticos (frecuencia alélica. 0,8 a 0,95)y menos frecuente en individuos de raza blanca del norte de Europa y Estados Unidos (frecuencia alélica, 0.17 a 0,48).


Niveles de apolipoproteína A-I.
La concentración plasmática varía de xxx y se correlaciona con riesgo de enfermedad cardíaca coronaria; el alelo para un nivel bajo de apoA-I es común entre los chinos, pero menos frecuente en otras poblaciones.


FENOTIPOS, GENOTIPOS Y FRECUENCIAS GÉNICAS

Si bien se requiere conocer el genotipo de una persona para determinar con precisión los riesgos de recurrencia, en la mayoría de las ocasiones sólo el fenotipo puede observarse y medirse de manera directa.
Así se requiere utilizar las cifras de frecuencia de una enfermedad hereditaria u otro rasgo genético para calcular las frecuencias alélicas individuales.
Para realizar esto se requieren conocimientos sobre la presencia de los genes en las poblaciones y su transmisión de generación en generación. 

Los fenotipos monogénicos, como los que se mencionaron en el capítulo 4, se segregan claramente en las familias y, por término medio, tienden a ocurrir en proporciones fijas y predecibles.
Un solo ejemplo de un rasgo hereditario autosómico común gobernado por un solo par de alelos puede utilizarse para ilustrar los principios básicos que rigen la herencia de los genes en las poblaciones.
Considérese el sistema de grupo sanguíneo MN (las glucoforinas), en el cual un par de alelos expresados en forma codominante M y N , generan tres genotipos, M/M, M/N y N/N y tres fenotipos correspondientes, M, MN y N (v. cap. 6).
Como queda claro a partir de los principios de la herencia monogénica que se presentaron en el capítulo 4, una unión específica de una mujer MN y un varón MN produciría tres tipos distinguibles de descendencia (M, MN y N), en las proporciones esperadas xxx .

Es posible extender estos mismos principios a las uniones de la población en general.
Un muestreo de individuos de una población proporciona números absolutos de fenotipos MN (y, por tanto, genotipos) que pueden convertirse en frecuencias relativas al dividirlos entre el número total de observaciones.
De este modo, para una población británica que estudiaron Race y Sanger( 1975): Se utilizan las frecuencias relativas observadas para calcular las frecuencias de los alelos M y N en esta población, porque éstas constituyen la estimación que se requiere para realizar predicciones acerca de uniones y sus probables resultados.
Basándonos en las frecuencias genotípicas y fenotípicas observadas, pueden determinarse directamente las frecuencias génicas teniendo en cuenta que cada genotipo autosómico tiene dos alelos.
En este ejemplo, entonces, la frecuencia observada del alelo M es: xxx .
De modo similar, es posible calcular la frecuencia de N (0,47) sumando directamente el número de alelos N (1.347 de un total de 2.838 alelos) o simplemente restando la frecuencia de M de 1 (por lo tanto, la frecuencia de N = xxx , ya que las frecuencias relativas de los dos alelos deben sumar 1.

Se ha empleado esta pequeña muestra de individuos en una población para obtener cálculos de la frecuencia relativa de los dos alelos en la población en conjunto. 
En este contexto, cuando nos referimos a la frecuencia poblacional de un gen (alelo), estamos considerando un acervo génico , que contiene todos los alelos en un locus particular para toda la población.
La probabilidad de que un individuo tenga un genotipo particular en el locus MN depende de la frecuencia de los alelos M y N en el acervo genético Si las frecuencias relativas de los dos alelos en el acervo génico son 0,53 (para M ) y 0,47 (para N ), entonces las proporciones relativas de las tres combinaciones de alelos (genotipos) en la población de la que se extrajo esta muestra de 1.419 sujetos son xxx (para obtener dos alelos M del acervo), xxx (para dos alelos N) y xxx (para un alelo M y otro N ).

LEY DE Hardy-Weinberg

Poniendo el ejemplo anterior en términos más generales, si p es la frecuencia de un alelo y q es la frecuencia del otro alelo, entonces las frecuencias de las tres combinaciones alélicas serán: xxx M/M M/N N/N .
Una consecuencia importante de las relaciones entre fenotipo, genotipo y frecuencias alélicas es que las proporciones de los genotipos no cambian de generación en generación. 
En la generación que sigue a uniones aleatorias en una población en la que los genotipos M/M, M/N y N/N están presentes en las proporciones xxx , los genotipos aparecen en las mismas proporciones relativas (tabla 7-3).

El hecho de que los genotipos se distribuyan en proporción a las frecuencias de alelos individuales en una población y permanezcan constantes de generación en generación constituye el principio básico de la piedra angular de la genética poblacional, la ley de Hardy-Weinberg .
Esta ley fue denominada así por George Hardy, un matemático inglés, y Wilhelm Weinberg, un médico alemán, que formularon de manera independiente esta tesis en 1908.
Para algunos resultó sorprendente en ese tiempo que un alelo dominante no incrementase su frecuencia de manera automática hasta reemplazar el alelo recesivo complementario o que un alelo recesivo deletéreo, como el que genera la fibrosis quística en niños de raza blanca (cap.

4), no disminuya automáticamente su frecuencia hasta que se elimina por completo de una población.
La ley de Hardy Weinberg explicó la base de la invariabilidad de las frecuencias génicas como una aplicación del teorema binomial.

Tabla 7.3

Frecuencias de tipos de uniones y descendencia para una población en equilibrio de Hardy-Weinberg con genotipos de los padres en la proporción xxx 

Distribución binomial . 
La distribución binomial, descubierta originalmente por Sir Isaac Newton, fue probada de manera rigurosa por primera vez por Jakob Bernoulli, un matemático. 
En términos generales, cuando existen dos eventos alternativos, uno con probabilidad p y otro con probabilidad 1 xxx , las frecuencias de las combinaciones posibles de p y q en una serie de n tentativas vienen dadas por el desarrollo de xxx .

Examinaremos ahora un ejemplo simple que tiene cierta relevancia clínica: la distribución de niños y niñas en nacimientos sucesivos.

Se supone que los nacimientos de varones y mujeres tienen una probabilidad de 1/2, aun cuando la probabilidad verdadera de nacimiento de varones humanos (p) es de hecho un poco más alta que 1/2.
Entonces, la distribución de grupos de hermanos con dos niños, un niño y una niña, y dos niñas en familias con dos hijos viene dada por el desarrollo del binomio xxx ; en familias con tres hijos, por el desarrollo de xxx y así sucesivamente,donde xxx .

Así, en grupos de hermanos con dos individuos, xxx xxx de grupos de hermanos con 2 niños; xxx con un niño y una niña; xxx con dos niñas.

En grupos de hermanos con tres miembros, xxx xxx de grupos de hermanos con tres niños; xxx con dos niños y una niña; xxx con un niño y dos niñas: xxx con tres niñas. 

Por supuesto, independientemente de la distribución de sexos de los niños precedentes en un grupo de hermanos, cada nuevo hermano posee probabilidades iguales de ser varón o mujer.

Los matemáticos quizá prefieran utilizar el término general del desarrollo binomial, que es: xxx donde n xxx número en series, p = probabilidad de un evento específico, q xxx probabilidad del evento alternativo, m = número de veces que ocurre p (esto es, el exponente de p) y xxx (n factorial) es n ( xxx ) ( xxx ).

En este ejemplo, los valores de p y q son cada uno de 1/2, pero la distribución binomial puede utilizarse también para otros valores.
El mismo método se aplica. 
por ejemplo, para proporcionar la distribución de un rasgo autosómico recesivo conocido entre la progenie de dos padres heterocigotos.
Aquí, la probabilidad de un evento (que un niño resulte afectado) es de 1/4. y la probabilidad del otro evento (que un niño no padezca la afección) es de 3/4.
De este modo, la probabilidad de engendrar un hijo afectado en un grupo de hermanos de cinco miembros es de: xxx .
De modo similar puede calcularse que la probabilidad de tener tres niños afectados es de xxx o aproximadamente 9 %.

Proporciones genéticas

Una de las utilidades importantes de la distribución binomial en genética poblacional es concretar si un determinado trastorno, cuyo patrón de herencia no se ha establecido previamente, es autosómico recesivo, examinando si su distribución en grupos de hermanos se ajusta al 25% esperado para la herencia autosómica recesiva.
Por ejemplo, suponiendo herencia recesiva, ¿qué proporción de familias con tres hijos tendrá 0, 1, 2 y 3 niños afectados?
De acuerdo con el procedimiento empleado antes, p (posibilidad de un niño clínicamente normal) es igual a 3/4, q (posibilidad de un hijo afectado) es igual a 1/4, n xxx y ( xxx ) xxx .
Así, la expectativa teórica. basada en herencia autosómica recesiva, es que: 27/64 de todas estas familias no tengan niños afectados; 27/64 tengan un hijo afectado; 9/64 tengan dos hijos afectados; 1/64 tengan tres hijos afectados.

Sesgo de detección

Se han determinado las proporciones esperadas de niños afectados en familias con tres hijos de padres que son portadores de un trastorno autosómico recesivo.

Como se mostró, si se detecta una familia sólo cuando tiene al menos un niño enfermo, y si se identifican todas estas familias, entonces se perderán 27 de las 64 familias con riesgo (42 %).
De las 37 familias detectadas, xxx tienen un hijo afectado: xxx tienen dos hijos afectados; xxx tienen tres hijos afectados.

Entonces, en el grupo detectado, el número total de niños afectados entre 111 niños en 37 familias es de xxx , muy por encima del 25 % que en teoría se esperaba para la herencia autosómica recesiva.
¿Dónde estuvo el error?

Este es un ejemplo de sesgo de detección , un sesgo que debe evitarse en la investigación médica, en especial en la genética médica. 
En pruebas para herencia autosómica recesiva, los grupos de hermanos sin miembros afectados inevitablemente se pierden, y dependiendo de la manera en que se realiza la detección, la posibilidad de que un grupo de hermanos se detecte puede ser mayor cuando en dicho grupo hay dos o más miembros afectados que cuando sólo hay uno.
Se conocen varios métodos estadísticos para corregir el sesgo en diferentes condiciones de detección y se describen en muchos textos de estadística. 

El sesgo de detección se menciona aquí brevemente. porque no reconocer su importancia puede conducir a errores graves.

Frecuencia de genes y genotipos autosómicos

Como hemos visto, las distribuciones de genotipos de Hardy-Weinberg en poblaciones constituyen distribuciones binomiales simples. 
Los símbolos p y q representan las frecuencias de dos alelos alternativos en un locus (donde xxx ), y xxx pudiendo esto hacerse extensivo al par de alelos en cualquier locus autosómico o a cualquier locus ligado al X en mujeres.
(Debido a que los varones son los únicos que tienes solamente un cromosoma X, en ellos las frecuencias de genes ligados al X se consideran de manera separada en una sección posterior).
Si un locus posee tres alelos, p, q y r, la distribución genotípica puede determinarse a partir de xxx .
En términos generales, mediante binomios es posible calcular las frecuencias genotípicas para cualquier número conocido de alelos.

En genética clínica, el empleo principal de la ley de Hardy-Weinberg es para el cálculo de las frecuencias genéricas y de heterocigotos cuando se conoce la frecuencia de un rasgo genético, como se ilustró antes en el caso de un rasgo codominante expresado para el locus MN.
Una aplicación relacionada para determinar la frecuencia de portadores de un trastorno autosómico recesivo se mostró en el capítulo 4 con referencia a la fibrosis quística; otros ejemplos de la aplicación de estos cálculos se proporcionan en la tabla 7-4, en donde se comparan las frecuencias génicas y de heterocigotos de algunos transtornos en poblaciones con incidencias altas y bajas.

Las frecuencias de genes autosómicos dominantes también pueden calcularse a partir de datos de incidencia. 
A menudo, los homocigotos para condiciones autosómicas dominantes son tan raros que pueden ser olvidados en el cálculo (tabla 7-5).
Debido a que casi todo paciente resulta, por lo tanto, un heterocigoto con un alelo normal y otro anormal, la frecuencia del alelo mutante (q) es aproximadamente la mitad de la incidencia de la enfermedad ( xxx o alrededor de xxx , puesto que el valor de p está muy cercano a 1).
La incidencia observada de hipercolesterolemia familiar (HF), por ejemplo, es aproximadamente de 1/500.
Por tanto, puede calcularse que la frecuencia del gen mutante de HF es la mitad de dicha cifra, o alrededor de 0,0001 (tabla 7-5).

Tabla 7.4

Incidencia y frecuencia génica y de heterocigotos para trastornos seleccionados autosómicos recesivos en diferentes poblaciones

EL MAPA GÉNICO HUMANO: MAPEO GÉNICO Y ANÁLISIS DE LIGAMIENTO

Los genetistas, que trabajan sobre muchos organismos diferentes, dependen del mapeo génico para establecer la identidad de los genes que controlan distintos rasgos.
El mapeo génico en genética humana y médica ha probado no ser una excepción a este patrón y además ha producido un rico dividendo al aportar información de enorme valor práctico para la biología y la medicina.
Como se señaló en capítulos anteriores, el mapeo génico humano constituye una de las áreas de estudio de más rápida expansión en la genética médica actual.
Esto se debe en gran medida a que dicha técnica puede proporcionar una vía directa para identificar genes que provocan enfermedades genéticas.
A medida que las aplicaciones de la información del mapeo génico en el diagnóstico de enfermedad, el aislamiento de genes y el consejo genético han ido siendo más ampliamente apreciadas, ha ido ganando fuerza el ímpetu para lograr un mapa génico humano completo-una enciclopedia de todos nuestros genes y sus trastornos genéticos-.
Hoy en día, las publicaciones médicas están llenas de comunicaciones que asignan las enfermedades genéticas a cromosomas particulares y los registros médicos contienen muchos informes de laboratorio en los que los marcadores genéticos se han utilizado para realizar consejo genético apropiado o diagnósticos precisos.

Existen dos procedimientos fundamentalmente diferentes para lograr los mapas génicos de los cromosomas humanos: el mapeo físico y el genético.
El mapeo físico emplea varios métodos para asignar los genes a determinadas localizaciones a lo largo del cromosoma. 
Con estos métodos,las posiciones del mapa se describen en unidades que constituyen un reflejo de algunas mediciones físicas realizadas con células somáticas en el laboratorio.
El mapeo genético , por otro lado, es la medición de la tendencia de dos genes a segregarse juntos en la meiosis en estudios familiares y, por lo tanto, constituye una descripción del comportamiento meiótico de un gen, más que su localización física.
En este capítulo se exponen ambos tipos de mapeo y se examinan varias de sus aplicaciones para identificar y aislar genes humanos determinados, así como para proporcionar información diagnóstica en genética médica.

El crecimiento de las células humanas en cultivo

Exigió largo tiempo que las técnicas para el cultivo celular se desarrollaran y fuesen ampliamente utilizadas, en parte debido a los estrictos requerimientos nutricionales de las células cultivadas y en parte porque el eficaz mantenimiento de un cultivo durante períodos largos requiere precauciones extremas para evitar la contaminación por microorganismos, levaduras u otras líneas celulares cultivadas. 
Los requerimientos nutricionales incluyen el uso de un medio semidefinido con el 5 al 15 % de suero de feto de ternera o ternera recién nacida (que contiene factores de crecimiento no identificados), así como glucosa, aminoácidos, vitaminas, minerales, sal y un sistema tampón.

Entre las clases de cultivos que se emplean con mayor frecuencia en la genética médica humana, incluyendo los estudios de mapeo génico, se encuentran los siguientes: Linfocitos de sangre periférica en cultivos a corto plazo .

Estas células no cumplen las premisas para la supervivencia a largo plazo y persisten en el cultivo sólo alrededor de 72 horas.
Estos cultivos a corto plazo son ampliamente utilizados para análisis cromosómico, como se señaló en el capítulo 2.

Fibroblastos .
Los fibroblastos se encuentran entre los tipos celulares más útiles para estudios genéticos en células somáticas. incluyendo el mapeo génico físico.
Se cultivan a partir de pequeños explantes de piel u otro tejido. 
Las células, que tienen una forma de huso característica crecen en monocapas unidas a un sustrato de plástico o vidrio, y deben subcultivarse a intervalos frecuentes debido a que la sobrepoblación genera inhibición del crecimiento por contacto.
Los cultivos de fibroblastos generalmente mantienen el cariotipo de las células originales (normal o anormal), aunque en raras ocasiones pueden aparecer clones de células con cariotipos alterados somáticamente en el cultivo.
Los cultivos de fibroblastos envejecen después de 30 a 100 generaciones.
La duración de la supervivencia en cultivo es mayor para las líneas celulares obtenidas de personas jóvenes que las de individuos de mayor edad. 
Los fibroblastos que se cultivan a partir de pacientes con sospecha de trastorno genético constituyen una fuente común de cromosomas en metafase para el análisis citogenético, de extractos celulares para estudios bioquímicos y de DNA para estudios moleculares.
Además, los fibroblastos humanos con frecuencia se emplean en experimentos de hibridación en células somáticas para mapeo génico.

Líneas permanentes de células transformadas .
En algunas líneas celulares se produce una transformación en el cultivo, y adquieren la habilidad de crecer ilimitadamente, ya sea de manera espontánea o experimental por infección viral.
Otras líneas celulares permanentes se establecen directamente a partir de tumores.
Estas líneas no muestran inhibición por contacto, no envejecen ni mantienen un cariotipo estable; sin embargo, con frecuencia presentan características del tipo de célula transformada de la que derivan.
Como tales, resultan muy útiles para el análisis de las bases genéticas de la transformación y la malignidad.

Líneas celulares de linfoblastos .
Si bien los cultivos de linfocitos de sangre periférica casi nunca persisten en cultivo, puede lograrse que esto ocurra cuando se transforman con el virus de Epstein-Barr.
Los cultivos linfoblásticos, que derivan de linajes de células B, crecen en suspensión, más que como monocapas de células adosadas, y no envejecen.
Así resultan una fuente esencialmente permanente de material de pacientes con trastornos genéticos.

Líneas celulares fetales .
Las células fetales pueden obtenerse prenatalmente mediante extracción de líquido amniótico, mediante amniocentesis o por muestras de vellosidad coriónica de una mujer embarazada (v.cap. 19).
Una proporción de las células fetales obtenidas de estas maneras conservan al menos algún potencial de crecimiento en cultivo, aunque estos cultivos no crecen tan bien (ni durante un período tan largo) como los fibroblastos de la piel.
Las células del líquido amniótico o los cultivos de vellosidad coriónica se emplean para determinar el cariotipo fetal, y efectuar análisis de niveles enzimáticos y de DNA con objeto de proporcionar diagnóstico prenatal.

MAPEO FÍSICO DE LOS GENES HUMANOS

Inicialmente, la asignación de genes a cromosomas humanos determinados sólo se basaba en estudios familiares de grandes genealogías para descubrir el modo de herencia. 
Los rasgos ligados al X podían asignarse al cromosoma X debido a su patrón único de transmisión. 
Además, como se aborda más adelante en este capítulo, unos cuantos rasgos autosómicos pudieron asignarse a cromosomas individuales debido al descubrimiento fortuito de su cotransmisión a través de muchas meiosis con otros genes autosómicos bien conocidos.
Sin embargo, ésta fue generalmente una tarea lenta y laboriosa hasta el descubrimiento, en los años 60, de que podía lograrse que los cromosomas de los mamíferos, y en particular los de los seres humanos, se «segregaran» en células somáticas en cultivo y no sólo en células germinales en los estudios familiares.
Con ello se estableció la llamada alternativa «parasexual» del mapeo génico.
Debido a su importancia central para el campo del mapeo génico, primero nos ocuparemos de la preparación y el crecimiento de células en cultivo y a continuación describiremos las aplicaciones específicas de este procedimiento al mapeo genético físico.

Genética de las células somáticas

En términos generales la genética de las células somáticas es el estudio de la organización, expresión y regulación de las células cultivadas de origen somático.
Ha resultado sumamente eficaz desde los años 60 como acercamiento a muchos problemas en genética humana y médica.
En su origen, la genética de las células somáticas se consideró como una vía de investigación de los trastornos cromosómicos y monogénicos en cultivos a largo plazo en el laboratorio más que en individuos vivos.
El procedimiento tiene muchas ventajas obvias. 
Si una línea celular de un paciente con un trastorno raro o un cariotipo inusual puede establecerse cuando el individuo está disponible, es posible congelar dicha línea celular de manera más o menos permanente en nitrógeno líquido, y diferentes grupos de investigadores en cualquier momento conveniente pueden estudiarla más tarde, incluso decenios después de la muerte del paciente.

El campo del mapeo génico ha obtenido ventajas de la capacidad de varios tipos diferentes de células de crecer, algunas veces de manera indefinida, en cultivos celulares. (v. recuadro en pág. 160).
En efecto, es dudoso que muchos de los avances en el mapeo génico ocurrieran de manera tan rápida sin la capacidad de cultivar células somáticas.

El mapeo físico de genes abarca varios métodos diferentes diseñados para proporcionar información de mapeo de distintos tipos y diferentes niveles de resolución que incluyen desde un cromosoma entero a un solo par de bases (tabla 8-1).

Tabla 8.1

Procedimientos para el mapeo génico físico

Híbridos de células somáticas y mapeo génico

Una de las principales aplicaciones de la genética de células somáticas al mapeo génico incluye la transferencia de material genético de una célula humana a otro tipo de célula en cultivo, procedimiento conocido como transferencia génica .
La cantidad de DNA transferido puede variar desde un segmento corto de DNA cromosómico a gran parte de un genoma entero.
La transferencia génica es un mecanismo bien conocido en genética bacteriana, viral y de hongos, y los genetistas saben desde hace tiempo que la frecuencia con la que dos marcadores genéticos se transfieren juntos puede utilizarse como una medida de su proximidad física en el genoma del donante.
Así, no mucho después de que se reconociese que las células humanas y de otros mamíferos podían mantenerse en cultivo, los genetistas humanos buscaron vías para transferir genes humanos de una a otra célula en el laboratorio como un medio parasexual de segregar genes diferentes y sus alelos.

El procedimiento más ampliamente utilizado, que se desarrolló en los años 70, consiste en la fusión de células de diferentes especies para generar híbridos de células somáticas interespecíficos. 
Las células somáticas cultivadas que crecen en monocapas o en suspensión, así como las células vivas obtenidas directamente de los pacientes, pueden fusionarse utilizando virus Sendai inactivados o polietilenglicol.
Estos agentes provocan la fusión de membranas de dos tipos de célula, con lo que se forman homocariones (si se fusionan dos células del mismo tipo) o heterocariones (si se fusionan dos células de diferente tipo), en los que se mantienen dos núcleos separados en el mismo citoplasma (figura 8-1).
Después de la mitosis y la fusión celular, los dos contenidos nucleares se mezclan en un solo núcleo "híbrido".
En condiciones selectivas apropiadas de cultivo, los dos tipos celulares parentales, así como los híbridos derivados de homocariones, no pueden sobrevivir, produciéndose una selección de híbridos intercelulares para el crecimiento.
Aunque se han desarrollado varios esquemas selectivos, uno de los primeros y más comúnmente empleados requiere la selección en medio que contenga hipoxantina, aminopterina y timidina (el llamado medio HAT).
Las células pueden crecer en el medio HAT sólo si poseen la enzima hipoxantín-guanín-fosforribosiltransferasa (HPRT ), que utiliza hipoxantina y guanina en la síntesis de DNA (v. figura 10-2).
Las células con deficiente actividad de HPRT (HPRT xxx ) requieren nueva síntesis de purinas para sobrevivir y no pueden hacerlo en el medio HAT, porque la aminopterina inhibe la biosíntesis de purina (y la de pirimidina).

Cuando líneas celulares HPRT de roedores (a menudo ratón o hámster) se fusionan con células humanas HPRT xxx , sólo las células somáticas híbridas roedor/humano pueden prolongar su crecimiento en el medio HAT: la línea parental de ratón o hámster muere en medio HAT y las células humanas parentales (según su fuente) se van eliminando paulatinamente (se contraseleccionan) a causa de su falta de potencial de crecimiento a largo plazo (si se emplean leucocitos periféricos), su crecimiento generalmente lento (si se usan fibroblastos primarios de la piel) o su sensibilidad fortuita y característica (en comparación con las células de roedores) al compuesto ouabaína (figura 8-1).

Los híbridos de células somáticas son originalmente poliploides y contienen el complemento cromosómico completo de las células parentales humana y de roedor. 
En su momento, sin embargo, muchos de los cromosomas de una de las especies se pierden, porque no se requiere su presencia continua para la supervivencia del híbrido, sólo el cromosoma portador del gen para un marcador seleccionado (p.ej., el cromosoma X, que porta el gen HPRT) está necesariamente presente en todas las células, aunque en general al menos otros cuantos cromosomas donadores también persisten. 
La base de la pérdida o retención cromosómica aún se desconoce; la característica importante de híbridos de células somáticas de roedor/humano es que los cromosomas de éste último, y no del roedor, son los que se pierden de manera preferente, lo cual provoca que las células híbridas conserven combinaciones y cifras diferentes de cromosomas humanos, como lo demuestran varias técnicas de cariotipificación que distinguen entre cromosomas de roedor y humanos (figuras 8-1 y 8-2).

Este sistema parasexual de segregación cromosómica humana hace posible el mapeo de cualquier gen o segmento de DNA que pueda distinguirse entre las especies en un cromosoma humano determinado.
Cuando los cromosomas humanos residuales presentes en un conjunto de híbridos de células somáticas independientes se comparan con la presencia o ausencia de un gen humano particular en la misma serie de híbridos, puede observarse que ese gen se correlaciona completamente con la presencia o ausencia de un cromosoma específico.
De esta manera, por ejemplo, puede mostrase que la presencia o ausencia del gen para la hexosaminidasa A (HEXA), cuyas mutaciones generan la enfermedad de Tay-Sachs (v.caps. 4 y 12), se correlaciona con el cromosoma humano 15 (tabla X-2).
Todos los híbridos que conservan un cromosoma 15 contienen un gen humano HEXA: todos aquéllos que ya no poseen dicho cromosoma no contienen el HEXA humano.
Esta concordancia perfecta se observó sólo para el cromosoma y no para otros, lo cual permitió asignar el gen HEXA a tal cromosoma.
En este tipo de análisis, la presencia de un gen humano puede detectarse mediante mediciones de actividad (si el gen se expresa en híbridos de célula somática), con electroforesis para distinguir la actividad celular humana de la del roedor o, más comúnmente hoy en día, con métodos de hibridación de DNA mediante técnica de manchas Southern y diferencias de enzima de restricción para distinguir el gen humano del que pertenece al roedor (figura 8-3).
Utilizando este tipo de análisis de concordancia en conjuntos de híbridos interespecíficos de células somáticas, muchos centenares de genes se han asignado a cromosomas humanos determinados. 
Los genes que se han mapeado en el mismo cromosoma se conocen como sinténicos (literalmente «en la misma fibra»), independientemente de lo cercanos o alejados entre sí que se hallen en este cromosoma.

Los estudios como éste que utilizan híbridos de humano/roedor permiten asignar un gen a un cromosoma determinado. 
Puede obtenerse resolución adicional mediante segregación de híbridos de cromosomas humanos estructuralmente anormales, de los que se describen muchos ejemplos en los capítulos 9 y 10.
Si un determinado gen cosegrega en un conjunto de híbridos de células somáticas de roedor/humano con un cromosoma delecionado o translocado, el gen puede localizarse en la porción del cromosoma retenido en los híbridos. 
Y al contrario, si se sabe que un gen mapea en un cromosoma determinado, pero está ausente de un hibrido que contiene una copia delecionada o translocada de dicho cromosoma, entonces el gen puede asignarse a la porción ausente del cromosoma. 
La figura 8-4 representa dos cromosomas X anormales estructuralmente utilizados en mapeo génico para localizar genes ligados al X y secuencias de DNA de regiones determinadas del cromosoma X.
Mediante el examen de híbridos que contienen porciones de estos cromosomas translocados, el cromosoma X puede dividirse en tres intervalos, y es posible asignar genes diferentes a cada uno.
La extensión de esta estrategia y el examen de regiones cada vez más pequeñas del cromosoma de interés en los híbridos han hecho posible localizar genes en regiones incluso más pequeñas.
Aunque este enfoque mejora de modo significativo el nivel de resolución del mapeo génico, las regiones cromosómicas definidas aún no son muy grandes en comparación con el tamaño medio de un gen.

Figura 8.1

Esquema de hibridación interespecífica de células somáticas

Después de la fusión celular, los clones de híbridos celulares somáticos humanos/roedores se seleccionan en un sistema selectivo, como el medio HAT.

Estos híbridos celulares pierden preferentemente cromosomas humanos, lo que hace posible aislar clones híbridos que contengan diferentes combinaciones de cromosomas humanos. 
Estos clones pueden entonces analizarse en busca de la presencia o ausencia de un determinado gen humano, permitiendo la asignación del gen a un cromosoma humano específico. (PEG, polietilenglicol).

Figura 8.2

Placa de cromosomas en metafase de un híbrido de células somáticas de ratón/humano 

Los cromosomas humanos pueden identificarse por medio de un método de detección que marca de modo específico el DNA humano, pero no el del ratón. 
Este híbrido contiene seis cromosomas humanos, que se identifican como los cromosomas 13, 20 (dos copias) y X (tres copias).

Tabla 8.2

Mapeo de un gen humano mediante híbridos de células somáticas

Figura 8.3

Experimento con manchas Southern para mapear el gen humano HEXA en el cromosoma 15, en una serie de híbridos de células somáticas de ratón/humano

Sólo los híbridos I, III, VI, VII y VIII contienen secuencias de HEXA humano, que pueden distinguirse de las secuencias génicas del ratón mediante lugares de restricción diferentes en el gen de las dos especies.

Figura 8.4

Mapeo regional de genes ligados al X mediante análisis de cromosomas con translocaciones X;autosoma en híbridos de células somáticas

Los productos recíprocos de dos translocaciones X;autosoma diferentes se segregan en clones híbridos de células somáticas y pueden analizarse en cuanto a la presencia o ausencia de genes del cromosoma X humano.
Los resultados combinados permiten la localización regional de los tres genes en porciones diferentes del X, como se indica a la derecha.

Mapeo mediante dosis génica

Un procedimiento conceptualmente relacionado también aprovecha los cromosomas con reordenamiento estructural para el mapeo génico, pero no requiere segregar primero los cromosomas anormales en híbridos de células somáticas. 
Este procedimiento se basa en la detección de diferencias de dosis en productos o secuencias génicas entre líneas celulares de los pacientes que contengan cifras distintas de copias de un gen determinado.
Si bien este método requiere análisis e interpretación muy cuidadosos, se ha utilizado para asignar o excluir genes de una región involucrada en una duplicación o una delación (v. cap. 9).

Originalmente se empleó para asignar genes al cromosoma 21 mediante la detección de niveles de actividad enzimática en líneas celulares de individuos con síndrome de Down (tres dosis del cromosoma 21), que resultaron más elevados que los niveles hallados en líneas celulares de individuos cromosómicamente normales (dos dosis).
A nivel de DNA, el procedimiento de dosis se ha utilizado de manera creciente para asignar marcadores de DNA al cromosoma X (comparando dosis de DNA en personas con un cromosoma X [esto es, un cariotipo de varón normal] a cinco [es decir, un cariotipo 49, XXXXXI (figura 8-5) o a regiones más pequeñas de un cromosoma determinado (examinando conjuntos de pacientes con trisomías parciales [tres copias o monosomías [una copia]: v. cap. 9).

Una de las aplicaciones más directas del mapeo mediante dosis génicas es la asignación regional de genes de enfermedad ligada al X examinando varones con deleciones citogenéticamente detectables de parte del cromosoma X.
En un caso bien estudiado, un niño (B.B.) sin antecedentes conocidos de ninguna enfermedad genética se presentó con cuatro trastornos ligados al X distintos: distrofia muscular de Duchenne (DMD), enfermedad granulomatosa crónica (EGC), retinitis pigmentaria (RP) y un raro fenotipo de eritrocitos.
El análisis citogenético cuidadoso reveló una pequeña delación, pero detectable en la banda xxx (figura 8-6).
La coexistencia de cuatro trastornos monogénicos con la pequeña delación cromosómica llevó a la conclusión de que los genes para estos cuatro rasgos ligados al X mapeaban en el intervalo delecionado. 
La extensión de este tipo de análisis a otros individuos afectados de modo simultáneo con múltiples trastornos ligados al X ha permitido la asignación regional de varios genes a esta región del cromosoma X (figura 8-6). 
El caso de B. B. resultó muy significativo para la genética médica porque (como se describe en forma más completa posteriormente en este capítulo) su cromosoma X delecionado se utilizó de manera directa para lograr la clonación de genes para la DMD y la EGC.
Esto proporcionó aún otro ejemplo de la forma en que el reconocimiento de lo inusual en medicina (en este caso, la presencia de múltiples enfermedades genéticas en un solo individuo) puede proporcionar importante información nueva acerca de los genes normales, su organización y función.

Figura 8.5

Ejemplos de mapeo mediante análisis de dosis

La sonda 1 de DNA utilizada a la izquierda puede mapearse en el cromosoma X, porque la intensidad de la hibridación parece ser una función del número de X presentes en cada muestra de DNA.
La sonda 2 de DNA muestra la misma intensidad de hibridación en las columnas 1 a 3, pero está ausente en la columna 4.
Este locas se mapea en el cromosoma Y.

Figura 8.6

Localización regional de genes ligados al X en pacientes con delaciones del cromosoma X

La correlación de tamaño de la delación citogenética con los trastornos particulares presentes en cada caso permite el mapeo preciso de los genes individuales de enfermedad en regiones determinadas del cromosoma X DMD, distrofia muscular de Duchenne, OTC, deficiencia de ornitín-transcarbamilasa. 
EGC, enfermedad granulomatosa crónica, RP, retinitis pigmentaria, G KD, deficiencia de glicerocinasa, HSC , hipoplasia suprarrenal congénita.

Nota: Un grupo de híbridos de células somáticas de ratón humano cada uno con 2 a 13 cromosomas humanos. 
Se utilizó para probar la presencia o ausencia del gen de la hexosaminidasa A humana (HEXA) mediante manchas Southern de DNA preparado a partir de los híbridos (v figura 8-3).
Existe una correlación perfecta entre la presencia o ausencia de HEXA y la presencia o ausencia del cromosoma humano 15 Para todos los demás cromosomas humanos, hay híbridos que contienen HEXA pero carecen del cromosoma e híbridos que contienen el cromosoma pero les falta HEXA. 
Así, estos resultados indican que el gen HEXA debe encontrarse en el cromosoma humano 15.

Human Genome Project

Los genetistas humanos y médicos han identificado y mapeado genes durante decenios.
Sin embargo, en 1986 Dulbecco propuso una desviación radical del enfoque de «mapear lo que se pueda», pues sugirió que, si los científicos realmente querían comprender el papel de los genes en el cáncer-sin mencionar otros trastornos genéticos en general-, todo lo que debían hacer era ¡secuenciar los 3.000 millones de pares de bases y encontrar todos los genes!
Después de mucha discusión y debate (algo de lo cual aún continúa) nació el Proyecto Genoma Humano, un esfuerzo internacional para primero mapear y finalmente secuenciar los 50.000 a 100.000 genes.
El proyecto inicialmente se centró en la construcción de mapas de ligamiento genético y físico de los 22 autosomas y los cromosomas sexuales, así como en el ensamblaje de colecciones de clones superpuestos y contiguos (denominados contigs) para cubrir cada cromosoma de telómero a telómero con objeto de facilitar la identificación y el aislamiento de los genes (figure 8-9).
En Estados Unidos, el proyecto es liderado por James D. Watson, quien junto con Francis Crick descubrió la estructura de la doble hélice en 1953.
La Organización Genoma Humano (HUGO) supervisa el esfuerzo internacional y organiza una serie de reuniones de Mapeo del Genoma Humano para comparar y ensamblar mapas formalmente, así como establecer una lista de todos los genes humanos mapeados.
Cada año se publica un catálogo actualizado (v.
Bibliografía), que se incluye en una base de datos informatizada que se mantiene al día continuamente incorporando todos los datos de mapeo actuales y los hace accesibles con medios electrónicos, junto con información sobre trastornos genéticos humanos (On-line Mendelian Inheritance in Man, OMIM ; accesible a través de Genome Data Base en la Johns Hopkins University, Baltimore, Maryland; McKusick, 1990; Pearson, 1991).

APLICACIONES DEL MAPEO GÉNICO HUMANO

Existen tres aplicaciones principales del mapeo génico en la genética médica: proporcionar información diagnóstica para el consejo genético, guiar los esfuerzos para clonar genes que provocan enfermedad genética y generar y demostrar hipótesis acerca de las bases de enfermedades genéticas determinadas.

En estos tres casos, la construcción de mapas genéticos y físicos resulta interdependiente.
El conocimiento de la posición física en el mapa de los marcadores polimórficos que se utilizan en el análisis de ligamiento ayuda al mapeo genético.
Los esfuerzos del mapeo físico para localizar con exactitud un gen pueden guiarse por la existencia de recombinaciones meióticas específicas que se detectan como parte del análisis de ligamiento.

Detección de genes mutantes mediante análisis de ligamiento

Un concepto genético clásico que se ha apreciado durante decenios es que un mapa de ligamiento genético puede utilizarse para diagnosticar enfermedades genéticas, incluso en ausencia de información concreta acerca de la naturaleza bioquímica o molecular del trastorno.
Sin embargo, a pesar de la importancia de este concepto, el mapeo de ligamiento ha sido aceptado ampliamente como relevante desde el punto de vista médico sólo desde la formulación de la idea de que los RFLP pueden formar la base de un mapa genético humano.

Como se señala en el capítulo 11, Kan y Dozy (1978) fueron los primeros en aplicar un polimorfismo de DNA al diagnóstico clínico; utilizaron un polimorfismo 3' del gen de la betaglobina para diagnosticar casos de enfermedad drepanocítica mediante análisis de ligamiento. 
Aunque la frecuencia de recombinación entre el lugar polimórfico y el de la mutación de la drepanocitosis en el gen de la betaglobina era insignificante, es importante reconocer que el procedimiento de ligamiento para la detección de mutación (incluso cuando se utilizan polimorfismos informativos en el gen clonado que provoca un defecto genético) es indirecto y conlleva un riesgo de recombinación entre el marcador genético y la mutación real.
En muchos casos, este riesgo probablemente es muy bajo.
Genes muy grandes, como el gen de la DMD, constituyen excepciones a esta regla general, porque la recombinación dentro del gen ocurre con una frecuencia detectable.
En la figura 8-19 se ilustra el empleo de genes clonados como marcadores polimórficos para el diagnóstico de enfermedades, utilizando como ejemplo otro defecto del locus de la xxx -globina, la xxx -talasemia.
Un trastorno autosómico recesivo frecuente que se describe con detalle en el capítulo 11.
Los ejemplos ilustran de nuevo los requerimientos según los cuales un marcador genético debe ser informativo y ha de ser posible determinar la fase para realizar análisis de ligamiento.

El valor de un mapa genético para el diagnóstico de enfermedad resulta más evidente cuando un gen se ha localizado en un cromosoma, pero aún no se ha clonado. 
En este caso, deben emplearse uno o más marcadores genéticos que muestren recombinación detectable con un locus de enfermedad.
Ya se ha aludido a esta situación al describir la detección de ligamiento en un apartado previo de este capítulo.
El diagnóstico en este caso se complica al tener que considerar la posibilidad de recombinación en meiosis.
Así, para un marcador genético que muestra una recombinación del 5% con un locus de enfermedad, el diagnóstico tiene una probabilidad de error del 5%.
Incluso aunque el marcador resulte completamente informativo y se conozca la fase (figura 8-30).
La posibilidad de recombinación también puede complicar la determinación de la fase cuando ésta se infiere de un niño, en lugar de deducirse de una generación de abuelos.
Por ejemplo, en la familia que se muestra en la figura 8-21, es posible un diagnóstico de distrofia miotónica (DM, una alteración autosómica dominante variable), con sólo una certeza de 82% en el individuo III-2 (v. figura 8-21), a pesar de que el marcador genético ligado que se utilizó muestra únicamente una recombinación del 10% con la mutación de DM.
Esto se debe a que la información de la fase proviene sólo del genotipo del niño afectado (III-1) en esta familia.

La exactitud del diagnóstico mediante el empleo de marcadores genéticamente ligados con una frecuencia de recombinación apreciable entre el marcador y el locus de la enfermedad puede incrementarse mucho utilizando dos marcadores genéticos informativos que flaqueen el gen de la enfermedad. 
En este caso, la posibilidad de un diagnóstico erróneo se reduce en forma significativa, ya que éste ocurre sólo si suceden dos recombinaciones, una a cada lado del gen de la enfermedad.
Por ejemplo, para marcadores de flanqueo que muestran cada uno un 10% de recombinación con un gen de enfermedad, el diagnóstico mediante ligamiento genético tiene una precisión del 99% (en vez del 90% con un solo marcador).
El beneficio de emplear marcadores de flanqueo refuerza el valor de un mapa de ligamiento genético preciso, con marcadores de orden y distancia conocidos bien mapeados. 

En los capítulos 18 y 19 se exponen ejemplos adicionales del empleo de análisis de ligamiento genético para el diagnóstico de enfermedades hereditarias.

Figura 8.19

Ejemplos de diagnóstico molecular en la xxx -talasemia, con polimorfismos en el locus de la xxx - globina

Se supone que la probabilidad de recombinación entre el marcador polimórfico y la mutación es insignificante.
En la familia A puede determinarse la fase del hermano afectado y también es posible diagnosticar al no afectado.
En la familia B, la fase no puede determinarse por completo; no es posible realizar ningún diagnóstico porque el segundo hijo quizá resulte afectado (probabilidad del 50%) o no afectado (probabilidad del 50%).
En la familia C, la fase no puede determinarse totalmente, pero es posible diagnosticar que el segundo hijo será un portador heterocigoto. 
En la familia D ningún diagnóstico es posible: esta familia es no informativa.

Figura 8.20

Ejemplo de diagnóstico molecular mediante ligamiento genético con un marcador polimórfico que se sabe que mapea a una distancia de aproximadamente 5 cM de un locus de enfermedad ligada al X

Se conoce la fase de la genealogía.
El alelo mutante debe estar localizado en el cromosoma X con el alelo del locus marcador de 4,0 kb.

Figura 8.21

Ejemplo de diagnóstico molecular mediante ligamiento genético en una genealogía de fase conocida con un marcador polimórfico que se sabe que mapea a una distancia de aproximadamente 10 cM del locus de la distrofia miotónica (DM)

La fase en el individuo II-2 no puede determinarse a partir de sus padres, porque es posible que su padre afecta do haya transmitido el alelo mutante de DM junto con cual quiera de los alelos A o a en el locus marcador.
El genotipo del niño afectado III-1 sugiere que la fase de la madre es DM-a, pero sólo presenta una certeza del 90 %, ya que existe una probabilidad del 10 % de que su fase sea DM-A y de que haya habido recombinación en la meiosis cuando concibió a su hija.

Por lo tanto, la probabilidad global de que III-2 resulte afectado es del 82 %, calculada como xxx .

Mapeo de genes de enfermedad mediante análisis de ligamiento

En la sección anterior se trató el uso de un mapa de ligamiento genético para obtener información diagnóstica cuando se conoce que un gen de enfermedad está genéticamente ligado con uno o más marcadores.
Sin embargo, cómo se descubrieron estas relaciones de ligamiento?

Los dos requerimientos esenciales para el mapeo de genes de enfermedad son, en primer lugar, un número suficiente de familias para establecer el ligamiento y, en segundo término. marcadores de DNA informativos adecuados.
Como se mencionó antes, el segundo requerimiento puede lograrse de manera relativamente fácil en la actualidad.
Encontrar familias adecuadas, sin embargo, puede constituir un reto, en particular para trastornos raros u otros en los que las personas afectadas mueren en edad temprana (y, por tanto, no se cuenta con ellas para el análisis). 
Se han utilizado dos procedimientos generales (figura 8-22).
En uno, se detecta un número pequeño de familias muy grandes y se obtiene DNA de miembros de la familia que son tipificados para polimorfismos de DNA.
La ventaja de este procedimiento es que se sabe que todos los miembros afectados de la genealogía tienen la misma enfermedad genética, causada por una mutación en un solo locus.
El procedimiento alternativo consiste en conseguir un gran número de familias más pequeñas.
Esto resulta más sencillo para enfermedades relativamente frecuentes, como la fibrosis quística (figura 8-22), pero implica el riesgo de que no todas las familias tengan un trastorno genéticamente idéntico. 
La heterogeneidad genética , que se ilustra mediante la situación en la que síntomas clínicos idénticos son causados por defectos en dos o más loci genéticos, puede confundir el análisis de ligamiento genético y dar la impresión de que un marcador no está ligado con un locus de enfermedad, cuando, de hecho, quizá si esté ligado, aunque sólo en un subgrupo de las familias analizadas.

Esta situación se ilustra más adelante en el caso de ligamiento en la retinitis pigmentaria autosómica dominante.

En la práctica de la genética humana cabe esperar el ligamiento sólo con una distancia de aproximadamente 20 cM o menos, ya que en distancias mayores casi nunca es posible encontrar suficiente material familiar para establecer evidencia significativa de ligamiento (una puntuación lod xxx ) frente a no ligamiento.
Así, por lo general un marcador polimórfico debe hallarse aproximadamente a menos de 20 millones de pares de bases del gen de la enfermedad en cuestión para que el ligamiento pueda detectarse.
Para un rasgo autosómico, esto es menos del 1% del genoma, y así es posible predecir que por término medio tendrán que ser punteados en el grupo familiar alrededor de 100 marcadores polimórficos autosómicos adecuadamente espaciados, antes de que se encuentre un resultado de ligamiento positivo.
De hecho, en las varias docenas de veces que se ha utilizado este procedimiento con éxito.
El intervalo abarca de 12 RFLP (para las búsquedas más afortunadas) a más de 200 (para las más difíciles). 
Resulta claro que la situación es mucho más sencilla para enfermedades ligadas al X, porque ya se conoce la localización cromosómica del gen.
Por lo tanto, sólo se utilizarán los marcadores polimórficos del cromosoma X para analizar su ligamiento con la enfermedad.

Cuando se comienza una búsqueda de ligamiento, lo esperado para cualquier marcador polimórfico es que, al azar, no se halle ligado con el gen de la enfermedad (de hecho, al azar, ni siquiera es probable que se encuentre en el mismo cromosoma del gen de la enfermedad).
Así, sólo se requerirán unas cuantas meiosis informativas para establecer que el polimorfismo no está cerca del gen en cuestión.
Cuando un marcador muestra una prueba sugestiva de ligamiento (p. ej., una puntuación lod máxima combinada entre 2 y 3), puede ayudar en la búsqueda saber dónde se mapea físicamente el locus marcador potencial.
Si, por ejemplo, el marcador se mapea en una posición en el brazo largo del cromosoma 7, entonces es posible centrar la atención en otros marcadores polimórficos en dicha región para confirmar de manera eficaz o rechazar el resultado de ligamiento sugerido. 
En este punto los extensos mapas físicos y genéticos de cada cromosoma humano resultan extremadamente valiosos.
Una vez que un supuesto ligamiento se ha confirmado, pueden probarse otros marcadores que se sabe mapean cerca del que se encontró en un intento para moverse cada vez más cerca del gen en cuestión.
Una vez que uno o más marcadores se encuentran aproximadamente a 5 cM del gen de la enfermedad, pueden utilizarse para el diagnóstico como se señaló en secciones previas.
También pueden servir como puntos de partida para estudios de mapeo preciso, con objeto de intentar aislar el gen de la enfermedad.

Mapeo génico humano y «clonación posicional»

La aplicación del mapeo génico en genética médica ha tenido un éxito espectacular en algunos casos.
La estrategia completa-el mapeo de la localización de un gen de enfermedad mediante el análisis de ligamiento para definir los marcadores que se utilizarán en el diagnóstico de la enfermedad y el consejo genético, así como los intentos posteriores para clonar el gen sobre la base de su posición en el mapa-puede ilustrarse mediante ejemplos específicos.
Los ejemplos que vamos a exponer ilustran varios procedimientos; aquel que en último término tendrá éxito para determinada enfermedad depende de las características y las circunstancias de esa enfermedad. 

El procedimiento de clonar un gen sobre la base de su posición en el mapa puede denominarse clonación posicional para distinguirlo de la estrategia más típica de la clonación génica en la que se comienza con una proteína conocida, se determina su secuencia de aminoácidos y se utiliza dicha información para aislar el gen (v. cap. 5).
La clonación posicional se basa sólo en la localización cromosómica del gen.
Esta estrategia se denominó en el pasado "genética inversa".
Sin embargo, de hecho, este procedimiento se enraiza firmemente en la tradición de la genética clásica.

Figura 8.22

Dos procedimientos para la obtención de familias para análisis de ligamiento 

El ligamiento con éxito de la enfermedad de Huntington con un marcador polimórfico en el cromosoma 4 se basó en gran parte en una sola genealogía venezolana de gran tamaño, una pequeña parte de la cual se muestra en A.
Sin embargo, el ligamiento con éxito de la fibrosis quística con un marcador polimórfico en el cromosoma 7 se basó en un conjunto de muchas familias pequeñas, algunas de las cuales aparecen en B.


Así se produce el cáncer 

Las bases moleculares del cáncer salen a la luz y sugieren nuevas aproximaciones terapéuticas. 

El desarrollo del cáncer ha dejado de ser un misterio. 
A lo largo de los últimos veinte años, los investigadores han realizado progresos espectaculares en el esclarecimiento de sus bases moleculares.
Los descubrimientos son sólidos, resistirán a las futuras generaciones de investigadores y sentarán las bases para tratamientos revolucionarios de la enfermedad.
Nadie puede predecir con exactitud cuándo se aplicarán las terapias que saldrán al paso de las alteraciones moleculares operadas en las células cancerosas, ya que la traducción de nuevos conocimientos en práctica clínica es complicada, lenta y cara.
Pero se trabaja en ello.

Bajo el término «cáncer» se amparan más de 100 formas de la enfermedad.
Casi todos los tejidos del cuerpo pueden llegar a desarrollar un estado maligno, y en algunos casos hasta varios tipos distintos.
Aunque cada cáncer tiene características únicas, los procesos básicos que los originan son muy similares. 
Por esa razón, me referiré en este artículo al «cáncer» en términos genéricos, recurriendo a tipos concretos para ilustrar reglas que parecen ser universales.

Los 30 billones de células que forman un cuerpo normal y sano viven en un condominio complejo e interdependiente, en el que unas regulan la proliferación de otras.
Las células normales sólo se reproducen cuando reciben las instrucciones adecuadas que les envían otras células vecinas.
Tal colaboración permanente asegura que cada tejido mantenga el tamaño y la arquitectura adecuada a las necesidades del cuerpo.

Las células cancerosas vulneran ese esquema.
Ignoran los controles normales de proliferación y siguen sus propias instrucciones internas de reproducción.
Gozan de una propiedad incluso más perniciosa:
son capaces de emigrar del sitio donde se producen, invadir otros tejidos y formar masas en lugares distantes del cuerpo. 
Con el tiempo, los tumores formados por esas células malignas se vuelven cada vez más agresivos y se tornan letales en cuanto empiezan a destrozar tejidos y órganos vitales.

En los últimos 20 años, la ciencia ha descubierto un ramillete de principios básicos que gobiernan el desarrollo del cáncer.
Sabemos ya que las células de un tumor descienden de una célula ancestral común, que en algún momento, generalmente décadas antes de que el tumor se manifieste, inició un programa de reproducción indebido.
La transformación maligna de una célula acontece después, por acumulación de mutaciones en unos genes específicos.
Esos genes son la clave para entender las raíces del cáncer.

Los genes residen en las moléculas de ADN de los cromosomas del núcleo celular.
Un gen cifra una secuencia de aminoácidos, que deben engarzarse entre sí para formar una proteína particular, que es la que finalmente realiza el trabajo del gen.
Cuando un gen se activa, la célula responde sintetizando la proteína cifrada.
Las mutaciones génicas que cambian la cantidad o la actividad del producto proteínico, pueden perturbar el funcionamiento de una célula. 
En la iniciación del cáncer desempeñan un papel fundamental dos clases de genes, que en conjunto constituyen sólo una pequeña proporción de toda la dotación génica del individuo.
En sus versiones normales, controlan el ciclo de vida de la célula, esto es, la compleja secuencia de eventos que hacen que una célula crezca y se divida. 
Los protooncogenes «activan» el crecimiento, mientras que los genes supresores de tumores lo inhiben.
Considerados en su conjunto, estos dos tipos de genes son responsables, en buena medida, de la proliferación celular incontrolada que se observa en los tumores humanos.

Cuando matan, los protooncogenes pueden convertirse en oncogenes carcinogénicos capaces de dirigir una multiplicación desenfrenada.
Las mutaciones pueden hacer que el protooncogén produzca un exceso de proteína estimuladora del crecimiento, o una forma hiperactiva de la misma.
Los genes supresores de tumores, por el contrario, contribuyen al cáncer cuando las mutaciones los silencien.
La falta de proteína supresora funcional priva a la célula del freno que, en situación de normalidad' impide el crecimiento desmesurado. 

Para que un tumor se desarrolle, deben ocurrir mutaciones en media docena o más de genes que controlan el crecimiento de las células fundadoras.
Formas alteradas de otras clases de genes participan también en la creación de un estado maligno. permitiendo específicamente que una célula que prolifera se torne invasiva y se disemine por todo el cuerpo (metástasis).

Las claves para entender el papel que desempeñan los genes supresores de tumores y los protooncogenes, mutados, en el desarrollo del cáncer. vienen dadas por el estudio de las funciones celulares que ejercen en condiciones de normalidad. es decir, de genes no mutados.
Dos décadas de investigaciones nos han permitido conocer tales funciones genéticas normales con una claridad y detalle sin precedentes.

Muchos protooncogenes cifran proteínas que participan en cadenas moleculares de transmisión de señales estimuladoras del crecimiento.
Esas señales se van transmitiendo desde el exterior celular hasta los sitios más recónditos del interior de la célula. 
El crecimiento de una célula se sale de control (se desregula) cuando en uno de sus protooncogenes se produce una mutación que altera una ruta estimuladora del crecimiento; en virtud de lo cual, la ruta se mantiene permanentemente activa cuando no debiera estarlo.

Esas rutas reciben y procesan señales estimuladoras del crecimiento emitidas por otras células en un tejido determinado.
Tal sistema de señales célula-célula suele comenzar cuando una célula libera factores de crecimiento. 
Una vez liberadas, estas proteínas se mueven a través del espacio intercelular y terminan engarzadas en receptores específicos (moléculas «antena») que hay en la superficie de otras células cercanas.
Los receptores se alojan en la membrana exterior de la célula diana, de suerte tal que uno de sus extremos se orienta hacia el espacio extracelular. y el otro se introduce en el interior de la célula o citoplasma.
Cuando un factor estimulador del crecimiento se une a un receptor. éste transmite una señal de proliferación a otras proteínas presentes en el citoplasma.
A su vez, estas proteínas emiten señales estimuladoras a toda una sucesión de proteínas distintas, en una cascada que acaba en el corazón de la célula, su núcleo.
En el núcleo, otras proteínas, los factores de transcripción, responden activando a un conjunto de genes que son los que coadyuvan a que la célula entre en su ciclo de crecimiento.

Algunos oncogenes obligan a la célula a sobreproducir factores de crecimiento.
Los sarcomas y gliomas(cánceres de tejido conectivo y células no neuronales del cerebro, respectivamente)liberan cantidades excesivas del factor de crecimiento derivado de plaquetas.
Otros tipos de cánceres secretan en demasía factor de crecimiento transformante de tipo alfa.
Estos factores actúan, como siempre, sobre células cercanas pero, lo que es más importante, pueden provocar también la proliferación de la propia célula que los fabrica.

Los investigadores han identificado también versiones oncogénicas de genes de receptores.
Los receptores aberrantes determinados por esos oncogenes liberan en el citoplasma celular un torrente de señales proliferativas, aunque no estén presentes los factores de crecimiento que urgen a la célula a replicarse.
En este orden, las células del cáncer de mama presentan a menudo moléculas receptoras de tipo Erb-B2 que muestran tal comportamiento.

Otros oncogenes perturban la cascada de señales en algún punto del citoplasma.
El ejemplo mejor conocido es la familia de oncogenes ras .
Las proteínas que cifran los oncogenes ras normales trasmiten señales estimuladoras, procedentes de los receptores de factores de crecimiento, a otras proteínas situadas curso abajo en la cascada.
Las proteínas cifradas por los genes ras mutantes, sin embargo, están siempre activas, aunque los receptores de los factores de crecimiento no les estén enviando señales.
En casi una cuarta parte de los tumores humanos, incluidos los carcinomas de colon, páncreas y pulmón, se encuentran proteínas Ras hiperactivas. 
(Los carcinomas son los cánceres más comunes.
Se originan en las células epiteliales que tapizan las cavidades corporales y forman la capa externa de la piel).

Otros oncogenes, así los de la familia myc , alteran la actividad de los factores de transcripción nucleares.
En condiciones normales, las células producen factores de transcripción Myc sólo cuando se estimulan por factores de crecimiento que afectan a la superficie celular.
Una vez sintetizadas, las proteínas Myc activan genes que inducen el crecimiento celular.
Pero en muchos tipos de cánceres, especialmente los que tienen que ver con los tejidos sanguíneos, los niveles de Myc se mantienen siempre altos, aun cuando falten los correspondientes factores de crecimiento.

El descubrimiento de cascadas de proteínas que portan mensajes proliferativos desde la superficie celular hasta el núcleo, ha supuesto algo más que una mera satisfacción intelectual.
Al ser las responsables de la activación de la multiplicación de las células malignas, se convierten en atractivas dianas para quienes laboran en el desarrollo de nuevas terapias contra el cáncer.
Media docena de empresas trabajan en drogas diseñadas para bloquear los receptores de factores de crecimiento aberrantes.
Al menos otros tres laboratorios farmacéuticos se afanan en la creación de compuestos que bloqueen la síntesis de las proteínas Ras aberrantes.
Esos dos tipos de agentes pueden frenar el exceso de señales en células cancerosas cultivadas, pero queda por demostrar su eficacia en el bloqueo de la evolución de procesos tumorales en animales y humanos.

Para convertirse en malignas, las células deben hacer algo más que sobreexcitar la maquinaria que promueve el crecimiento.
Deben también encontrar la forma de evadir o ignorar las señales de freno emitidas por sus células vecinas normales.

Los mensajes inhibidores recibidos por una célula normal fluyen hacia el núcleo, lo mismo que las señales estimuladoras, a través de cascadas moleculares.
En las células cancerosas, esas cascadas deben inactivarse, para que la célula haga caso omiso de las potentes señales inhibidoras que llegan a su superficie.
Los componentes críticos de esas cascadas moleculares, que son los productos de los genes supresores de tumores faltan o están silenciados en muchos tipos de células cancerosas.

El factor de crecimiento transformante tipo beta ( TGF-beta ) puede detener el crecimiento de varios tipos de células normales.
Algunas células de cáncer de colon se vuelven indiferentes al TGF-beta porque tienen inactivo un gen que cifra un receptor para esa sustancia secretada.
Algunos cánceres de páncreas inactivan el gen DPC4 , cuyo producto proteínico actúa curso abajo del receptor del factor de crecimiento.
Y otros cánceres silencien el gen p15 , que cifra una proteína que, en su versión normal, responde a las señales del TGF-beta deteniendo la maquinaria implicada en el ciclo de división celular.

Las proteínas supresoras de tumores pueden recurrir también a otros caminos para suspender la proliferación celular.
Algunas, por ejemplo, bloquean el flujo de señales que discurren por los circuitos estimuladores del crecimiento. 
Uno de esos supresores es el producto del gen NF-1 , molécula citoplasmática que inutiliza a la proteína Ras antes de que ésta pueda emitir órdenes promotoras del crecimiento.
Las células que no tienen NF-1 carecen, pues, de un factor importante para contrarrestar la acción de Ras y, por tanto, la proliferación incontrolada.

Varios estudios han demostrado que la introducción de un gen supresor de tumores en células cancerosas que carecen del mismo puede devolver cierto grado de normalidad a las células.
Esos resultados sugieren posibles vías para combatir el cáncer, suministrando a las células malignas versiones intactas de los genes supresores de tumores que han perdido durante el desarrollo del tumor.
Pese al atractivo que rodea a dicha idea, este tipo de estrategias se ve frenada por las dificultades técnicas con las que todavía tropieza la terapia génica de muchas enfermedades. 
Con los procedimientos actuales no se pueden introducir genes en un número suficiente de células de un tumor.
Mientras no se remonte este obstáculo logístico, el uso de la terapia génica para curar el cáncer seguirá siendo una idea muy hermosa, pero inalcanzable. 

A lo largo de los últimos cinco años se han ido acumulando datos y observaciones que permiten desentrañar el destino intracelular de las rutas estimuladoras e inhibidoras. 
Convergen en el reloj del ciclo celular, un aparato molecular que hay en el núcleo.
El reloj es el director ejecutivo de la célula, que, por lo que parece, se descontrola en prácticamente todos los tipos de cánceres humanos. 
En la célula normal, el reloj integra el conjunto de señales reguladoras del crecimiento recibidas por la célula, y decide si ésta debe entrar o no en el ciclo de división.
Si la respuesta es positiva, el reloj dirige todo el proceso.

El ciclo celular consta de cuatro etapas.
En la fase G1 (de «gap», intervalo), la célula aumenta de tamaño y se prepara para copiar su ADN .
El copiado ocurre en la fase siguiente, denominada S (síntesis), durante la cual la célula duplica con precisión su complemento cromosómico.
Una vez que los cromosomas se han replicado, la célula entra en la fase G2 , durante la cual se prepara para la última fase, M (mitosis), fase en la que la célula se divide para engendrar dos células hijas, cada una de las cuales recibe una serie completa de cromosomas. 
Las nuevas células hijas entran inmediatamente en fase G1 y continúan el ciclo de nuevo.
En una opción alternativa, pueden detener el ciclo, temporal o permanentemente.

Para programar esta elaborada sucesión de acontecimientos, el reloj del ciclo celular se vale de diversos tipos de moléculas. 
Sus dos componentes esenciales, las ciclinas y las quinasas dependientes de ciclinas ( CDK ), se asocian entre sí e inician la entrada en los diferentes estadios del ciclo celular.
En G1 , por ejemplo, las ciclinas de tipo D se unen a la CDK4 o CDK6 , y el complejo que resulta opera sobre una potente molécula inhibidora del crecimiento, la proteína pRB .
Esta acción contrarresta el freno impuesto por pRB y permite que la célula progrese hacia el final de la fase G1 y entre, por tanto, en la fase S (síntesis de ADN ).

Diversas proteínas inhibidoras pueden reprimir el ciclo celular en alguna de sus fases.
Las proteínas p15 (mencionada antes) y p16 bloquean la actividad de las CDK asociadas a la ciclina D , impidiendo así que la célula progrese de G1 a la fase S .
Otro inhibidor de CDK , la proteína p21 , puede actuar a lo largo del ciclo celular entero.
La p21 está bajo el control de una proteína supresora de tumores, p53 , que controla la salud celular, la integridad del ADN de sus cromosomas y la correcta terminación de las diferentes etapas del ciclo.

Las células del cáncer de mama suelen producir un exceso de ciclina D y ciclina E .
En muchos melanomas, las células epiteliales han perdido el gen que cifra la proteína p16 , encargada de servir de freno.
En la mitad de todos los tipos de tumores humanos la proteína p53 no es funcional.
Y en cánceres de cuello uterino producidos por infecciones de las células con un papilomavirus humano, tanto la pRB como la p53 suelen estar desarticuladas, eliminándose así dos de los principales inhibidores del reloj.
El resultado final en todos los casos es que el reloj empieza a correr sin control, ignorando cualquier señal de alarma externa que le ordene detenerse.
Si los investigadores consiguiesen idear estrategias para bloquear las ciclinas y CDK activas se podría detener el avance de las células cancerosas.

Hasta ahora he comentado dos formas de las que se valen, en condiciones normales, nuestros tejidos para detener la proliferación celular y evitar el cáncer.
Previenen el exceso de multiplicación privando a las células de factores estimuladores del crecimiento o, alternativamente, inundándolas de factores antiproliferación.
Sin embargo, como hemos visto, las células, en su camino hacia el cáncer, suelen ignorar esos controles, ya que se autoestimulan sin respetar las señales inhibidoras.
El cuerpo humano equipa a sus células para esas eventualidades con ciertos sistemas auxiliares que protegen contra la división desenfrenada. 
Pero mutaciones ulteriores del repertorio genético de la célula pueden desarmar también esos sistemas de defensa y dejar paso a la tumoración.

Uno de esos sistemas auxiliares, presente en todas las células humanas, provoca el suicidio celular («apoptosis») si alguno de sus componentes esenciales resulta dañado o si sus sistemas de control se desregulan.
Así, un daño en el ADN cromosómico puede poner en marcha la apoptosis.
Investigaciones recientes indican que la creación de un oncogén o la inutilización de un gen supresor de tumores pueden también inducir esa respuesta.
La destrucción de una célula dañada es un hecho grave para la célula implicada, pero explicable si se contempla al cuerpo en su conjunto, ya que los peligros potenciales que las mutaciones carcinogénicas causan al organismo revisten mayor gravedad que el pequeño precio que se paga por la pérdida de una simple célula. 
Los tumores que aparecen en nuestros tejidos se producirían, por tanto, a partir de las células genéticamente alteradas, muy pocas, que logran evadir el mecanismo de apoptosis conectado a su circuito de control.

En su transformación cancerosa, la célula utiliza varios medios para orillar la apoptosis.
Entre sus muchas funciones, la proteína p53 coadyuva a poner en marcha el suicidio celular. 
Su inactivación en muchas células tumorales reduce la probabilidad de eliminación de las células genéticamente alteradas.
Una producción excesiva de la proteína Bcl-2 , en las células cancerosas, protege eficazmente contra la apoptosis.

Recientemente, los científicos han comprobado que la capacidad para escapar de la apoptosis puede poner en peligro a los pacientes, no sólo porque contribuye a la expansión de un tumor, sino también porque los tumores se vuelven resistentes a la terapia.
Creíase antaño que la radioterapia y muchos fármacos utilizados en quimioterapia aniquilaban directamente las células malignas al provocar estragos en su ADN .
Hoy sabemos que los daños que producen en el ADN tales tratamientos no suelen ser tan arrasadores.
Lo que ocurre es que las células afectadas perciben que el daño no puede repararse fácilmente y ellas mismas se suicidan. 
Este descubrimiento implica que las células cancerosas que han burlado la apoptosis son menos sensibles al tratamiento. 
También sugiere que una terapia capaz de restaurar la capacidad de las células para suicidarse sería buena para combatir el cáncer, y mejoraría la eficacia de las medidas actuales, basadas en la radio y quimioterapia. 

Nuestras células cuentan, además, con una segunda defensa contra la proliferación desenfrenada, muy distinta del programa de apoptosis.
Es un mecanismo capaz de contar y limitar el número total de veces que la célula puede autorreproducirse.

Mucho de lo que se sabe sobre este sistema de protección viene de los estudios con células cultivadas en placas de petri.
Cuando se toman células de un embrión humano o múrido y se cultivan, la población se duplica, aproximadamente, cada día.
Pero después de un número más o menos fijo de duplicaciones (50 o 60 en células humanas), el crecimiento se detiene, momento en el cual se dice que las células se tornan senescentes. 
Esto, al menos, es lo que sucede cuando las células tienen sus genes RB y p53 intactos.
Las células que sufren mutaciones inactivadoras en cualquiera de esos genes continúan dividiéndose cuando sus compañeras normales entran en senescencia.
Con el tiempo, sin embargo, las supervivientes alcanzan una segunda fase, denominada crisis, en que muchas de las células mueren.
Ocasionalmente, alguna célula escapa a esta muerte y se hace inmortal:
ella y sus descendientes se multiplican sin tasa.

Estos eventos implican la existencia de un mecanismo encargado de contar el número de duplicaciones que experimenta una población celular.
En los últimos años, la investigación ha descubierto el mecanismo molecular que lleva cabo la cuenta.
Ciertos segmentos de ADN , denominados telómeros, presentes en los extremos de los cromosomas, son los que computan el número de generaciones de replicación que tienen lugar en una población de células y, en el momento adecuado, inician el proceso de senescencia y de crisis.
Así se limita la expansión indefinida de las poblaciones celulares.

Igual que las puntas plastificadas de los cordones de los zapatos, los telómeros protegen de posibles daños los extremos cromosómicos.
En la mayoría de las células humanas.
Los telómeros se van acortando un poquito cada vez que el cromosoma se replica durante la fase S del ciclo celular. 
Cuando la disminución de los telómeros sobrepasa cierta longitud crítica, suena una alarma que avisa a las células para que inicien la fase de senescencia.
Si las células hacen caso omiso, la progresión en el acortamiento de los telómeros dispara la crisis, ya que el excesivo acortamiento de los telómeros provoca que los cromosomas se fusionen unos con otros o se rompan, creando un caos genético que es fatal para la célula.

Si el sistema de conteo basado en los telómeros funcionase adecuadamente en las células cancerosas, su proliferación excesiva se abortaría mucho antes de que el tumor alcanzase un tamaño muy grande.
La expansión peligrosa sería cortada por el programa de senescencia o, si la célula evade este bloqueo, por el caos cromosómico que se produce en la fase de crisis. 
Pero esta última defensa se rompe durante el desarrollo de la mayoría de las células cancerosas, gracias a la activación de un gen que cifra una telomerasa.

Esta enzima, virtualmente ausente en la mayoría de las células sanas, pero presente en casi todas las células tumorales, reemplaza sistemáticamente los segmentos teloméricos que se pierden en cada ciclo celular. 
La telomerasa mantiene así la integridad de los telómeros y permite que las células se repliquen sin fin. 
La inmortalidad celular que resulta puede ser problemática por dos razones.
Primera, porque hace que los tumores sean mayores: lo que es obvio.
Segunda, porque da tiempo para que las células precancerosas o ya cancerosas acumulen mutaciones adicionales que incrementarán su capacidad replicativa, invasora y. finalmente, de metástasis.

Desde el punto de vista de la célula cancerosa, la producción de una sola enzima es una forma inteligente de derribar la barrera de la mortalidad.
Sin embargo, la dependencia de una sola enzima puede convertirse en su talón de Aquiles.
Si se pudiese bloquear la telomerasa en las células cancerosas, sus telómeros volverían a acortarse en cada división celular.
Las células entrarían en crisis y morirían.
Esa es la razón de que varios laboratorios farmacéuticos traten de desarrollar drogas dirigidas contra la telomerasa. 

Para que un tumor incipiente acumule todas las mutaciones que requiere su desarrollo maligno han de transcurrir, por lo común, varias décadas.
En algunos individuos, sin embargo, ese intervalo temporal se acorta mucho, y contraen ciertos tipos de cáncer mucho antes de lo que suele ser habitual en los tumores en cuestión.
¿Cómo se puede acelerar la formación de un tumor?

En muchos casos, esta temprana aparición se explica porque el paciente hereda de alguno de sus progenitores un gen mutante causante del cáncer.
Cuando un óvulo fecundado empieza a dividirse y replicarse, se copia todo el conjunto de genes que aportan óvulo y espermatozoide y se va distribuyendo a todas las células del cuerpo.
De esta manera, un evento raro, como es una mutación en un gen crítico encargado de controlar el crecimiento, se convierte en ubicuo, ya que la mutación acaba implantándose en todas las células de cuerpo y no sólo en algunas al azar.
En otras palabras, el proceso de formación del tumor se acelera, porque no hay que esperar a que se produzcan sus primeras etapas, las más lentas.
En consecuencia, el desarrollo tumoral, que suele requerir tres o cuatro décadas para llegar a su fin, puede culminarse en sólo una o dos. 
Como esos genes mutantes pasan de generación en generación, muchos miembros de una misma familia pueden desarrollar un cáncer prematuramente.

Ejemplo paradigmático de ello es una variante hereditaria de cáncer de colon.
La mayoría de los casos de cáncer de colon ocurren esporádicamente, como resultado de eventos genéticos aleatorios que se producen durante la vida de una persona.
En ciertas familias, sin embargo, muchos individuos se ven afectados por tumores colónicos prematuros, producidos por un gen heredado.
En los casos esporádicos, una mutación rara silencia a un gen supresor de tumor, denominado APC , en células epiteliales del intestino.
La proliferación de estas células mutantes produce un pólipo benigno que puede convertirse en un carcinoma maligno.
En ciertas familias, algunas formas defectivas de APC pasan de padres a hijos.
Los miembros de esas familias desarrollan cientos e incluso miles de pólipos durante las primeras décadas de sus vidas, y algunos de ellos se convierten en carcinomas.

Va creciendo la lista de síndromes cancerosos familiares que pueden ya relacionarse con la herencia de un gen supresor de tumores mutado.
A este respecto versiones defectivas del gen de la pRB suelen provocar el desarrollo de un cáncer de ojo, o retinoblastoma, en niños.
Esas mismas mutaciones, más adelante, incrementan el riesgo de contraer osteosarcomas (cánceres óseos).
Versiones mutantes heredadas del gen supresor de tumores p53 producen tumores en múltiples sitios, en lo que se conoce como síndrome de Li-Fraumeni.
Y los genes BRCA1 y BRCA2 . recientemente aislados, son responsables de los casos de cánceres de mama de tipo familiar, que constituyen un alto porcentaje de los cánceres de mama premenopáusicos y una proporción también importante de los cánceres de ovario de tipo familiar. 

La aparición temprana de tumores puede explicarse también por mutaciones en otra clase de genes.
Como he dado por sentado, en la mayoría de la gente el cáncer no aparece hasta bien tarde en su vida, o no se desarrolla nunca, porque vienen al mundo con genes «sanos». 
Con los años, sin embargo, nuestros genes se ven atacados por carcinógenos presentes en el ambiente y por agentes químicos que producen nuestras propias células. 
También se pueden introducir errores genéticos cuando las enzimas que replican el ADN durante el ciclo celular cometen errores.
En la mayoría de los casos, esos errores los corrige rápidamente un sistema de reparación que poseen todas las células. 
Si el sistema no repara el error, la mutación correspondiente se perpetúa en las siguientes generaciones celulares.

La alta eficacia reparadora del sistema constituye una de las razones de que puedan pasar muchas décadas antes de que todas las mutaciones necesarias para que se desarrolle un tumor se reúnan, por azar, en una misma célula. 
Ciertos defectos hereditarios, sin embargo, pueden acelerar el desarrollo de un tumor impidiendo que las proteínas encargadas de reparar los daños del ADN actúen con eficacia.
La consecuencia de ello es que las mutaciones que de suyo se acumularían muy lentamente, aparezcan con una frecuencia alarmante por todo el ADN de las células.
Entre los afectados están, inevitablemente, los genes que controlan la proliferación celular.

Ese es el caso de otro tipo de cáncer de colon hereditario, no polipósico.
Los individuos afectados fabrican versiones defectuosas de una proteína implicada en el mecanismo de reparación de los errores causados por el mecanismo de replicación del ADN .
Debido a ello, las células del colon acumulan mutaciones muy deprisa, acelerándose el desarrollo del cáncer unas dos décadas o incluso más.
Las personas afectadas por otro síndrome canceroso familiar, el xeroderma pigmentosum , heredan una copia defectuosa de un gen que dirige la reparación del ADN dañado por las radiaciones ultravioleta.
Estos pacientes son propensos a varios tipos de cánceres de piel inducidos por la luz solar.

De la misma manera, las células de las personas que nacen con un gen ATM defectuoso encuentran dificultades para reconocer la presencia de ciertas lesiones en el ADN y poner en marcha la adecuada respuesta reparadora.
Estas personas son susceptibles a degeneraciones neurológicas, malformaciones de vasos sanguíneos y a una variedad de tumores.
Según algunos investigadores, hasta un 10 por ciento de los cánceres de mama hereditarios pueden corresponder a pacientes con una copia defectuosa de ese gen.

En los próximos diez años, la lista de genes de susceptibilidad al cáncer crecer de manera espectacular merced al Proyecto Genoma Humano (cuyo objetivo es identificar todos los genes presentes en la célula humana).
Junto con el desarrollo de técnicas de análisis de ADN cada vez más poderosas, el conocimiento de esos genes nos permitirá predecir qué miembros de las familias propensas al cáncer correrán riesgo de contraerlo y cuáles serán los afortunados que han heredado copias intactas de esos genes.

Aunque es mucho lo que ya sabemos de las bases genéticas de la proliferación celular incontrolada, apenas conocemos los rudimentos de los genes mutantes que contribuyen a los estadios posteriores del desarrollo tumoral, en particular de los que permiten a las células tumorales atraer a los vasos sanguíneos para nutrirse, para invadir los tejidos adyacentes y para llevar a cabo la metástasis.
Pero las investigaciones en estas áreas están progresando con celeridad. 

Estamos ya cerca de poder escribir la historia pormenorizada de muchos tumores humanos, desde el comienzo hasta su final fatal, biografías que se escribirán en el lenguaje de los genes y las moléculas.
De aquí, a diez años conoceremos con suma precisión la sucesión de eventos que integran esa compleja transformación de células normales en malignas.

Entenderemos entonces por qué ciertas masas locales nunca progresan más allá de su estado benigno y no invasivo.
Estas formas de crecimiento benignas se encuentran en casi todos los órganos del cuerpo.
Quizá también comprenderemos por qué unos genes mutantes contribuyen a la formación de algunos tipos de cánceres, pero no a otros.
Por ejemplo, en los retinoblastomas, cánceres de vejiga y carcinomas de pulmón aparecen con frecuencia versiones mutantes del gen supresor de tumores RB , lo que ocurre sólo ocasionalmente en los carcinomas de colon y mama.
Muy probablemente, muchas de las soluciones a estos misterios vendrán de las investigaciones en biología del desarrollo (embriología). 
Después de todo, los genes que gobiernan el desarrollo embrionario son, más tarde,las fuentes de nuestras malignidades. 

La cantidad de información acumulada en las dos últimas décadas sobre los orígenes del cáncer no tiene parangón en la historia de las investigaciones biomédicas.
Parte de estos conocimientos se están ya aprovechando para el desarrollo de herramientas moleculares que nos permitan detectar y determinar la agresividad de ciertos tumores.
No obstante, a pesar de tantos progresos, resulta difícil pensar en nuevas terapias curativas.
Una de las razones es que las células tumorales difieren muy poco de las sanas.
Sólo una pequeña parte de las decenas de miles de genes que tiene una célula sufren daños durante la transformación maligna. 
Significa ello que el amigo normal y el enemigo maligno están hechos de la misma madera, y los disparos dirigidos contra el adversario pueden lesionar también al compañero. 

Pese a todo, el signo de la batalla está cambiando. 
Las diferencias entre células normales y cancerosas, aunque sutiles, son reales.
Y algunas características únicas de los tumores son excelentes dianas para ciertos fármacos de creación reciente.

Etapas del desarrollo de un tumor

Representación esquemática del desarrollo de un tumor maligno de tejido epitelial.
Los cánceres epiteliales, las especies más comunes, se denominan carcinomas.
La masa que se ve aquí resulta de mutaciones operadas en cuatro genes, aunque puede variar el número de genes implicados en los tumores reales.

1. El desarrollo del tumor comienza cuando una célula ( naranja ) de una población normal ( beige ) sufre una mutación genética que refuerza su tendencia a proliferar, siendo así que, en condiciones de normalidad, debería estar quiescente

2. La célula alterada y su progenie conservan su apariencia normal, pero se reproducen en exceso; experimentan, pues, un fenómeno de hiperplasia.
Al cabo de los años, una de entre un millón ( rosa ) sufre otra mutación que le mina, todavía más, el control del crecimiento celular

3. Además de una proliferación desmesurada, la progenie de esta célula presenta un aspecto anormal en su morfología y orientación.
Se dice que el tejido presenta displasia.
De nuevo, transcurrido cierto tiempo, se produce una mutación poco frecuente que altera el comportamiento celular ( morado )

4. Las células afectadas muestran anomalías crecientes en su desarrollo y aspecto.
Si el tumor no ha traspasado aún ninguna barrera para invadir otro tejido, se habla de un cáncer in situ .
El tumor puede permanecer así indefinidamente. 
Sin embargo, algunas células pueden sufrir nuevas mutaciones ( azul )

5. Si los cambios genéticos facilitan la invasión, por el tumor, de tejido circundante y la entrada de las células en el torrente sanguíneo o en la linfa, calificamos entonces como maligna masa tumoral.
Las células invasoras pueden iniciar nuevos tumores en otras partes del cuerpo (metástasis), que pueden ser letales si afectan a un órgano vital


Figura 1

CASCADAS DE SEÑALES que, en las células normales, transmiten los mensajes de control del crecimiento desde la superficie externa hasta el núcleo

Allí, un complejo molecular, el reloj del ciclo celular, recoge todos los mensajes y decide si la célula debe o no dividirse.
Las células cancerosas proliferan desmesuradamente porque algunas mutaciones genéticas hacen que la cascada estimuladora ( verde )envíe demasiados señales de «marcha» o porque las cascadas inhibidoras ( rojo ) dejan de enviar señales de «parar».

Una cascada estimuladora se vuelve hiperactiva si la mutación hace que algún componente, como el receptor de un factor de crecimiento( recuadro de la izquierda ), envíe mensajes estimuladores de forma autónoma, sin esperar a las órdenes que en situación de normalidad les envían otros componentes situados curso arriba del proceso.
En opción alternativa, las cascadas inhibidoras se bloquean cuando se elimina algún componente, un transmisor citoplasmático ( recuadro de la derecha ), y se rompe la cadena de señales.

Algunos genes implicados en cánceres humanos

Los protooncogenes cifran proteínas que estimulan la división celular.
Sus formas mutadas, los oncogenes, pueden determinar que las proteínas estimuladoras sean hiperactivas y que las células proliferen en demasía. 
Los genes supresores de tumores cifran proteínas que inhiben la división celular.
Las mutaciones pueden hacer que las proteínas se inactiven y priven, por tanto, a las células de unos frenos necesarios para evitar la proliferación.
Los investigadores se esfuerzan por descifrar las funciones específicas de muchos genes supresores de tumores.

- PDGF: cifra el factor de crecimiento derivado de plaquetas.
Implicado en los gliomas (un cáncer de cerebro)

- erb-B: cifra el receptor del factor de crecimiento epidérmico.
Implicado en el glioblastoma (un cáncer de cerebro) y en el cáncer de mama

- erb-B2, también denominado HER-2 o neu.
Cifra un receptor de factor de crecimiento.
Implicado en cánceres de mama, glándulas salivales y ovario 

- RET: cifra un receptor de factor de crecimiento.
Implicado en el cáncer de tiroides


- Ki-ras: implicado en cánceres de pulmón, ovario, colon y páncreas
- N-ras: implicado en leucemias

- c-myc: implicado en leucemias y cánceres de mama, estómago y pulmón 
- N-myc: implicado en neuroblastomas (cánceres de neuronas) y glioblastomas
- L-myc: implicado en el cáncer de pulmón
- Bcl-2: cifra una proteína que, en situación de normalidad, bloquea el suicidio celular.
Implicado en linfomas de células B.

- Bcl-1, también denominado PRAD1.
Cifra la ciclina D1, un componente estimulador del reloj del ciclo celular.
Implicado en cánceres de mama, cabeza y garganta.

- MDM2: cifra un antagonista de la proteína supresora de tumores p53.
Implicado en sarcomas (cánceres de tejido conectivo) y otros tumores.


- APC: implicado en cánceres de colon y estómago
- DPC4: cifra una molécula transmisora de un cascada de señales que inhiben la división celular.
Implicado en cánceres de pancreasa.

- NF-1: cifra una proteína que inhibe a una proteína estimuladora (Ras).
Implicado en neurofibromas, feocromocitomas (cánceres del sistema nervioso periférico) y leucemia mieloide.

- NF-2: implicados en meningiomas, ependimomas (cánceres de cerebro) y schwannomas (afectan a los revestimientos de los nervios periféricos)

- MTS1: cifra la proteína p16, uno de los componentes-freno del reloj del ciclo celular.
Implicado en muchos cánceres 

- RB: cifra la proteína pRB , uno de los frenos principales del ciclo celular.
Implicado en retinoblastomas y cánceres óseos, de vejiga,pulmón y mama.

- p53: cifra la proteína p53, que puede detener la división celular e inducir a las células anormales a suicidarse.
Implicado en muchos cánceres

- WT1: implicado en el tumor de Wilms (riñón) 

- BRCA1: implicado en cánceres de mama y ovario
- BRCA2: implicado en cáncer de mama
- VHL: implicado en cánceres de células renales 




Reloj del ciclo celular y cáncer 

Si no todos, la mayoría de los cánceres humanos crecen sin mesura no sólo porque se perturban las cascadas de señales celulares, sino también porque el reloj del ciclo celular se vuelve loco.
El reloj, formado por un conjunto de proteínas nucleares que interaccionan entre si, integra, en condiciones normales, mensajes procedentes de las cascadas estimuladoras e inhibidoras y, si prevalecen los mensajes estimuladores, pone en marcha el programa de división celular.
La progresión a través de las cuatro etapas del ciclo celular (a) depende en gran medida de que se alcancen niveles elevados de ciclinas, unas proteínas: primero la de tipo D , luego la E , A y finalmente, la B .

Un momento crucial del ciclo tiene lugar al final de la fase G1 , en el punto restrictivo ( R ), cuando la célula decide si debe o no cerrar el ciclo.
Para que la célula pase a través de ese punto y entre en la fase S , es preciso que un conmutador molecular pase del estado apagado («off») a encendido («on»).
Tal conmutación procede según se detalla (b):
conforme suben los niveles de ciclinas D y E , estas proteínas se combinan con unas quinasas dependientes de ciclinas, y las activan (1).
Estas enzimas (mientras forman parte del complejo quinasa-ciclina) arrebatan grupos fosfatos (2) de las moléculas de ATP (trifosfato de adenosina) y los transfieren a una proteína pRB , el freno maestro del reloj del ciclo celular.
Si pRB no está fosforilada, bloquea el ciclo celular (conmutador en posición «off») secuestrando otras proteínas, los denominados factores de transcripción.
Pero cuando el complejo ciclina-quinasa añade suficientes fosfatos a pRB , El freno deja de actuar (3,abajo) y libera los factores de transcripción, que quedan libres para actuar sobre los genes (3, arriba).
Los factores liberados estimulan la producción de varias proteínas requeridas en la continuación del ciclo celular.

En la figura c se resumen las múltiples interacciones moleculares que regulan el ciclo celular.
Encima del punto R se pueden ver los cambios que ocurren para que el conmutador molecular esté en posición de encendido («on»).
En ciertos cánceres se ha detectado una sobreactividad de proteínas estimuladoras, las ciclinas D y E y la quinasa CDK4 .
También, la inactivación de algunas proteínas inhibidoras. 
Entre las proteínas afectadas se encuentran p53 (ausente o ineficaz en más de la mitad de todos los tipos de tumores), pRB , p16 y pl5 .
El efecto neto de cualquiera de esos cambios es la desarticulación del reloj y, en consecuencia, una proliferación celular excesiva.

Figura 2

CROMOSOMAS HUMANOS de una célula normal ( arriba ) dispuestos por parejas idénticas

Sólo se muestran las parejas del 8 al 18 Por contra, los cromosomas de una célula de cáncer de cuello uterino presentan machas anomalías ( abajo ).
Con el cromosoma 8, por ejemplo, se observan tres alteraciones: mayor número de copias, deleción de material genético en algunas de ellas, y la rotura y posterior empalme de dos segmentos que no pertenecen al mismo cromosoma ( copia de la derecha ).
La pérdida de copias, como en el caso del cromosoma 13, es también frecuente.
Estos cambios favorecen la progresión de un tumor si activan un oncogén, incrementan el número de copias de un oncogén o eliminan un gen supresor de tumores.


Problemas de la terapia génica 

La idea de utilizar genes para tratar enfermedades constituye un objetivo apremiante, pero queda mucho camino que recorrer antes de alcanzar esa meta.

A finales del siglo XX, los colegas de Daniel H., Burnham, arquitecto de algunos de los primeros rascacielos modernos, se mostraban escépticos ante la idea de levantar edificios que se perdiesen en las nubes.
Según se cuenta, Burnham les animaba a rebelarse contra los «proyectos de poca monta», faltos de «magia para hacer bullir la sangre».
Les incitaba a transgredir los límites de la arquitectura tradicional, a pensar lo inconcebible y a perder el miedo a la imaginación desbocada.
En definitiva, a revolucionar la arquitectura.

En medicina se han producido también cambios revolucionarios en los últimos siglos.
Nuestra praxis y conocimientos deben muchísimo a la introducción de la microscopía, anestesia, vacunación, antibióticos y al desarrollo de las técnicas de trasplantes.
La medicina se prepara ahora para acometer otro salto histórico, para entrar en una era en que se emplearán de forma rutinaria los genes en la sanación o alivio de enfermedades de todo tipo, heredadas y adquiridas.

Pero ese día aún no ha llegado.

Ciertos investigadores, industriales y periodistas han puesto un énfasis excesivo en las expectativas y han silenciado las dificultades o restado su importancia.
Con semejante actuación, han sembrado la idea de que la terapia génica era ya un hecho y su aplicación generalizada algo a la vuelta de la esquina.
Y no es verdad.

En su parte teórica, la revolución de la terapia génica sí se ha producido.

Siempre que se descubre un nuevo gen, la pregunta inmediata es la relativa a sus posibilidades de aplicación en el tratamiento de alguna enfermedad, aun cuando existan métodos tradicionales para hacerle frente.
Pero la parte técnica de la revolución, su capacidad real de corregir las enfermedades, eso es harina de otro costal.
Se han cubierto ya las primeras etapas.
Se ha demostrado que puede conseguirse que los genes transferidos funcionen en el cuerpo humano, y lo hagan, a veces, durante años.
Sin embargo, ningún ensayo ha logrado todavía restablecer la salud de uno solo de los más de 2000 pacientes que participan en las pruebas de terapia génica que se desarrollan por todo el mundo.

Esta falta de eficacia terapéutica convincente resulta preocupante.
Pero sería un error dudar del potencial futuro de la terapia génica.
No se olvide que nos hallamos ante un campo muy joven.
En los Estados Unidos, los ensayos con pacientes tienen menos de diez años.

Interpretaremos con mayor veracidad esa carencia de efectos clínicos espectaculares si la consideramos reflejo de los titubeos iniciales de los investigadores en la utilización de una técnica nueva y difícil y si reconocemos que los obstáculos son mayores de lo esperado.

Uno de los retos, apuntaba en 1995 un informe federal sobre la investigación en terapia génica, estriba en refinar los métodos empleados para insertar genes terapéuticos en las células.

Con frecuencia, los genes que se introducen en los pacientes no llegan a un número suficiente de células adecuadas o, por razones no siempre claras, funcionan mal, cuando no dejan de hacerlo pasado un tiempo.
En esas condiciones, un gen potencialmente beneficioso tendría pocas posibilidades de influir en el desarrollo de una enfermedad.

Me limitaré a describir algunos de los principales obstáculos técnicos que impiden el éxito de la transferencia génica; expondré, asimismo, las estrategias abordadas para superar tales dificultades.
Me ceñiré a las terapias que afectan a las células somáticas, las que no son ni espermatozoides ni óvulos. 
De momento, las investigaciones sobre terapia génica humana han evitado las manipulaciones que pudieran afectar, de forma involuntaria quizás, a los descendientes de los individuos tratados.

Para entender los obstáculos con que se enfrenta la terapia génica es preciso conocer al menos los rudimentos sobre el funcionamiento de los genes y sobre las líneas generales de la terapia génica que se pretende.
Un gen de una célula humana es un segmento de ADN que, en la mayoría de los casos, cifra la información para fabricar una proteína específica.
Todas las células del cuerpo portan los mismos genes en los cromosomas que están en el núcleo.
Pero las células nerviosas, por ejemplo, no se comportan igual que las hepáticas.
Células distintas utilizan, o expresan, subgrupos diferentes de genes y, por tanto, fabrican grupos diversos de proteínas (principales operarios celulares). 

Dicho con mayor exactitud, cada célula copia sólo un grupo selecto de genes en moléculas de ARN mensajero, que son las que sirven de molde para la síntesis de proteínas. 

Si un gen particular muta, su producto proteínico puede no fabricarse, funcionar poco o hacerlo de forma demasiado agresiva.
En cualquier caso, el defecto puede alterar funciones vitales de las células y tejidos que utilizan el producto génico normal y, en consecuencia, originar alguna enfermedad.

Históricamente, el tratamiento que han dado los médicos a las enfermedades debidas a mutaciones genéticas hereditarias no ha ido en el sentido de alterar los genes, sino en el de intervenir sobre las consecuencias biológicas de la mutación.

Por ejemplo, en caso de fenilcetonuria, patología en la que la pérdida de un gen determina la acumulación de productos tóxicos procedentes del metabolismo de la fenilalanina, se prescriben dietas muy restrictivas, carentes de ese aminoácido.

Por desgracia, la eficacia de las intervenciones no genéticas ante enfermedades hereditarias suele ser parcial.

A principios de los años setenta, esa observación, sumada al conocimiento cada vez mayor del funcionamiento de los genes y al descubrimiento de los genes responsables de muchas enfermedades, indujo a pensar en las posibilidades de obtener mejores resultados atacando a las enfermedades congénitas en su origen.

Entre las enfermedades genéticas investigadas recordaremos la fibrosis quística (que afecta principalmente a los pulmones), la distrofia muscular, la deficiencia en desaminasa de adenosina (que debilita el sistema inmunitario) y la hipercolesterolemia familiar (que determina una aparición temprana de aterosclerosis grave).

Para sorpresa de todos, andando el tiempo se comprobó que las propias enfermedades adquiridas presentaban a menudo un componente genético que, en teoría, podría ser blanco de una estrategia genética de corrección de la enfermedad.

La verdad es que más de la mitad de todos los ensayos clínicos de terapia génica que se están llevando a cabo se refieren al cáncer, que en la mayoría de los casos no es hereditario, sino consecuencia de daños genéticos acumulados desde el nacimiento.
Otros ensayos se centran en el sida, causado por el virus de inmunodeficiencia humana ( VIH).

En principio, un gen normal podría insertarse directamente en un cromosoma de una célula, donde ocupara el lugar de su versión defectuosa.
En la práctica, sin embargo, no podemos realizar todavía en una persona semejante sustitución.
Mas, para nuestra fortuna, no siempre se requiere tal procedimiento.
La mayoría de los ensayos de terapia génica introducen el gen sano en un tipo celular seleccionado, para compensar la falta o mal funcionamiento de la versión endógena, o para crear una propiedad nueva.
Muchas de las terapias génicas propuestas contra el cáncer trabajan en esa línea; se proponen que las células cancerosas fabriquen sustancias que sean tóxicas para ellas mismas, provoquen una enérgica respuesta del sistema inmunitario o corten el suministro de sangre que los tumores necesitan para seguir desarrollándose. 

Algunos grupos están también ensayando estrategias para compensar mutaciones genéticas que determinan la producción de proteínas destructivas.
Así ocurre en la terapia antisentido, planteamiento que se funda en la producción de cadenas cortas de ADN sintético que se unen a los transcritos de ARN mensajero de los genes mutantes, impidiendo, por ende, su traducción en proteínas anormales.
Otras tácticas parecidas utilizan ribozimas, pequeñas moléculas de ARN que degradan los ARN mensajeros de los genes aberrantes.
Un caso diferente nos lo ofrecen los genes que fabrican anticuerpos intracelulares, que bloquean la actividad de la proteína mutante.

Por fin, hay estrategias terapéuticas que se basan en el diseño de moléculas híbridas de ADN y ARN que podrían dirigir la reparación del gen mutado.

Contamos con dos formas principales de suministrar genes a los pacientes.
En ambos procedimientos los genes se introducen de antemano en vectores, moléculas que transportan los genes foráneos hasta el interior de las células. 
En el método usual, se extraen células de un tejido seleccionado del paciente, se incuban en el laboratorio con los vectores que transportan los genes y se reimplantan las células corregidas genéticamente en el paciente (terapia ex vivo ).
En otros casos, se introducen los vectores directamente en el cuerpo, de ordinario en los tejidos que se van a tratar (terapia in vivo ).
El objetivo último, por supuesto, es conseguir que los vectores penetren en el torrente sanguíneo o en otros sitios y encuentren su camino hacia las células deseadas, por ejemplo, órganos que son difíciles de alcanzar o depósitos cancerosos ocultos.
No se dispone aún de ninguno de esos transportadores dirigidos para ensayarlo en pacientes, pero se progresa por días en ese sentido.

En el cuerpo, ciertos genes sólo son útiles si su expresión está regulada de forma precisa. 
Dicho de otra manera: deben producir la cantidad justa de proteína en el momento adecuado.
No se ha alcanzado todavía ese grado de precisión en los genes foráneos introducidos en el cuerpo.

Para muchos casos de terapia génica, sin embargo, no resulta imprescindible una regulación tan exquisita. 
No siempre será obligado introducir los genes en las mismas células que los van a necesitar.
A veces, algunos tipos celulares más accesibles (por ejemplo, el músculo o la piel) podrían convertirse en factorías de proteínas.

Esas centrales liberarán las proteínas que otras células cercanas demandan o segregarán proteínas al torrente sanguíneo para su traslado hasta sitios más alejados.

La clave del éxito de cualquier estrategia reside en el vector, en su capacidad de transportar genes de una forma segura y eficaz.
Desde un principio, los virus, que son poco más que genes que se autorreplican envueltos en una cubierta de proteínas, han sido los vectores potenciales que han recabado mayor atención.
Por una razón comprensible:
la evolución los ha diseñado específicamente para que entren en las células y expresen allí sus genes.
Además, podemos sustituir algunos de los genes implicados en la replicación vírica y en la virulencia por uno o más genes potencialmente terapéuticos.
En teoría, un virus así manipulado y alterado introduciría los genes beneficiosos en las células, pero no se multiplicaría ni causaría enfermedades.

Los virus más socorridos son los retrovirus, que introducen sus genes, de forma permanente, en los cromo somas de las células que invaden.

Los genes integrados se copian y transmiten a todos los descendientes de esas células.
Muchos otros tipos de virus, sin embargo, no integran su material genético en los cromo somas de sus hospedadores.
Sus genes acostumbran comportarse en el cuerpo de una manera más fugaz, en parte porque no se replican cuando las células receptoras se dividen.

Un grupo ideal de células diana para los vectores basados en retrovirus son las células madre o troncales, que persisten indefinidamente y producen también descendientes especializados.
Las células hematopoyéticas, por ejemplo, originan todos los tipos celulares de la sangre (eritrocitos, leucocitos del sistema inmunitario, etc.) y reconstituyen la sangre cuando es necesario; también producen copias de sí mismas.
El problema está en que resulta sumamente difícil identificar células madre humanas y modificarlas de un modo predecible y seguro.

Pese a su indudable atractivo, los retrovirus, que empezaron a utilizarse como vectores a principios de los años ochenta, plantean algunos problemas.
Son promiscuos y depositan sus genes en los cromosomas de muchos tipos celulares distintos. 
Esta falta de especificidad en cuanto al tipo de célula hospedadora constituye un claro inconveniente contra el uso directo de esos vectores en el cuerpo humano.
Podrían provocar que los genes foráneos penetraran en células inadecuadas, con lo que disminuiría la eficacia de incorporación en las células diana; sin mencionar, además, los efectos fisiológicos no deseados que ello comportaría. 
Por otra parte, los retrovirus con los que se trabaja actualmente no pueden transferir genes a tipos celulares incapaces de dividirse o que lo hacen raramente (como las neuronas maduras y las células del músculo esquelético). 
Los vectores retrovíricos en uso se integran en los cromosomas sólo cuando las membranas que rodean al núcleo de las células hospedadoras se disuelven, lo que sólo ocurre durante la división celular. 

Otro problema que plantean los retrovirus es que integran su ADN al azar en el cromosoma de la célula hospedadora, en vez de hacerlo en sitios predecibles.
Según donde se insiera, puede destrozar un gen esencial o alterarlo de suerte tal que induzca un desarrollo canceroso.
La posibilidad de que se produzca un tumor puede ser remota, pero aun así tal riesgo no debe despreciarse.

Se han registrado ciertos avances encaminados a paliar las carencias de los retrovirus en su tarea vectora de genes. 
Así, para incrementar la especificidad y hacer que los retrovirus, una vez en el cuerpo, se dirijan a determinados tipos celulares, se investiga en la modificación de la cubierta del virus (la superficie exterior).
Como ocurre con otros virus, los retrovirus sólo introducen su carga genética en el interior de una célula si unas proteínas de su superficie encuentran sitios específicos a los que unirse (receptores), que se hallan en la superficie celular. 
La unión de las proteínas víricas con los receptores celulares permite que la cubierta externa del retrovirus se funda con la membrana celular y pueda así introducir genes víricos y proteínas en el interior de la célula.
Para conseguir retrovirus más selectivos en relación con las células que invade, se pretende reemplazar o modificar las proteínas naturales de la cubierta, así como añadir nuevas proteínas o porciones de proteínas a las ya existentes.

En un experimento que demostraba la viabilidad de la estrategia de sustitución, Jiing-Kuan Yee, de la Universidad de California en San Diego, sustituyó la proteína de la cubierta del virus de la leucemia de ratón por la del virus de la estomatitis vesicular humana.
(El virus de ratón, que no causa ninguna enfermedad conocida en humanos, es el retrovirus con más posibilidades de convertirse en vector en terapia génica.) 
El retrovirus de ratón alterado infectó células que portaban receptores para el virus de la estomatitis vesicular humana, pero dejó intactas las que portaban receptores para el propio virus de ratón.

También se han cosechado importantes progresos en las investigaciones encaminadas a modificar las proteínas ya existentes en la cubierta vírica.
El grupo encabezado por Yuet Wai Kan, de la Universidad de San Francisco, han unido una hormona proteínica a la proteína de la cubierta del virus de la leucemia de ratón.
Esa hormona permitía al virus infectar células humanas que presentaban el receptor para esa hormona.

Se confía en que, pronto, se preparen vectores retrovíricos capaces de insertar genes terapéuticos en los cromosomas de células que no se dividen.
Inder M., Verma, Didier Trono y su equipo, del Instituto Salk, han aprovechado la capacidad que tiene el VIH (un retrovirus) para depositar sus genes en el núcleo de células del cerebro que no se dividen, sin esperar a que la membrana nuclear se disuelva durante la división celular.

Sustituyeron los genes responsables de la reproducción del VIH por un gen para una proteína cuyo rastro podía seguirse bien.
El vector trasladó luego ese gen hasta células que no se replicaban; lo hizo primero en cultivos y luego cuando se inyectó directamente en el cerebro de ratas.
El VIH podría convertirse en vehículo eficaz si se consigue descartar totalmente la posibilidad de que los vectores manipulados se tornen patogénicos.
Otra posible táctica consiste en transferir ciertos genes útiles del VIH, en particular los que cifran las proteínas que transportan los genes hasta el núcleo, a retrovirus que no producen enfermedades en humanos.

Por último, hay quienes investigan sobre cómo recortar la aleatoriedad con que los vectores retrovíricos insertan genes en los cromosomas humanos.
Se parte del conocimiento adquirido en las levaduras y otros organismos, donde se puede integrar genes en sitios predecibles de su ADN.

Los vectores víricos, retrovíricos aparte, presentan sus propias ventajas y desventajas.
Los que se fundan en adenovirus humanos, ubicuos, constituyen la opción alternativa a los retrovirus más acariciada, en parte por su notable seguridad.
Lo peor que puede esperarse de las formas habituales de adenovirus es un resfriado de pecho. 
Además, infectan fácilmente a las células humanas y, al menos inicialmente, tienden a producir elevados niveles de la proteína terapéutica.

Los vectores adenovíricos, aunque introducen genes en el núcleo, no los insertan en los cromosomas.
Se evita así la posibilidad de que se alteren genes vitales o de que coadyuven al desarrollo de un cáncer.
Tienen en su contra que, para algunas aplicaciones, la eficacia de los genes es temporal.

Puesto que el ADN acaba desapareciendo, los tratamientos de la fibrosis quística y otras enfermedades crónicas deberían repetirse con cierta periodicidad (de meses o años).
En algunas situaciones, sin embargo, en las que una proteína sólo se necesita transitoriamente para inducir una respuesta inmunitaria al cáncer o a un patógeno, puede resultar deseable que el gen foráneo sólo se exprese durante un breve intervalo.
Otro inconveniente, compartido con los retrovirus, es la falta de especificidad celular.
No obstante, igual que acontece con los retrovirus, se trabaja en métodos para encarrilar los vectores adenovíricos hacia los tejidos deseados. 

De momento, el principal obstáculo que se opone al uso de vectores adenovíricos en pacientes estriba en la enérgica respuesta inmunitaria que provocan.
Tras un primer tratamiento, esos vectores podrían infectar las células apropiadas y generar grandes cantidades de las proteínas deseadas.

Pero, enseguida, las defensas del hospedador se ponen en marcha, matando a las células alteradas e inactivando a sus nuevos genes.
Una vez, alertado el sistema inmunitario contra los virus, éstos no tardan en quedar eliminados en cuanto se introducen por segunda vez.
A ese tipo de respuestas podría atribuirse la caída de la expresión génica en muchos de los estudios de transferencia génica mediada por adenovirus realizados con pacientes.
El conocimiento preciso de las limitaciones que presentan los adenovirus está permitiendo avanzar en el desarrollo de una nueva generación de vectores, diseñados para reducir las interferencias defensivas.
Se han alcanzado ya algunas mejoras desechando o matando los genes adenovíricos que ejercen un mayor protagonismo en la repuesta inmunitaria.

También se están estudiando las posibilidades vehiculares que ofrecen otros virus.
Me refiero a los virus asociados con los adenovirus, los herpesvirus, alfavirus y poxvirus.
Aunque ninguno reúne todas las perfecciones, es posible que exista alguna aplicación terapéutica para cada uno de ellos.
Por ejemplo, el interés de los virus asociados con los adenovirus reside en que no producen enfermedades conocidas en el hombre.
Además, sus formas naturales integran sus genes en los cromosomas humanos. 
Pueden mostrarse útiles en algunas aplicaciones que ahora se reservan a los retrovirus; ahora bien, al ser más pequeños, presentan el inconveniente de su incapacidad para incorporar genes grandes.
Los herpesvirus, por contra, no integran sus genes en el ADN de sus hospedadores; pero son atraídos hacia las neuronas, algunas de las cuales retienen a los virus en un estado más o menos inocuo durante toda la vida de las personas afectadas.

De ahí que se haya pensado en los herpesvirus para emplearlos como vectores en las terapias de enfermedades neurológicas. 

En conjunto, los vectores víricos siguen siendo muy prometedores, aunque debamos extremar las precauciones para asegurarnos de que los virus no experimentarán cambios que induzcan enfermedades.
Esta y otras consideraciones han potenciado el desarrollo de métodos que prescinden del recurso a los virus para transferir genes terapéuticos. 
Al igual que los virus, esos agentes sintéticos son básicamente ADN combinado con moléculas que pueden condensar el ADN, introducirlo en las células y protegerlo de su posible degradación en el interior celular. 
Y, lo mismo que los vectores víricos, es casi seguro que se utilizarán en la práctica médica cuando logren refinarse mucho más.

En el laboratorio, los genes transferidos por los vectores no víricos se integran en los cromosomas de las células receptoras; su eficacia es, sin embargo, muy baja cuando se introducen en el cuerpo.
El que la falta de integración sea una ventaja o una desventaja dependerá, como ya hemos mencionado, de los objetivos particulares de la terapia.

Se llaman liposomas unas perlas pequeñas de grasa. 
En ellos se viene investigando desde hace casi tanto tiempo como en los retrovirus.
Esas esferas sintéticas pueden prepararse para que encierren en su interior un plásmido, un bucle de ADN estable, que porte los genes terapéuticos. 
La transferencia génica mediada por liposomas ("lipoplejos") es mucho menos eficaz que la transferencia mediada por virus, pero se ha desarrollado lo suficiente como para que ensayarla en tratamientos del cáncer y la fibrosis quística. 
Mientras tanto, para resolver el problema de la eficacia, se están estudiando los efectos de la alteración de la composición química de los liposomas; de hecho, se han empezado a crear vectores que remedan a los virus en su capacidad para alcanzar dianas específicas y en su destreza para transferir genes.

Otros vectores utilizan revestimientos no lipídicos para proteger el ADN.
Algunos de esos revestimientos están formados por polímeros de aminoácidos y otras sustancias cuya función es dirigir los genes terapéuticos hacia las células adecuadas en el cuerpo y proteger a los genes de la degradación por enzimas celulares.
Esos complejos, investigados por Max Birnstiel y Matt Cotten, del Instituto de Patología Molecular de Viena, y por David T. Curiel, de la Universidad de Alabama en Birmingham, se han comportado bien en cultivos celulares.
Ya se están perfeccionando para su uso en animales y en pacientes.

Hay quienes están experimentando con la posibilidad de la inyección directa de ADN desnudo, esto es, sin la envoltura lipídica, en los pacientes.

De sus resultados iniciales se desprende que se trata de una estrategia prometedora en la inmunización contra enfermedades infecciosas e incluso contra ciertos tipos de cáncer. 

También se buscan alternativas a los plásmidos. 
En particular se están preparando cromosomas en miniatura, o cromosomas humanos artificiales, en los que se puedan incluir los genes terapéuticos.
Esas construcciones llevan la mínima cantidad de material genético necesaria para evitar su degradación en el núcleo y su pérdida durante la división celular.

Incorporan, asimismo, elementos que permiten la replicación fiel de los cromosomas artificiales cada vez que una célula se divide, igual que lo hacen los cromosomas normales.

En el futuro, como ahora, los investigadores elegirán el método de transferencia génica en función de los objetivos terapéuticos.
Si un paciente ha heredado un defecto genético y necesita un aporte continuo del producto génico normal durante toda su vida, la mejor opción puede ser un vehículo que integre de forma estable el gen terapéutico en los cromosomas el paciente. 
En ese caso, se elegirá un retrovirus o un virus asociado a los adenovirus.
Si sólo se necesita una actividad génica transitoria, como para estimular al sistema inmunitario contra células cancerosas o agentes infecciosos, lo más adecuado puede ser un vehículo no integrativo, como los adenovirus, liposomas o incluso el ADN desnudo.

Pero, casi con toda seguridad, las herramientas del futuro no van a ser los prototipos que se están ensayando hoy en los laboratorios.
Y no habrá una técnica ideal para cada enfermedad, sino que existirán muchas opciones.
Los sistemas de transferencia génica ideales serán los que combinen las mejores características de diferentes vectores.
Cada sistema se adaptará al tejido específico o tipo celular que requiera la modificación, a la duración precisa de la actividad génica y al efecto fisiológico deseado del producto génico. 
La ciencia se afanará por desarrollar procedimientos para alterar el nivel de expresión génica a voluntad y de inactivar o eliminar los genes introducidos si la terapia ha fracasado.

Pero los problemas no habrán acabado, ni siquiera cuando se consiga perfeccionar los vectores para transferir genes. 
Sabemos, por ejemplo, que las células modifican a veces los genes foráneos y los inactivan.

En ello se trabaja, pero no se ha encontrado todavía la solución.
Tampoco conocemos cómo responderán los sistemas defensivos de los pacientes ante la presencia de una proteína procedente de un gen terapéutico. 
Para impedir una reacción inactivante por parte del sistema inmunitario, los médicos quizá tengan que inyectar a los pacientes fármacos antirrechazo o inducir una tolerancia inmunitaria frente a la proteína foránea, realizando la terapia génica en un momento muy temprano de la vida del paciente (antes de que su sistema inmunitario haya madurado).

Aunque he insistido en los obstáculos técnicos con que se enfrenta la terapia génica, no por ello dejo de ser muy optimista en cuanto a los resultados esperanzadores, que quizá pronto veamos en ciertas enfermedades.
Nuestras herramientas mejoran rápidamente y algunos de los ensayos clínicos que se llevan a cabo están a punto de demostrar su eficacia terapéutica, incluso con las limitaciones de las técnicas actuales.
En especial, las inmunoterapias basadas en la utilización de genes, aplicadas contra ciertas condiciones malignas, como los neuroblastomas y melanomas, parece que van a demostrar convincentemente su eficacia a la hora de reducir el desarrollo de la patología y de promover la regresión de los tumores ya formados. 
Ello permitirá aumentar la eficacia de las terapias ya existentes.
Pero debo insistir en que sólo con una ciencia rigurosa, estudios clínicos cuidadosamente diseñados y una más sosegada difusión de los resultados experimentales, los investigadores podrán asegurar el oportuno, ético y eficaz desarrollo de este nuevo y excitante campo de la medicina.

Figura 1

LA INTRODUCCIÓN DE GENES en humanos se puede llevar a cabo encarrilando directamente ( flecha naranja ) los vectores (agentes que portan los genes con capacidad terapéutica) hacia los tejidos deseados ( in vivo )

No obstante, la estrategia ex vivo ( flechas azules ) es más frecuente.

En ella, los médicos extraen células de un paciente, les agregan en el laboratorio el gen deseado y las vuelven a introducir en el paciente una vez corregidas genéticamente. 
Una de las estrategias in vivo actualmente en estudio se basa en la utilización de vectores idóneos, que se inyecten en la corriente sanguínea o en otra parte y se dirijan hacia tipos celulares específicos de cualquier parte del cuerpo.

Figura 2

LOS VIRUS ( dibujo inferior ) introducen su material genético en las células

Integren o no sus genes en el ADN de las células infectadas, enseguida comienzan a dirigir la síntesis de nuevas partículas víricas, que pueden dañar a la célula e infectar a otras.
Para convertir un virus silvestre en un vector seguro de terapia génica, se sustituyen genes víricos por otros que determinan proteínas terapéuticas ( dibujo superior ), dejando sólo las secuencias víricas necesarias para la expresión génica.
Tales vectores entran en las células y producen las proteínas benefactoras, pero no se multiplican.

Tabla 1

Algunas ventajas potenciales

Retrovirus.
Los genes se integran en los cromosomas hospedadores, ofreciendo estabilidad a largo plazo .
Adenovirus.
La mayoría no produce enfermedades graves; notable capacidad para albergar genes foráneos .
Virus asociados a los adenovirus.
Los genes se integran en los cromosomas hospedadores; no causan enfermedades conocidas en humanos.
Liposomas.
No portan genes víricos y, por tanto, no producen enfermedades.
ADN «desnudo».
Lo mismo que los liposomas; puede ser útil en vacunas.
Algunos inconvenientes de los vectores existentes.
Retrovirus.
Los genes se integran aleatoriamente y pueden, pues, fragmentar genes del hospedador; muchos infectan sólo células en división.
Adenovirus.
Los genes pueden funcionar transitoriamente, debido a la falta de integración o a los ataques del sistema inmunitario.
Virus asociados a los adenovirus.
Escasa capacidad para genes foráneos .
Liposomas.
Transfieren genes a las células con menos eficacia que los virus.
ADN «desnudo» .
Ineficaz a la hora de transferir genes; inestable en la mayoría de los tejidos.


Figura 3

LOS VECTORES que se diseñan para introducir genes en las células pueden ser víricos y no víricos 

Cada vector prototípico presenta ventajas e inconvenientes; se trabaja en la modificación precisa de los mismos que permita incrementar su eficacia en los pacientes.

Figura 4

LOS VIRUS DEBEN UNIRSE a moléculas específicas presentes en la superficie celular, o receptores, para poder atravesarla

El virus de la leucemia de ratón utiliza una proteína de su cubierta para unirse a un receptor que hay en muchos tipos celulares ( izquierda ). 
Los expertos han alterado esa proteína de la cubierta, añadiéndole nuevos componentes ( centro ) o sustituyéndola por otras ( derecha ), y han conseguido que el virus se una a células que no son las que reconoce normalmente.
Con tácticas similares se pueden enviar otros vectores a tipos celulares específicos. 

Figura 5

CÉLULA DE CEREBRO HUMANO a la que se le ha introducido un vector basado en el virus VIH

El vector porta un gen que determina una proteína de fácil seguimiento ( amarillo ).
Se demuestra así que estas formas atenuadas del VIH, un retrovirus, pueden servir para introducir genes terapéuticos en las neuronas.
Estas células, que no se dividen, son resistentes a los vectores retrovíricos tradicionales.


Síndrome de Williams

Para adentrarse en las complejidades de la organización del cerebro, los científicos empiezan a interesarse por una enfermedad de la que hasta ahora se sabe muy poco En cierta ocasión pidieron a una adolescente con un cociente intelectual de 49 que dibujará un elefante y relatara por escrito todo lo que conocía acerca del animal.
No hubo quien entendiera el garabato.
Pero su descripción resultó de una riqueza impresionante, con ribetes de lirismo. 
Entre otras cosas, decía:
«Tiene unas enormes orejas grises, orejas como abanicos, orejas que hacen soplar el viento».

La expresividad de la joven en cuestión es típica de personas con síndrome de Williams.
Se trata de una afección poco común que ha comenzado a despertar el interés de distintas ramas de la ciencia.
Aunque no todos los individuos que padecen el síndrome presentan idéntica sintomatología, suelen parecerse entre sí.
Se les aplica con frecuencia el diagnóstico de «retraso mental» moderado y, por lo general, su cociente intelectual, según reflejan las pruebas al uso, es bajo.
Leen y escriben mal y tienen muchas dificultades con las operaciones aritméticas, incluso las sencillas. 
En otros campos, sin embargo, hacen gala de una finura extraordinaria.
No sólo tienen facilidad para la expresión oral, sino también para el reconocimiento de caras.
En conjunto, tienden a la locuacidad y la sociabilidad. 
Su empatía es manifiesta.

Además, y aunque los datos sean anecdóticos, algunos de estos pacientes poseen un talento musical extraordinario. 
Aun cuando su grado de atención para la mayoría de las tareas sea limitado, muchos escuchan música o tocan instrumentos con una persistencia sorprendente.
Pese a que la mayoría se revela incapaz de leer las notas musicales, destacan por su oído finísimo y un sorprendente sentido del ritmo.
Un muchacho aprendió con gran facilidad a tocar simultáneamente con una mano un tambor con un tiempo 7/4, y con la otra otro tambor con un tiempo 4/4. 
Muchos de estos individuos retienen en su memoria durante años composiciones musicales complejas y recuerdan la melodía y la letra de largas baladas.
Conocemos a uno que canta canciones en 25 idiomas.
Los más experimentados improvisan e interpretan melodías con extraordinaria facilidad.

Este tipo de anécdotas ha despertado el interés por el estudio sistemático de la capacidad musical de los niños afectados por el síndrome de Williams. 
Los resultados indican que los jóvenes distinguen bien las melodías; se interesan por la música y responden emotivamente con viveza mucho mayor que el resto de la población.
Cierto muchacho confesaba que «la música es mi manera favorita de pensar».

Los científicos se sienten atraídos por el síndrome de Williams, en parte porque sospechan que las excelencias y las carencias de los individuos afectados proporcionarán una nueva ventana que permita observar la organización y maleabilidad del cerebro normal.
Algunos grupos intentan localizar habilidades características en el cerebro de estos enfermos y determinar la influencia de esas habilidades en la actividad intelectual o de otro tipo.
Se proponen, además, descubrir las anomalías genéticas que subyacen en el síndrome de Williams.

En 1993 se descubrió que esta afección se debía a la pérdida de cierta región minúscula de una de las dos copias del cromosoma 7 presente en cada una de las células del organismo.
El fragmento que ha sufrido la deleción contendrá 15 o más genes.
A medida que los genes afectados por la deleción se vayan conociendo, se irá determinando el mecanismo en virtud del cual su ausencia conduce a alteraciones neuroanatómicas y engendra los tipos de comportamiento observados.
Este enfoque integrado del estudio del síndrome de Williams - que busca la interrelación entre genes, neurobiología y comportamiento- puede convertirse en un modelo para explorar la forma en que los genes inciden en el desarrollo y función del cerebro.

La patología médica se interesa obviamente por el síndrome de Williams.
El análisis de los genes en la región delecionada permite explicar por qué los enfermos de Williams padecen además otras alteraciones físicas.
Se han acometido ensayos prenatales que facilitan un diagnóstico precoz de la enfermedad, medidas de enorme interés para que los pacientes reciban la ayuda y cuidados necesarios desde los primeros momentos de la vida y conseguir que desarrollen al máximo todas sus capacidades.
El desconocimiento clínico de las características del síndrome de Williams y la ausencia de pruebas fiables han obstaculizado en el pasado el diagnóstico precoz de esta patología.

Figura 1

LOS ENFERMOS CON SINDROME DE WILLIAMS mezclan excelencias y limitaciones

Cuando se pidió a una muchacha con cociente intelectual de 49 que dibujase y describiese un elefante trazó unos garabatos, incomprensibles en sí mismos.
Agregó, sin embargo, una vívida descripción oral.
Algunos individuos poseen un notable talento musical; el pasado verano, los que aparecen en la fotografía de la izquierda- de arriba abajo:
Julia Tuttle, Brian Johnson y Gloria Lenhoff (hija de Howard M., Lenhoff, uno de los autores de este artículo)- participaron en un campamento de música y artes para enfermos con el síndrome de Williams celebrado en Belvoir Terrace en Lenox, estado de Massachusetts.

Progreso lento de la medicina

Aunque el síndrome de Williams, que se presenta en uno de cada 20.000 nacimientos en la población mundial, haya atraído un creciente interés en los últimos tiempos, no era en absoluto desconocido.
Del trabajo de uno de los autores (Lenhoff)se desprende que los enfermos inspiraron historias en que intervenían una pléyade de personajes fantásticos: elfos, gnomos, duendes y otros.

Pero, por lo que al mundo médico se refiere, éste no se ha percatado de la existencia del síndrome hasta muy recientemente; no hace más de 40 años.
En 1961 J., C., P., Williams, un cardiólogo neozelandés, advirtió que un subgrupo de sus pacientes pediátricos compartían determinados síntomas.
Además de problemas cardiovasculares semejantes, tenían rasgos de elfos (como la nariz respingona y barbilla reducida) y eran, al parecer, retrasados mentales.
Entre los problemas que Williams observó aparecían soplos cardiacos y estrechamiento de los vasos principales.
En particular, los enfermos con el síndrome de Williams padecen con frecuencia estenosis supravalvular aórtica ( ESVA ), una constricción entre leve y severa de la aorta.

De entonces acá, los médicos han observado la presencia de otros rasgos, algunos de los cuales se manifiestan desde los primeros momentos de la vida.
Muestran dificultades para la lactación y sufren dolores de estómago, estreñimiento y hernias.
Duermen mal y pueden ser muy irritables, un comportamiento causado por otro signo frecuente: concentraciones elevadas de calcio en la sangre.
A medida que crecen, su voz se hace ronca y proceden con suma lentitud en el desarrollo, tanto físico como mental.
Por término medio empiezan a andar hacia los 21 meses, a menudo sobre los talones, algo que suele persistir a lo largo de toda la vida y que les confiere un aspecto extraño.
El control motor fino está también alterado.
Además, estos pacientes, muy sensibles al ruido, suelen tener una estatura inferior a la de los niños de su edad y parecen envejecer prematuramente (con arrugas prematuras y cabello gris).

Estas descripciones dieron paso a una comprensión genética, gracias en parte a un estudio de la ESVA en personas que no presentaban el síndrome de Williams.
En 1993, Amanda K., Ewart y Mark T., Keating, de la Universidad de Utah, Colleen A., Morris, de la Universidad de Nevada, y otros descubrieron que, para un segmento de esta población, la ESVA tenía su origen en una mutación heredada en una copia del gen que cifraba la elastina, proteína que confiere elasticidad a muchos órganos y tejidos, como las arterias, pulmones,intestinos y piel.

Figura 2

UNA DELECIÓN MINÚSCULA en una de las dos copias del cromosoma 7 de las células del organismo es la causa del síndrome de Williams (dibujo)

La región ausente puede contener 15 o más genes, de los que sólo han podido identificarse algunos. 
Una prueba diagnóstica se basa en un descubrimiento reciente:
el gen de la elastina se encuentra entre los que se han perdido.
La prueba consiste en señalar las copias del cromosoma 7 con un marcador fluorescente de color verde y el gen con otro marcador fluorescente de color rojo. 
Los cromosomas ( azul ) de un sujeto normal (microfotografía, arriba) muestran dos señales verdes y dos rojas, prueba de que ambas copias del cromosoma 7 están presentes y que cada una lleva el gen de la elastina. 
Los enfermos de Williams, sin embargo, carecen de una de las copias del gen y, por tanto, una de las copias del cromosoma 7 no presenta la señal roja ( microfotografía, abajo ).

Identificación de los genes ausentes

Sabedores de que la ESVA era una condición común en los pacientes con síndrome de Williams y que los pacientes con sólo ESVA familiar y los individuos con síndrome de Williams presentan alteraciones en órganos que requieren elasticidad, los investigadores se plantearon la posibilidad de que el síndrome de Williams implicara también algún cambio en el gen de la elastina.
La búsqueda dio fruto.
Se descubrió la existencia de una deleción que afectaba a una de las dos copias del gen en el cromosoma 7 de las células.
Dicha deleción ocurre aproximadamente en el 95 por ciento de los pacientes con síndrome de Williams.
La pérdida es nociva, por la razón presumible de que se necesiten ambas copias del gen para producir la proteína elastina en cantidades adecuadas.

Se sabía con anterioridad que una reducción en el suministro de elastina podía ser responsable de varios rasgos físicos del síndrome de Williams (como la ESVA , hernias y la aparición prematura de arrugas), aunque no podía explicar por sí sola las alteraciones cognitivas o del comportamiento.
Después de todo, los primeros individuos estudiados, que padecían sólo la ESVA sin alteraciones cognitivas, hubieran revelado también un cociente intelectual bajo, si una disminución de la elastina hubiera sido la única responsable de todos los síntomas del síndrome de Williams.
Esto indujo a sospechar la posible implicación de otros genes.
Hipótesis que recibió el respaldo de los resultados obtenidos tras el examen directo de los cromosomas de pacientes de Williams, que indicaban que la región en que se había producido la deleción del cromosoma 7 se extendía hasta los límites del gen de la elastina, y con toda seguridad abarcaba otros muchos genes.

Empezamos ya a conocer los genes implicados.
De ellos, tres ( LIMquinasa 1 , FZD3 y WSCRI ) son activos en el cerebro, prueba de que podrían influir en el desarrollo y función del cerebro.
Se desconocen las actividades exactas de las proteínas codificadas, aunque Ewart y sus colaboradores han propuesto que la LIMquinasa 1 (invariablemente delecionada junto con el gen de la elastina) pudiera intervenir en la capacidad para captar las relaciones espaciales.

Esta función podría ayudar a comprender por qué los pacientes con síndrome de Williams hallan dificultad a la hora de dibujar de memoria objetos elementales. 
Otro gen de la zona delecionada, el RFC2 , especifica una proteína involucrada en la replicación del ADN , aunque no ha podido confirmarse su participación en el síndrome de Williams.

Queda mucho por conocer sobre el sustrato genético del síndrome de Williams.
Al descubrimiento de la deleción en el cromosoma 7 han seguido algunas recompensas de índole práctica.
Que la deleción ocurra en todas las células del organismo en los pacientes de Williams obliga a descartar la posibilidad de que se debiera a algo que la madre hubiera hecho u omitido durante el embarazo. 
La alteración tiene su origen en un óvulo o un espermatozoide que sufrieron la pérdida de genes en el cromosoma 7 antes de entregar sus cromosomas para la creación de un embrión.
Esto explica también por qué en los hermanos sanos de los pacientes con síndrome de Williams no está presente esa deleción en el cromosoma 7.
Así pues, la posibilidad de que éstos engendren hijos con el síndrome de Williams no es mayor que en el resto de la población.
Por último, las técnicas microscópicas que revelaron la deleción del gen de la elastina - hibridación in situ fluorescente, o FISH - se han adoptado como prueba diagnóstica.

Emerge un perfil cognitivo

El trabajo sobre la genética del síndrome de Williams sirve de complemento a los esfuerzos encaminados a especificar las características neurobiológicas de esta afección.
Esas investigaciones, que en la actualidad se llevan a cabo en varios laboratorios, comenzaron hace unos 15 años, cuando uno de nosotros (Bellugi) contestó una llamada telefónica, ya entrada la noche, en su laboratorio del Instituto Salk de Estudios Biológicos en La Jolla, California.
Al otro lado del cable estaba una madre que sabía que la doctora Bellugi investigaba las bases neurobiológicas del lenguaje y pensaba que su hija, que tenía el síndrome de Williams, sería de interés para el grupo del Instituto Salk.
La niña, de 13 años y cociente intelectual 50, la habían catalogado como deficiente mental. 
De acuerdo con ese perfil, la niña leía y escribía como si tuviera 5 o 6 años.
Y, sin embargo, se expresaba con una extraordinaria belleza.

No estaba entonces solucionado, ni lo está ahora, el problema de distinguir los procesos cerebrales que controlan el lenguaje de los que controlan el razonamiento.
En la población en general, las capacidades cognitivas y las del lenguaje van de la mano.
La dicotomía que presentaba la hija sugería que el estudio de los pacientes con el síndrome de Williams podría contribuir a desglosar ambos procesos.

Fascinada por la noticia, Bellugi aceptó recibir la visita de la niña, y desde entonces la siguió viendo con regularidad.
Empezó a buscar en la bibliografía descripciones sobre las excelencias y limitaciones de los pacientes de Williams, aunque sólo encontró ideas muy vagas. 
Antes de que Bellugi pudiera pensar en descubrir las áreas del cerebro y los procesos neurobiológicos que explicasen las características, un tanto especiales. 
De los pacientes de Williams, necesitaba un perfil mucho más ceñido que le permitiera encajar los rasgos distintivos y diferenciales en la comparación con otros individuos.
Bellugi y sus colaboradores comenzaron a diseñar pruebas para estudiar capacidades especificas y comparar el grupo de enfermos de Williams con el resto de la población y el grupo de otros enfermos con alteraciones de sus capacidades cognitivas: el de los enfermos con síndrome de Down.

En las investigaciones ahora en curso se examinan poblaciones de adolescentes según sexo, edad y cociente intelectual.
(Los pacientes de Williams tienen cocientes intelectuales entre 40 y 100, con una media alrededor de 60).
Muy pronto, el equipo investigador vio que los enfermos de Williams, en contraste con sus limitaciones para las pruebas generales de capacidad cognitiva, se desenvolvían con soltura en el manejo de la gramática a la hora de expresarse de forma espontánea. 
En conjunto, aventajaban a los individuos del grupo del síndrome de Down en las tareas que implicaban comprensión gramatical y creatividad.

Algunos resolvían con dominio tareas complejas del tipo de elaboración de preguntas así:
« ¿no es verdad que a ella le gusta el pescado?
» matizando la fuerza expresiva.
En alguna de estas pruebas hay que leer una frase inicial («A María le gusta el pescado»).
Después tiene que sustituir el nombre por un pronombre («A ella le gusta el pescado»). 
Y después construir una pregunta con una cierta complejidad gramatical («¿no es verdad que?»). 

Los investigadores del Instituto Salk se dieron cuenta también de algo que otros observaron más tarde: 
que los enfermos con síndrome de Williams utilizaban un vocabulario bastante más amplio de lo que cabía esperar de su edad mental.
Cuando se les pedía que citaran algunos animales, no se contentaban con incluir palabras fáciles e incluían ejemplos exóticos como yak, mofeta, yaguané, cóndor, unicornio.

Además de poseer un vocabulario más rico, los individuos con síndrome de Williams tendían a ser más expresivos que los niños normales de su edad.
Esta animación se ponía de manifiesto de manera divertida cuando a los pacientes se les pedía que construyeran una historia basada en una serie de dibujos sin palabras.
A medida que relataban su cuento, alteraban el tono, el volumen, la longitud de las palabras o el ritmo para aportar mayor viveza y fuerza emocional al relato.
También añadían interjecciones para captar y mantener despierto el interés del auditorio («y de repente, ¡zas, cataplum!»; «y bum») con más frecuencia con que lo hacían los enfermos con síndrome de Down.
Por desgracia ocurre a veces que ese don de sociabilidad y fuerza expresiva puede confundir a los profesores, quienes fácilmente creen que esos niños tienen una capacidad de razonar mucho mayor que la que en realidad poseen; en esos casos esos niños pueden verse privados del apoyo que necesitan en sus tareas escolares.
Una posible explicación de la capacidad para la expresión oral de que hacen gala los afectados con el síndrome de Williams es que su defecto cromosómico, en contraste con lo que ocurre con los pacientes de Down, puede no alterar ciertas facultades de apoyo del lenguaje. 
Otros investigadores, por ejemplo, han señalado que la memoria a corto plazo para los sonidos del lenguaje hablado, o «memoria fonológica», que al parecer está implicada en el aprendizaje de la lengua y en la comprensión, se conserva bastante bien en los pacientes con síndrome de Williams.

Interesa resaltar que los estudios realizados recientemente con niños italianos y franceses con síndrome de Williams inducen a pensar que quizá no se conserve tan bien como aparenta en ellos la morfología, parte de la gramática que trata de la conjugación de los verbos, el género y la formación del plural.
(Esos idiomas, al igual que ocurre con el español, son mucho más ricos morfológicamente que el inglés.)
De esa observación cabría inferir que las regiones cerebrales conservadas en el síndrome de Williams y la presencia de una memoria a corto plazo para los sonidos del lenguaje sirven de apoyo a muchas de las aptitudes verbales, aunque quizá no basten para el dominio completo del idioma.

En contraste con los buenos resultados que obtienen en las pruebas orales, los enfermos de Williams realizan peor las tareas que requieren elaborar las imágenes.
El dibujar, por ejemplo.
A menudo los fallos en este tipo de tareas difieren de los que se dan en los enfermos con síndrome de Down.
Esa discrepancia abona la hipótesis de que las deficiencias en ambos grupos posean una base anatómico - cerebral.
En este sentido, los enfermos con síndrome de Williams, al igual que los pacientes que han sufrido una apoplejía del hemisferio derecho del cerebro, pueden captar ciertos componentes de las imágenes, aunque no consigan apreciar el conjunto de la figura.
Los enfermos con síndrome de Down, sin embargo, pueden tener más facilidad para percibir la organización global, pero sin fijarse en muchos de los detalles, como ocurre también en individuos que han sufrido una apoplejía del hemisferio izquierdo.

El perfil general que revelan las diversas pruebas cognitivas implica que el defecto cromosómico en el síndrome de Williams no afecta al hemisferio izquierdo (la región más importante para el lenguaje en la mayoría de la gente) y en cambio altera el hemisferio derecho (del que depende en mayor medida la visión espacial).
Pero la expresividad emotiva de los pacientes con síndrome de Williams (que también se asocia con la actividad funcional del lado derecho) y, al menos, otro de los hallazgos hacen dudar de una explicación tan reduccionista.
Los enfermos de Williams reconocen y distinguen bastante bien fotografías de caras que no les son familiares (una facultad en la que interviene el hemisferio derecho).
En efecto, en este punto su capacidad es igual a la de los adultos de la población general. 

Figura 3

LA ANATOMÍA BÁSICA del cerebro es normal en los enfermos con síndrome de Williams, aunque su volumen es algo menor

Las áreas que parecen estar mejor conservadas comprenden los lóbulos frontales y el neocerebelo (a), así como el área límbica (b), el área auditiva primaria y el planum temporale (c).

Un perfil cognitivo en formación 

En su empeño por acotar los rasgos característicos del síndrome de Williams, los expertos han comparado individuos que sufren este síndrome con otros que padecen síndrome de Down.
Se atenían a los resultados de pruebas que evaluaban destrezas especificas.
Una de estas pruebas ( arriba )- en que se pedía que el muchacho copiara de memoria una letra D dibujada mediante letras Y pequeñas- reveló la incapacidad para integrar los detalles en una configuración más amplia.
Los afectados con el síndrome de Williams tendían a dibujar sólo las letras Y pequeñas, mientras que los que padecían el síndrome de Down tendían a conservar la configuración de conjunto, aunque omitiendo los detalles.
Otra prueba- en la que se pedía a los individuos que inventaran una historia basada en una serie de dibujos sin palabras- reveló que los pacientes de Williams pueden crear a menudo narraciones bien estructuradas.

Enfermo de Williams, edad 17 años, cociente intelectual 50

«Erase una vez cuando se hizo oscuro de noche, el niño tenia una rana.

El niño estaba mirando a la rana, sentado en la silla, en la mesa, y el perro estaba mirando a la rana en el tarro.
Aquella noche se durmió el niño y durmió mucho tiempo, y también el perro.
Pero la rana no se iba a dormir.
La rana salió del tarro.
Y cuando la rana salió, el niño y el perro estaban todavía durmiendo.
A la mañana siguiente era muy hermoso por la mañana.
Había mucha luz, y el sol era agradable y calentaba.
Entonces, de repente, cuando abrió los ojos, miró al tarro y entonces, de repente, la rana ya no estaba allí.
El tarro estaba vacío.
No había rana por ningún lado».

Enfermo de Down, edad 18 años, cociente intelectual 55" «La rana está en el tarro. 
El tarro está en el suelo.
El tarro en el suelo.

Eso es.
El taburete está roto.
Los vestidos está tirado por ahí».

Síndrome de Williams: ¿inspiración para cuentos de gnomos?

En las narraciones populares de muchas culturas aparecen «individuos pequeñitos» con poderes mágicos: gnomos, elfos, duendes, geniecillos y hadas.
Un buen número de semejanzas y modos de comportarse sugieren que al menos en algunas de esas historias ciertos personajes parecen modelados según el síndrome de Williams.
Se trata de una hipótesis acorde con la idea mantenida por los historiadores según la cual el folclor y la mitología parten de acontecimientos de la vida real.

Los rasgos faciales de los enfermos de Williams se describen a menudo como propios de gnomos o elfos.
En común con estos personajes fantásticos del folclor, muchos pacientes tienen una nariz respingona, chata, ojos saltones, orejas ovaladas y boca grande con labios abultados v rematados por una barbilla pequeña.
Estos rasgos son, en efecto, frecuentes en los niños con síndrome de Williams que se parecen entre sí más que a sus parientes cercanos, sobre todo en la infancia.
El síndrome se acompaña de un crecimiento y desarrollo lento, lo que hace que muchos de los individuos que lo padecen sean de baja estatura.

Los personajes míticos de los cuentos son a menudo músicos y narradores.
Las hadas «repiten una y otra vez las canciones que han oído» y pueden «seducir» a los hombres con sus melodías. 
Cosas parecidas podrían decirse también de los enfermos con el síndrome de Williams, que a pesar de tener cocientes intelectuales típicamente subnormales, poseen unas dotes narrativas fuera de lo común y un gran talento musical.
(Las grandes orejas puntiagudas se asocian a menudo con estos personajes míticos y pueden representar de manera simbólica la sensibilidad de estos individuos- en común con los enfermos de Williams- por la música y por los sonidos en general).

Como grupo, los enfermos con síndrome de Williams son gente cariñosa, que inspiran confianza, llenos de ternura y muy sensibles hacia los sentimientos de quienes les rodean.
De manera semejante, las hadas aparecen frecuentemente como «madrinas», muy amables y llenas de gentileza. 
Por último, los enfermos de Williams, de manera similar a los personajes de las leyendas, esperan orden y previsibilidad. 
En los individuos afectados por este síndrome tal necesidad se manifiesta en su fidelidad a las rutinas cotidianas y una constante urgencia por prever y despejar cualquier eventualidad. 

En el pasado, los escritores tejieron historias con personajes imaginarios para ayudar a explicar fenómenos que no entendían, incluyendo tal vez los rasgos distintivos, físicos y de comportamiento, de los enfermos con síndrome de Williams. 
Hoy día, los científicos dirigen su atención hacia estos enfermos en un intento de comprender lo desconocido, con la esperanza de descifrar algunos de los secretos del funcionamiento del cerebro.

Figura 4

NIÑOS DE FAMILIAS DISTINTAS con los típicos rasgos faciales de un elfo que los clínicos asocian con el síndrome de Williams

El dibujo del elfo que aparece a la derecha es obra de Richard Doyle del siglo XIX, un tío del creador de Sherlock Holmes.

Los estudios neurológicos aportan nuevas luces

El examen de los cerebros mediante resonancia magnética y los estudios anatomopatológicos que se llevan a cabo en el Instituto Salk apoyan la probabilidad de que la deleción cromosómica responsable del síndrome de Williams altere el cerebro de una manera más complicada.
La deleción parece producir cambios anatómicos (acumulaciones anormales de neuronas en las áreas visuales) que alteran la capacidad de percepción espacial.
Pero el defecto cromosómico parece no afectar a la red que abarca estructuras de los lóbulos frontales, lóbulo temporal y cerebelo.
Esta red conservada puede hacer función de andamiaje neuroanatómico para las capacidades excelentes e inesperadas de los enfermos de Williams.

Para concretar más, los estudios neuroanatómicos indican que el volumen cortical medio en individuos afectados por los síndromes de Williams o de Down es menor que el de los sujetos normales de la misma edad.
Pero los volúmenes de las distintas regiones difieren en aquellos.
Así, los lóbulos frontales y la región límbica de los lóbulos temporales están mejor conservados en los pacientes con síndrome de Williams.
El sistema límbico, que también incluye otras estructuras, interviene en las actividades cerebrales relacionadas con la memoria y las emociones; la conservación de la región límbica podría explicar por qué los enfermos de Williams son bastante emotivos y manifiestan una notable empatía. 

El análisis del cerebelo pone al descubierto otras diferencias entre enfermos de Williams y de Down.
Mientras que el volumen del cerebelo es pequeño en los enfermos de Down, en los de Williams es normal.
Y en éstos el neocerebelo, desde el punto de vista evolutivo la región más joven del cerebelo, es igual o mayor que el de los individuos coetáneos del resto de la población, mientras que en los enfermos de Down es menor.

El descubrimiento de que el neocerebelo está conservado en los enfermos de Williams es aún más curioso cuando se contempla dentro del contexto de otras investigaciones. 
Hasta fechas recientes se admitía que el cerebelo estaba asociado sobre todo con el movimiento.
El equipo encabezado por Steven E., Petersen, de la Universidad de Washington, ha demostrado que el neocerebelo se activa cuando uno intenta pensar en una asociación de un verbo con un determinado nombre sustantivo (como «sentarse» con «silla»). 
Y no sólo eso, sino que cuando se realizan pruebas en pacientes con lesiones cerebelares se ponen de manifiesto alteraciones de las funciones cognitivas, además de las motoras.
Los anatomistas señalan que el neocerebelo comunica con una parte de la corteza frontal que, en común con el neocerebelo, es mayor en el hombre que en los antropoides. 

Dado que el hombre habla y el antropoide no, algunos han propuesto que el neocerebelo y la región vinculada de la corteza frontal podrían estar bajo el control de los mismos genes, lo que permitió que surgieran juntos en la evolución sirviendo de apoyo al lenguaje emergente. 
La conservación relativa de la corteza frontal y el aumento de tamaño del neocerebelo en los pacientes de Williams, junto con la fluidez de lenguaje, presta cierta credibilidad a esta última hipótesis y a la de que el cerebelo interviene en la elaboración del lenguaje hablado.

Otros análisis anatómicos recientes han identificado rasgos adicionales que podrían explicar el más que mediano talento musical de los enfermos con síndrome de Williams.
La corteza auditiva primaria (localizada en el lóbulo temporal) y una región auditiva adyacente, el planum temporale (importante para el lenguaje y el sentido musical), tienen un tamaño agrandado en los escasos cerebros de enfermos de Williams estudiados hasta la fecha.
Además, el planum temporale es más amplio en el hemisferio izquierdo que en el derecho, aunque en algunos enfermos de Williams la región izquierda tiene un tamaño grande, una característica frecuente entre los músicos profesionales.
Estos hallazgos casan bien con las observaciones de Audrey Don, de la Universidad de Windsor, la investigadora que inició los estudios sobre la capacidad musical de los pacientes con síndrome de Williams.
Llega a la conclusión de que la percepción intacta de los patrones auditivos puede explicar en gran medida la facilidad para la música y la expresión oral que se observan en los pacientes con síndrome de Williams; en coherencia con este resultado, las estructuras cerebrales deben estar también intactas. 

Las sondas fisiológicas que permiten comparar la actividad eléctrica en el cerebro de pacientes de Williams y otros individuos durante la realización de determinadas tareas nos ayudan a avanzar en el conocimiento del desarrollo del cerebro.
En respuesta a los estímulos gramaticales, por ejemplo, los individuos normales muestran una mayor actividad en el hemisferio izquierdo que en el derecho, como cabe esperar de una tarea relacionada con el lenguaje.
Pero los enfermos de Williams muestran respuestas simétricas en los dos hemisferios, una señal de que no se ha producido la especialización típica relacionada con el lenguaje que tiene lugar en el hemisferio izquierdo.
Además, mientras que en un adulto normal hay por lo general una mayor actividad en el hemisferio derecho que en el izquierdo cuando se procesan imágenes de rostros, en los pacientes con síndrome de Williams se da el patrón opuesto.
Estas observaciones respaldan la tesis según la cual cuando algo chirría en los procesos de desarrollo, el cerebro acostumbra repartir las responsabilidades, generando nuevos circuitos que desempeñen las funciones de los que se han estropeado.

Las investigaciones sobre el síndrome de Williams acaban de despegar y ya comienzan a arrojar una luz nueva sobre la organización del cerebro.
Están consiguiendo también que los científicos se acerquen al «retardado mental» con una visión renovada.
Los estudios sobre el síndrome de Williams han demostrado que un cociente intelectual bajo puede enmascarar la existencia de capacidades insospechadas.
Y advierten también que otros grupos de personas etiquetadas como retrasados mentales podrían encerrar un potencial inexplorado que aguarda para manifestarse a que los expertos y la misma sociedad se tomen la molestia de buscarlo y cultivarlo.


Bases biológicas de la homosexualidad masculina 

Datos relativos a la estructura del cerebro y al ligamiento genético apoyan la existencia de un componente biológico en la homosexualidad masculina.

La mayoría de los hombres sienten atracción sexual por las mujeres, y viceversa.

Para mucha gente, ése es el orden natural de las cosas, la manifestación cabal del instinto biológico, reforzado por la educación, la religión y las leyes.

Sin embargo, una minoría notable de varones y mujeres, cifrados entre el uno y el cinco por ciento, sólo sienten atracción por personas de su mismo sexo.

Otros, por último, se muestran atraídos, en grado variable, por hombres y mujeres.

¿Cómo explicar tal heterogeneidad en la orientación sexual?
¿Tiene que ver con nuestros genes, nuestra fisiología, nuestra historia personal o, quizá, con una convergencia de todo ello?
¿Es cuestión de elección más que de tendencia compulsiva?

Probablemente, ningún factor, por sí solo, puede determinar un carácter tan complejo y variable como la orientación sexual.
Pero estudios recientes de varios laboratorios, entre ellos el nuestro, indican que genes y desarrollo cerebral desempeñan un papel significativo, aunque todavía no sepamos cuál.
Los genes podrían estar detrás de la diferenciación sexual del cerebro y su interacción con el mundo exterior, diversificando su ya amplia gama de respuestas a los estímulos sexuales.

Las investigaciones sobre las raíces biológicas de la orientación sexual han seguido dos grandes líneas.
La primera gira en torno a observaciones realizadas en el curso de un tipo de estudios distinto: la búsqueda de posibles diferencias físicas entre el cerebro del varón y el cerebro de la mujer; según veremos, el cerebro del homosexual masculino y el del heterosexual pueden tener diferencias curiosamente análogas.
El segundo enfoque, de corte genético, se basa en la búsqueda de pautas familiares de homosexualidad y en el examen directo del material hereditario, el ADN .

Desde hace tiempo, los investigadores buscan en el cerebro humano algún tipo de manifestaciones que guarden correlación con el sexo.
Tal suerte de diferenciación sexual de las estructuras cerebrales, conocida por dimorfismo sexual, no es fácil de establecer.
Por término medio, el cerebro del varón tiene un tamaño ligeramente mayor, acorde con el mayor tamaño del cuerpo.

Aparte de eso, un examen somero no revela ninguna diferencia obvia entre los dos sexos.
Incluso al microscopio, la arquitectura cerebral del varón es muy similar a la de la mujer.
A nadie debe sorprenderle, pues, que las primeras observaciones significativas de dimorfismo sexual se realizaran en animales de laboratorio.

Reviste particular importancia un estudio realizado en ratas, dirigido por Roger A., Gorski, de la Universidad de California en Los Angeles.
En 1978, Gorski inspeccionaba el hipotálamo de rata, una región situada en la base del cerebro, que se halla implicada en el comportamiento instintivo y en la regulación del metabolismo.
Observó que, en las ratas macho, el tamaño de cierto grupo de células próximas a la parte anterior del hipotálamo multiplicaba varias veces el tamaño del mismo conjunto en las hembras.
Pese a tratarse de un grupo celular muy pequeño, de menos de un milímetro de diámetro en los machos, la diferencia entre los sexos es perceptible, incluso sin la ayuda del microscopio, cuando miramos cortes de tejido debidamente teñidos.

El descubrimiento de Gorski tenía su intríngulis.
A la región general del hipotálamo donde reside ese grupo de células, el área preóptica medial, se la ha involucrado en la generación del comportamiento sexual, en particular, en la conducta típicamente masculina.
A modo de ejemplo, monos machos con áreas preópticas lesionadas se manifiestan indiferentes a la interacción sexual con las hembras; la estimulación eléctrica de esa región provoca que el macho inactivo se acerque a una hembra para montarla.
Conviene señalar, sin embargo, que no se ha encontrado todavía en los monos un grupo de células análogo al identificado, sexualmente dimórfico, en ratas.

Ni tampoco se conoce con exactitud qué función desempeña el grupo de células sexualmente dimórficas de las ratas.
Lo que se sabe, de un estudio realizado por Gorski y sus colaboradores, es que los andrógenos - hormonas masculinas- cumplen una misión clave en la generación del dimorfismo durante el desarrollo.
Las neuronas de ese grupo de células abundan en receptores de hormonas sexuales, lo mismo de andrógenos - así la testosterona- que de estrógenos, hormonas femeninas.
Aunque las ratas machos y hembras parten de un número casi parejo de neuronas en el área preóptica medial, se produce un pulso de testosterona secretada por los testículos de los fetos machos cuando se acerca el alumbramiento para estabilizar su población neuronal.
En las hembras, la falta de testosterona determina la muerte de muchas neuronas de ese grupo, razón de dicha estructura típicamente menor.
Curiosamente, las neuronas preópticas son sensibles a los andrógenos sólo unos pocos días antes y después del nacimiento.

Si, por castración, eliminamos los andrógenos en una rata adulta, no mueren las neuronas.

El grupo de Gorski, de la UCLA , en particular su alumna Laura S., Allen, descubrió también estructuras dimórficas en el cerebro humano.
El conjunto de células que responde al acrónimo NIHA3 («tercer núcleo intersticial del hipotálamo anterior»), de la región preóptica medial del hipotálamo, triplica en los varones el tamaño que presenta en las mujeres.(Lo que no obsta para que el tamaño varíe bastante dentro de un mismo sexo).

En 1990, LeVay, uno de los autores, decidió comprobar si el NIHA3 o cualquier otro grupo de células del área preóptica medial tenía un tamaño que guardase correlación con la inclinación sexual y con el sexo.
La hipótesis de trabajo era bastante arriesgada, pues dominaba la idea establecida de que la orientación sexual era uno de los aspectos «elevados» de la personalidad, en el que el ambiente y la cultura desempeñaban un papel esencial.
Las informaciones procedentes de fuentes de tal nivel se supone que son procesadas primariamente por la corteza cerebral, y no por centros «menores», como el hipotálamo.

LeVay examinó el hipotálamo en muestras de autopsias de 19 varones homosexuales, todos los cuales habían fallecido a causa de complicaciones relacionadas con el sida, y en muestras de autopsias de 16 varones heterosexuales, seis de los cuales también habían muerto de sida. 
(La orientación de los que habían fallecido por motivos ajenos al sida no se determinó.)
Sin embargo, suponiendo una distribución similar a la de la población general, probablemente los homosexuales no serían más de uno o dos).
LeVay incluyó también muestras de seis mujeres cuya orientación sexual se ignoraba.

Tras etiquetar cada muestra, para eliminar todo sesgo subjetivo de las mismas, realizaron cortes hipotalámicos secuenciales, los tiñeron para marcar los grupos de neuronas y midieron las áreas correspondientes en un microscopio.
Con la información sobre el tamaño del área y el grosor del trozo analizado, calcularon el volumen de cada grupo de células.
Además del núcleo sexualmente dimórfico descrito por Allen y Gorski, el NIHA3 , LeVay examinó otros tres grupos vecinos, los NIHA1 , NIHA2 y NIHA4 .

Como Allen y Gorski, LeVay observó que, en el varón, el tamaño de NIHA3 duplicaba, de lejos, su tamaño en la mujer.
Pero el NIHA3 era también entre dos y tres veces mayor en el hombre heterosexual que en el varón homosexual.
En algunos varones homosexuales, el grupo de células faltaba por completo.
De los análisis estadísticos se desprendía que la probabilidad de que este resultado se debiese al azar era de uno entre mil.
No había diferencia significativa entre los volúmenes del NIHA3 de varones homosexuales y mujeres.
Así pues, las investigaciones sugerían la existencia de un dimorfismo relacionado con la orientación sexual del varón, de magnitud pareja a la que se daba en relación al sexo.

En este tipo de estudios, es obligado comprobar si las diferencias estructurales observadas se deben a variables distintas de las que nos interesan.
Nos rondaba por la cabeza, como principal sospechoso, el sida.
El virus del sida, así como otros agentes infecciosos que se aprovechan de un sistema inmunitario mermado, puede producir graves daños en las células del cerebro. 
¿Residía ahí la razón del pequeño tamaño del NIHA3 en los varones homosexuales, todos los cuales murieron de ese mal?

Por indicios de índole dispar, parece que no.
El volumen del NIHA3 de los varones heterosexuales que fallecieron de sida no difería del volumen de quienes murieron por otras causas.
Además, las historias clínicas de las víctimas del sida con NIHA3 pequeños no diferían de las historias clínicas de quienes presentaban de tamaño notable el grupo de NIHA3 .

Resultaba también que los restantes grupos de células del área preóptica medial, NIHA1 , NIHA2 y NIHA4 , no eran menores en las víctimas del sida.
Si la enfermedad tuviese un efecto destructivo inespecífico, cabría suponer otra cosa.

Por último, tras completar el estudio principal, LeVay consiguió el hipotálamo de un homosexual masculino que murió de causas no relacionadas con el sida.
Esta muestra, que se procesó «ciega» con otras procedentes de varones heterosexuales de edad similar, confirmó el estudio principal:
el volumen del NIHA3 del varón homosexual era menos de la mitad que el de los heterosexuales.

Allen y Gorski aportaron otras características del cerebro relacionadas con la inclinación sexual.
Vieron que la comisura anterior, fascículo de fibras que cruza la línea media del cerebro, es pequeña en los varones heterosexuales, grande en las mujeres y mayor aún en los varones homosexuales.
Una vez hecha la corrección correspondiente atendiendo al tamaño del cerebro, la comisura anterior de las mujeres y la de homosexuales masculinos alcanzaban un tamaño equiparable.

¿Qué decir de estas correlaciones manifiestas entre orientación sexual y estructura cerebral?
Caben tres posibilidades lógicas.
Primera:
las diferencias estructurales están presentes desde un principio, quizás incluso antes del nacimiento, lo que contribuiría a establecer la orientación sexual del varón.
Segunda posibilidad:
las diferencias se producen en la madurez, como resultado de las inclinaciones sexuales del varón o de su comportamiento.

Tercera posibilidad:
aunque no existe relación causal, ambas (esto es, orientación sexual y estructura cerebral) se hallan vinculadas a una tercera variable, como podría ser determinado episodio del desarrollo durante la vida perinatal.

No disponemos de datos suficientes para decidirnos por una de las tres.
Con todo, lo obtenido en la investigación animal nos permite asegurar la improbabilidad de la segunda posibilidad, según la cual las diferencias estructurales se producen en la etapa adulta.
En ratas, por ejemplo, el grupo de células sexualmente dimórficas del área preóptica medial manifiesta cierta plasticidad en su respuesta a los andrógenos durante las primeras fases del desarrollo del cerebro, pero más tarde se torna muy resistente al cambio.

Nosotros, que optamos por la primera posibilidad, creemos que las diferencias estructurales surgen durante el desarrollo cerebral y contribuyen, pues, al comportamiento sexual.
La región preóptica medial del hipotálamo de monos está implicada en el comportamiento sexual y, por tanto, el tamaño del NIHA3 de los varones podría condicionar la orientación sexual; pero semejante relación causal es todavía pura especulación.

Admitamos que algunos individuos portaran ya diferencias estructurales relativas a la orientación sexual en el momento del nacimiento. 
¿Cómo surgieron tales diferencias?
Pudieron deberse a la interacción entre esteroides gonadales y cerebro en formación; esa interacción es responsable de las diferencias estructurales entre el cerebro del varón y el de la mujer.
Entre los especialistas, hay quienes han sugerido la posibilidad de que algunos fetos queden condicionados o determinados para, alcanzada la madurez, mostrarse homosexuales, por haber padecido niveles de andrógenos atípicos.
Sugieren, en particular, que los niveles de andrógenos en los fetos masculinos que devienen homosexuales son insólitamente bajos e insólitamente altos en fetos femeninos que devienen lesbianas.

Parece más probable, sin embargo, que medien diferencias intrínsecas en la respuesta de cada cerebro a los andrógenos durante el desarrollo, aun cuando no diverjan los niveles de hormonas.
Este tipo de respuesta requiere una maquinaria molecular compleja, que incluiría a los receptores de andrógenos, y presumiblemente a toda una gama de proteínas y genes cuya identidad y funciones son aún desconocidas.

De entrada, puede parecer absurda la propia noción de genes homosexuales masculinos. 
¿Cómo podrían sobrevivir a la «presión» reproductiva darwinista unos genes que «dirigen» a hombres y mujeres hacia personas de su mismo sexo?
¿No son heterosexuales la mayoría de los progenitores de homosexuales y mujeres lesbianas?
Ante tales aparentes incongruencias, los investigadores se han centrado en genes cuyo papel sería el de influir en la orientación sexual, más que determinarla.
En la búsqueda de esos genes se han tomado dos caminos principales: estudios de gemelos y familias, y análisis de ligamiento en el ADN .

Los estudios de gemelos y árboles familiares parten del principio según el cual los rasgos de base genética se transmiten entre los miembros de la familia.
El primer estudio moderno sobre patrones de homosexualidad en familias fue publicado en 1985 por Richard C., Pillard y James D., Weinrich, de la Universidad de Boston.
Desde entonces han aparecido otros cinco estudios sistemáticos sobre gemelos y hermanos de varones homosexuales y mujeres lesbianas.

Y éstas son las cifras recogidas para el caso de los varones:
aproximadamente el 57% de los gemelos idénticos, el 24% de los gemelos fraternos y el 13% de los hermanos de varones homosexuales son también homosexuales.
Para las mujeres, aproximadamente el 50% de las gemelas idénticas, el 16% de las gemelas fraternas y el 13% de las hermanas de lesbianas, son también lesbianas.

Cuando se comparan esos datos con las tasas normales de homosexualidad, resulta evidente la existencia, en los dos sexos, de un agrupamiento familiar importante en la orientación sexual.
En ese contexto, J., Michael Bailey y sus colaboradores, de la Universidad del Noroeste, estiman que la heredabilidad global de la orientación sexual (proporción de varianza debida a los genes) es del 53% para los hombres y del 52% para las mujeres. 
(El agrupamiento familiar es más evidente para parientes del mismo sexo, y no tanto para las parejas hombre - mujer).

Para evaluar el componente genético de la orientación sexual y aclarar su modo de herencia, hay que abordar un rastro sistemático y exhaustivo de las familias de homosexuales y lesbianas.
Hamer (coautor de este artículo), Stella Hu, Victoria L., Magnuson, Nan Hu y Angela M., L., Pattatucci, del norteamericano Instituto Nacional de la Salud, han incoado esa línea de trabajo, que forma parte de otra más amplia y cuyo objetivo es investigar los factores de riesgo para determinados cánceres que se concentran en ciertos segmentos de la población homosexual masculina.

Los estudios provisionales sobre varones, realizados por el equipo de Hamer, confirman los resultados sobre hermanos, llevados a cabo por Pillard y Weinrich.

El hermano de un varón homosexual tiene un 14% de probabilidad de serlo él también, cuando en el caso de los que no tienen un hermano homosexual masculino es del 2%. 
(El estudio utilizó una definición hiperrestrictiva de homosexualidad, lo que explica la baja tasa media.)
Entre parientes más lejanos se descubrieron pautas menos esperadas:
los tíos maternos tenían un 7% de probabilidad de ser homosexuales, mientras que los hijos de tías maternas tenían una probabilidad del 8%.
Padres, tíos paternos y los otros tres tipos de primos no presentaban correlación alguna.

Aunque la investigación apuntara hacia la existencia de un componente genético, la homosexualidad ocurría con mucha menos frecuencia de lo que cabría esperar para un gen con una herencia mendeliana simple.
Cabe la posibilidad de que los genes sean más importantes en unas familias que en otras; y eso es lo que parece observarse en las familias con dos hermanos varones homosexuales.
Comparadas con familias elegidos al azar, la tasa de homosexualidad en tíos maternos sube del 7 al 10%, y en primos maternos del 8 al 13%.
Este agrupamiento familiar, incluso en parientes fuera del núcleo familiar principal, respalda la hipótesis de una raíz genética en la inclinación sexual.

¿Por qué los varones homosexuales tienen más parientes masculinos de su misma inclinación en la rama materna de la familia?
Una posibilidad, que los individuos conocieran mejor sus parientes maternos, parece improbable, porque los parientes homosexuales masculinos, de sexo opuesto, de homosexuales y lesbianas presentaban una similar distribución en ambas ramas familiares.
Otra explicación es que la homosexualidad, aunque transmitida por ambos progenitores, sólo se expresa en un sexo, en este caso los varones.
Cuando se expresa, el carácter reduce la tasa reproductiva y, por tanto, se transmite por la madre de forma desproporcionada.
Un efecto de ese tipo puede explicar parcialmente la concentración de parientes homosexuales de varones homosexuales en la rama materna de la familia.
Mas, para confirmar esta hipótesis, será necesario encontrar un gen apropiado en un cromosoma autosómico, y que además lo transmita cualquier progenitor.

Una tercera posibilidad reside en el ligamiento al cromosoma X .
Los varones portan dos cromosomas sexuales:
el Y , que heredan de su padre, y el X , que heredan de su madre, que tiene dos.
Por tanto, cualquier carácter condicionado por un gen presente en el cromosoma X tenderá a ser heredado por la rama materna, y se observará preferentemente en los hermanos, tíos maternos y primos maternos, lo que coincide exactamente con el patrón observado.

Para comprobar esta hipótesis, Hamer y sus colegas se embarcaron en un estudio de ligamiento del cromosoma X en varones homosexuales.
El análisis de ligamiento se funda en dos principios genéticos.
Si un carácter está condicionado genéticamente, los parientes que comparten el carácter compartirán también el gen, con una frecuencia mayor de lo que cabría esperar por puro azar.
Eso es cierto, aunque la influencia ejercida por el gen sea mínima.
Además, los genes que se alojan próximos en el cromosoma se heredan casi siempre juntos.
Por consiguiente, si hay un gen que influye sobre la orientación sexual, debería estar «ligado» a algún marcador de ADN cercano que tienda a «viajar» junto a él por la familia.

Para caracteres afectados por un solo gen, el ligamiento puede localizar al gen con bastante precisión dentro de un cromosoma.
Para caracteres complejos, como la orientación sexual, el ligamiento también ayuda a determinar si realmente hay o no un componente genético.

A la hora de iniciar un análisis de ligamiento de la orientación sexual masculina, resultaba imprescindible comenzar por hallar marcadores informativos, esto es, segmentos de ADN que «señalen» posiciones en un cromosoma.
Para nuestra fortuna, el Proyecto Genoma Humano ha generado ya un largo elenco de marcadores que cubren todo el cromosoma X .
Los más útiles son unas secuencias breves de ADN repetido que presentan longitudes ligeramente diferentes en personas distintas.
Para detectar los marcadores, se recurrió a la reacción en cadena de la polimerasa ( RCP ), que permite generar miles de millones de copias de regiones específicas del cromosoma; a continuación, se separaron los diferentes fragmentos por el método de electroforesis en gel.

El segundo paso en el análisis de ligamiento consistió en localizar familias adecuadas.
Cuando se estudian caracteres sencillos determinados por un solo gen (como la ceguera a los colores o la anemia falciforme), suelen rastrearse extensas familias, con muchas generaciones, en los que cada miembro porta o no, sin ambigüedad, el rasgo en cuestión.
Este tipo de enfoque no sirve para la investigación sobre la orientación sexual.
Hay que empezar por identificar la orientación sexual de las personas, lo que no es fácil.
Muchos encubren su auténtica inclinación sexual, o no son conscientes de ella.
Como en el pasado la homosexualidad constituía un estigma más vergonzante, el rastreo plurigeneracional en una familia se torna más problemático.
Además, los modelos genéticos muestran que, para los rasgos determinados por varios genes diferentes que se expresan a varios niveles, el estudio de familias extensas disminuye la probabilidad de encontrar un gen ligado:
son demasiadas las excepciones que hay que considerar.

Por esas razones, el equipo de Hamer decidió centrarse en núcleos familiares con dos hijos homosexuales masculinos.
En este enfoque, y eso constituye una ventaja, los individuos que se confiesan homosexuales no suelen mentir.
Además, permite detectar un único gen ligado, aunque para su expresión se requieran otros genes o factores no hereditarios; así, por ejemplo, en la hipótesis de que, para ser homosexual masculino, se requiera un gen ligado al cromosoma X , otro gen presente en un autosoma y una serie de circunstancias del entorno.

El estudio de los hermanos homosexuales masculinos ha de proporcionar un resultado claro, ya que los dos deben tener el gen del cromosoma X .
Por contra, los hermanos heterosexuales de varones homosexuales a veces compartirán el gen del cromosoma X y a veces no, lo que puede provocar confusión en los resultados.

Los analistas genéticos creen que los estudios de hermanos encierran la clave para descubrir los caracteres afectados por muchos elementos.
Como Hamer y sus colegas estaban más interesados en encontrar un gen que se expresara sólo en los hombres y se transmitiese a través de las mujeres, restringieron su búsqueda a familias con varones homosexuales, aunque sin pares padre homosexual - hijo homosexual.

Seleccionaron 14 familias con esas características.
Prepararon muestras de ADN de los hermanos homosexuales y, cuando fue posible, de sus madres o hermanas.
Las muestras se analizaron para 22 marcadores que cubrían todo el cromosoma X , desde el extremo del brazo corto hasta el final del brazo largo.

Para cada marcador, se consideró que un par de hermanos homosexuales era concordante si heredaba de su madre marcadores idénticos; discordante, si heredaban marcadores diferentes.
Se esperaba que el 50 por ciento de los marcadores fuesen idénticos por puro azar.
Hicieron también correcciones para tomar en cuenta la posibilidad de que la madre portase dos copias del mismo marcador.

Los resultados de este estudio fueron sorprendentes.
En la mayor parte del cromosoma X , los marcadores estaban aleatoriamente distribuidos entre hermanos homosexuales.
Mas, para un marcador situado en el extremo del brazo largo del cromosoma X , en la región Xq28 , había un número elevado de hermanos concordantes:
33 pares compartían el mismo marcador, mientras que sólo 7 pares no lo compartían.
Pese al discreto tamaño de la muestra, el resultado era estadísticamente significativo, ya que la probabilidad de que dichas proporciones ocurriesen por azar no llegaba a 1 de cada 200.
En un grupo control de 314 pares elegidos al azar, la mayoría de los cuales eran presumiblemente heterosexuales, los marcadores Xq28 estaban distribuidos aleatoriamente.

Si nos atenemos a una interpretación directa de estos descubrimientos, la región cromosómica Xq28 contiene un gen que influye en la orientación sexual de los varones.
El estudio proporciona la prueba más contundente de que la sexualidad humana está condicionada por la herencia, ya que examina directamente la información genética, el ADN .

Pero, igual que ocurría con los primeros estudios,hemos de tomar varias precauciones.

En primer lugar, se exige que el resultado sea repetible.
Otros casos en que se habían descubierto genes que parecían estar relacionados con caracteres de la personalidad resultaron a la postre controvertidos.
Conviene saber, en segundo lugar, que todavía no se ha aislado el gen.
El estudio lo sitúa en una región del cromosoma X que tiene una longitud aproximada de 4 millones de pares de bases.
Si bien esa región representa menos del 0,2% del genoma humano, sigue siendo muy extensa: puede abarcar varios cientos de genes.
Para encontrar la aguja en ese pajar habría que trabajar con un gran número de familias o disponer de una información más completa sobre la secuencia de ADN , para identificar todas las regiones susceptibles de cifrar proteínas.
Casualmente, la región Xq28 es extraordinariamente rica en loci genéticos, por lo que resulta probable que se trate de una de las primeras regiones del genoma humano que se secuencia en su integridad.

En tercer lugar, desconocemos la importancia cuantitativa del papel de Xq28 en la orientación sexual de los varones.
En la población de hermanos homosexuales estudiada, 7 de los 40 hermanos no compartían esos marcadores.
Suponiendo que 20 hermanos debían heredar marcadores idénticos por puro azar, cabe estimar que el 36% de los hermanos homosexuales no presentan ligamiento entre homosexualidad y Xq28 .
Quizás esos varones heredaron genes distintos, estaban condicionados por factores fisiológicos no genéticos o lo estaban por el entorno.
Para el conjunto de varones homosexuales, la mayoría de los cuales sin hermanos homosexuales, la influencia que ejerce Xq28 es incluso menos evidente.
También se desconoce el papel de Xq28 y de otros loci genéticos en la inclinación sexual de las mujeres.

¿De qué modo podría afectar a la sexualidad un locus genético de la región Xq28 ?
Cabría que el hipotético gen influyera en la síntesis o metabolismo de una hormona.
Un posible candidato para ese gen era el locus del receptor de andrógenos, que cifra una proteína esencial para la masculinización del cerebro humano y se aloja, además, en el cromosoma X .
Para comprobar dicha hipótesis, Jeremy Nathans, Jennifer P., Macke, Van L., King y Terry R., Brown, de la Universidad Johus Hopkins, sumaron sus fuerzas a las de Bailey, de la Universidad del Noroeste, y Hamer, Hu y Hu, del Instituto Nacional de la Salud.

Compararon la estructura molecular del gen del receptor de andrógenos en 197 varones homosexuales y 213 predominantemente heterosexuales.
No encontraron variaciones significativas en sus secuencias.
Ni tampoco los estudios de ligamiento hallaron correlación entre homosexualidad en hermanos y herencia del locus del receptor de andrógenos.
Y lo más significativo:
el locus resultó estar en Xq11 , muy distanciado de la región Xq28 .
Quedó así excluido el receptor de andrógenos entre los factores condicionantes de la inclinación sexual de los varones.

Vayamos con la segunda posibilidad:
que el hipotético gen actúe indirectamente, a través de la personalidad o el temperamento, y no directamente en la elección del objeto sexual.
Dicho de una manera plástica, la gente genéticamente muy segura de sí misma tendrá menos problemas a la hora de manifestar sus inclinaciones hacia el mismo sexo que las personas que dependan de que otros aprueben o le afeen su conducta.

¿Podría, por último, ocurrir que el producto génico de Xq28 ejerciera una acción directa sobre el desarrollo de regiones del cerebro sexualmente dimórficas, como la NIHA3 ?
En su versión elemental, tal agente podría actuar autónomamente, quizás en el útero, estimulando la supervivencia de neuronas específicas de varones preheterosexuales, o promoviendo su destrucción en varones prehomosexuales y mujeres.
En un modelo más complejo, el producto génico podría cambiar el grado de sensibilidad de un circuito neuronal del hipotálamo a ciertos estímulos ambientales, tal vez en los primeros años de vida.
De acontecer eso, los genes servirían para predisponer, más que para predeterminar.
Queda por ver si todo esto tiene algo de verosímil.
Lo cierto es que se puede comprobar experimentalmente, utilizando las herramientas actuales de la genética y neurobiología molecular.

Nuestras investigaciones han despertado el interés de la opinión pública, no porque suponga un desafío conceptual - la idea de que genes y cerebro estén implicados en el comportamiento humano no es nueva -, sino porque viene a poner el dedo en la llaga de un conflicto profundo en la sociedad.
Pensamos que la investigación científica puede ayudar a disipar algunos de los mitos sobre la homosexualidad, que en el pasado han oscurecido la imagen de los hombres y las mujeres con esta inclinación.

Figura 1

HIPOTÁLAMO de cerebro, examinado para buscar diferencias relacionadas con la orientación sexual 

Se tiñeron los hipotálamos de 41 personas y se marcaron grupos de neuronas.
En varones, el grupo NIHA3 , del área preóptica medial, tenía un tamaño que duplicaba el de las mujeres.
De dos a tres veces era también la diferencia entre varones heterosexuales y homosexuales (micrografías de la derecha).
Este descubrimiento sugiere la existencia de diferencias relacionadas con la inclinación sexual de los varones, de una magnitud parecida a las que se observan en relación con el sexo.

Figura 2

COMPARTICIÓN DE GENES de la región Xq28 entre hermanos varones homosexuales, con una frecuencia significativamente mayor que la observada en la población general

De 40 pares de hermanos homosexuales estudiados, 33 pares compartían la región Xq28 .
En un grupo control de 314 pares de hermanos, seleccionados de forma aleatoria, los marcadores Xq28 se encontraban distribuidos prácticamente al azar.

Figura 3

BÚSQUEDA DE GENES compartidos en hermanos homosexuales masculinos (marrón oscuro)

Se requiere primero el aislamiento de ADN de los individuos que hay que analizar.
Con la técnica de reacción en cadena de la polimerasa se obtienen varios miles de millones de regiones específicas del cromosoma X , y los fragmentos conseguidos se separan en una electroforesis en gel.
Los hermanos homosexuales comparten un marcador de la región Xq28 , en este ejemplo hipotético el CA11 , con una frecuencia mucho mayor de la esperada del azar.

Arboles genealógicos y cromosoma X

Los árboles genealógicos de la orientación sexual masculina muestran mayores índices de homosexualidad ( marrón oscuro ) en la rama materna.
En la paterna dicho índice no supera mucho el 2% observado para la población general.
Este descubrimiento apunta a un posible papel del cromosoma X(abajo).
Los varones tienen dos cromosomas sexuales, uno Y que heredan del padre, y otro X , que heredan de la madre.
Por tanto, un carácter que se herede por vía materna puede estar condicionado por un gen que esté en uno de sus cromosomas X ( en rojo ).
De hecho, experimentos ulteriores demostraron que una proporción grande de hermanos varones homosexuales comparten la región Xq28 , situada en el extremo del cromosoma X .


Bases genéticas y ambientales de la conducta

¿Está el comportamiento humano determinado por aspectos genéticos o por el entorno?
Quizás ha llegado el momento de abandonar la dicotomía.

Los defensores del comportamiento innato y los partidarios del adquirido han estado enfrentados durante más tiempo del que puedo recordar.
Mientras los biólogos han creído desde siempre que los genes intervenían en el comportamiento humano, los sociólogos han militado en masa en el bando contrario, el que afirma que somos obra nuestra, libres de las cadenas de la biología.

Viví el fragor de ese debate en los años setenta.
Si en mis conferencias abiertas aludía a las diferencias existentes entre los sexos en los chimpancés -mayor agresividad y ambición en los machos que en las hembras-, se levantaban protestas airadas.
¿No estaría proyectando mis propios valores sobre esos pobres animales?
¿Hasta qué punto eran rigurosos mis métodos? 
¿Por qué me molestaba en comparar los sexos? 
¿Acaso escondía segundas intenciones?

Hoy, esa misma información aburre al auditorio. 
Ni siquiera las comparaciones directas entre el comportamiento humano y el de los primates, tema antaño tabú, consigue llamar la atención de nadie.
Todo el mundo ha oído hablar de que los hombres provienen de Marte y las mujeres de Venus.
Todo el mundo ha visto en los semanarios gráficos tomografías de emisión de positrones de cerebros humanos tomadas durante la ejecución de di versas tareas, y en las que aparecen iluminadas áreas diferentes según se trate de un varón o de una mujer.

Pero en esta ocasión la preocupación me acucia a mi.
En vez de celebrar la victoria del enfoque biológico, considero que algunas de las dicotomías contemporáneas entre hombres y mujeres son simplificaciones extremas, acordes con un supuesto buen gusto social y realizadas con resabios antimachistas (por ejemplo, cuando se habla de "envenenamiento de testosterona" para aludir a efectos hormonales normales).
Estamos todavía muy lejos de entender, en su sutil complejidad, la relación existente entre genética y entorno.
La sociedad ha permitido el movimiento pendular desde el comportamiento aprendido al innato, dejando atrás un gran número de sociólogos confundidos.
Todavía nos gusta expresarlo todo basándonos en una u otra tendencia, en lugar de considerar ambas a la vez.

Es imposible saber hacia dónde evolucionaremos en los próximos 50 años, sin remontarnos un intervalo equivalente en la historia de la controversia entre la conducta innata y la adquirida.
El debate está cargado de visceralidad, pues cualquier postura que se adopte trae consigo graves implicaciones políticas.
Las posiciones varían desde una infundada fe en la flexibilidad humana, sostenida por los reformistas, hasta una obsesión por la estirpe y la raza, que caracteriza a los conservadores.
Estas posturas, cada una a su manera, han causado un incalculable sufrimiento a la humanidad durante los últimos cien años. 

Figura 1

Para averiguar qué parte corresponde a la genética y qué parte a la educación y el entorno en el comportamiento humano, se han estudiado gemelos criados por separado

Estos hermanos se reencontraron ya de adultos. 
Ambos eran bomberos y se habían dejado bigote. 

Aprendizaje e instinto

Hace medio siglo, las dos corrientes de pensamiento dominantes sobre el comportamiento animal y humano presentaban puntos de vista opuestos.
Enseñando a los animales acciones arbitrarias, presionar una palanca por ejemplo, los conductistas norteamericanos llegaron a la conclusión de que todo comportamiento era el resultado de un aprendizaje realizado por ensayo y error. 
Este proceso se consideró tan universal, que las diferencias entre las especies se reputaron irrelevantes:
el concepto de aprendizaje se aplicó a todos los animales, hombre incluido.
Lo sentenció B. F. Skinner, fundador del conductismo:
"Palomas, ratas, monos,
¿importa cuál sea uno u otro?"

Por contra, la escuela etológica europea se centró en el comportamiento innato.
Cada especie animal nace con un número de patrones de conducta, que apenas sufren modificaciones por la acción del entorno.
Estos y otros comportamientos propios de cada especie representan adaptaciones evolutivas.
Así, nadie necesita enseñar a los humanos cómo reír o llorar:
se trata de respuestas innatas, universalmente utilizadas y entendidas. 
De forma similar, tampoco la araña necesita aprender a construir su tela.
Nace con una batería de hileras (tubos conectados a una glándula que produce seda), así como con un programa de comportamiento que le "enseña" a tejer los hilos.

Debido a su simplicidad, ambos planteamientos del comportamiento presentaban un enorme atractivo.
Y aunque ambos aceptaban la teoría de la evolución, a veces su influencia era epidérmica.
Los conductistas recalcaban la continuidad entre humanos y otros animales, atribuyendo dicho vínculo a la evolución.
Pero, dado que para ellos el comportamiento se aprendía, no se heredaba, ignoraron el aspecto genético, sobre el que recae la evolución. 
Aunque es cierto que la evolución implica continuidad, también exige diversidad:
cada animal se adapta a un determinado modo de vida en un determinado ambiente.
Como se evidencia a partir de los tratados de Skinner, este punto se pasó por alto sin más.

Por su parte, algunos otólogos manejaban nociones harto imprecisas sobre la evolución, poniendo mayor énfasis en la filogenia que en la selección natural.
Entendieron que la inhibición de la agresividad y otros aspectos del comportamiento eran beneficiosos para las especies.
Argumentaban que si unos animales mataban a otros en luchas, la especie acabará por extinguirse.
Aunque eso podría ser cierto, los animales tienen razones egoístas para evitar una escalada de violencia que fuera dañina para ellos y para sus congéneres. 
De ahí que estas ideas se estén reemplazando por teorías que tratan sobre la manera en que ciertas pautas de comportamiento benefician al individuo y a los con él emparentados.
Los efectos ejercidos sobre la especie en cuanto tal se consideran algo colateral.

El conductismo comenzó a tambalearse con el descubrimiento de que el aprendizaje difiere según las circunstancias y según la especie.
Así, una rata vincula acciones y efectos sólo si ambos son consecutivos.
El roedor tardaría mucho en aprender a presionar una barra si la recompensa se demorara unos minutos.
Sin embargo, cuando éste ingiere comida que le provoca malestar, persistirá en su aversión por la comida aunque haya un retraso de horas entre la ingestión y la sensación negativa. 
Todo indica que los animales aprenden de forma selectiva, alcanzando su mayor habilidad en las contingencias que son más importantes para su supervivencia.

A la vez que los conductistas se vieron forzados a aceptar las premisas de la biología evolutiva y a tener en cuenta el mundo fuera del laboratorio, los otólogos y ecólogos fueron estableciendo las bases de la revolución neodarwinista de los años setenta.
Se adelantó Nikolaas Tinbergen, etólogo holandés.
Acometió brillantes experimentos de campo sobre la importancia de la supervivencia en el comportamiento animal.
Así, explicó por qué muchas aves quitan las cáscaras de los huevos del nido tras la eclosión de los polluelos.
La coloración de la parte externa del cascarón ejerce una función de camuflaje por mimesis, pero la interna no:
los cuervos y otros depredadores podrán entonces localizar el resto de la puesta si cáscaras rotas y huevos enteros se encontraran próximos.
La remoción de los restos constituye una respuesta automática primada por la selección natural:
las aves que practican este comportamiento cuentan con una progenie superviviente más numerosa.

Otros desarrollaron teorías para explicar comportamientos que, a primera vista, no parecen ayudar al sujeto actor, sino a otros.
Obsérvese ese comportamiento "altruista" en las hormigas soldado, que dan su vida en defensa de la colonia, o en los delfines, que sacan a la superficie al compañero que se está ahogando.
Los biólogos suponían que la selección natural admitiría la ayuda entre los parientes si con ello se perpetuaban los mismos genes.
Si dos animales no estaban emparentados, el favor concedido por uno debería ser devuelto en un futuro.

Los científicos se sintieron tan seguros de sus explicaciones sobre la cooperación de las sociedades animales, que no se resistieron a generalizarlas a nuestra propia especie. 
Consideraron que la iniciativa solidaria de la sociedad se fundaba sobre la misma base que los valores de familia y transacción económica.

En 1975, Edward O. Wilson, experto norteamericano en hormigas, proclamó que había llegado el momento de aplicar la teoría de Darwin al comportamiento humano y que la sociología debería prepararse para trabajar, codo con codo con los biólogos, en dicho empeño.
Hasta entonces, una y otra disciplina habían seguido trayectorias independientes, si bien desde el punto de vista de un biólogo la sociología no era mucho más que el estudio del comportamiento animal centrado en una sola especie: la nuestra.
No siendo ésa la perspectiva del profesional de la sociología, las propuestas a favor de un marco unificado de trabajo no fueron bien recibidas.
Cierto oponente destemplado de Wilson echó agua fría sobre la cabeza de éste, que acababa de dictar una conferencia. 
Por las razones que veremos la "sociobiología", denominación de la nueva síntesis de Wilson, fue equiparada con las persecuciones raciales del pasado y, en concreto, con el "Holocausto".

Aunque la crítica fue manifiestamente injusta -Wilson aportaba explicaciones evolutivas, no propuestas políticas-, a nadie debiera sorprender que la cuestión de la biología humana despierte grandes pasiones.

Figura 2

Ambas posturas, llevadas al extremo, entrañan un grave riesgo

Recuérdese el determinismo biológico de los nazis y el determinismo social de los comunistas.

Cargas del pasado

Solía creerse que, por ser aprendida, parte del comportamiento humano podía cambiarse sin dificultad, en tanto que otra, heredada, se resistía a la modificación. 

Ideólogos de todo cuño se han valido de esta división para defender la naturaleza innata de ciertas características humanas (presunta diferencia racial en punto a la inteligencia) y la plasticidad de otras (capacidad para superar los estereotipos entre sexos).
De este modo, el comunismo se fundó sobre la base de una gran confianza en la maleabilidad humana.
Puesto que las personas, a diferencia de los insectos sociales, son reacias a sacrificar su individualidad por el bien común, algunos regímenes acompañaron sus revoluciones con campañas de adoctrinamiento masivas.
Pero resultó en vano.
El comunismo se vino abajo debido a una estructura económica de incentivos que no hería la naturaleza humana.
Por desgracia, esto sólo se produjo tras haber causado miseria y muerte. 

Más desastrosa fue la convivencia entre la biología y el nazismo.
Aquí, también, el pueblo ( das Volk ) se antepuso al individuo, pero en vez de apoyarse en la maquinaria social, se eligió el método de la manipulación genética. 
Se clasificaron las personas entre pertenecientes a un tipo "superior" y las de un tipo "inferior"; había que proteger al primero de la contaminación del segundo.
En el horrible lenguaje de los nazis, un Volk sano requería la extirpación de todos los elementos "cancerosos".
Esta idea fue llevada al extremo de forma tal, que la civilización occidental se ha comprometido a no olvidarlo nunca.

Pero no se crea que la ideología seleccionista subyacente se restringió a esa época y lugar.
En la primera parte del siglo XX, el movimiento eugenésico -que perseguía la mejora de la humanidad por medio del cruce entre los "mejor adaptados"-gozaba de gran predicamento entre los intelectuales de Estados Unidos y Gran Bretaña.
Con un punto de apoyo en ideas que se remontaban a la República de Platón, se aceptaba por buena la esterilización de los discapacitados mentales y de los criminales.
Y el darwinismo social -tesis según la cual en una economía capitalista no intervencionista el fuerte dejará fuera de la competencia al débil, redundando en una mejora general de la población- inspira todavía hoy determinadas políticas.
En esa línea, no deberíamos ayudar a los pobres en su lucha por la subsistencia para no perturbar el orden natural. 

Con semejantes proclamas, se comprende que las clases oprimidas-minorías y mujeres-no vean un aliado en la biología.
Pero yo afirmaría que el peligro puede venir de ambas direcciones, del determinismo biológico y de su antagonista, es decir, el rechazo de las necesidades básicas humanas y la creencia de que podemos ser lo que nos propongamos.
Las comunas "hippies" de los sesenta, los kibutzim de Israel y la revolución feminista persiguieron redefinir la humanidad.
Sin embargo, sólo perdurará el rechazo a los celos sexuales, al vínculo entre padres e hijos 0 a las diferencias entre los sexos hasta que un movimiento en contra intente equilibrar las tendencias culturales y las inclinaciones humanas.

Hoy, mientras el genocidio de la Segunda Guerra Mundial se desvanece en el recuerdo, se nos ofrecen pruebas abundantes de la conexión entre genética y comportamiento. 
Los estudios realizados con gemelos que han crecido separados han llegado a ser de conocimiento general, y no hay semana sin que los periódicos se hagan eco de un nuevo gen humano. 
Está documentado el origen genético de la esquizofrenia, la epilepsia y la enfermedad de Alzheimer e incluso de comportamientos habituales tales como temblar de miedo. 
Conocemos mejor las diferencias genéticas y neurológicas entre varones y mujeres, entre homosexuales y heterosexuales. 
Se sabe que una pequeña región del cerebro de los varones transexuales (que se visten y comportan como mujeres) se asemeja a la misma región del cerebro de la mujer. 

La lista de estos avances científicos aumenta por días y genera una acumulación crítica de pruebas imposibles de ignorar.
Es comprensible que los estudiosos que han dedicado su vida a condenar la idea de una influencia de la biología en el comportamiento humano se resistan a cambiar.
Pero se quedan atrás, superados por el público, que parece haber aceptado la intervención de los genes en cuanto hacemos y somos.
Al propio tiempo, las reticencias a las comparaciones con otros animales se han disipado gracias al torrente de programas y documentales sobre la naturaleza, que han traído a nuestros hogares la vida salvaje y nos han mostrado a los animales un poco más inteligentes e interesantes de lo que la gente creía. 

Los estudios sobre chimpancés y bonobos, como los realizados por Jane Goodall y el autor, ponen de manifiesto que existen paralelismos entre innumerables prácticas y capacidades humanas (desde la política y la educación de los niños hasta la violencia e incluso la moral) y la vida de nuestros parientes animales más cercanos.
¿Cómo podemos mantener los dualismos del pasado -entre humanos y animales y entre cuerpo y mente- ante tamaños indicios en contra? 
El actual conocimiento sobre nuestros antecedentes biológicos no permite un retorno a las ideas del pasado.

No obstante, esto no resuelve el problema del abuso ideológico. 
Si acaso, empeora las cosas.
Mientras la gente tenga compromisos políticos, se describirá la naturaleza humana de una u otra manera de acuerdo con sus propios intereses. 
A los conservadores les gusta señalar el aspecto egoísta de la naturaleza humana, mientras que los liberales sostienen que hemos evolucionado para ser sociales y solidarios. 
La evidente exactitud de ambas deducciones muestra lo erróneo del ingenuo determinismo genético.

Figura 3

Con nuestros parientes animales más próximos - como esta familia de bonobos - compartimos muchos comportamientos

Los programas televisivos sobre naturaleza han traído hasta nuestros hogares la influencia de la biología en el comportamiento humano.

Lo mejor de ambos mundos

Dado que el lenguaje genético se ha introducido en nuestra cultura de términos pegadizos, hay muchas razones para mostrar al público que los genes, por ellos mismos, son como semillas esparcidas sobre el pavimento:
incapaces de dar fruto alguno.
Cuando los científicos dicen de un rasgo que es heredado, lo que quieren indicar es que parte de su variabilidad se explica por factores genéticos. 
Mas tiende a olvidarse que los factores ambientales suelen explicar otro tanto.

Como Hans Kummer, un primatólogo suizo, señaló hace años, intentar determinar qué fracción de un rasgo se debe a los genes y qué fracción al entorno, es tan inútil como preguntarse si el sonido de un tambor que retumba lejano lo produce el percusionista o el instrumento.
Por otro lado, si recogemos distintos sonidos en diferentes ocasiones podemos preguntarnos con razón si la variación se debe a diferentes percusionistas o a distintos tambores.
Esta es la pregunta que la ciencia se formula cuando aborda la relación entre efectos genéticos y ambientales.

Preveo un estrechamiento de los lazos entre genética y comportamiento, un conocimiento mucho más preciso de las funciones cerebrales y una adopción gradual de los modelos evolutivos en las ciencias sociales.
Por fin, el retrato de Charles Darwin terminará por presidir los departamentos de sociología y psicología.
Pero cabría esperar que todo esto fuera acompañado de una continua valoración de las implicaciones éticas y políticas de la ciencia de la conducta.

Tradicionalmente, los científicos han actuado como si no fuera asunto suyo el uso que se haga de la información que proporcionan.
Ha habido períodos en que han tomado incluso parte activa en políticas abusivas.
Una notable excepción fue Albert Einstein, quien pudiera servir de modelo del tipo de conciencia moral que se requiere en ciencias sociales y del comportamiento.
Si algo nos enseña la historia, es la importancia de permanecer alerta ante malas interpretaciones y simplificaciones.
Nadie se halla en mejor posición que los propios científicos para advertirnos contra las distorsiones y para explicar los conceptos complejos.

La dirección en la que el pensamiento podría desarrollarse quizá se ilustre con un ejemplo extraído del cruce de caminos donde coinciden la antropología cultural y evolutiva.
Sigmund Freud y muchos antropólogos tradicionales, Claude Lévi-Strauss entre ellos, han aceptado que el tabú del incesto humano sirve para suprimir el impulso sexual entre miembros de una misma familia.
Freud creía que las primeras fantasías sexuales en los humanos eran siempre de carácter incestuoso.
De ahí que el tabú del incesto se viera como la victoria definitiva de la cultura sobre la naturaleza.

En el polo opuesto, Edward Westermarck un sociólogo finés contemporáneo de Freud, postulaba que el trato familiar en edades tempranas (el que se da entre madre e hijo o entre hermanos) mataba el deseo sexual.
En su opinión, entre individuos que han crecido juntos apenas encontramos, si es que la encontramos, atracción sexual. 
Westermarck, darwinista convencido, lo atribuía a un mecanismo adquirido por evolución e ideado para evitar las consecuencias perniciosas de la endogamia.

En el mayor estudio a gran escala realizado sobre este tema hasta la fecha, Arthur P., Wolf, antropólogo de la Universidad de Stanford, examinó las historias matrimoniales de 14.400 mujeres.
Llevó a cabo su "experimento natural" en Taiwan.
Las familias de ese país adoptaban y criaban las futuras nueras.
Lo que indicaba que se buscaban maridos con los que se había crecido juntos desde la niñez. 
Wolf comparó esos matrimonios con los formados entre hombres y mujeres que no se conocían hasta el día de su boda.
Tomando el número de divorcios y la tasa de natalidad como medidas, respectivamente, de la felicidad matrimonial y de la actividad sexual los resultados respaldaban el efecto Westermarck:
la convivencia desde los primeros años de vida comprometía manifiestamente la compatibilidad matrimonial en la edad adulta.
Los primates están sujetos al mismo mecanismo.
Muchos primates evitan la endogamia a través de la migración de un sexo o del otro al llegar a la pubertad.
El sexo que emigra conoce nuevos compañeros con los que no se ha relacionado, mientras que el sexo residente gana diversidad genética del exterior. 
Además, los parientes cercanos que permanecen en el mismo entorno, suelen evitar las relaciones sexuales.

En los años cincuenta, Kisaburo Tokuda observó el mismo fenómeno en un grupo de macacos japoneses del zoo de Kyoto.
Cierto joven macho que había ascendido al rango superior de la jerarquía, utilizó sus privilegios sexuales copulando frecuentemente con todas y cada una de las hembras excepto con una: su madre.
No se trataba de ningún caso aislado.
Las cópulas madre-hijo se encuentran reprimidas en todos los primates.
Incluso entre los bonobos -a buen seguro, los primates de mayor actividad sexual sobre la faz de la Tierra- semejante acoplamiento es rarísimo, si no inexistente.
La anulación del incesto ha quedado ya demostrada en numerosos primates; se admite que el mecanismo responsable es la relación de familiaridad en edad temprana.

El efecto Westermarck sirve de escaparate del enfoque darwinista aplicado al comportamiento humano.
Por la sencilla razón de que esa hipótesis se apoya en una combinación de factor genético y factor educacional.
El cuadro consta de un componente de desarrollo (aversión sexual aprendida), un componente innato (el efecto de los vínculos familiares a temprana edad), un componente cultural (algunas culturas crían juntos niños sin mutua relación de parentesco, otras crían, separados, hermanos de distinto sexo, pero la mayoría de las familias se organizan de tal manera, que automáticamente se crean inhibiciones sexuales entre parientes), una adecuada razón evolutiva (supresión de la endogamia) y paralelismos directos con el comportamiento animal.
Corona todos esos componentes el tabú cultural. exclusivo de nuestra especie.
Sería bueno saber si el tabú del incesto sirve sólo para formalizar y fortalecer el efecto Westermarck o si añade una dimensión de nuevo cuño. 

La inesperada riqueza de un programa de investigación que integra el desarrollo, la genética, la evolución y la cultura al abordar un fenómeno circunscrito, demuestra cuánto se gana con la supresión de viejas barreras entre las disciplinas.
En el futuro, cabe presumirlo, el enfoque evolutivo del comportamiento humano adquirirá un refinamiento progresivo, pues irá incorporando de una manera explícita la flexibilidad cultural.
Por tanto, la dicotomía de los dos planteamientos tradicionales, el aprendizaje o el instinto, cederá paso a una perspectiva integradora.
Mientras tanto, los estudiosos del comportamiento animal se interesarán más por los efectos que el entorno ejerce sobre el comportamiento y, de forma especial -en primates y mamíferos marinos-, en la posibilidad de transmisión cultural de información y costumbres.
Hay comunidades de chimpancés que usan piedras para cascar nueces en la selva. mientras que otras comunidades tienen a su alcance las mismas nueces y piedras, y parece que no sepan qué hacer con ellas.
Para explicar esa disparidad no podemos apelar a la variación genética.

Más que mirar la cultura como antítesis de la naturaleza, obtendremos un conocimiento más profundo del comportamiento humano provocando silenciosamente el olvido del viejo debate factor genético/factor educacional.


Glucómica

Los azúcares desempeñan funciones críticas en muchas actividades celulares.
Pese a ello, su estudio había quedado rezagado respecto a la investigación sobre genes y proteínas.
Un retraso que empieza a recuperarse.

Descifrado el genoma humano, ha emergido en el horizonte con todo su atractivo el proteoma, conjunto de proteínas que nuestras células fabrican siguiendo las indicaciones de los genes.
Cumple a las proteínas realizar la mayoría de las tareas de nuestro organismo.
Parecería, pues, que la comprensión de su comportamiento debería traducirse en fuente de ideas para remediar los males que nos aquejan.
Pero en las células hay algo más que genes y proteínas.
Hidratos de carbono y lípidos desempeñan también funciones cruciales.
Sin su conocimiento no se entiende la forma de operar del organismo ni cabe esperar paliar las enfermedades.

Los hidratos de carbono, en particular, realizan una asombrosa gama de tareas.
No hace todavía mucho, se les reservaba una función de meros generadores de energía (en el caso de la glucosa y el glucógeno) o de elementos estructurales. 
Sabemos ya que se combinan con proteínas y lípidos en las superficies celulares; allí instalados, influyen en las comunicaciones intercelulares, el funcionamiento del sistema inmunitario, la capacidad patogénica de agentes infecciosos y la metástasis.
Contribuyen a la identificación celular y al control del tráfico de las células móviles por todo el organismo.
Tal es la ubicuidad de estas moléculas, que las células se presentan ante otras células y ante el sistema inmunitario revestidas de su manto.

Tras reconocer la importancia de los glúcidos en la salud y en la enfermedad, la ciencia básica y la industria farmacéutica se empeñan ahora en descubrir su estructura y actividad, con la vista puesta en la obtención de nuevos agentes terapéuticos.
Las autoridades políticas se han percatado también de su interés.
Por citar un ejemplo significativo, en octubre de 2001 los norteamericanos Institutos Nacionales de la Salud (NIH) concedieron una ayuda de 34 millones de dólares por un período de cinco años al Consorcio para la Glucómica Funcional, un grupo de 54 investigadores repartidos por todo el mundo involucrados en el desarrollo de una biblioteca de cadenas sintéticas de hidratos de carbono y una base de datos estructural.
James C. Paulson, del Instituto de Investigación Scripps de La Jolla, dirige los trabajos.

Complejidad

Los azúcares simples -como la glucosa y la sacarosa, que están formadas por unos cuantos átomos de carbono, oxígeno e hidrógeno- reciben los nombres técnicos de monosacáridos, disacáridos, etcétera, de acuerdo con el número de unidades de azúcar que contengan.
Si la cadena se prolonga, hablamos de oligosacáridos; 
los polisacáridos constituyen genuinas macromoléculas. 
Son glucoconjugados las moléculas integradas por hidratos de carbono asociados a proteínas (glucoproteínas) o a lípidos (glucolípidos).

El principal escollo con que se encontraban los bioquímicos del pasado residía en la penuria de herramientas para descifrar la estructura de las versiones complejas y para sintetizar sus moléculas de una manera reproducible.
Los problemas arrancaban de la extraordinaria variabilidad estructural de los glúcidos.
Los cuatro nucleótidos que intervienen en el ADN, y los veinte aminoácidos que forman las proteínas, se unen entre sí de manera lineal, a la manera de las cuentas de un rosario; se engarzan, además, mediante el mismo tipo de enlace químico siempre.
Por el contrario, los azúcares simples, diez más o menos, que son los habitualmente presentes en los hidratos de carbono de los mamíferos, pueden unirse entre sí por diversos puntos y formar estructuras ramificadas complejas.
Más aún, dos unidades enlazadas no siempre se orientan de la misma manera: a veces, una molécula cae perpendicular a uno de los planos de la otra unidad y, en otras ocasiones, perpendicular al plano opuesto.

Los cuatro nucleótidos del "alfabeto" del ADN pueden combinarse para producir 256 estructuras de cuatro unidades, diferentes entre sí.
Los 20 aminoácidos de las proteínas pueden producir hasta 16.000 configuraciones de cuatro unidades.
Pero los hidratos de carbono más simples del organismo pueden unirse, en teoría, para dar más de 15 millones de organizaciones distintas de cuatro componentes.
Aunque en la naturaleza no se presentan todas estas combinaciones, las posibilidades resultan en verdad asombrosas. 

Constituye, pues, un reto formidable determinar las secuencias de los componentes de los glúcidos complejos, para proceder luego a su síntesis.
El progreso en la glucómica, más aún que en la genómica, dependerá de los avances que se den en técnicas moleculares de secuenciación y en bioinformática.

Posibilidades

De entrada, un mejor dominio de los glúcidos comportaría un mayor refinamiento de los tratamientos existentes.
Fijémonos en la heparina, un fármaco común desde los años treinta.
Se indica este glúcido anticoagulante para evitar que se formen trombos durante una intervención quirúrgica.
Sin embargo, la mayoría de las preparaciones comerciales, extraídas del revestimiento intestinal del cerdo, son una mezcla heterogénea y pobremente caracterizada de compuestos constituidos por una cadena que contiene entre 200 y 250 unidades de monosacáridos. 
La potencia de la heparina y los efectos secundarios posibles varían no sólo de una compañía farmacéutica a otra, sino también de un lote al siguiente; ello exige comprobar sus propiedades conforme se va fabricando.

Los laboratorios farmacéuticos venden hoy versiones más pequeñas de la heparina, de masas moleculares bajas.
Han recortado partes que no son necesarias para la actividad del fármaco, con el ahorro consiguiente de efectos secundarios.
Pero también en esas nuevas presentaciones cuesta lograr partidas homogéneas.

Así las cosas, hace dos años, el equipo dirigido por Ram Sasisekharan, del Instituto de Tecnología de Massachusetts, decidió poner en práctica las herramientas que habían desarrollado para descifrar la secuencia completa del centro activo de la heparina.
(Es decir, la región responsable de la actividad biológica del compuesto).
Merced a la información obtenida, avanzan ahora los procedimientos de síntesis de heparinas más potentes y fiables, de masa molecular baja, al tiempo que se consigue encauzar sus propiedades farmacológicas para aplicaciones específicas. 

El paulatino control de los hidratos de carbono debería igualmente repercutir en la eficacia de las proteínas producidas con técnicas de ADN recombinante.
Para operar según es debido, ciertas proteínas han de llevar determinados glúcidos, agregados en lugares precisos. 
Pero las posibilidades técnicas no son a menudo suficientes.
Consideremos la eritropoyetina, un fármaco recombinante que se administra para estimular la producción de hematíes en pacientes anémicos o sometidos a diálisis renal.
Durante años, los laboratorios Amgen se vieron obligados a desechar hasta el 80 por ciento del fármaco que fabricaban, debido a una glucosilación inadecuada en virtud de la cual la molécula desaparecía en seguida de la sangre.
La compañía encontró entonces la manera de añadir dos glúcidos adicionales a los habituales en la eritropoyetina.
La nueva versión, comercializada con el nombre de Aranesp, permanece en la sangre mucho más tiempo que el fármaco primitivo, por lo que requiere una dosificación menos frecuente.

Además de mejorar fármacos ya existentes, importa ahondar en los hidratos de carbono que permitan introducir terapias innovadoras en múltiples patologías.
En unos casos, tales fármacos podrían consistir en glúcidos o glucoconjugados; en otros, podrían ser moléculas que influyan sobre las interacciones entre hidratos de carbono y otras moléculas, incluidas las interacciones con enzimas (catalizadores biológicos) que controlan la síntesis o degradación de las moléculas que portan azúcares.

Contra las infecciones

Entre otros ámbitos de interés, la investigación centra su atención en las enfermedades infecciosas, dominio en el que los fármacos glucídicos han cosechado ya un éxito extraordinario.
Sírvanos de botón de muestra la vacuna dirigida contra el Hemophilus influenzae del tipo b (Hib).
Su administración ha librado a muchos de la meningitis, letal a menudo, causada por el Hib.
Al presentar un glúcido de la bacteria ante el sistema inmunitario, la vacuna actúa como cebador de éste para que destruya de inmediato al microorganismo en cuanto penetre en el cuerpo.
Una primera versión que tenía sólo una cadena de hidrato de carbono del Hib resultó ineficaz.
Pero desde finales de los años ochenta contamos ya con preparaciones de glucoconjugados, sumamente eficaces, en las que el azúcar está unido a una proteína que activa la respuesta inmunitaria.
Hay en estudio otras vacunas con glucoconjugados para enfermedades infecciosas, incluida la que debe proteger contra infecciones de Staphylococcus aureus en pacientes hospitalizados.

Diversos organismos patógenos se sirven de hidratos de carbono para reconocer e interaccionar con sus células huésped.
Algunos fármacos existentes y otros en elaboración contienen glúcidos o moléculas que remedan a aquéllos con el fin de bloquear dicho contacto. 
El virus de la gripe, por ejemplo, penetra en las células que infecta, después de unirse con el ácido siálico que sobresale de las glucoproteínas en la superficie celular. 
La unión a ese azúcar abre las "puertas" celulares, dejando que el virus penetre y se replique en el interior de la célula.
Cuando los virus recién formados emergen entonces de la célula, pueden quedar atrapados por el mismo azúcar y han de desplegar la enzima neuraminidasa para que corte el enlace con el azúcar y queden liberados. 
Dos fármacos, comercializados con los nombres de Tamiflu y Relenza, acortan la duración de la gripe al unirse fuertemente al centro activo de la enzima, evitando así su acción sobre el ácido siálico. 
Neutralizada la neuraminidasa, no le será fácil al virus propagarse e infectar a otras células.

En el caso del virus de la gripe, el fármaco inutiliza al glúcido genuino.
Accede así a la enzima e inhibe su actividad, en un proceso de inhibición competitiva. 
La aplicación de ese mecanismo empleando análogos sintéticos de los glúcidos problemáticos podría revestir interés en la lucha contra otras enfermedades infecciosas.
Así, en el caso de Helicobacter pylori , bacteria que produce úlceras de estómago e inflamación; se instala ésta en el organismo al unirse a un glúcido de la superficie de las células que tapizan el estómago.
O en el de Shigella disenteryae , bacteria responsable de epidemias de diarrea mortales, que sintetiza una toxina que se une a un glúcido de las células intestinales.
Moléculas imitadoras de azúcares, que actúan de señuelos, al unirse a H. pylori o a la toxina de S. disenteryae , y evitan con ello su anclaje celular, constituyen ya una promesa en las pruebas de laboratorio.

Suele seguirse una estrategia similar en la lucha contra el choque séptico (una afección de la circulación a menudo fatal), causado por una bacteria gram negativa. (Las bacterias se denominan "gram positivas" o "gram negativas" según su reacción ante un colorante determinado).
Se produce el choque séptico cuando las bacterias mueren -a raíz de un tratamiento con antibióticos- y liberan un glucolípido, el lípido A, en el torrente sanguíneo desencadenando una respuesta inflamatoria catastrófica.
La administración de un análogo del lípido A, que no inste una drástica respuesta inmunitaria, podría reducir o eliminar el choque al actuar de cebo y mantener las células del sistema inmunitario alejadas del lípido A genuino del organismo.
Esos sucedáneos podrían limitar también la replicación bacteriana y la producción de lípido A. Casi todas las enfermedades infecciosas están producidas por virus, bacterias, hongos o parásitos.
Sin embargo, algunas afecciones cerebrales, pensemos en la enfermedad de Creutzfeldt-Jakob, podrían deberse a priones, proteínas mal plegadas. 
De acuerdo con las investigaciones de John Collinge, del Hospital St.
Mary de Londres, la resistencia peligrosa que oponen los priones cabría atribuirla a una glucosilación inadecuada de las proteínas, que se tornan así protegidas contra la degradación enzimática fisiológica. 

Restauración del equilibrio 

Los fármacos basados en hidratos de carbono podrían tener también un papel en la lucha contra una serie de enfermedades no infecciosas.
Nos referimos, en particular, a las afecciones marcadas por un exceso de inflamación. 
Ante una herida o infección, las células endoteliales que tapizan los vasos sanguíneos comienzan a desplegar una batería de selectinas, proteínas que se unen a hidratos de carbono.
Las selectinas de las células endoteliales se enlazan débilmente a sialil Lewisx (hidratos de carbono específicos) en la superficie de los leucocitos circulantes del sistema inmunitario.
Como una pelota de tenis que rueda sobre una cinta de velcro, los leucocitos tropiezan en la pared del vaso y se frenan lo suficiente para emigrar fuera de la pared hacia el tejido lesionado, donde se instalan para contener la amenaza.
Esa respuesta, importante para la defensa de la salud, puede convertirse en causa de enfermedad, si llega a hacerse crónica o peca de desproporcionada.
Las sustancias que impiden el contacto entre sialil Lewis x y selectinas se hallan hoy en fase de desarrollo para su aplicación como fármacos antiinflamatorios potenciales.

La investigación explora también estrategias glucídicas para combatir el cáncer.
Sabido es que las células malignas despliegan a menudo hidratos de carbono incompletos o anormales en su superficie.
Se pretende, en consecuencia, incorporar esos azúcares en vacunas que induzcan al sistema inmunitario a reconocer y destruir las células cancerosas que lleven tales glúcidos. 

A este respecto, el grupo de Sasisekharan, del MIT, demostró recientemente en el ratón que los heparansulfatos, azúcares que se encuentran en las células normales y malignas, pueden potenciar o limitar el crecimiento canceroso; depende que ocurra lo uno o lo otro de la forma en que dichos glúcidos se degraden o eliminen por las enzimas celulares.
Este descubrimiento ha llevado a sugerir un tratamiento del cáncer mediante la administración de fragmentos del azúcar frenadores del crecimiento o mediante la administración de alguna sustancia que hiciese que las propias células cancerosas produjeran una cantidad más saludable del fragmento deseable.

De la letalidad del cáncer es culpable la formación de metástasis.
Se da ese fenómeno cuando las células malignas se desprenden del tumor y horadan el tejido conjuntivo para adentrarse en el torrente sanguíneo. 
A través de la sangre (o la linfa) viajan hasta tejidos alejados, donde abandonan la circulación y establecen nuevos tumores.
Una de las moléculas que parecen estar vinculadas con ese curso es una proteína enlazada a un hidrato de carbono.
La galectina-3, así se llama la proteína, fomenta además las metástasis con su participación en la angiogénesis (formación de nuevos vasos sanguíneos): ayuda a las células tumorales a resistir las señales de autodestrucción. 
Para bloquear tales efectos dañinos, los laboratorios Glyco-Genesys están realizando pruebas clínicas con un hidrato de carbono derivado de la pectina del cidro que se une a la galectina-3.

Para formar glucoconjugados las células siguen una serie de pasos, durante los cuales diversas enzimas añaden o eliminan grupos de glúcidos.
Intervienen luego ciertas enzimas situadas en el interior de lisosomas, compartimentos rodeados por membranas, que degradan los glucolípidos y las glucoproteínas inservibles.
En las enfermedades de Gaucher y Tay-Sachs acontece que la enzima lisosómica involucrada es defectuosa, con la consiguiente acumulación destructiva de glucolípidos en el organismo.
La enfermedad de Gaucher puede frenarse en la actualidad por la administración de la enzima normal, una vez modificada de suerte tal que despliegue un hidrato de carbono que la encamine hacia el tipo celular específico.
En concreto, la molécula de manosa dirige la enzima que degrada glucolípidos hacia los macrófagos, que son especialmente sensibles a la pérdida de la enzima.

La terapia enzimática tiene que administrarse por vía intravenosa.
Por tratarse de proteínas, las enzimas se degradarían en el tracto digestivo si se administrasen por vía oral.
Más aún, las enzimas no atraviesan la barrera hematoencefálica y, por tanto, no pueden combatir las lesiones de las células nerviosas del cerebro.
La investigación se centra, pues, en limitar la síntesis de glucolípidos mediante la reducción de la cantidad producida en primer lugar; en particular, a través de la administración de compuestos pequeños, así sucedáneos glucídicos, que estén capacitados para inhibir las enzimas implicadas en la síntesis de glucolípidos.
Uno de estos fármacos, desarrollado por Oxford Glyco Sciences, se administraría por vía oral.
Se ha comprobado la eficacia del mismo en las pruebas con humanos.

De la investigación glucómica cabría incluso esperar que aporte las bases para posibles trasplantes de órganos de cerdo a los humanos.
Entre los obstáculos que se interponen en el camino de los xenotrasplantes destaca uno:
el tejido del cerdo presenta un hidrato de carbono que no se encuentra en los tejidos humanos.
Su introducción despertaría una reacción rápida de destrucción del injerto por el sistema inmunitario del receptor.
Este impedimento podría superarse tal vez mediante la administración de sucedáneos de azúcares que sirvieran de señuelo o mediante la manipulación genética de cerdos, para que sus enzimas no promuevan el glúcido indebido. 

El desarrollo de fármacos glucídicos ha de hacer frente a graves dificultades, en especial cuando se trata de preparados que comprenden hidratos de carbono genuinos.
El sistema digestivo considera alimentos a los azúcares, por lo que tendrían que empaquetarse para evitar su degradación, o inyectarse.
En la sangre, las enzimas podrían degradarlos también.
Además, puesto que los hidratos de carbono operan mediante su unión lábil a muchos sitios y no por enlaces vigorosos a unos pocos, habría que administrarlos en grandes cantidades.
Con todo, ninguno de estos obstáculos parece insuperable.


Origen africano reciente de los humanos 

La genética nos revela que una mujer africana, de hace 200.000 años, fue nuestro antepasado común En la búsqueda de hechos relativos a la evolución humana, los genéticos moleculares nos hemos empeñado, por segunda vez, en un debate con los paleontólogos.
Tiempo atrás, creían éstos que la división entre los humanos y los antropomorfos ocurrió hace 25 millones de años;
nosotros manteníamos que los genes de humanos y antropomorfos eran demasiado parecidos para que el cisma se remontara más allá de algunos, pocos, millones de años.
Tras 15 años de porfía, ganamos la partida, al admitir los paleontólogos que la razón estaba de parte nuestra.

De nuevo andamos a la greña; ahora, a propósito de la última fase de la evolución humana.
Afirman los paleontólogos que los humanos modernos evolucionaron, desde sus predecesores arcaicos dispersos por todo el mundo, durante el último millón de años.
A ello contraponemos las comparaciones genéticas, que nos dan pie para sostener que todos los humanos vivientes pueden remontarse por líneas genealógicas maternas de descendencia hasta una mujer que vivió hace unos 200.000 años, probablemente en Africa.

Los humanos modernos surgieron en un lugar y de ahí se extendieron a otros.

ALLAN C. WILSON y REBECCA L. CANN han aplicado técnicas genéticas a la paleontología en muchas de sus colaboraciones. 
Hasta su muerte en 1991 Wilson fue profesor de bioquímica en la Universidad de California en Berkeley.
Nacido en Nueva Zelanda, inició su formación en la Universidad de Otago en 1955, que prosiguió en la estatal de Washington hasta recibir el doctorado por Berkeley.
Trabajó en el Instituto Weizmann de la Ciencia, la Universidad de Nairobi y la de Harvard.
Cann da clases de genética y biología molecular en la facultad de medicina John A. Burns adscrita a la Universidad de Hawai en Manoa.
Se doctoró en antropología por Berkeley, donde trabajó luego como becaria de Wilson.
Una de sus líneas de investigación actuales está centrada en la aplicación del ADN mitocondrial a la determinación de la diversidad genética en aves de las islas Hawai.

Ni la información genética de los vivos, por sí sola, ni los restos fósiles de los muertos, de manera exclusiva, pueden explicar cómo, cuándo y dónde se originaron las poblaciones.
Pero la primera goza de ventaja decisiva al poder determinar la estructura de los árboles genealógicos:
los genes vivos deben tener antecesores, mientras que los muertos fósiles pueden no tener descendientes.
Los biólogos moleculares saben que los genes objeto de su estudio tuvieron que pasar a través de linajes que sobrevivieron hasta hoy; los paleontólogos no pueden estar seguros de que los fósiles que examinan no conducen a una calle cortada.

El método molecular está exento de otras limitaciones de la paleontología.
No necesita fósiles bien datados o utensilios de cada parte del árbol familiar que espera describir.
No está viciado por dudas sobre si los utensilios hallados cerca de los fósiles fueron hechos y usados por la población que éstos representan.
Finalmente, se ocupa de una serie de características que es completa y objetiva.

Del genoma, o serie total de genes, decimos que está completo porque contiene toda la información biológica heredada por un individuo.
Más aún, todas las variantes del genoma que aparecen en el seno de una población -grupo de individuos interfecundos- pueden igualmente estudiarse, de forma que las peculiaridades específicas no tienen por qué distorsionar la interpretación de los datos. 
Los genomas son fuentes objetivas de datos porque ofrecen pruebas no calificadas, a fin de cuentas, por ningún modelo evolutivo.
Las secuencias génicas, empíricamente verificables, no se moldean con prejuicios teóricos.

El registro fósil, por otra parte, es bellacamente inconsistente, pues un montón de huesos conservados podría no representar a la mayoría de organismos que no dejaron vestigio.
Los fósiles no admiten, por principio, una interpretación objetiva;
los caracteres físicos por los que son clasificados reflejan necesariamente los modelos que los paleontólogos quieren probar.
Si se clasifica, por ejemplo, una pelvis como humana porque soportó una postura erguida, se está presuponiendo que la bipedestación distinguió a los primeros homínidos de los antropomorfos. 
Tal discurso tiende a la circularidad.
La perspectiva del paleontólogo porta un sesgo estructural que limita su poder de observación.

Los biólogos expertos en teoría de la evolución han de rechazar la idea de que los fósiles proporcionan la prueba más directa del proceder real de la evolución humana.

Los fósiles ayudan a rellenar el conocimiento de cómo operaron en el pasado los procesos biológicos, pero no deben cegarnos ante nuevas líneas de prueba ni ante nuevas interpretaciones de materiales arqueológicos mal entendidos y fechados de manera provisional.

Todas las ventajas de nuestra disciplina quedaron patentes en 1967, cuando Vincent M. Sarich, trabajando en el laboratorio de Wilson en la Universidad de California en Berkeley, se enfrentó a Ramapithecus , un primate fósil. 
Los paleontólogos habían datado sus restos en unos 25 millones de años.
Sobre la base del espesor del esmalte dental y otros caracteres del esqueleto creían que el ramapiteco había aparecido después de la divergencia de los linajes humano y antropomorfo y que era directo antecesor de los humanos.

Sarich midió la distancia evolutiva entre humanos y chimpancés estudiando sus seroproteínas.
Sabía que las diferencias entre éstas reflejan mutaciones acumuladas desde su separación.
(En aquel tiempo, era más fácil comparar sutiles diferencias de proteínas que cotejar las secuencias genéticas que codifican las proteínas.)
Para comprobar que las mutaciones habían ocurrido con idéntica cadencia en ambos linajes, comparó chimpancés y humanos con una especie de referencia y encontró que todas las distancias genéticas encajaban.

Sarich disponía ya de un reloj molecular; el paso siguiente era calibrarlo.
Para lo cual, calculó el índice de mutación en otras especies cuyas divergencias podían datarse con verosimilitud por fósiles.
Luego aplicó el reloj a la separación entre chimpancés y humanos, datándola entre hace cinco y siete millones de años -mucho más tarde de lo que nadie imaginara. 

Al principio, la mayoría de los paleontólogos se aferraban al dato mucho más antiguo.

Pero nuevos hallazgos fósiles minaban el status humano de Ramapithecus ;
hoy está claro que Ramapithecus es igual que Sivapithecus , un antecesor de los orangutanes, no de antropomorfo africano alguno.
Más aún, la edad de algunos sivapitecos se rebajó hasta sólo unos seis millones de años.
A comienzos de la década de los ochenta, casi todos los paleontólogos acabaron aceptando el dato más reciente de Sarich para la separación de las líneas humana y de antropomorfos.
A los que seguían rechazando sus métodos no les quedó otro asidero que el de aducir que el biólogo molecular había dado con la solución correcta por pura suerte.

De las primeras comparaciones entre proteínas de especies diferentes brotaron dos nuevas ideas: la de las mutaciones neutras y la del reloj molecular.
Con respecto a la primera, la evolución molecular parece dominada por esas mutaciones fútiles que se acumulan con una cadencia sorprendentemente regular en los linajes supervivientes.

Dicho de otro modo, la evolución génica es, sobre todo, resultado de la acumulación inflexible de mutaciones que no parecen ser dañinas ni beneficiosas. 
La segunda idea, la de los relojes moleculares, surgió de la observación de que el ritmo de cambio genético según mutaciones puntuales (cambios en determinados pares de bases de ADN) era tan regular, en largos períodos, que se las podía usar para datar divergencias de troncos comunes.

Pudimos aplicar estos métodos a la reconstrucción de las últimas etapas de la evolución humana después de 1980, cuando el análisis de restricción del ADN permitió explorar las diferencias genéticas con alta resolución.
En Berkeley, Wes Brown, Mark Stoneking y los autores recurrimos a ese medio para rastrear las genealogías maternas de muestras de personas en todo el mundo.

El ADN que estudiamos reside en las mitocondrias, orgánulos celulares que convierten alimentos en energía disponible para el resto de la célula.
A diferencia del ADN nuclear, que forma haces de largas fibras, constituida cada una por una doble hélice revestida de proteínas, el ADN mitocondrial se presenta en pequeños anillos de doble filamento.
Mientras el ADN nuclear codifica unos 100.000 genes -casi toda la información necesaria para formar un ser humano, el ADN mitocondrial sólo codifica 37.

En este puñado de genes, cada uno es imprescindible: 
sabemos que una sola mutación adversa en uno de ellos causa graves enfermedades nerviosas.

El ADN mitocondrial tiene una doble ventaja sobre el ADN del núcleo, para los científicos que investigan el momento de separación de los linajes.
Primera, las secuencias de ADN mitocondrial que nos interesan acumulan mutaciones rápida y constantemente, según observaciones empíricas.
Por tratarse de mutaciones que, en muchos casos, no alteran la función de la mitocondria, son de hecho neutras, y la selección natural no las elimina. 

Este ADN mitocondrial se comporta, pues, como un reloj de rápido tic-tac, lo que es fundamental para identificar cambios genéticos recientes.
Cualquier par de humanos escogidos al azar de cualquier punto del planeta se parecen tanto en casi todas sus secuencias de ADN, que podemos medir la evolución de nuestra especie concentrándonos sólo en los genes que mutan más deprisa.
Los genes que controlan los rasgos esqueléticos no pertenecen a este grupo.

Segunda ventaja: a diferencia del ADN nuclear, el de la mitocondria se hereda sólo de la madre, sin más cambio que las eventuales mutaciones.
La contribución paterna acaba en la papelera, como quien dice, de los recortes. 
Los genes nucleares, a los que el padre contribuye, se propagan en lo que podemos llamar linajes corrientes, que son por supuesto importantes para la transmisión de rasgos físicos.
Para nuestros estudios sobre origen de los humanos modernos, sin embargo, nos fijamos en los linajes mitocondriales y maternos.

Los linajes maternos son los más idénticos entre hermanos, porque su ADN mitocondrial sólo ha tenido una generación para incorporar mutaciones.
El grado de proximidad decrece paso a paso, según se avanza en la genealogía, desde los primos hermanos que descienden de la misma abuela materna hasta los primos segundos que descienden de la misma bisabuela materna, y así sucesivamente.
Según nos remontamos en la genealogía, mayor se hace el círculo de parientes maternos, hasta incluir a todos los vivientes.

De ello se infiere, en pura lógica, que todo el ADN mitocondrial humano debe haber tenido una última antecesora común.
Pero es fácil mostrar que ésta no vivió necesariamente en una pequeña población, ni fue la única mujer de su generación.

Imaginemos una población estática que siempre mantiene 15 madres.
Cada nueva generación tendrá 15 hijas, pero algunas madres no tendrán hija, mientras que otras tendrán dos o más.

Como las líneas maternas se extinguen siempre que no haya hija para continuarlas, es sólo cuestión de tiempo la desaparición de todos los linajes menos uno. 
En una población estable el tiempo para esta reducción a un linaje materno es la duración de una generación multiplicada por el doble del tamaño de la población. 

Podemos llamar "Eva" a la feliz mujer cuyo linaje permanece. 
Téngase en cuenta, no obstante, que en tiempos de esa Eva vivían otras mujeres y que Eva no tenía un puesto de favor en el tablero de crianza.
Es puramente beneficiaria de la suerte.
Más aún, si fuéramos a reconstruir los linajes de la población, nos retrotraerían a muchos de los hombres y mujeres contemporáneos de Eva. 
Los genéticos de poblaciones Daniel I. Hartl, de la facultad de medicina de la Universidad de Washington, y Andrew G. Clark, de la Universidad estatal de Pennsylvania, estiman que podían vivir entonces hasta 10.000 personas.
Por tanto, el nombre de Eva puede despistar -no es la fuente última de todos los linajes ordinarios, como fue la Eva bíblica.

Con los datos del ADN mitocondrial podemos definir los linajes maternos de personas vivas remontándonos hasta un antecesor común.
En teoría, un gran número de árboles genealógicos distintos pudo dar origen a cualquier serie de datos genéticos.
Para reconocer al más probablemente correcto, se puede aplicar el principio de parsimonia, que exige que los individuos estén relacionados del modo más sencillo posible.
El árbol hipotéticamente más eficaz debe contrastarse comparándolo con otros datos para ver si es coherente con ellos.
Si el árbol se mantiene, se le analiza como prueba de la historia geográfica ínsita en sus elementos.

En 1988 Thomas D. Kocher, entonces en Berkeley y hoy en la Universidad de New Hampshire, aplicó una interpretación parsimoniosa así a la interrelación del ADN mitocondrial de 14 individuos de todo el mundo.
Determinó que 13 nudos de ramificación eran el mínimo que podían reconocerse para las diferencias que encontró.
Teniendo en cuenta las consideraciones geográficas, concluyó que Africa era la patria humana más remota:
la distribución global de los tipos de ADN mitocondrial que vio podían explicarse, pues, del modo más simple con no más de tres hechos migratorios a otros continentes. 

Un supuesto crucial en este análisis es que todos los linajes mitocondriales evolucionan con la misma velocidad. 
Por esta razón, cuando Kocher realizó su comparación de ADN mitocondriales humanos, incluyó también secuencias análogas de cuatro chimpancés. 
Si las líneas humanas hubieran diferido en la velocidad con que sumaron mutaciones entonces unas de las 14 secuencias humanas hubieran estado más cerca o más lejos de las secuencias de los chimpancés que otras.
De hecho, las 14 secuencias humanas son todas casi equidistantes de las secuencias de los chimpancés, lo que implica una clara uniformidad en la velocidad de cambio de las humanas.

Los datos de los chimpancés ilustraron también la notable homogeneidad genética entre humanos: la variabilidad genética de los chimpancés decuplica la observada en humanos.

Dato que, por sí solo, sugiere que toda la humanidad moderna brotó de un lote bastante restringido de antepasados comunes.

Trabajando en Berkeley con Stoneking, generalizamos la investigación de Kocher con el examen de un árbol genealógico más amplio: 182 tipos distintos de ADN mitocondrial de 241 individuos.
Las presencias múltiples de tipos de ADN mitocondrial concurrían siempre entre gente de un mismo continente y normalmente en personas que vivían dentro de un radio de unos 150 kilómetros.
El árbol que construimos constaba de dos ramas principales y ambas reconducían a Africa, lo que respaldaba la hipótesis del origen africano de los humanos modernos.

De nuestro estudio se desprende que, si bien las barreras geográficas influyen en el ADN mitocondrial de una población, los habitantes del continente que consideremos no pertenecen todos, por lo común, a una misma línea materna. 
Los neoguineanos constituyen un ejemplo arquetípico. 
Sospechábase su diversidad genética a raíz de los análisis filológicos de sus familias lingüísticas -muy diversas, aunque clasificadas comúnmente como papú- habladas en esta sola isla.
En nuestro árbol genealógico, los neoguineanos mostraron varias ramas diferentes, señal de que la antepasada común de todos los neoguineanos no era de Nueva Guinea.
La población de Nueva Guinea debió de fundarse por muchas madres cuyas líneas maternas guardaban estrecho parentesco con las de Asia.

Este descubrimiento es lo que cabía esperar si la hipótesis del origen africano fuese cierta;
al salir de Africa hacia el Este, la gente tenía que cruzar Asia.
El viaje era probablemente lento, y, durante el tiempo que costó llegar a Nueva Guinea, se añadieron mutaciones tanto en los linajes que quedaron en Asia como en los que fueron más lejos.

Así pues, pobladores que se manifiestan emparentados en una misma raza geográfica no tienen por qué guardar un estrecho parentesco en su ADN mitocondrial.
En razón de las mitocondrias, no podemos abordar las razas como si fueran especies biológicas.
Las características anatómicas que unen a los neoguineanos no se recibieron, por herencia, de los primeros ocupantes;
antes bien, creemos que se desarrollaron tras nuevas colonizaciones, máxime como resultado de mutaciones en los genes nucleares, extendidas mediante el sexo y la recombinación por toda Nueva Guinea. 
Abundando en la misma idea, el color claro de la piel de muchos blancos es probablemente un desarrollo tardío en Europa, posterior a la colonización de este continente por africanos.

Durante los primeros años de la década de los ochenta, cuando construíamos nuestro árbol genealógico, nos apoyábamos en negros americanos, a falta de africanos, cuyo ADN mitocondrial era difícil de obtener en la cantidad necesaria.
Para nuestra ventura, la invención reciente de la reacción en cadena de la polimerasa ha vencido ese freno.

La técnica posibilita duplicar secuencias de ADN fácilmente, hasta el infinito;
una muestra inicial pequeña de ADN puede multiplicarse en una provisión inagotable.

La reacción en cadena de la polimerasa permitió a Linda Vigilant, hoy en la Universidad estatal de Pennsylvania, rehacer nuestro estudio usando datos de ADN mitocondrial de 120 africanos que representaban seis partes distintas de la región subsahariana.

Vigilant trazó un árbol genealógico cuyas 14 ramas más profundas conducen exclusivamente hasta Africa: la decimoquinta rama lleva a africanos y no africanos. 
Los no-africanos se hallan en ramas secundarias superficiales que brotan de la rama 15.
Considerando el número de ADN mitocondriales africanos y no-africanos estudiados, la probabilidad de que las 14 ramas más remotas no fueran exclusivamente africanas es de 1 contra 10.000 para un árbol con este orden de ramificación.

Satoshi Horai y Kenji Hayasaka, del Instituto Nacional de Genética de Japón, estudiaron de modo análogo muestras que incluían muchos más asiáticos e individuos de menos partes de Africa:
los linajes mitocondriales reconducían, de nuevo, a Africa.
La probabilidad, por nosotros calculada, de que llegaran accidentalmente a esta conclusión era sólo de 4 contra 100.
Aunque estas evaluaciones estadísticas no son pruebas incontrovertibles ni definitivas, confieren verosimilitud a la teoría del origen africano para el ADN mitocondrial humano.

Como nuestras comparaciones con los datos de chimpancés demostraban que el reloj de ADN mitocondrial humano poseyó un tic-tac constante durante millones de años, sabíamos que era posible calcular cuándo vivió la madre común de la humanidad.
Partimos del supuesto de que los linajes humano y del chimpancé se separaron hace cinco millones de años, como demostrara el trabajo de Sarich.
Después calculamos cuánto se habían separado los humanos entre sí con relación a su distanciamiento de los chimpancés;
esto es, encontramos el índice entre la divergencia del ADN mitocondrial humano y la que separa a humanos y chimpancés.

Usando dos series diferentes de datos, resolvimos que el índice era inferior a 1/25.

Por tanto, los linajes maternos humanos crecieron aparte en un período de menos de 1/2SaVa parte de cinco millones, o sea, menos de 200.000.
Con una tercera serie de datos sobre cambios en una sección del ADN mitocondrial llamada la región de control, obteníamos una fecha más antigua para la madre común, fecha que, sin embargo, resulta menos segura, porque persisten dudas sobre la corrección necesaria ante la avalancha de mutaciones que presenta la región de control.

Cabe objetar que un reloj molecular que se sabe exacto para cinco millones de años puede no ser fiable en períodos más cortos.
Es concebible, por ejemplo, que haya intervalos de estancamiento genético, interrumpidos por cortos estallidos de cambio, cuando acaso un nuevo mutágeno entre en el ambiente, un virus infecte células de la línea germinal o un intensa selección natural afecte todos los segmentos de ADN.
Para excluir la posibilidad de que el reloj funcione con pausas y arranques, medimos cuánto ADN mitocondrial ha evolucionado en poblaciones fundadas en un tiempo conocido.

Se estima que las poblaciones aborígenes de Nueva Guinea y Australia se fundaron hace menos de 50.000 o 60.000 años.
La cantidad de evolución operada desde entonces, en cada uno de esos lugares, parece cifrarse en un tercio del total de la especie humana.
De esto podemos deducir que Eva vivió hace el triple de 50.000 o 60.000 años, o sea, hace unos 150.000 a 180.000 años. 
Todos nuestros cálculos coinciden así en que la dispersión ocurrió hace cerca de 200.000 años.

Cálculos que encajan con una línea de prueba fósil, por lo menos.
Los restos humanos de anatomía moderna aparecen primero en Africa, luego en el Próximo Oriente y más tarde en Europa y Asia oriental.
Los antropólogos suponen que, en Africa oriental, la transición de humanos anatómicamente arcaicos a modernos ocurrió hace no más de 130.000 años. 

Por contra, una segunda línea de prueba no se acomoda a ese modo de ver las cosas.
El registro fósil muestra claramente que las partes meridionales de Eurasia se ocuparon por humanos arcaicos que emigraran de Africa a Asia hace cerca de un millón de años.

Fósiles tan célebres como el Hombre de Java y el Hombre de Pekín son de esa clase.
Este descubrimiento y la hipótesis de que la población eurasiática arcaica sufrió cambios anatómicos que los hizo más parecidos a los humanos modernos condujeron al modelo de evolución multirregional:
cambios evolutivos semejantes en diversas regiones geográficas convirtieron a los habitantes arcaicos microencefálicos en tipos modernos macroencefálicos.

Con todo, se hubieran necesitado altos niveles de flujo génico para mantener a las poblaciones humanas como una sola especie biológica.
El modelo de evolución multirregional también predice que al menos algunos genes de la moderna población esteasiática se hallarían más ligados a los de sus predecesores asiáticos arcaicos que a los de los africanos modernos.
Podríamos esperar encontrar linajes remotos en Eurasia, sobre todo en Extremo Oriente.
En cambio, las prospecciones en nuestros laboratorios y en otros, con más de 1000 personas de Eurasia y sus satélites mitocondriales (Australia, Oceanía y las Américas), no han dado indicios de tal resultado. 

Parece, pues, muy improbable que algún linaje verdaderamente antiguo haya sobrevivido oculto en Eurasia.
Sencillamente, no vemos el resultado que predice el modelo regional.

Más aún, los genéticos Masatoshi Nei, de Pensylvania, Kenneth K. Kidd, de Yale, James Wainscoat, de Oxford, y Luigi L. Cavalli-Sforza, de Stanford, encuentran apoyo al modelo de origen africano en sus estudios de genes nucleares. 

Los defensores del modelo multiregional de evolución ponderan que han documentado la continuidad de morfologías anatómicas entre residentes arcaicos y modernos de diversas regiones;
insisten en la improbabilidad de que esas morfologías se desarrollaran independientemente en un pueblo invasor.
Para que este argumento valga, debe mostrarse que los rasgos craneales en cuestión son en verdad independientes entre sí;
esto es, que la selección natural no tiende a favorecer ciertas constelaciones de formas funcionalmente relacionadas.
Pero sabemos que unos potentes músculos masticadores pueden imponer cambios en la mandíbula, el toro supraorbitario y otros puntos craneales;
las circunstancias que favorecieron la evolución de estas formas en una población pueden hacerlo de nuevo en otra población emparentada. 

Otros paleontólogos ponen en tela de juicio la existencia de tal continuidad.
Arguyen que las poblaciones modernas no están ligadas a las antiguas por características morfológicas que se desarrollaran sólo en el registro fósil.
Por contra, poblaciones fósiles y modernas están unidas por la retención común de rasgos ancestrales más antiguos.
La continuidad que ven los creyentes en la evolución multiregional podría ser mera ilusión.

La idea de que los humanos modernos cohabitaran en una región con arcaicos y acabaran sustituyéndolos por completo sin mezcla alguna puede parecer improbable.
Sin embargo, hay datos fósiles que respaldan la hipótesis. 
Los descubrimientos en las cuevas de Qafzeh en Israel sugieren que los Neandertales y los humanos modernos vivieron codo con codo durante 40.000 años, sin dejar apenas traza de mestizaje.

El modo en que una población humana reemplazó a los humanos arcaicos sin mezcla genética detectable continúa envuelto en el misterio.
Uno de nosotros (Cann) sospecha que las enfermedades infecciosas contribuyeron al proceso, promoviendo la eliminación de un grupo.
Cavalli-Sforza especula con el desarrollo en los antepasados de los humanos modernos de algún rasgo moderno, como la capacidad de lenguaje avanzado, que les impidió eficazmente cruzarse con otros homínidos.
Estas cuestiones y otras ligadas cederán cuando los biólogos moleculares aprendan a ligar secuencias genéticas específicas con rasgos físicos y de conducta.

E incluso antes, la investigación en el ADN nuclear y mitocondrial habrá de rendir árboles genéticos portadores de mayor información.
Especialmente seductivas son las secuencias del cromosoma Y, que determina la masculinidad y que, por tanto, se hereda sólo del padre. 
El laboratorio de Gerard Lucotte, en el College de France, ha comparado indirectamente tales secuencias en un esfuerzo por retrazar linajes paternos hasta un único progenitor -"Adán", si se quiere.
Estos resultados preliminares también apuntan a una patria africana;
con ulterior refinamiento, este trabajo sobre linajes paternos podría ofrecer quizás un valioso contraste para nuestros resultados sobre linajes maternos. 

Por desgracia, los cambios de bases se acumulan despacio en las regiones útiles del cromosoma Y, lo que hace difícil llevar un análisis genealógico detallado.

Mayor progreso cabe aún esperar en el futuro inmediato, a medida que los biólogos moleculares vayan aplicando sus técnicas a materiales descubiertos por nuestros amistosos rivales, los paleontólogos.
Se han realizado los primeros estudios sobre ADN de tejidos momificados hallados en un pantano de Florida y datados en hace 7500 años.

Métodos avanzados para obtener ADN de huesos fósiles más viejos parecen ya al alcance de la mano.
Con ellos, podemos empezar a construir el árbol familiar desde una raíz que viviera en la juventud de la familia humana. 


Ensayo aleatorio sobre irinotecan más tratamiento de soporte frente a tratamiento de soporte solo después del fracaso del fluorouracilo en pacientes con cáncer colorrectal metastásico

Introducción

El cáncer colorrectal es uno de los tumores malignos más frecuentes del adulto y afecta a una de cada 20 personas en los EE.UU., así como en la mayor parte de países desarrollados.
A pesar de que un 40-50% de los pacientes pueden curarse con la intervención quirúrgica, muchos de ellos desarrollarán una enfermedad metastásica. 
En estos pacientes, el tratamiento con fluorouracilo, modulado habitualmente mediante ácido folínico, es la única opción, y su supervivencia mediana es de 10-12 meses.
Cuando el tumor progresa después de un tratamiento de primera línea con fluorouracilo, no se dispone de tratamiento estándar.
La supervivencia es corta y se asocia a pérdida de peso, a inicio o empeoramiento de los síntomas relacionados con el tumor y a mala calidad de vidas.
Puesto que con los agentes citotóxicos se obtienen bajas tasas de respuesta y pueden presentarse graves efectos adversos, numerosos pacientes reciben tratamiento de soporte.
En la quimioterapia de primera línea, el fluorouracilo con ácido folínico es mejor que el tratamiento de soporte (o la quimioterapia retardada) en cuanto a supervivencia y calidad de vida, pero no se ha demostrado todavía el valor de la quimioterapia de segunda línea en el cáncer colorrectal metastásico.

El irinotecan es un inhibidor de la topoisomerasa I que bloquea el paso de la replicación enzimática del ADN, produciendo múltiples interrupciones del ADN de una sola cadena, lo que finalmente bloquea la división celular. 
En los estudios en fase II, el irinotecan posee actividad antitumoral objetiva en los pacientes con cáncer colorrectal metastásico, incluso en los pacientes con tumores documentados resistentes al fluorouracilo con tasas de respuesta del 11-23%. 
Un 40% adicional de pacientes experimentaron estabilización del tumor durante una mediana de 5 meses.
Los efectos adversos frecuentes incluyeron diarrea retardada, neutropenia, síndrome colinérgico precoz, náuseas y vómitos, alopecia y astenia.
Los efectos tóxicos limitantes de la dosis fueron diarrea grave y neutropenia.

El ensayo que describimos se inició en 1995 para comparar el irinotecan con el tratamiento de soporte solo con respecto a la supervivencia, calidad de vida y otras variables clínicas.
Los participantes fueron pacientes con cáncer colorrectal metastásico en los que la quimioterapia con fluorouracilo había fracasado.

Métodos

Objetivos

La supervivencia global fue el objetivo principal.
Los objetivos secundarios fueron el impacto del tratamiento sobre el estado general, el peso corporal, los síntomas relacionados con el tumor y la calidad de vida.

Selección de los pacientes 

Para ser candidatos a la aleatorización, los pacientes tenían que cumplir los siguientes criterios: cáncer colorrectal metastásico demostrado histológicamente; 
enfermedad metastásica progresiva documentada partiendo de la base de un aumento del 25% del tamaño de las lesiones diana o de un incremento del antígeno carcinoembrionario de 1,25 veces el valor de referencia inicial y con un valor banal superior a xxx , lo que permitió la inclusión de pacientes con una enfermedad no mensurable (carcinomatosis peritoneal y recidivas pélvicas);
progresión documentada por dos determinaciones no separadas por más de 6 meses y progresión del tumor mientras eran tratados con fluorouracilo o al cabo de 6 meses de la última perfusión de fluorouracilo;
haber recibido tratamiento adyuvante y/o no más de dos regímenes paliativos basados en el fluorouracilo; edad, 18-75 años; estado general de la OMS 0-2; neutrófilos xxx o superiores; plaquetas xxx o superiores; bilirrubina total xxx el limite superior de la normalidad de cada centro (LSN) o inferiores; transaminasas hepáticas 3 x LSN (en el caso de metástasis hepática bilirrubina xxx LSN y transaminasas xxx LSN), o superiores; creatinina sérica xxx o inferiores; lavado de 4 semanas para la radioterapia ola quimioterapia, y consentimiento informado por escrito.

No fueron candidatos los pacientes con los siguientes criterios: tratamiento previo con inhibidores de la topoisomerasa I, enfermedad extensa (que afectara a más del 50% del volumen del hígado o a un 25% del volumen pulmonar o una masa abdominal xxx ), con metástasis en el sistema nervioso central o con obstrucción intestinal o diarrea no resuelta.

Aleatorización y tratamientos del estudio

Las hojas de registro se emitieron a Rhóne-Poulenc Rorer Research and Development (Antony., Francia) donde se verificaron electrónicamente los criterios de elegibilidad.
Si se cumplían todos los criterios, la aleatorización se llevaba a cabo electrónicamente en la proporción de irinotecan con respecto al tratamiento de soporte 2:1, con una estratificación por centro.

En el grupo irinotecan, los pacientes recibieron el mejor tratamiento de soporte más 350 mg/m2 de irinotecan, diluido en 250 ml de suero fisiológico o dextrosa, administrado mediante una perfusión intravenosa de 90 min. cada 3 semanas (o 300 mg/m2 si la edad era > 70 años o el estado general de 2 de la OMS, de acuerdo con los factores de riesgo identificados previamente para el desarrollo de toxicidad).
Se proporcionaron directrices para el tratamiento de los efectos adversos (panel). 
El tratamiento tenía que iniciarse no después de 8 días tras la aleatorización.
En el grupo de tratamiento de soporte solo, los pacientes recibieron el mejor tratamiento de soporte y fueron visitados cada 3 semanas. 
El tratamiento de soporte se definió como el mejor tratamiento disponible según lo considerado por el médico responsable, de acuerdo con los patrones institucionales de cada centro.
El tratamiento de soporte incluyó antibióticos, analgésicos, transfusiones, corticoides o cualquier otro tratamiento sintomático (excepto irinotecan u otro inhibidor de la topoisomerasa I) y/o la asistencia de un psicoterapeuta. 
Se permitió la radioterapia localizada para aliviar síntomas, como el dolor, siempre que la dosis total administrada se encontrara dentro de los límites paliativos de acuerdo con los patrones institucionales.

Análisis del mejor tratamiento de soporte

El tratamiento de soporte y las medicaciones concomitantes se mencionaron en cada visita (cada 3 semanas en ambos grupos). 
Se clasificaron con el diccionario de la OMS; se efectuó una subclasificación adicional con el código de la OMS para la clase terapéutica anatómica (CTA). 
Con estas clasificaciones, los analgésicos se dividieron en opioides y no opioides y se analizaron en bloques de 3 semanas.

Seguimiento

Los pacientes visitaron al investigador para evaluación y tratamiento cada 3 semanas.
Se mencionaron todos los electos adversos de acuerdo con los criterios de toxicidad común del National Cancer Institute.
Después de la interrupción del tratamiento en el grupo irinotecan, los pacientes fueron evaluados regularmente al igual que el grupo de tratamiento de soporte (estado del tumor, síntomas o efectos adversos cada 3 semanas) hasta la muerte o durante al menos un año.
Después de un año, sólo se controló la fecha de la muerte.

Calidad de vida

La calidad de vida se evaluó con el cuestionario QLQ-C30 de la European Organization for Research and Treatment of Cancer (EORTC) (incluyendo cinco escalas funcionales, una escala del estado de salud global y nueve escalas de síntomas) que se cumplimentó en el período inicial y a las 3 y 6 semanas y, posteriormente, cada 6 semanas.
Después de la interrupción del tratamiento en el grupo irinotecan, los pacientes continuaron cumplimentando los cuestionarios QLQ-C30 cada 6 semanas al igual que el grupo de tratamiento de soporte.

Análisis estadístico

Las curvas de supervivencia se estimaron con el método de Kaplan-Meier en una población aleatorizada y se compararon mediante una prueba de rangos logarítmicos bilateral. 
Con una p=0,05 y una potencia de 0,80, se requerían 264 pacientes (176 en el grupo irinotecan y 88 en el grupo de tratamiento de soporte) para poner de manifiesto una diferencia significativa de la supervivencia al año desde el 20% (tratamiento de soporte) hasta el 35% (irinotecan).

La aleatorización fue estratificada por centros. 
Se planificó una estratificación retrospectiva para tener en cuenta los factores pronósticos basales (sexo, edad, estado general, pérdida de peso, presencia de metástasis hepáticas, localización primitiva del tumor, numero de localizaciones metastásicas, respuesta al tratamiento con fluorouracilo y duración de éste, así como su objetivo [adyuvante o paliativo], hemoglobina, recuento de leucocitos, plaquetas. lacticodeshidrogenasa, transaminasas, fosfatasa alcalina, bilirrubina, proteínas y antígeno carcinoembrionario).

El análisis se basó en el principio de intención de tratar y los pacientes fueron analizados de acuerdo con el grupo al que fueron asignados.
La asociación de factores pronósticos con la supervivencia se estimó con el modelo de riesgos proporcionales de Cox para datos de supervivencia censurados.
La selección del modelo para identificar las variables con efecto sobre la supervivencia se baso en un procedimiento gradual progresivo.
Los valores de p para la inclusión y la retirada fueron 0,05 y 0,06, respectivamente.
Después de haber determinado el modelo pronóstico, el efecto del tratamiento después de un ajuste para otros factores pronósticos se estimó incluyéndolo en el modelo.
La supervivencia sin una perdida de peso de más de 5%, la supervivencia sin deterioro del estado general, y la supervivencia libre de dolor se estimaron mediante el método de Kaplan-Meier y se compararon con una prueba de rangos logarítmicos bilateral.

Las variables de calidad de vida se compararon con análisis multivariables y univariables de los valores en el periodo inicial, durante el estudio, de la peor puntuación de los pacientes durante el periodo del ensayo y de los cambios desde el periodo inicial.
Las estimaciones de Kaplan-Meier y las pruebas de rangos logarítmicos se llevaron a cabo en relación con el tiempo hasta un deterioro definitivo de la calidad de vida con ocho umbrales diferentes para el deterioro.

Resultados

Datos de los pacientes

Fueron asignados aleatoriamente a irinotecan más tratamiento de soporte 189 pacientes y a tratamiento de soporte solo 90 pacientes (fig. 1).
En la tabla 1 se exponen las características de los pacientes.
Éstas fueron similares para ambos grupos, excepto para el estado general (un mayor número de pacientes tenían un mal estado general en el grupo de tratamiento de soporte) y la anemia.

Sólo 27 pacientes del grupo irinotecan (14%) y nueve del grupo de tratamiento de soporte (10%) fueron evaluados mediante el antígeno carcinoembrionario solo, con razones medias del antígeno en relación a los valores de referencia de 2,24 y 2,83, respectivamente.

No recibieron irinotecan 6 pacientes de este grupo.
El irinotecan se administró durante una mediana de 4,1 (0,7-12,6) meses y 172 pacientes (91%) recibieron su primera perfusión en los primeros 8 días desde la aleatorización. 
En el grupo irinotecan, 40 pacientes (21%) recibieron quimioterapias anticáncer posteriores (31 un régimen de fluorouracilo y nueve un fármaco diferente del irinotecan). 
En el grupo de tratamiento de soporte, 28 pacientes (31%) recibieron quimioterapia (21 un régimen de fluorouracilo, nueve otros fármacos y uno irinotecan).
En el grupo de tratamiento de soporte, 17 pacientes (19%) recibieron quimioterapia dentro del primer mes desde la aleatorización en comparación con dos (1%) del grupo irinotecan.
Las medicaciones concomitantes en el período basal fueron similares en ambos grupos y consistieron principalmente en analgésicos. 

Supervivencia global

En la figura 2 se presentan las estimaciones de Kaplan-Meier. 
En el grupo irinotecan se identificaron 123 acontecimientos (65%), la duración mediana de los cuales fue de 9,2 meses (límites, 0-18,9).
La probabilidad de supervivencia para este grupo a los 6 meses fue del 72,2%, a los 9 meses del 52,6% y a los 12 meses del 36,2%.
En el grupo de tratamiento de soporte, se identificaron 71 acontecimientos (79%), la duración mediana de los cuales fue de 6,5 meses (límites, 0,7-19,3). 
En este grupo, la probabilidad de supervivencia a los 6 meses fue del 54,1%,, a los 9 meses del 29,1% y a los 12 meses del 13,8%.
El seguimiento mediano fue de 12,9 meses.
Los pacientes del grupo irinotecan vivieron durante un tiempo significativamente más prolongado que los del grupo de tratamiento de soporte ( p = 0,0001 ).
Este beneficio apareció después de 2 meses y se hizo más evidente durante todo el período de estudio.
La supervivencia al año fue del 36,2 y del 13,8% en el grupo irinotecan y de tratamiento de soporte, respectivamente.
La supervivencia mediana fue de 9,2 y 6,5 meses, respectivamente.

Un modelo de Cox univariable puso de manifiesto que los pacientes con un estado general de 0 o 1 compartieron el mismo pronóstico, que fue notablemente mejor que el de los pacientes con un estado general de 2.
En la figura 3 se observa que el beneficio del tratamiento se identificó en todos los grupos de estado general.
Con un estado general inferior a 2, se identificaron 98 acontecimientos (60%) en el grupo irinotecan, la duración media de los cuales fue de 10,5 meses (límites, 0-18,9).
La probabilidad de supervivencia para este grupo a los 6 meses fue el 76,3%, a los 9 meses del 57,2% y a los 12 meses del 40%.
En el grupo de tratamiento de soporte solo se identificaron 53acontecimientos (77%), la duración mediana de los cuales fue ele 7,4 meses (0,7-19,3).
En este grupo la probabilidad de supervivencia a los 6 meses fue del 62,5%, a los 9 meses del 33,2% y a los 12 meses del 16,2%. 
En el grupo irinotecan con un estado general de 2, se identificaron 24 acontecimientos (92%), la duración mediana de los cuales fue de 5,1 meses (límites. 0,3-18,5).
La probabilidad de supervivencia para este grupo a los 6 meses fue del 46,2%, a los 9 meses del 26,9%, y a los 12 meses del 15,4%.
En el grupo de tratamiento de soporte se identificaron 18 acontecimientos (86%), la duración mediana de los cuales fue de 3,5 meses (límites, l,1-11,0).
En este grupo la probabilidad de supervivencia a los 6 meses fue del 26,5%, a los 9 meses del 15,9% y a los 12 meses del 7,9%.
La regresión multivariable de Cox confirmó la importancia de otros factores pronósticos conocidos (tabla 2).
Cuando en el modelo se incluyó el grupo de tratamiento, el beneficio de supervivencia para el grupo irinotecan siguió siendo significativo ( p = 0,001 ).

Objetivos secundarios

La supervivencia sin una pérdida de peso superior al 5% ( p = 0,018 ) y la supervivencia sin un deterioro del estado general ( p = 0,0001 ) fueron significativamente Irás prolonga das en el grupo irinotecan.
En un mayor número de pacientes con un estado general peor que 0 en el periodo inicial mejoró su estado general: el 35 frente al 11%, ( p=0,002 ).
La supervivencia libre de dolor en los pacientes sin dolor en el período inicial (fig. 4) fue significativamente mas prolongada en el grupo irinotecan que en el grupo de tratamiento de soporte ( p = 0,003 ), a pesar de una mayor proporción de pacientes tratados con opioides en el grupo de tratamiento de soporte.
Se identificaron 61 acontecimientos (73%) en el grupo irinotecan, la duración mediana de los cuales fue de 6,9 meses (límites, 0,3-17,2).
La probabilidad de supervivencia para este grupo a los 6 meses fue del 56,8%, a los 9 meses del 36,6% y a los 12 meses del 18,8%.
En el grupo de tratamiento de soporte se identificaron 26 acontecimientos (79%), la duración mediana de los cuales fue de 2.0 meses (0-13,0).
En este grupo, la probabilidad de supervivencia a los 6 meses fue del 31,5%, a los 9 meses del 14,7% y a los 12 meses del 4,9%.
Al cabo de 23 semanas de la aleatorización, la tasa de pacientes con un consumo de opioides por bloques de 3 semanas fluctuó desde el 25 al 34% en el grupo irinotecan y del 40 al 56'7, en el grupo de tratamiento de soporte.

En el análisis de la calidad de vida, el cumplimiento de los pacientes fue de alrededor del 80% en ambos grupos al inicio del estudio y disminuyó durante el estudio hasta alrededor del 50%.
El cumplimiento disminuyó más rápidamente en el grupo de tratamiento de soporte, lo que pudo deberse al deterioro más precoz de los pacientes. 
La diferencia desde el periodo inicial en el análisis multivariable de la variancia fue significativa ( p = 0,0001 ).

Los análisis univariables de la variancia favorecieron significativamente al grupo irinotecan para la puntuación de funcionamiento cognitivo ( xxx ), puntuación de la calidad de vida global ( xxx ), puntuación del dolor ( xxx ), puntuación de la disnea ( xxx ), pérdida del apetito ( xxx ) y del impacto económico ( xxx ).
La puntuación de diarrea fue significativamente mejor en el grupo de tratamiento de soporte ( xxx ).
Los análisis sobre la peor puntuación del paciente durante el estudio se exponen en la tabla 3: todos los resultados favorecieron significativamente al irinotecan excepto las puntuaciones emocional, de náuseas, alteración del sueño y económica.
La puntuación de diarrea fue significativamente más baja en el grupo de tratamiento de soporte.
El tiempo hasta el deterioro definitivo de la calidad de vida fue significativamente más prolongado en el grupo irinotecan, con cualquiera que fuera el umbral escogido del deterioro (todos los valores xxx ).

Seguridad

En la tabla 4 se expone la incidencia de efectos adversos (relacionados o no con el fármaco) de grados 3 o 4 por paciente.
Los pacientes que recibieron únicamente el tratamiento de soporte experimentaron una elevada incidencia de efectos adversos graves, en especial dolor y astenia, a pesar de que un número significativamente mayor de pacientes del grupo irinotecan experimentó efectos adversos, sobre todo neutropenia, náuseas, vómitos y diarrea.
De los 183 pacientes tratados con irinotecan, dos fallecieron (1,1%) por causas relacionadas con el fármaco, aunque en uno no se ha establecido de forma clara la asociación con los acontecimientos adversos (diarrea y/o neutropenia febril). 

Los ingresos por efectos adversos se produjeron en 136 pacientes (72%) del grupo irinotecan y 57 pacientes (63%) del grupo de tratamiento de soporte solo, durante una mediana acumulada de 15 días (límites, 1-168) y 11 días (2-87), respectivamente.

Discusión

Este estudio puso de manifiesto que el tratamiento con irinotecan más tratamiento de soporte en comparación con el tratamiento de soporte prolongó la vida de los pacientes con cáncer colorrectal metastásico.
Este beneficio fue clínicamente significativo porque la probabilidad de supervivencia al año fue 2,6 veces superior en los pacientes tratados con irinotecan que la de los pacientes tratados con tratamiento de soporte solo.
Otros objetivos de la eficacia, como la supervivencia sin pérdida de peso, la supervivencia sin deterioro del estado general, la supervivencia libre de dolor y la calidad de vida, favorecieron significativamente al irinotecan.

La ventaja de supervivencia para el grupo irinotecan siguió siendo muy significativa después de un ajuste para el estado general, un factor pronóstico bien reconocido. 
De un modo más general, la regresión multivariable puso de manifiesto que la diferencia de supervivencia entre ambos grupos siguió siendo significativa incluso después del ajuste para el efecto de los bien conocidos factores pronósticos. 
Los datos de supervivencia también concordaron con los observados en los estudios en fase 11 sobre irinotecan. 

El estudio también puso de manifiesto una ventaja para el irinotecan con respecto a la calidad de vida.
Los resultados concordaron con otras variables clínicas (deterioro del estado general, pérdida de peso y control del dolor).
El análisis de la escala de síntomas EORTC QLQ-C30 concordó con los datos de seguridad, lo que sugiere que el cuestionario EORTC QLQ-C 30 es sensible, al contrario de lo que ocurría con una observación previa.

El perfil de seguridad del irinotecan fue aceptable.
La incidencia de diarrea de grados 3 o 4 relacionada con el tratamiento con irinotecan (21%) fue más baja que lo mencionarlo en estudios previos (39%).
Este resultado, observado en un ensayo multicéntrico, probablemente se explica por una puesta en práctica más rigurosa de las directrices de tratamiento de la diarrea que en los estudios previos.
En el 22% de los pacientes se mencionó una neutropenia de grados 3-4 y en el 14% de los pacientes, vómitos de grados 3-4.
La astenia, el dolor y los síntomas neurológicos fueron más frecuentes en el grupo de tratamiento de soporte.

Algunos investigadores señalan que los pacientes con un cáncer avanzado que son tratados con tratamiento de soporte solo vivirán un menor número de días pero los últimos días de su existencia serán más tranquilos, protegidos de los efectos adversos de la quimioterapia.
Nuestro estudio puso de manifiesto que un 67% de los pacientes que recibieron tratamiento de soporte experimentaron síntomas graves y, como consecuencia, un 63% fueron ingresados durante una mediana de 11 días. 
Además, el irinotecan mejoró los síntomas de los pacientes y retrasó el inicio de los síntomas relacionados con el tumor, como el deterioro del estado general, la pérdida de peso y el dolor.
Estos datos estuvieron apoyados por el análisis de la calidad de vida, que sugirió que los efectos adversos del irinotecan se compensaron favorablemente mediante la reducción de los acontecimientos relacionados con el tumor.

Hemos puesto de manifiesto una ventaja en cuanto a supervivencia y un beneficio clínico mediante quimioterapia de segunda línea en pacientes con cáncer colorrectal metastásico que ha dejado de responder al fluorouracilo.
En numerosos pacientes se identificaron factores pronósticos desfavorables, lo que implica que los resultados pueden ser aplicables en la práctica diaria.
El valor del irinotecan se confirmó en términos de supervivencia, calidad de vida y otras variables clínicas.
Por consiguiente, el irinotecan puede recomendarse como tratamiento estándar de segunda línea en el cáncer colorrectal y como una nueva referencia para los ensayos clínicos venideros.

Tabla 1

Directrices de seguridad para el tratamiento con irinotecan

Efecto Adverso.
Tratamiento.


Síndrome colinérgico.
Sulfato de afropina (0,25 mg por vía subcutánea), si es grave, tratamiento profiláctico en los siguientes ciclos.


Náuseas / vómitos.
Antieméticos profilácticos sistémicos.


Primera deposición diarreica.
Loperamida a dosis altas de forma precoz (4 mg a la primera deposición diarreica, después 2 mg cada 2 h durante 12 h después de la última deposición diarreica durante un máximo de 48 h).


Diarrea durante > 24.
Antibioterapia de amplio espectro profiláctica oral, por ejemplo, fluoroquinolona (para prevenir el riesgo de infección grave).


Diarrea durante > 48.
Ingreso hospitalario para soporte parenteral y sustitución del tratamiento antidiarreico, por ejemplo, por octreótida.


Neutropenia de grado 4 o diarrea de grados 3 o 4.
Reducción de la dosis en los siguientes ciclos: 300 mg/m2 , después 250 mg/m2 (o 250 mg/m2, después 200 mg/m2 si la dosis de inicio era 300 mg/m2).


Tabla 2

Características al inicio

Irinotecan ( n=189 ).
Tratamiento de soporte ( n=90 ).


Sexo (varones/mujeres).


Edad mediana (límites, años).


Estado general de la OMS .


Pérdida de peso menor del 5% . .


Con síntomas.


Localización primitiva del tumor.


Colon derecho.


Colon izquierdo.


Recto.


Número de órganos afectados.


Localización de las metástasis.


Hígado.


Pulmón 69.


Peritoneo.


CEA.


Concentración mediana de CEA (límites: ug/l ).


Documentación de la progresión del tumor solo mediante aumento de CEA.


Aumento medio de (CEA_CEA)para los evaluados sólo mediante CEA.


Progresión documentada del tumor mientras seguía el tratamiento previo con fluorouracilo.


Tratamiento previo.


Cirugía.


Radioterapia.


Quimioterapia.


Solo adyuvante.


Un régimen para enfermedad metastásica con o sin tratamiento adyuvante.


> 2 regímenes para enfermedad metastásica con o sin tratamiento adyuvante.


Mejor respuesta al fluorouracilo previo para enfermedad metastásica.


Regresión completa / parcial.


Estabilización.


Progresión.


Valor biológico patológico.


Hemoglobina < 110 g/l .


Número de leucocitos > 8 x 109/l.


LDH / LSN.


Fosfatasas alcalinas > LSN.


CEA > 10 ug/l .


Concentración mediana de hemoglobina (límites; g/l).


Figura 1

Probabilidad de supervivencia de los 279 pacientes

Figura 2

Probabilidad de supervivencia de acuerdo con el estado general

Figura 3

Probabilidad de supervivencia libre de dolor en 116 pacientes sin dolor en el período inicial

Tabla 3

Tabla 3 Análisis multivariable de Cox

Razón de riesgo.


Grupo de tratamiento.


Irinotecan más tratamiento de soporte.


Tratamiento de soporte solo .


Pérdida de peso durante los 3 meses previos (%).


Estado general de la OMS.


Hemoglobina (g/l).


Metástasis hepáticas.


Fosfatasas alcalinas (x LSN).


Número de órganos afectados.


Tabla 4

Peor puntuación durante el estudio para cada escala EORTC QLQ-C30 

Irinotecan ( n=158 )Media (EE).
Tratamiento p de soporte solo ( n=73 ) Media (EE).


Escala funcional.


Física.


Capacidad.


Cognitiva.


Emocional.


Social.


Calidad global de vida.


Síntomas.


Fatiga.


Náuseas/vómitos.


Dolor.


Disnea.


Alteración del sueño.


Perdida del apetito.


Estreñimiento.


Diarrea.


Impacto económico.


Tabla 5

Tabla 5 Pacientes con criterios de toxicidad común de grados 3 o 4 del National Cancer Institute

Acontecimiento adverso.
Irinotecan.
Tratamiento de soporte solo.


Anemia.


Leucopenia/neutropenia.


Trombocitopenia.


Fiebre o infección con neutropenia grado 3 o 4.


Nauseas.


Vómitos.


Diarrea.


Estreñimiento.


Síndrome colinérgico.


Astenia.


Mucositis.


Anorexia.


Signos cutáneos.


Síntomas neurológicos.


Síntomas cardiovasculares .


Dolor (excluido el abdominal).


Dolor abdominal.


Infección (sin neutropenia grado 3 o 4).


Cualquier grado 3 o 4.



Clonación con fines médicos 

Una vez logradas las modificaciones genéticas de los mamíferos y su reproducción ulterior, la biomedicina busca sacarles partido esas posibilidades técnicas.

El nacimiento en el verano de 1995 de dos corderos en el Instituto Roslin, cerca de Edimburgo, anunciaba lo que en opinión de muchos habría de ser un período de oportunidades revolucionarias en biología y medicina.
Megan y Morag, gestados los dos por una madre subrogada ("de alquiler"), no resultaron de la unión de un espermatozoide y un óvulo.
Su material genético procedía de cultivos de células extraídas de un embrión de nueve días, lo que Convirtió a Megan y a Morag en copias genéticas, en clónicos del embrión.

Antes del caso de los corderos, los expertos sabían ya cómo obtener ovejas, vacas v otros animales mediante la copia genética de células extraídas de embriones precoces.
Constituía, sin embargo, una tarea laboriosa.
Nuestro trabajo auguraba una clonación más cómoda, gracias a las propias condiciones de la manipulación de células en cultivo.
Además, según quedaba evidenciado con Megan y Morag, no importa que las células se hallen parcialmente especializadas o diferenciadas, para que pueda reprogramarse su genética y entonces funcionar como si se tratara de células de un embrión precoz.
Una posibilidad en la que pocos biólogos hubieran creído.

Dimos un paso más y nos aprestamos a clonar animales a partir de cultivos de células extraídas de fetos de 26 días y de una oveja madura.
De las células de la oveja surgió Dolly, el primer mamífero clonado a partir de un adulto.
El anuncio del nacimiento de Dolly en febrero de 1997 despertó el interés de todos los medios de comunicación.
Sobre todo porque sugería la posibilidad teórica de clonación de humanos, día que no me gustaría ver amanecer.
Eso aparte, la consecución de clónicos a partir de células en cultivo procedentes de tejido fácilmente obtenido habría de traer numerosos beneficios prácticos para la mejora animal y medicina además de resolver problemas biológicos fundamentales.

La clonación se basa en la transferencia nuclear, la misma técnica empleada desde hace varios años para obtener animales idénticos a partir de células embrionarias.
La transferencia nuclear requiere el concurso de dos células.
La célula receptora suele ser un óvulo sin fecundar y reciente.
Esos óvulos están listos para empezar a desarrollarse en cuanto reciben el estímulo apropiado.
La célula donante es la célula a copiar.
Bajo un microscopio potente, el experto sostiene, mediante succión, el óvulo receptor en el extremo de una pipeta fina:
con una micropipeta sutilísima absorbe los cromosomas, esos corpúsculos rechonchos que portan el ADN de la célula. (En esa etapa, los cromosomas no están encerrados en un núcleo definido).
Luego, lo normal es que la célula donante, con su núcleo desarrollado, se fusione con el óvulo receptor.

Algunas células fusionadas empiezan a desarrollarse como un embrión normal y producen descendencia si se implantan en el útero de una madre de alquiler.

En los experimentos que realizamos con cultivos, adoptamos medidas especiales para hacer compatibles las células donante y receptora.
De forma especial nos esforzamos por coordinar los ciclos de replicación del ADN y los de producción de ARN mensajero, molécula que se transcribe del ADN y dirige la síntesis de proteínas.
Optamos por utilizar células donantes cuyo ADN no se estaba replicando en el momento de la transferencia.
Para ello, trabajamos con células que obligamos a permanecer inactivas mediante reducción de la concentración de nutrientes en el cultivo.
Después de la transferencia nuclear, aplicamos al óvulo pulsos de corriente eléctrica para inducir la fusión de las células e imitar la estimulación que, en condiciones normales, proporciona el espermatozoide.

Una vez demostrado, con el nacimiento de Megan y Morag, que podíamos producir una descendencia variable a partir de cultivos procedentes de embriones, registramos la patente.
Avanzamos un poco más y abordamos experimentos para comprobar si podía lograrse descendencia a partir de cultivos cuyas células se hallaran en un nivel de diferenciación más acabado.
En colaboración con PPL Therapeutics, ensayamos con fibroblastos (células comunes del tejido conjuntivo) procedentes de fetos y células extraídas de la ubre de una oveja en su tercer mes y medio de gestación.
Seleccionamos una adulta preñada porque las células mamarias crecen con mucho vigor en esa fase de la gestación, indicio de que pueden prosperar en cultivo.
Presentan, además, cromosomas estables lo que sugiere que conservan toda su información genética.
La clonación satisfactoria de Doly a partir de un cultivo de células mamarias y la clonación de otros corderos a partir de fibroblastos cultivados revelaba que el protocolo de Roslin era sólido y repetible.

Según cabía esperar, toda la descendencia clónica de nuestros experimentos se parecía a la progenie de la oveja que aportó el núcleo originante, no a las madres subrogadas ni a las donantes del óvulo.
Las pruebas genéticas demuestran más allá de toda duda que Dolly es clónica de un adulto.
Lo más probable es que procediera de una célula mamaria plenamente diferenciada, aunque es imposible afirmarlo con absoluta certeza, pues el cultivo contenía también células no tan diferenciadas que hay en la glándula mamaria, aunque su número es escaso.
De entonces acá otros laboratorios han empleado una técnica similar para crear clónicos sanos de ganado y de ratón a partir de células de cultivo, entre ellas células procedentes de hembras sin preñar.

Que la clonación mediante transferencia nuclear sea repetible no significa que carezca de contratiempos.
Algunas vacas y ovejas clónicas son desmesuradamente grandes, efecto observado también cuando se cultivan embriones antes de avanzar la gestación.
Y lo que importa más, la transferencia nuclear no alcanza todavía un nivel satisfactorio de eficacia.
Hace una treintena de años, John B. Gurdon observó, cuando realizaba experimentos de transferencia nuclear con ranas, que el número de embriones que llegaban al estadio renacuajo era menor si las células donantes procedían de animales en un estadio de desarrollo avanzado.

Nuestros primeros resultados con mamíferos reflejaron una pauta similar.
Todos los estudios de clonación descritos hasta ahora muestran un patrón continuo de muertes durante el desarrollo embrionario y fetal.
Sólo llegan a término entre un 1 y un 2 por ciento de los embriones.
Por si fuera poco, algunos de los clones que sobreviven al parto mueren en breve plazo.

Sigue sin conocerse la causa de esos fracasos, signo posible de la complejidad que rodea a la reprogramación genética necesaria para que nazca una descendencia sana.
Basta con que un gen no se exprese bien o no consiga cifrar una proteína decisiva en un punto decisivo para provocar un desastre.
En la reprogramación podría exigirse la regulación de millares de genes, en un proceso con cierto margen de aleatoriedad.
Las mejoras técnicas, una de ellas el recurso a otras células donantes, podrían traer un recorte de la mortalidad.

La capacidad alcanzada de obtener descendencia a partir de células de cultivo facilita el camino que conduce a la modificación genética de los organismos, a los animales transgénicos.
Además de su interés objetivo en investigación, esos individuos manipulados pueden fabricar proteínas humanas de importancia clínica.

La técnica habitual para obtener animales transgénicos peca de una lentitud exasperante.
Requiere la microinyección de un constructo genético (una secuencia de ADN que incorpora el gen deseado) en un buen número de huevos fecundados.

Algunos incorporan el ADN introducido y la descendencia resultante lo expresa.
A continuación, estos animales se crían para que transmitan el ADN del constructo.

Frente a ello, basta un simple tratamiento químico para convencer a las células en cultivo de que incorporen un constructo de ADN.
Si esas células sirven luego de donantes en una transferencia nuclear, toda la descendencia clónica resultante portará dicho ADN.
En el Instituto Roslin y en PPL Therapeutics se ha aplicado la técnica para obtener animales transgénicos con eficacia mayor que la lograda a través de microinyección.

Hemos incorporado en ovejas el gen del factor IX humano, proteína de la coagulación sanguínea utilizada para el tratamiento de la hemofilia B. En nuestro experimento, junto con el gen del factor IX transferimos a las células donantes un gen de resistencia contra antibióticos; al añadir al cultivo una dosis tóxica del antibiótico neomicina, murieron todas las células que no habían incorporado el ADN añadido.
Pese a esta alteración genética, la proporción de embriones que se desarrollaron a término después de la transferencia nuclear se mantenía acorde con nuestros resultados anteriores.

La primera oveja transgénica así obtenida.
Polly nació en el verano de 1997.

Polly y otros clónicos transgénicos segregan la proteína humana en su leche.
De ello se desprende que, una vez perfeccionadas las técnicas de recuperación de óvulos de especies dispares, la clonación permitirá la introducción de cambios genéticos precisos en cualquier mamífero y la creación de múltiples individuos portadores de la alteración.

Los cultivos de células de glándula mamaria podrían ofrecer una ventaja añadida.

Hasta hace poco, la única forma práctica de evaluar si un constructo de ADN produciría la secreción de una proteína en la leche consistía en transferirlo a ratones hembra:
luego, analizar la leche.
Pero se ha de poder realizar la comprobación directa en cultivos de células mamarias.
Será cuando podremos acelerar el proceso de localización de buenos constructos de ADN y de las células que los hayan incorporado, rindiendo una secreción eficiente de la proteína.

La clonación ofrece muchas más posibilidades.
Así, la generación de órganos animales sometidos a manipulación genética para acomodarlos a su trasplante en humanos.
Cada año mueren por millares los pacientes que necesitan un nuevo corazón, un hígado o un riñón de sustitución.
Un órgano de un cerdo normal trasplantado quedaría de inmediato fuera de servicio por una reacción inmunitaria "hiperaguda".
Esta reacción viene desencadenada por proteínas de las células del cerdo que han sido modificadas por la alfagalactosil transferasa, una enzima.
Parece lógico, pues, que un órgano procedente de un cerdo al que se le ha sometido a manipulación genética para privarle de dicha enzima, podría ser bien tolerado, siempre que los médicos dieran al receptor fármacos supresores de otras reacciones inmunitarias menos extremas.

Prometedora resulta también la producción rápida de animales portadores de defectos genéticos que imiten la fibrosis quística y otras enfermedades humanas.

Cierta información se ha recabado de los ratones, pero éstos y los humanos discrepan en la constitución de los genes de la fibrosis quística.
Se espera que las ovejas aporten mayor provecho, habida cuenta de que sus pulmones se parecen a los humanos.
Además, la esperanza de vida de la oveja es de años, lo que posibilita una evaluación a largo plazo de su respuesta ante el tratamiento.

La creación de animales con defectos genéticos plantea cuestiones éticas espinosas.
Existe una aceptación social mayoritaria de la investigación en animales, siempre que las enfermedades que se vayan a estudiar sean graves y se evite el ensañamiento o sufrimiento innecesarios.

La capacidad de la bioingeniería para producir animales con una constitución genética determinada podría aplicarse a terapias celulares de patologías hoy incurables.
Pienso en la enfermedad de Parhinson, la diabetes y la distrofia muscular, en que aparecen lesionadas poblaciones celulares especificas, incapaces de repararse o sustituirse a sí mismas.
Se están explorando varios enfoques originales que proporcionarían células nuevas (procedentes del propio paciente y cultivadas, donadas por otros humanos o extraídas de animales).

Las células transferidas no deberán transmitir enfermedades nuevas y habrán de adaptarse a las necesidades fisiológicas del paciente.
Cualquier respuesta inmunitaria que desencadenen deberá ser conjurable.
Los animales clónicos, con las modificaciones genéticas precisas que reduzcan al mínimo la respuesta inmunitaria, podrían constituir un reservorio de las células a suplir y producir incluso células con propiedades especiales.
Pero no olvidemos que cualquier modificación expondría al riesgo de aparición de una reacción inmunitaria más vigorosa.

A través de la clonación podrían formarse rebaños de reses exentos del gen responsable de la síntesis de la proteína del prión.
Por culpa de dicho gen el ganado vacuno queda expuesto a la infección por priones, agentes de la encefalitis espongiforme bovina (EEB) o enfermedad de las vacas locas.
Puesto que muchos medicamentos contienen gelatina u otros productos procedentes del ganado, existe el lógico temor de que los priones pasen a los humanos.
La clonación podría formar rebaños que, al carecer del gen de la proteína priónica, serían una fuente de componentes medicamentosos sin contaminar con priones.

Además, la técnica podría cortar la transmisión de enfermedades genéticas.
Se investigan vías para complementar o sustituir genes defectuosos.
Ello no impedirá que los pacientes, incluidos los beneficiados con el tratamiento, sigan transmitiendo los genes defectuosos a su descendencia.
Pero si se pudiera tratar el embrión precoz.
Los núcleos de las células embrionarias modificadas podrían transferirse a óvulos de los que se desarrollarían niños exentos de una enfermedad determinada.

Algunos de los proyectos más ambiciosos prevén la producción de células donantes humanas universales.
A partir de embriones de ratón en estados muy precoces podemos aislar células madre pluripotenciales indiferenciadas, capacitadas para intervenir en todos los tejidos de un adulto.
Pueden obtenerse células equivalentes en otras especies, sin que los humanos tengan que ser la excepción.

Se empieza a dominar la diferenciación in vitro de células madre pluripotenciales, para su plausible aplicación en funciones reparadoras o reemplazadoras de un tejido deteriorado.

En principio, podrían fabricarse células pluripotenciales adaptadas a tal o cual paciente, creando un embrión por transferencia nuclear para ese propósito, mediante el empleo de una de las células del paciente como donante v un óvulo humano como receptor.
Se permitiría que el embrión se desarrollara sólo hasta la etapa necesaria para separar y cultivar las células pluripotenciales:
en ese momento el embrión tiene unos pocos centenares de células, que no han empezado a diferenciarse.
En particular, el sistema nervioso no ha empezado a desarrollarse, por lo que el embrión no sufre dolor ni es sensible a los estímulos del medio.
Las células extraídas del mismo podrían utilizarse para tratar diversas enfermedades graves causadas por lesión celular, quizás el sida, la enfermedad de Parkinson, la distrofia muscular y la diabetes.

Estos marcos hipotéticos en los que se precisa crecimiento de embriones humanos para conseguir las células resultan hondamente preocupantes para unos, pues los embriones poseen capacidad para devenir personas humanas.
Deben respetarse los puntos de vista de quienes consideran inviolable la vida a partir de la concepción.
Un punto de vista que no comparto.
Para mi, el embrión es un grupo de células que no adquiere sensibilidad hasta una etapa ulterior del desarrollo; en mi opinión, no es todavía una persona.

Crear un embrión para curar un paciente puede resultar caro.
Más práctico seria establecer líneas de células pluripotenciales, estables y permanentes, a partir de embriones humanos clónicos.
Luego las células podrían diferenciarse conforme fueran requeridas.
Las células implantadas así obtenidas no serian idénticas desde el punto de vista genético, si bien podría controlarse la reacción inmunitaria.
Quizá se logre algún día obtener, mediante "desdiferenciación" directa, células madre pluripotenciales genéticamente idénticas a las del paciente:
es decir, que no deba mediar un embrión ad hoc .

Hay quien defiende, para algunos casos, la clonación de personas hechas y derechas.
Se habla de un sustituto de un pariente moribundo.
Pero, se objeta, el clónico nunca sería recibido como un individuo completo, sujeto siempre a lo que la familia esperaría de él en función del conocimiento del "gemelo" genético.

Una postura familiar errónea, porque la personalidad humana está determinada sólo en parte por los genes.
El clónico de una persona extrovertida podría tener una conducta retraída.
Los clónicos de deportistas, estrellas de cine, empresarios o científicos podrían elegir carreras diferentes.

Hay quien aboga por la clonación en parejas estériles.
No parece buena solución, por las consecuencias que acarrearía en el niño copia de uno pero no del otro componente de la pareja.
Puesto que disponemos de otros métodos para el tratamiento de todos los tipos conocidos de esterilidad.
Las vías terapéuticas al uso parecen más apropiadas.
En mi opinión, ninguno de los usos sugeridos de la clonación para la fabricación de copias de personas es éticamente aceptable.

Ni que decir tiene que me opongo enérgicamente a que se permita el desarrollo de embriones humanos clónicos para que puedan convertirse en donantes de tejidos.

No obstante, la clonación a partir de células cultivados ofrecerá importantes oportunidades médicas.
Las predicciones sobre nuevas técnicas suelen equivocarse.
Las actitudes sociales cambian; se producen avances inesperados.
El tiempo lo dirá.

¿Es la inactividad la clave de la clonación? 

En nuestros experimentos de transferencia nuclear, todas las células donantes estaban inactivas, esto es, no fabricaban ARN mensajero.
La mayoría de las células dedica buena parte de su ciclo biológico a la transcripción de ADN en ARN mensajero, que dirige la síntesis de proteínas.
Nosotros optamos por experimentar con células inactivas porque persisten durante días en un estado uniforme.
Keith H. S. Campbell cayó en la cuenta que podrían ser muy adecuadas para clonación.

Supuso que una transferencia nuclear cabal exigía, como condición previa, la inhibición de la producción natural de ARN en el núcleo donante.
Por una razón:
en fase embrionaria muy precoz, las células están controladas por proteínas y por ARN fabricado en el precursor del óvulo parental.
El embrión sólo empieza a fabricar su propio ARN unos tres días después de la fecundación.

Puesto que los cromosomas propios del óvulo no fabricarían normalmente ARN, los núcleos de las células inactivas gozarían quizá de una mayor oportunidad de desarrollo si se realizaba la transferencia.

Cabe también que los cromosomas de los núcleos inactivos se encuentren en un estado físico favorable.
En nuestra opinión, las moléculas reguladoras del óvulo receptor actúan sobre los núcleos transferidos para reprogramarlos.
Aunque no sabemos lo que son esas moléculas, quizá los cromosomas de una célula inactiva sean más accesibles a ellas.

Ahora, ratones clónicos 

El equipo de Ryuzo Yanagimachi, de la Universidad de Hawai en Honolulú, acaba de clonar ratones mediante la transferencia, a óvulos, de núcleos donantes (no de células completas).
Tomaron núcleos de células cumulares, que rodean el ovario y son inactivas en estado natural.
Nadie ha demostrado todavía que pueda producirse descendencia a partir de células diferenciadas que no estén inactivas.

La madre subrogada (centro) está flanqueada por la descendencia clónica del donante de núcleo.

Figura 1

MEGAN Y MORAG( arriba ) fueron los primeros mamíferos clónicos obtenidos a partir de células de cultivo

La técnica ha permitido engendrar ovejas clónicas portadoras de genes humanos.

Esos animales producen leche que puede procesarse ( izquierda ) para producir proteínas de interés terapéutico.

Así se crearon Megan y Morag.
Se combinaron células de cultivo con óvulos para producir embriones que se transformaron en descendientes clónicos.

Figura 2

DOLLY ( derecha ) saltó a la fama en 1997 por ser el primer mamífero clonado a partir de células de un adulto

Llegada a la madurez, la oveja parió un cordero sano, Bonnie ( izquierda ), fruto de un apareamiento y una gestación normales.

Así se preparan las células para obtener clónicos transgénicos.
Se combinan secuencias de ADN de una oveja y un humano:
luego se introducen en células de oveja que se utilizarán como donantes de núcleos.

Figura 3

POLLY ( izquierda ) es clónica transgénica de una oveja de lana Dorset

Se añadió el gen del factor IX de la coagulación humana a la célula portadora de la herencia genética del cordero.
Polly lleva, pues, el gen humano.
La oveja que hizo de madre "de alquiler" de Polly ( derecha ) es una carinegra escocesa.


Proteosomas

Compete a los proteosomas reciclar las proteínas celulares.
Pero si cumplen mal su cometido, por exceso o por defecto, aparecen diversas enfermedades habituales.

Cada minuto se reproducen en el interior de nuestras células escenas que parecerían tomadas de una película de aventuras.
La "infeliz" proteína que ahora cumple su función, dentro de un instante portará el marchamo de su destino "desventurado": una suerte de túnel donde se romperá en pedazos.
Es el túnel, si se nos permite seguir con la metáfora, la viva imagen del potro de castigo.
En su rueda, la proteína sufre un estiramiento y sobre ella caen los cuchillos de las enzimas que la despedazan.
Todavía les aguarda a los trozos del despiece, fuera ya del túnel, la acción sajadora de otras enzimas más simples que culminarán la destrucción de la proteína.

Podría pensarse que esta hipérbole intracelular carece de interés, salvo para la proteína en cuestión. 
Nada más lejos de la verdad.
En muchos laboratorios, el nuestro incluido, se ve con claridad creciente que los proteosomas, nombre técnico de esas salas de despiece molecular, intervienen de una forma decisiva en las vías que regulan una gavilla entera de procesos celulares.
Hay, en una célula normal del organismo, unos 30.000 proteosomas.
Cuando fracasan en su misión -bien sea porque destruyen proteínas importantes o bien sea porque no degradan las dañadas o malformadas- pueden aparecer enfermedades.
El virus de la inmunodeficiencia humana (VIH) y otros han desarrollado mecanismos para manipular la degradación proteosómica de proteínas en su propio interés. 
Se espera que los fármacos del futuro destinados a remediar el cáncer y otras patologías similares consistan en compuestos químicos que actúen sobre los proteosomas y las vías que transportan las proteínas hacia el interior de éstos.

En ciertos laboratorios farmacéuticos se investigan compuestos inhibidores de la vía del proteosoma; hay dos fármacos en fase de prueba en el hombre.

La renovación es el camino

Del paño de las proteínas están hechas las células.
Algunas proteínas actúan también como enzimas, que dirigen las reacciones de la vida.
Los tipos de proteínas que una célula produce dependen de los genes que se hallen activos en cualquier momento.
Determinan éstos la secuencia en que deben unirse las 20 subunidades básicas, los aminoácidos. 
Las cadenas se pliegan en hélices y bucles compactos para constituir diferentes clases de proteínas, dotada cada una de una función específica que le viene impuesta por su forma y su composición química. 

¿Qué ocurre cuando deja de necesitarse la presencia de esa proteína?
Durante años se dio por supuesto que la parte del león de la degradación de las proteínas se la llevaban los lisosomas, sacos de enzimas digestivas habituales en la mayoría de las células del organismo.
Pero a comienzos de los años setenta uno de los autores (Goldberg) demostró que las bacterias y los hematíes inmaduros, células privadas de lisosomas, podían, no obstante, destruir proteínas anómalas. 
Y más aún, el proceso requería energía, lo que no acontecía con otros procesos de degradación. 

El equipo de Goldberg consiguió que ese proceso de degradación dependiente de energía funcionase en el tubo de ensayo.
Merced a ello, varios grupos de investigación descubrirían, a finales de los años setenta y a lo largo del decenio siguiente, las enzimas responsables. 
Por último, en 1988, dos grupos - dirigido uno por Goldberg y el otro por Martin C. Rechsteiner, de la Universidad de Utah- observaron que de la degradación de las proteínas se encargaban grandes complejos multienzimáticos.
A esas macroestructuras el grupo de Goldberg les dio el nombre de proteosomas.

Se les llamó así porque contienen muchas proteasas, enzimas que trocean las proteínas.
Pero los proteosomas centuplican el tamaño de otras proteasas y revisten mayor complejidad.
Una vez colocada una proteína en el umbral de un proteosoma, penetra en el interior de la partícula y allí se descompone en los aminoácidos que la constituían, piezas que podrán luego aprovecharse para la síntesis de nuevas proteínas.
La mayoría de las proteínas se renuevan al cabo de unos días, incluso en las células que raramente se dividen, como hepatocitos o neuronas.
Según de qué proteína hablemos, así será su ritmo de degradación: unas tienen períodos de semivida de sólo 20 minutos, mientras que otras, en la misma célula, pueden subsistir días o semanas. 
Pero esas tasas de degradación varían de forma drástica a tenor de las condiciones cambiantes de nuestro cuerpo.

A primera vista, la destrucción continua de los constituyentes de la célula parece un derroche; presta, sin embargo, un servicio inestimable a numerosas funciones esenciales. 
La degradación de una enzima importante o una proteína reguladora, por ejemplo, es un mecanismo habitual que las células utilizan para frenar o suspender una reacción bioquímica.
Por otra parte, muchos procesos celulares se activan con la degradación de una proteína inhibidora clave, igual que el agua fluye de una bañera cuando se levanta el tapón.
Esta eliminación de proteínas reguladoras reviste particular interés en la temporización de las transiciones entre las etapas del ciclo de la división celular.

La degradación de proteínas desempeña un papel crucial en la regulación general del metabolismo corporal.
En un momento de menesterosidad, pensemos en el estado de desnutrición o de enfermedad, la vía de los proteosomas se aviva en nuestros músculos, proporcionando aminoácidos que pueden convertirse en glucosa requerida para la combustión energética.
Se refleja esa degradación excesiva de proteínas en el desgaste y debilidad muscular de los famélicos y en los pacientes con cáncer avanzado, sidosos y diabéticos sin tratar. 

Nuestro sistema inmunitario, en su constante búsqueda de células infectadas con virus o cancerosas que eliminar, depende también de los proteosomas para generar las señales que identifiquen a tales células dañinas.
En efecto, aunque suelen degradarse las proteínas celulares hasta dejarlas en sus aminoácidos, los proteosomas dejan sueltos unos pocos fragmentos, de entre ocho y diez aminoácidos, que son capturados y presentados en la superficie de la célula, donde el sistema inmunitario comprueba si son normales o anormales. 
En estados patológicos y en el bazo y los ganglios linfáticos se producen inmunoproteosomas, tipos de proteosomas especializados que potencian la eficacia de este mecanismo de vigilancia.

Gracias a la degradación de las proteínas por los proteosomas se evita la acumulación de proteínas aberrantes, potencialmente tóxicas.
Las células de mamíferos y las bacterianas destruyen de una manera selectiva las proteínas que muestran conformaciones anómalas causadas por una mutación, por errores en la síntesis o por alteraciones de otra clase.

En múltiples enfermedades genéticas humanas se deja sentir la importancia de la degradación de proteínas anómalas.
En varias anemias hereditarias, un gen mutante determina la síntesis de moléculas anormales de hemoglobina, que no se pliegan adecuadamente y las destruyen los proteosomas inmediatamente después de la síntesis. 
Por su parte, la fibrosis quística está causada por una mutación en el gen que cifra una proteína en forma de poro, que traslada el cloruro a través de la membrana externa de la célula.
Debido a que estos mutantes de los transportadores de cloruro desarrollan a menudo una conformación inadecuada, los proteosomas los degradan antes de que alcancen la membrana celular.
El moco pegajoso que se acumula en los pulmones y en otros órganos de las personas con fibrosis quística obedece a la falta de transportadores de cloruro normales.

Otras enfermedades podrían ser, en parte, resultado de un fracaso de los proteosomas en su función degradadora de proteínas anómalas.
Hay conjuntos de proteínas mal plegadas que se acumulan junto con proteosomas en determinadas neuronas del cerebro de sujetos con la enfermedad de Parkinson, la de Huntington o la de Alzheimer.
Averiguar por qué las neuronas de individuos afectados con estos trastornos neurodegenerativos dejan de degradar las proteínas anormales constituye hoy un campo de intensa investigación. 

En la panza de la bestia

Desde la perspectiva de la proteína, los proteosomas son estructuras gigantes.

Mientras que una proteína de tamaño medio tiene entre 40.000 y 80.000 daltons, la mayoría de los proteosomas de los organismos superiores pesan de largo dos millones de daltons.
A mediados de los años noventa, un grupo dirigido por Wolfgang Baumeister y Robert Huber, del Instituto Max Planck de Bioquímica en Martinsried, recurrió al microscopio electrónico y a la técnica de la difracción de rayos X para determinar la arquitectura molecular de los proteosomas.
Cada uno de ellos está constituido por una partícula central con aspecto de túnel a la que acompañan una o dos partículas reguladoras, menores, situadas en un extremo o en ambos.
La partícula central consta de cuatro anillos apilados -cada uno compuesto a su vez por siete subunidades- que rodean un canal central, que constituye el tracto digestivo del proteosoma.
Los dos anillos externos actúan como puertas de control y alejan así el peligro de que alguna proteína extraviada penetre en la cámara de destrucción.

A su vez, las partículas de los extremos se comportan como guardianas exigentes del paso hacia la partícula central.
Son partículas reguladoras que reconocen y se unen a proteínas marcadas para la destrucción; luego, recurren a la energía necesaria para desplegar las proteínas y arrojarlas hacia la partícula central, donde se trocean en fragmentos de tamaño variable.

Se han sintetizado y aislado compuestos que inhiben de una manera selectiva el proteosoma sin afectar otras enzimas celulares, lo que podría comportar efectos secundarios. 
A través de tales inhibidores se nos ha venido revelando la complejidad de la vía proteosómica. 
A dosis elevadas, los inhibidores acaban produciendo la muerte de las células, lo que no debe sorprendernos si consideramos las importantes funciones desempeñadas por los proteosomas.
No podemos omitir que, in vivo e in vitro , las células cancerosas parecen más sensibles a esos efectos letales que las células normales.
Los laboratorios Millennium Pharmaceutical están ensayando un inhibidor experimental del proteosoma cuya acción se extenderá a varios tipos de cáncer, incluido el mieloma múltiple.
La misma empresa ha empezado las fases iniciales de inocuidad para el hombre de otro inhibidor destinado al tratamiento del ictus cerebral y el infarto de miocardio. 

El beso de la muerte

El proteosoma no elige proteínas al azar para destruirlas. 
Corresponde a la célula señalar las proteínas destinadas a ese fin.
En su inmensa mayoría, esas proteínas se etiquetan primero con ubiquitina, que debe su nombre a que se encuentra en muchos organismos diferentes. 
Con sólo 76 aminoácidos, la ubiquitina es una proteína bastante pequeña; se une a proteínas mayores formando largas cadenas.
Estas colas de poliubiquitina actúan como códigos postales que encaminan y aceleran las proteínas selladas hacia el proteosoma.

El control del momento de la muerte de una proteína no reside en la propia destrucción en el proteosoma, sino en el proceso de la incorporación de las cadenas de ubiquitina, la ubiquitinación, que requiere energía.
Avram Hershko y Aaron Ciechanover, del Instituto Technion de Israel en Haifa, en colaboración con Irwin A. Rose, del Centro Fox Chase de Investigaciones Oncológicas en Filadelfia, han puesto en claro las líneas esenciales de la ubiquitinación.

El proceso de ubiquitinación de las proteínas se desarrolla en varias etapas.
En él participan tres enzimas: E1, E2 y E3.
La enzima E1 activa la ubiquitina y la asocia a E2.
La tercera enzima, E3, facilita entonces la transferencia de la ubiquitina activada desde E2 hasta la proteína.
El proceso se repite hasta que una larga cola de ubiquitinas cuelga de la proteína.
El proteosoma reconoce en ese instante la cadena de marras y atrae la proteína hacia su interior.

Para conocer por qué se elige una determinada proteína para la ubiquitinación hay que profundizar en las enzimas E3.
Dos de los autores (Elledge y Harper), y otros, han descubierto que existen centenares de E3 distintas que reconocen la información encerrada en las secuencias de aminoácidos de otras proteínas que las convierten en objetivo de la ubiquitinación.
Ante condiciones fisiológicas alteradas, pensemos en una infección o una falta de nutrientes, las células pueden modificar las proteínas mediante la adición de grupos fosfato.

Esa fosforilación puede trastornar la actividad de una proteína o su capacidad para unirse a las E3.
Estas enzimas reconocen también las proteínas que fracasan en el plegamiento o que están alteradas; las hacen entrar en el proceso de limpieza después de marcarlas para que el proteosoma las aprehenda.
Muchos procesos celulares clave se apoyan en la estabilidad de las proteínas; por tanto, si desciframos cómo se controla la estabilidad, tendremos la llave de numerosos secretos de la biología. 

Mediante el control de la estabilidad de proteínas cruciales, las enzimas E3 regulan el desarrollo de las extremidades, la respuesta inmunitaria, la división celular y la comunicación intercelular, entre otros.
Los propios ritmos circadianos y la floración en las plantas caen en la órbita de las enzimas E3.
De varias proteínas E3 se conoce incluso su función supresora de tumores u oncogénica, lo que vincula la ubiquitinación al comienzo del proceso canceroso.

Así acontece con el supresor de tumores Von Hippel Lindau (VHL), una E3 que sufre a menudo una mutación en los tumores renales.
La función de VHL consiste en demorar el crecimiento celular limitando el desarrollo de los vasos celulares en los tejidos; cuando se produce la mutación, los tumores recién formados generan un rico suministro sanguíneo y crecen rápidamente.
Se acaba de encontrar que cierta forma hereditaria de la enfermedad de Parkinson se origina en virtud de una mutación del gen de un tipo de la enzima E3, inductora de la acumulación de proteínas en ciertas células cerebrales y de su aniquilamiento.

Los virus, que se distinguen por derivar en su propio beneficio los procesos celulares, han evolucionado hasta adquirir medios para secuestrar el mecanismo de ubiquitinación y degradación de proteínas en sus ominosos intereses.
Sirven de ejemplo los virus del papiloma humano (VPH), que provocan verrugas genitales, cáncer de útero o cáncer de recto.
La transformación hacia el desarrollo tumoral se bloquea, habitualmente, por la p53 , una de las proteínas supresoras de tumores en el organismo.
El VPH recurre a una estratagema para evitar el sistema defensivo celular: fabrica una proteína que se enlaza simultáneamente a la p53 y a una enzima E3.
La fatídica unión insta la ubiquitinación de la p53 , que así queda marcada para su destino fatal en el proteosoma. 
Las células, indefensas, se convierten más fácilmente entonces en un cáncer incipiente.

De un truco similar se vale el VIH para destruir la CD4 de la superficie celular, proteína que necesita el virus para infectar las células, aunque bloquea la producción de más virus en una fase ulterior.
CD4 constituye el atracadero que le permite al VIH penetrar en las células T del sistema inmunitario; se une a la proteína gp160 que sobresale de la superficie del virus.
Pero cuando el VIH se apresta a replicarse en las células recién infectada, la proteína CD4 puede constituir un problema: se adhiere a las proteínas gp160 recién sintetizadas, evitando así que se reúnan con otras proteínas víricas y formar nuevos virus.
Para obviar tal obstáculo, el VIH ha desarrollado la Vpu, una proteína que pone a la CD4 en la vía del disparadero.
Vpu se une a la CD4 y a un complejo en el que se integra una enzima E3, lo que hace que la proteína CD4 se ubiquitine y caiga después en el proteosoma destructor. 

Están apareciendo nuevos datos que realzan la importancia de las E3.
No cabe duda de que estas enzimas atraerán el desarrollo de fármacos en el futuro.

Puesto que cada E3 es responsable de la destrucción de un número restringido de proteínas, los inhibidores de las E3 deberían ser fármacos muy específicos con pocos efectos secundarios.
La identificación reciente de grandes familias de enzimas E3 ha abierto nuevos caminos para la creación de medicinas.
De esos hallazgos apasionantes se espera un mejor conocimiento de los fenómenos reguladores y de la propia biología humana. 
Cuanto más sepamos sobre los proteosomas y la maquinaria de selección para la ubiquitinación, tanto mejor apreciaremos cuánto debe la vida a la muerte de las proteínas.


Hipertensión en los afroamericanos 

De la hipertensión arterial común entre los americanos de origen africano suele culparse a los genes.
Ocurre, sin embargo, que, en Africa, tal hipertensión no se da.
Nos encontramos ante un ejemplo óptimo de interacción entre genoma y entorno.

Casi todos los norteamericanos experimentan, con los años, un incremento persistente de la presión sanguínea. 
Cerca de una cuarta parte rebasa el dintel de hipertensión, término con el que nos referimos a una elevación crónica de la presión sanguínea.
Esta afección contribuye de forma silenciosa a la enfermedad cardíaca, al ictus y a la insuficiencia renal; se la relaciona con unas 500.000 muertes anuales.
La situación es más acuciante entre los afroamericanos:
el 35 por ciento de ellos padece hipertensión.
La afección, particularmente letal en esta población, se halla implicada en el 20 por ciento de las muertes entre los negros, una cifra que dobla la de los blancos.

La explicación al uso de esta disparidad entre negros y blancos atribuye a las personas de origen africano una "proclividad intrínseca" hacia la presión sanguínea elevada, en virtud de una genética vagamente definida.
Semejante juicio, amén de no corresponderse con los datos observados, resulta inquietante.

Deriva, en efecto, de una actitud ante las razas muy arraigada en la investigación en salud pública; prevención que, a veces, lleva a interpretaciones reduccionistas, al destacar la importancia de los rasgos raciales o genéticos.
La raza se convierte en causa de la enfermedad, cuando en realidad se trata de una característica que engloba muchas otras variables que influyen en el desarrollo de la enfermedad, como por ejemplo las relacionadas con el nivel socioeconómico. 

Estamos convencidos de que la superación de las hipótesis habituales relativas a la raza proporcionaría un enfoque más fructífero para entender la elevada prevalencia de hipertensión entre los afroamericanos. 
La hipertensión es un resultado al que se puede llegar por distintas vías:
complejas interacciones entre factores externos (estrés o dieta), fisiología interna (sistemas biológicos que regulan la presión sanguínea) y genes implicados en la regulación de la presión de la sangre.
Cuando se conozcan las interacciones entre estos tres elementos del modelo, comprenderemos los mecanismos por los que se desarrolla un aumento persistente de la tensión arterial.
Los investigadores podrán luego replantearse con mayor éxito la cuestión de la alta prevalencia de este trastorno entre los afroamericanos y alcanzar mejores tratamientos para todos los pacientes.

Para descubrir la importancia proporcional de los diferentes factores ambientales, se podría mantener constante el acervo génico de una población y observar las variaciones de sus condiciones de vida o de su comportamiento en entornos distintos.
No se puede llevar a cabo este tipo de experimento en condiciones ideales, sobre todo, porque un gran número de americanos presenta, cuando menos uno, y frecuentemente varios, de los factores y las conductas de riesgo de la hipertensión: el sobrepaso, la dieta rica en sal, el estrés psicológico de larga duración, la inactividad física y el consumo excesivo de alcohol.
Se puede establecer una analogía con la identificación de las causas de cáncer de pulmón en una sociedad en la que todo el mundo fuma.
Sin un grupo de control formado por personas que no fumen, los investigadores nunca sabrán que el tabaco contribuye de forma determinante al cáncer de pulmón.

Una solución a este problema podría pasar por volver a Africa.
En 1991, comenzamos un proyecto de investigación centrado en la diáspora, la emigración forzada de africanos occidentales ocurrida entre los siglos XVI y XIX.
En este vergonzoso capitulo de la historia, los traficantes europeos de esclavos compraron y capturaron en la costa occidental de Africa diez millones de personas y las transportaron al Caribe y al continente americano, donde gradualmente se mezclaron con europeos y americanos nativos.
Hoy en día sus descendientes habitan todo el hemisferio occidental.

Desde hace tiempo se sabe que la prevalencia de la hipertensión en las zonas rurales de Africa occidental es la menor del mundo, si dejamos de lado ciertos enclaves de la cuenca amazónica y del Pacifico Sur.
Los norteamericanos y británicos descendientes de africanos occidentales, por contra, presentan una llamativa prevalencia, de las más altas del mundo. 
Esta discrepancia apunta a factores ambientales o conductuales, más que a factores genéticos, como causa fundamental del incremento de la tendencia a la hipertensión.

Para determinar las causas de hipertensión en estas poblaciones, instalamos centros de estudio en comunidades de Nigeria, Camerún, Zimbabwe, Santa Lucía, Barbados, Jamaica y los EE.UU.
Con el proyecto en marcha, centramos nuestra atención en Nigeria, Jamaica y los EE.UU.
Consideramos que los tres países nos permitirían, en cierta medida, captar los efectos médicos del desplazamiento de los africanos desde sus países de origen hacia occidente. 
Seleccionamos de forma aleatoria individuos de cada uno de esos sitios para determinar la prevalencia general de la hipertensión y de sus factores de riesgo (dieta rica en sal, obesidad o inactividad física).

Como era de esperar, se observaron importantes diferencias entre estas tres sociedades.
En Nigeria se estudió, con la colaboración de la facultad de medicina de la Universidad de Ibadan, una comunidad rural del distrito de Igbo- Ora.
La poligamia es allí una práctica frecuente y las familias son grandes y complejas; las mujeres tienen una media de cinco hijos.
Los habitantes de Igbo- Ora, de complexión delgada, se ocupan en tareas agrícolas y ganaderas de subsistencia, exigentes desde el punto de vista físico, y siguen la dieta tradicional nigeriana, que se basa en arroz, tubérculos y fruta.

Las naciones del Africa subsahariana no suelen llevar registros de mortalidad o de esperanza de vida.
Pero podemos suponer, a partir de estudios locales, que las infecciones, la malaria sobre todo, son la primera causa de muerte.
Nuestra investigación puso de manifiesto que los habitantes de Igbo-Ora corren un riesgo anual de mortalidad de entre uno y dos por ciento, lo que se puede considerar alto comparado con cualquier referencia occidental. 
Los que alcanzan edades avanzadas acostumbran llegar bastante sanos.
En particular, la tensión arterial no aumenta con la edad y, aunque se dan casos de hipertensión, éstos son raros.
(Nos complace haber podido coordinar con el personal médico de la región el tratamiento de los pacientes a los que se diagnosticó hipertensión). 

En Jamaica, una economía industrial emergente con riesgo muy bajo de enfermedades infecciosas, las crónicas tienen, en cambio, una mayor prevalencia que en Nigeria.
Nuestro equipo estableció su base de operaciones en Spanish Town, la antigua capital colonial de Jamaica.
Con sus 90.000 habitantes, Spanish Town of rece un corte transversal de la sociedad jamaicana.
Se encargó del proyecto la unidad de investigación del metabolismo tropical de la Universidad de las Indias Occidentales en Mona Campus.

La estructura familiar de Jamaica ha evolucionado hasta apartarse del patriarcado africano.
Existen numerosas unidades familiares, generalmente pequeñas y a menudo fragmentadas, gobernadas por las mujeres.
El desempleo crónico ha favorecido la marginación de los hombres y ha minado su posición social.
Son comunes las labores exigentes desde el punto de vista físico.
La dieta se compone de productos locales y otros modernos.
A pesar de la pobreza generalizada, la esperanza de vida en Jamaica sextuplica la de los afroamericanos estadounidenses, debido a una menor tasa de enfermedad cardiovascular y de cáncer. 

En EE.UU. trabajamos en el área metropolitana de Chicago, en la ciudad de Maywood, cuya población es mayoritariamente afroamericana.
Muchos de sus ancianos nacieron en el sur, en Mississippi, Alabama o Arkansas.
Con la emigración mejoraron situación económica y salud.
El trabajo en la industria pesada, regulado por los sindicatos, ofrece mayores oportunidades a los hombres; las mujeres se han incorporado a la población activa en empleos de diversa categoría.
La dieta es típicamente americana: rica en sal y grasas.
La generación ahora en la edad adulta tardía ha disfrutado de incrementos sustanciales en la esperanza de vida, aunque el progreso ha sido desigual en los últimos diez años.

Antes de estudiar sociedades tan dispares, nos aseguramos de que los sujetos participantes presentaran una dotación genética similar.
Hallamos que los afroamericanos y jamaicanos que participaron en el estudio compartían con los nigerianos un promedio del setenta y cinco por ciento de su herencia genética.

Este bagaje genético común contrastaba con importantes diferencias.

Consideremos en primer lugar la prevalencia de la hipertensión entre los participantes.
En Nigeria, sólo un siete por ciento de la población rural presentaba hipertensión, observándose un incremento en el área urbana.
Alrededor del 26 por ciento de los negros de Jamaica y del 33 por ciento de los estadounidenses padecían hipertensión o se medicaban para controlarla.
Algunos factores de riesgo de la hipertensión adquirían mayor frecuencia conforme nos deslizábamos a occidente.
De Africa a los EE.UU. se registraba un incremento persistente en la ingesta de sal y en el índice de masa corporal, una medida que relaciona el peso con la altura.
El análisis de estos datos indica que el sobrepaso, la falta de ejercicio y una dieta pobre podrían justificar entre el 40 y el 50 por ciento del incremento de hipertensión de los afroamericanos en comparación con los nigerianos.
Se supone que las variaciones en la ingesta diaria de sal fomentan también el riesgo.

La diáspora africana ha demostrado resultar una potente herramienta para evaluar los efectos que los cambios sociales y ambientales pueden provocar sobre un bagaje genético estable.
Se plantea, asimismo, la cuestión de si el incremento de la tensión arterial es un riesgo inevitable de la vida moderna, independientemente del color de la piel. 
El sistema cardiovascular humano evolucionó en un medio, el Africa rural, donde la obesidad es infrecuente, moderada la ingesta de sal, baja en grasas la dieta y elevada la actividad física requerida.
La vida de subsistencia en Africa no ha cambiado mucho, al menos en estos aspectos.
Hemos observado que la tensión arterial de las personas que viven en tales condiciones no aumenta con la edad y que la aterosclerosis es una afección casi desconocida.
Los campesinos africanos constituyen, pues, un interesante grupo de control con el que los epidemiólogos pueden comparar poblaciones de sociedades industriales.

Una variación de estas condiciones basales, moderada incluso, produce cambios notables en el riesgo de hipertensión. 
La presión sanguínea es substancialmente más alta en la ciudad de Ibadan que en áreas rurales próximas.
Esto es así, a pesar de que las diferencias entre ambos grupos, en cuanto a obesidad e ingesta de sodio, son pequeñas.
El estrés psicológico y la falta de actividad física ayudarían también a explicar el incremento.

El estrés psicológico y el social, difíciles de medir, divergen de una cultura a otra.
Pocos objetarán, sin embargo, que los negros de Norteamérica y Europa se enfrentan a un tipo singular de estrés: la discriminación racial.
Se desconocen los efectos a largo plazo del racismo sobre la presión sanguínea; si se sabe que los negros de Trinidad, Cuba y zonas rurales de Puerto Rico muestran niveles de tensión arterial similares a los de otros grupos raciales.
Tal vez, se podría conjeturar que las relaciones interraciales en estas sociedades sean menos agresivas para el sistema cardiovascular que en EE.UU.

En cuanto epidemiólogos, nos interesa ir más allá de la mera descripción de los agentes relacionados con el incremento de riesgo de hipertensión y analizar la interacción entre factores ambientales y biológicos que producen la enfermedad.

No se conoce en su pormenor el mecanismo regulador de la presión sanguínea.
Si se sabe que el riñón desempeña un papel fundamental mediante el control de la concentración en la sangre de los iones de sodio (que provienen de la sal común -cloruro sódico- de la dieta), que a su vez incide en el volumen de sangre y su presión. 

Los riñones evolucionaron en una situación en que la dieta humana era pobre en sodio, por lo que desarrollaron una enorme capacidad de retención de este ion vital.
Cuando filtran la sangre, los riñones retienen hasta un 98 por ciento del sodio, que luego termina por volver a la circulación.
Pero si reciben mucho sodio, lo secretan en grandes cantidades, con el incremento consiguiente de la presión arterial.
Un exceso de sal en los riñones daña el mecanismo interno de filtración, lo que también redunda en un aumento mantenido de la presión.

Para saber si los riñones de nuestros pacientes regulaban bien la concentración de sodio, nos pusimos a medir la actividad de una importante vía bioquímica que interviene en la regulación de la presión sanguínea. 
El sistema renina- angiotensina-aldosterona (SRAA, nombre derivado de tres de los compuestos que participan) es una complicada cadena de reacciones químicas que regula la concentración sanguínea de la proteína angiotensina II.
Cuenta ésta entre sus varias funciones la de inducir la constricción de la luz de los vasos sanguíneos. 
El aumento de la presión sanguínea que ello provoca, dispara la liberación de aldosterona, sustancia que incrementa la recuperación renal de sodio.
En resumen, a una intensa actividad de la vía SRAA debería corresponder una presión sanguínea alta.

Al objeto de establecer la actividad SRAA de los participantes, extrajimos muestras de sangre y determinamos la concentración en ellas de angiotensinógeno, una de las sustancias implicadas en el primer paso del SRAA.
La elección de este compuesto se basa en su concentración en sangre, que, a diferencia de otros compuestos de la misma vía de vida media corta, se revela bastante constante.

De acuerdo con lo esperado, los niveles elevados de angiotensinógeno incrementaban la probabilidad de hipertensión.
Esta asociación era más débil en las mujeres (las variaciones en los estrógenos también afectan a su presión sanguínea).
Además, la concentración media de angiotensinógeno en cada grupo estudiado tendía a aumentar conforme nos trasladamos de Nigeria a Jamaica y de aquí a los Estados Unidos.
Se observó idéntico patrón en el caso de la hipertensión, lo mismo en mujeres que en varones.

A tenor de nuestros resultados, algunos factores de riesgo de la hipertensión operan mediante la elevación de los niveles sanguíneos de angiotensinógeno. 

Ocurre, por ejemplo, con la obesidad.
Una cantidad excesiva de grasa corporal conlleva aparejada una elevación en los niveles de angiotensinógeno.
La incidencia de la obesidad aumentó de forma más o menos paralela a los niveles de hipertensión y de angiotensinógeno en nuestros grupos de estudio.
Verdad es que correlación no es causalidad, pero los hallazgos apuntan a que la obesidad induce hipertensión, al menos en parte, mediante el incremento de la producción de angiotensinógeno.

Ciertos hallazgos del ámbito de la genética parecen respaldar el determinante papel que el incremento de angiotensinógeno desempeña en la aparición de la hipertensión.
Hay personas portadoras de ciertos alelos de los genes que cifran la producción de angiotensinógeno.
Estas variantes alélicas favorecen un incremento de su concentración y sus portadores acostumbran correr mayor riesgo de hipertensión.

Hace unos años, investigadores de la Universidad de Utah y del College de France informaron de la asociación mostrada entre dos alelos del gen del angiotensinógeno (235T y 174M) y niveles elevados de angiotensinógeno, en personas de ascendencia europea.
Se ignora si estos alelos intervienen en el control de los niveles de angiotensinógeno o si se trata de simples marcadores que se heredan junto con otros alelos que sí dejan sentir su influencia.

Un gen asociado a una mayor proclividad a la hipertensión no tiene necesariamente que hallarse implicado en la patogenia de la enfermedad.
Por tanto, los portadores del gen no deben forzosamente sufrir hipertensión.
Se ha determinado que los factores genéticos explican del 25 al 40 por ciento de la variabilidad de la presión sanguínea.
Se calcula que los genes relacionados con esta variabilidad pueden ser 10 o 15, lo que indica que cada gen contribuye únicamente con un dos o un cuatro por ciento de las diferencias de presión sanguínea entre distintas personas.
Que un gen induzca hipertensión dependerá en medida notable de la presencia de influencias ambientales necesarias para instar la "expresión" real de semejante susceptibilidad.

Los resultados de genética a los que hemos llegado nosotros ilustran lo que acabamos de afirmar.
Hemos descubierto que el alelo 235T duplica, en afroamericanos, la frecuencia que acostumbra en euroamericanos.
Sin embargo, los negros con esta variante del gen no parecen correr un riesgo mayor de hipertensión que los negros que no lo presentan.
Entre los nigerianos encontramos una elevación moderada de los niveles de angiotensinógeno en los portadores de la variante del gen 235T; de nuevo, este factor no se tradujo en un mayor riesgo de hipertensión.
Aún más, el 90 por ciento de los africanos que estudiamos portaban dicho alelo, mientras que la tasa de hipertensión en esa comunidad es, según lo comentado, bajísima. 
La frecuencia del alelo 147M resultó ser equivalente en ambos grupos.

Podría suceder que los niveles elevados de angiotensinógeno no bastaran para disparar la hipertensión en personas de ascendencia africana.
Para inducir la enfermedad se requeriría la presencia de factores genéticos, fisiológicos o ambientales adicionales.
Cabe también que el papel del alelo 235T en el desarrollo de la hipertensión no sea el mismo en todos los grupos étnicos.

Según nuestras investigaciones, un aspecto al menos de la nutrición puede modificar la fisiología de una persona y producir hipertensión.
Pero hay que andar cautos a la hora de las generalizaciones fáciles. 
Para explicar la frecuencia de la hipertensión entre afroamericanos no basta un solo alelo o un único factor externo.
De acuerdo con nuestro rastreo de la diáspora africana, un individuo con una dotación genética dada puede presentar cierta proclividad a la hipertensión, aunque ésta sólo se materializará en un entorno determinado.
El reto estriba en acotar los factores genéticos y ambientales que afecten a la hipertensión, para luego juntar todas las piezas y establecer las múltiples maneras en que tales factores condicionan una elevación crónica de la presión sanguínea.

La hipertensión produce un siete por ciento de las muertes en todo el mundo, una proporción que aumentará conforme crezca el número de sociedades que adopten los hábitos y el estilo de vida de, los países industrializados. 
Sin retorno posible al medio de donde salimos, compete a la ciencia buscar soluciones.
La revolución sanitaria llegó tras conocerse los mecanismos del contagio.
Pudo tratarse la cardiopatía cuando quedó manifiesta la importancia de la dieta en el metabolismo del colesterol. 
La prevención y el tratamiento de la hipertensión también están sometidos al conocimiento sobre la forma en que los genes y el ambiente unen sus fuerzas para obstruir la regulación de la presión sanguínea.

El desentrañamiento de los fenómenos implicados en el desarrollo de la hipertensión en los afroamericanos instará el replanteamiento de las divisiones étnicas y raciales de nuestra especie.
El concepto de raza carece de fundamento biológico; a lo sumo se trata de un constructo social sin perfiles científicos.

Hace tiempo que los antropólogos renunciaron a clasificar Homo sapiens en razas o subespecies.
Pese a ello, la medicina y la epidemiología continúan asignando un sentido biológico a las clasificaciones raciales, argumentando que la raza es útil no sólo para distinguir entre grupos de personas, sino también para explicar la prevalencia de determinados trastornos. 
Sin embargo, las clasificaciones raciales que incorporan no se basan en criterios científicos, sino en categorías políticas o burocráticas como las que se usan en los censos.

No podemos olvidar el contexto social de la idea de raza. 
Vivimos en un mundo donde las designaciones raciales asumen un significado desafortunado.
Los efectos destructivos del racismo complican cualquier estudio de enfermedades que, como la hipertensión, afectan a las minorías.
A medida que exploremos las complejas interacciones entre los factores de riesgo externos (estrés y obesidad) y los genes asociados con la regulación de la presión sanguínea, los resultados deberán ser útiles para todos, al margen del color de su piel.

Lo que significa la lectura de la presión

La presión sanguínea se mide con un esfigmomanómetro que proporciona dos resultados: la presión sistólica y la diastólica.
La sistólica indica la presión máxima ejercida por la sangre en la pared de las arterias; parece cuando el ventrículo izquierdo del corazón se contrae y empuja la sangre hacia las arterias. 
La presión diastólica mide la presión mínima sobre la pared de los vasos sanguíneos; aparece cuando el ventrículo izquierdo se relaja y se llena de sangre.
La presión sanguínea normal ronda los 120 mm., de mercurio para la sistólica y los 80 para la diastólica.

Muchas personas experimentan aumentos transitorios de su presión sanguínea, particularmente en situaciones de estrés.
Cuando la presión sanguínea se mantiene de forma persistente por encima de 140/90 los médicos diagnostican hipertensión.
La enfermedad se puede tratar con dietas especiales, regímenes de ejercicios y medicación.

Hipertensión y tráfico de esclavos

Una controvertida explicación que se aduce con frecuencia sobre la prevalencia de la hipertensión entre los estadounidenses negros, se basa en el viaje recorrido desde Africa a América por barcos de esclavos, el pasaje medio .
Según dicha teoría, durante los viajes, los esclavos habrían experimentado una situación darwinista de "supervivencia del más adaptado", en la que la supervivencia dependería de la disponibilidad de los genes adecuados. 

Estos mismos genes serian los que hoy conferirían un incremento del riesgo de hipertensión.

A menudo se invocan teorías evolucionistas para explicar el motivo en cuya virtud cierto grupo racial o étnico muestra un riesgo mayor de desarrollar una enfermedad particular. 
El argumento suele atenerse al siguiente dictado:
sobre la población actué cierta presión de selección que favoreció la supervivencia de algunos miembros del grupo (y de sus genes) y eliminó otros.
Si la población resultante no mezcló su genes con otros grupos raciales o étnicos, algunas características genéticas podrían aparecer con una frecuencia creciente.

Suponiendo que los afroamericanos tengan una predisposición genética a la hipertensión, los teóricos evolucionistas se preguntan cuál fue esa única y extrema presión de selección que ha hecho tan común un rasgo tan lesivo.

Algunos proponen que el viaje infrahumano en barcos de esclavos es un suceso de este tipo.
Los esclavos padecían unos índices de mortalidad extraordinariamente elevados antes, durante y después de llegar a las plantaciones americanas.

Muchas de estas muertes se debieron a lo que los médicos llaman enfermedades con pérdida de sal: diarrea deshidratación y ciertas infecciones.
La capacidad para retener sal podría haber tenido un valor capital para la supervivencia de los africanos arrastrados a América.
En circunstancias modernas, la retención de sal predispondría a sus descendientes a la hipertensión.

Dada su aparente linealidad, se ha aceptado de forma acrítica la hipótesis de la esclavitud.
La verdad, sin embargo, pudiera ser muy otra.
Los estudiosos de la historia de Africa han cuestionado la veracidad de ese relato.
No existen pruebas fehacientes de que las enfermedades con pérdida de sal fueran la primera causa de muerte en los barcos de esclavos. 
Los africanos morían a bordo por otras razones, entre ellas la tuberculosis (una infección que no se asocia a la pérdida de sal) y la violencia.

Si atendemos al fundamento biológico de la tesis, resulta harto endeble.
La diarrea y otras enfermedades con pérdida de sal, en niños sobre todo, han sido las más letales en cualquier población durante toda la historia de la evolución humana.
La presión de selección resultante de tales enfermedades debería esperarse que afectara a todos los grupos raciales y étnicos. 
Al menos en el Caribe y durante el siglo XVIII, los blancos tuvieron tasas de supervivencia apenas mejores que las de los esclavos, lo que indica de nuevo que cualquier presión de evolución no estaba limitada a los africanos.
Por último, los datos actuales sugieren que los africanos que han emigrado a Europa en las últimas décadas también presentan una mayor presión arterial que los blancos, lo que apunta a efectos ambientales o algo común al bagaje genético africano general.

Desconocemos todavía los genes relacionados con la sensibilidad a la sal, que permitirían comprobar de forma directa la hipótesis del pasaje medio .
Si éste hubiera funcionado coreo un cuello de botella evolutivo, debería haber reducido el tamaño de la población y la variabilidad genética, pues sólo habrían sobrevivido las personas con una dotación genética muy especifica.
Los datos disponibles abogan por una gran diversidad genética entre los afroamericanos, lejos de una presumida uniformidad.

La hipótesis cómoda de la esclavitud para explicar la tasa de hipertensión refleja el atajo de las teorías genéticas y raciales.
Su aceptación expresa la proclividad a atribuir razones genéticas de las diferencias entre blancos y no blancos sin evaluar las pruebas disponibles.
Se trata de un sesgo tendencioso y anticientífico. 
La investigación genética es cada vez más objetiva y la capacidad de medir variaciones reales en las secuencias de ADN puede forzar a la abandonar los prejuicios raciales y étnicos.
O también revestirlos de nueva legitimidad.
Lo que ocurra dependerá de cómo interpreten los científicos los hallazgos en un contexto que tenga en consideración las complejidades de la sociedad y de la historia.

Figura 0

El sistema renina-angiotensina-aldosterona 

Esta vía bioquímica opera sobre la presión sanguínea.
Las personas con una elevada actividad del sistema suelen padecer hipertensión arterial.

1 El angiotensinógeno se sintetiza en el hígado de forma continua.

2 Los riñones liberan renina como respuesta al estrés, tanto fisiológico (ejercicio físico, cambios de dieta) como emocional.
3 La angiotensina I resulta de la reacción del angiotensinógeno y de la renina.
Cuando la sangre llega a los pulmones, la angiotensina I reacciona con la enzima ECA (enzima convertidora de la angiotensina).

4 La angiotensina II es el producto de la reacción de angiotensina I y la ECA.
La angiotensina II tiene dos efectos principales:
induce la liberación de aldosterona por las glándulas suprarrenales y estimula la contracción del músculo liso vascular, lo que incrementa la presión sanguínea.

5 La aldosterona induce la retención de sal y agua por el riñón, lo que incrementa la presión sanguínea.

Figura 1

LA INCIDENCIA DE LA HIPERTENSION se ha estudiado en africanos y en norteamericanos y caribes descendientes de africanos

Se observa un progresivo descenso de la proporción de hipertensos desde los EE.UU., a través del Atlántico y hasta Africa ( gráfico ).
La máxima diferencia se dio entre afroamericanos de medio urbano ( abajo, a la derecha ) y nigerianos de entorno rural ( abajo, a la izquierda ). 
Los hallazgos indican que la hipertensión puede ser, en buena medida, una enfermedad de la vida moderna y que los genes no bastan por sí solos para explicar la cifra de hipertensas entre afroamericanos.

Figura 2

EL INIDICE DE MASA CORPORAL es una medida de la razón de peso a altura

Se habla de sobrepeso en índices superiores a 25.
En el estudio desarrollado por los autores entre descendientes de africanos, un índice medio bajo de una población se correspondía con una tasa baja de hipertensión.
A medida que aumentaba el índice de masa corporal medio, crecía la prevalencia de la hipertensión. 
La obesidad contribuye al desarrollo de la hipertensión arterial.

Figura 3

LA PROPORCION de portadores del alelo 235T de cierto gen y la hipertensión en distintos grupos étnicos constituye un rompecabezas

Se creía que los portadores de 235T tendrían una alta incidencia de hipertensión, pero se ha observado que tal asociación no puede generalizarse.

Así, la variante 235T es muy común entre los nigerianos, que apenas si conocen la hipertensión. 
No basta sólo un gen para controlar el desarrollo de hipertensión.


Base genética de la respuesta inmunitaria

CONCEPTOS DEL CAPÍTULO

El sistema inmunitario de los vertebrados es un mecanismo controlado genéticamente que defiende al cuerpo de los organismos causantes de enfermedades.
Una vasta recombinación genética en células somáticas baraja un número limitado de genes en nuevas combinaciones que producen literalmente, millones de anticuerpos y de receptores de superficie celular diferentes.
Los antígenos de superficie celular desempeñan una función en la determinación de los grupos sanguíneos y en el éxito de las transfusiones y de los transplantes de órganos.
La proliferación, la diferenciación y la función de las células del sistema inmunitario están controladas genéticamente, y mutaciones en estos genes resultan en enfermedades del sistema inmunitario. 

Al evolucionar los vertebrados, desarrollaron un mecanismo genético especial y complejo, crucial para su supervivencia. 
Este mecanismo, denominado sistema inmunitario , coordina una serie de respuestas defensivas ante la entrada de substancias extrañas o ante la invasión de virus y microorganismos en el cuerpo.
Estas respuestas son específicas e implican dos fases: una respuesta primaria ante la exposición inicial, y una respuesta secundaria ante posteriores exposiciones al mismo agente. 

Desde el punto de vista genético, el sistema inmunitario tiene dos características fundamentales.
Primero, en cada individuo debe reconocer «lo que es propio», de manera que las células y los tejidos del organismo no sean atacadas por su propio sistema inmunitario.
Este reconocimiento implica un grupo de genes que, en la especie humana, se localizan en el cromosoma 6.
Segundo, el sistema inmunitario debe poder producir células específicas (células asesinas y otras células) y productos génicos (anticuerpos) que neutralicen y posteriormente destruyan los agentes «foráneos» (antígenos).

La producción de anticuerpos contra antígenos potenciales es una extraordinaria hazaña genética. 
A partir de un grupo de menos de 300 genes, sucesos de recombinación durante el desarrollo de las células productoras de anticuerpos crean un vasto potencial inmunológico. 
Como consecuencia del barajamiento de este número limitado de genes se pueden producir decenas de millones de anticuerpos diferentes.

En este capítulo examinaremos las células que forman el sistema inmunitario y cómo estas células y sus productos génicos se movilizan para engarzar una respuesta inmunitaria.
También examinaremos la función del sistema inmunitario en la determinación de los grupos sanguíneos y en la incompatibilidad madre- feto, cómo los marcadores de superficie celular se utilizan en transplantes, y cómo estos marcadores pueden utilizarse como sistema de predicción para determinar el factor de riesgo de una amplia gama de enfermedades.
Finalmente, consideraremos varias enfermedades genéticas del sistema inmunitario, como mutaciones en un solo gen que inactivan completamente la respuesta inmunitaria.

También describiremos cómo el síndrome de inmunodeficiencia adquirida (SIDA) asociado a la infección del virus de inmunodeficiencia humano (HIV) desmantela la respuesta inmunitaria de las personas infectadas.
En conjunto, la investigación del sistema inmunitario constituye una de las áreas más excitantes de la genética, un área en que las nuevas informaciones y los nuevos descubrimientos se producen cada vez con más frecuencia.

Componentes del sistema inmunitario 

El sistema inmunitario es una barrera efectiva contra la invasión de substancias foráneas potencialmente dañinas.
El sistema inmunitario reconoce los virus, las bacterias, los hongos y los parásitos invasores como no propios, y posteriormente los secuestra, los inactiva y los destruye.
Generalmente, esta repuesta inmunitaria implica anticuerpos específicos contra las substancias extrañas.
Los agentes que activan la producción de anticuerpos se denominan antígenos .
Los anticuerpos son proteínas producidas y secretadas por células específicas del sistema inmunitario.
Hay muchos tipos de moléculas diferentes que pueden actuar de antígenos, como las proteínas, los polisacáridos y, más raramente, otras moléculas como los ácidos nucleicos.
Normalmente, una característica estructural específica del antígeno, denominada epítopo , estimula la producción de anticuerpos.
Los antígenos invasores pueden ser moléculas libres o pueden formar parte de la superficie de una célula, de un microorganismo o de un virus.
Sea como sea, los organismos que tienen sistema inmunitario poseen la capacidad de hacer anticuerpos contra cualquier antígeno que encuentren.

Los dos tipos principales de inmunidad del sistema inmunitario son:
(1) la inmunidad mediada por anticuerpos , asociada a dos tipos de células sanguíneas, las células T que reconocen los antígenos, y las células B que producen los anticuerpos; y
(2) la inmunidad mediada por células , que implica un tipo de células T denominadas células T citotóxicas o asesinas .


Las células del sistema inmunitario 

Una de las células clave del sistema inmunitario es el fagocito , un glóbulo blanco que fagocita y destruye las moléculas y los microorganismos foráneos.
Un tipo de fagocito, denominado macrófago , desempeña una función esencial tanto en la inmunidad mediada por anticuerpos como en la inmunidad mediada por células ya que señala a otras células del sistema inmunitario que ha; antígenos foráneos presentes.
En esta función de señalización, los macrófagos se conocen como células presentadoras de antígenos .
Todos los fagocitos surgen de células madre, que son células mitóticamente activas que se encuentran en la médula ósea (Figura 22.1).
Los fagocitos maduros pueden dejar el sistema circulatorio pasando por las paredes de los capilares para entrar en tejidos inflamados o dañados para identificar, fagocitar y destruir antígenos.

Tabla 22.1

Células del sistema inmunitario 

Tipo celular.
Función.


Células T4.
También denominadas células T cooperadoras, tienen funciones en la respuesta inmunitaria mediada por células y en la mediada por anticuerpos; conmutador general del sistema inmunitario.


Células T8 supresoras.
Subtipo de células T que participa en la respuesta inmunitaria mediada por células y por anticuerpos; frenan y/o desconectan la respuesta inmunitaria .


Células B.
Precursores de las células plasmáticas, reconocen los antígenos y son activadas por las células T4.


Células plasmáticas.
Sintetizan grandes cantidades de un solo anticuerpo; producidas por mitosis de una célula B activada.


Células de memoria.
Permite una respuesta rápida en un segundo encuentro con un anticuerpo específico.


Células asesinas.
Subtipo de célula T que puede detectar y destruir células infectadas por virus.


Un segundo tipo de célula, las células T, también se producen por divisiones mitóticas de células madre (Figura 22.1).
Las células T inmaduras recién producidas migran al timo, donde se desarrollan en diversos subtipos celulares, como las células cooperadoras , las células supresoras y las células asesinas .
Durante este periodo de maduración, las células T se programan para producir un único tipo de receptor de superficie celular.
Cada célula T produce un solo tipo de receptor, denominado receptor antigénico , que a su vez se unirá a un solo tipo de antígeno [1] . 
Hay literalmente decenas o centenares de millones de antígenos potenciales diferentes, y hay un número parecido de células T programadas de manera diferente.
La colección de receptores de las células T se produce a partir de un pequeño número de genes mediante sucesos de recombinación genética.

Las células T maduras se distribuyen desde el timo por todo el sistema circulatorio y linfático, y quedan apartadas en los nódulos linfáticos y en el bazo. 
Las células T maduras pueden dividirse, y toda la descendencia de una sola célula T forma un grupo familiar o clon.
Hay dos tipos generales de células T: las células T4 cooperadoras/inductoras y las células T8 citotóxicas/supresoras.

Las células T4 cooperadoras son el conmutador general del sistema inmunitario; activan la respuesta inmunitaria.
Las células T8 supresoras son el conmutador que desconecta el sistema inmunitario; detienen o reducen la respuesta inmunitaria (Tabla 22.1).

El tercer componente del sistema inmunitario, las células B, se originan en la médula ósea (Figura 22.1) y migran al bazo y a los nódulos linfáticos.
Al madurar, estas células se programan genéticamente para producir anticuerpos; cada célula B produce un solo tipo de anticuerpos.
La interacción de las células B con las células T4 cooperadoras activa la producción de anticuerpos.
Las células B activadas proliferan y se convierten en células plasmáticas , que producen grandes cantidades de anticuerpos.
Las células de memoria representan una subpoblación de células B que participan en posteriores encuentros con el antígeno específico.

La respuesta inmunitaria

La respuesta inmunitaria implica cuatro estadios:
(1) la detección de los antígenos foráneos,
(2) la activación de células del sistema inmunitario,
(3) la inactivación del antígeno y
(4) la desactivación de la respuesta inmunitaria. 

Cada uno de estos estadios está dirigido por al menos un tipo celular.
Tras la exposición a un antígeno y la generación de una respuesta inmunitaria, los posteriores encuentros del sistema inmunitario con el mismo antígeno desencadenan una respuesta inmunitaria secundaria , también denominada memoria inmunológica .
Esta respuesta inmunitaria secundaria también está dirigida por tipos celulares específicos.
Primero examinaremos las respuestas inmunitarias mediadas por anticuerpos y por células, y después consideraremos cómo la memoria inmunológica protege el cuerpo contra una nueva exposición al antígeno. 

Inmunidad mediada por anticuerpos .
Los macrófagos vagan por el sistema circulatorio y por los tejidos inflamados y pueden responder a la presencia de moléculas foráneas, de virus y de microorganismos.
Cuando encuentran un antígeno, lo engloban y lo ingieren (Figura 22.2).
Después de ingerirlo, se incorporan fragmentos del antígeno en la membrana plasmática del macrófago, y éste los expone en su superficie externa dentro de un contexto molecular específico.
Al moverse este macrófago presentador de antígenos por el sistema circulatorio o por los espacios intercelulares puede encontrase con una célula T4 cooperadora que tenga un receptor antigénico complementario.
Los receptores de superficie de la célula T4 reaccionan con el antígeno, lo que activa la célula T cooperadora.
A su vez, la célula T activada identifica y moviliza células B que sintetizan anticuerpos contra el antígeno que el macrófago le ha presentado. 
La célula B estimulada empieza a dividirse y a diferenciarse para formar dos tipos de células hija.

Unas son las células de memoria , y las otras son las células plasmáticas , que sintetizan y secretan de 2.000 a 20.000 moléculas de anticuerpo por segundo en el torrente sanguíneo. 

Los anticuerpos son efectivos contra antígenos extracelulares como células bacterianas, partículas víricas libres, toxinas secretadas y protozoos parásitos presentes en el torrente sanguíneo, o contra células infectadas por virus que exhiban antígenos víricos en su superficie. 
Los anticuerpos actúan de diferentes maneras para inactivar los antígenos.
Los anticuerpos pueden interaccionar con los antígenos para formar complejos antígeno-anticuerpo.
La agrupación del complejo antígeno-anticuerpo marca los antígenos para que sean destruidos, siendo ingeridos y destruidos por los fagocitos. 
La combinación de determinados anticuerpos con antígenos activa el complemento , un sistema proteolítico presente en el suero sanguíneo que lisa las células bacterianas invasoras.
Los anticuerpos también pueden interaccionar con antígenos destruyendo su capacidad funcional.
Por ejemplo, algunos virus tienen proteínas en su superficie que ayudan a que el virus se una a la superficie de las células como preludio a la infección vírica.
Los anticuerpos pueden reaccionar con esta proteína de unión e inactivarla, lo que deja al virus incapacitado para infectar células y lo expone a los fagocitos para que éstos lo ingieran y lo destruyan.

Las células de memoria producidas por células B activadas también producen anticuerpos, pero en vez de tener un periodo vital de varios días, como es el caso de las células plasmáticas, tienen un periodo vital prolongado de varios meses o años.
Estas células desempeñan una función importante en la respuesta inmunitaria secundaria (que se expone a continuación). 

Las células T8 supresoras siguen la respuesta inmunitaria completa y, cuando los antígenos detectables se han inactivado o destruido, detienen la proliferación de las células B y desactivan la producción de anticuerpos.
De esta manera, las células T8 actúan de conmutador para desconectar el sistema inmunitario.

Inmunidad mediada por células 

En comparación a la acción de los anticuerpos, que marcan a los antígenos para que sean destruidos por fagocitos, la interacción célula-célula directa denominada inmunidad mediada por células , llevada a cabo por un tipo de células T, también puede inactivar antígenos.
Las células T que participan en este tipo de reacción inmunitaria se denominan células T asesinas o células T citotóxicas .
Como otras células T, las células asesinas se originan en la médula ósea y maduran en el timo.

Cuando un virus infecta una célula, pequeños fragmentos de la cubierta proteica del virus se incorporan en la membrana plasmática de la célula.
Estos fragmentos proteicas se muestran como un nuevo grupo de antígenos en la superficie de la célula infectada, lo que permite que las células T asesinas identifiquen y maten las células infectadas por los virus.
El reconocimiento por parte de las células asesinas implica al antígeno vírico y a un grupo de marcadores de superficie celular denominados antígenos de histocompatibilidad . 
Estos antígenos están codificados por el complejo antigénico leucocitario humano (HLA), localizado en el brazo corto del cromosoma 6.
Estos marcadores también desempeñan una función crucial en los injertos y en los transplantes.
Su función en la inmunidad mediada por células ayuda a asegurar que las células asesinas sólo ataquen a las células infectadas por los virus.

Una vez identificadas las células diana, las células asesinas las atacan secretando una proteína denominada perforina , que se inserta en la membrana plasmática de la célula infectada por virus.
Las moléculas de perforina, cilíndricas, se unen entre sí para formar poros en la membrana plasmática. 
El citoplasma de la célula diana sale por estos poros, y la célula infectada muere.
La célula asesina se desengancha de la célula muerta, pudiendo identificar y matar otras células infectadas.
Además de matar células infectadas por virus, las células T citotóxicas y un segundo tipo de células asesinas denominadas células asesinas naturales atacan y matan células cancerígenas, hongos v algunos tipos de parásitos.
Las células T citotóxicas también atacan y matan células introducidas en el cuerpo durante los transplantes de tejidos o de órganos siempre que reconozcan a las células transplantadas corma foráneas.

Memoria inmunológica e inmunización 

Ya desde tiempos antiguos se sabía que las personas expuestas a determinadas enfermedades no podían contraer la misma enfermedad de nuevo.
Por ejemplo, las personas que han tenido el sarampión (una enfermedad infecciosa provocada por un virus) no pueden contraerlo otra vez, aunque estén expuestas a individuos infectados.
La resistencia a una segunda infección está controlada por la respuesta inmunitaria secundaria y por la producción de células de memoria B y T durante la primera exposición al antígeno.
Debido a las células de memoria, una segunda exposición a un antígeno resulta en la producción inmediata y a gran escala de anticuerpos y de células T asesinas. 
Gracias a la presencia de células de memoria, la respuesta inmunitaria secundaria es más rápida, se produce a mayor escala, y dura más que la respuesta inmunitaria primaria.

La existencia de respuesta inmunitaria secundaria es la base para la vacunación contra diversas enfermedades infecciosas. 
Una vacuna es una preparación diseñada para estimular la producción de células de memoria contra un agente patógeno.
En este procedimiento, el antígeno se administra oralmente o mediante inyección. 

Una vez el antígeno está en el cuerpo, provoca una respuesta inmunitaria primaria que incluye la producción de células de memoria.
A menudo se administra una dosis secundaria o de recuerdo para producir una respuesta que incremente el número de células de memoria.
Las vacunas pueden prepararse a partir de patógenos muertos o de cepas atenuadas que pueden estimular el sistema inmunitario pero no pueden producir los síntomas de la enfermedad. 
También se utilizan las técnicas de DNA recombinante para producir vacunas.
Por ejemplo, se utilizan fragmentos del gen que codifica la proteína de cubierta del virus de la hepatitis B para producir un antígeno no infeccioso que se utiliza como vacuna.

Diversidad genética en el sistema inmunitario

Entre los vertebrados, cada individuo puede producir millones de tipos diferentes de anticuerpos, cada uno respondiendo a un antígeno diferente [2] .
Del mismo modo, también las células T tienen millones de sitios de reconocimiento antigénico diferentes en su superficie. 
La base de esta diversidad molecular yace en la secuencia aminoacídica que forma estas proteínas, denominadas receptores de las células T (TCR).
Los TCR están codificados por una familia de genes, y la reordenación de estos genes durante el desarrollo de las células B y de las células T es la base de esta diversidad.

Los antígenos de histocompatibilidad, un tercer grupo de proteínas implicadas en la respuesta inmunitaria, también están codificados por una familia de genes, el complejo HLA, pero en este caso la diversidad se logra mediante un gran número de alelos en cada locas de la familia, en vez de mediante reorganizaciones génicas.
En las siguientes secciones examinaremos las bases moleculares de la diversidad de los anticuerpos y de los TCR.
La diversidad del complejo HLA la consideraremos en una sección posterior. 

Anticuerpos

En la especie humana, las células plasmáticas, un tipo de célula B, producen los anticuerpos.
Se han reconocido cinco clases de anticuerpos o de inmunoglobulinas (Ig): IgG, IgA, IgM, IgD e IgE (Tabla 22.2).
El primer tipo, las IgG , representan aproximadamente el 80 por ciento de los anticuerpos encontrados en sangre y son el tipo de anticuerpos más extensamente caracterizado. 
Las IgG están asociadas a la memoria inmunológica. 
Las inmunoglobulinas IgA (que se encuentran en la leche materna) pueden secretarse a través de las membranas plasmáticas y están asociadas a la resistencia inmunitaria a las infecciones de los tractos respiratorio y digestivo.
Los anticuerpos IgM son, generalmente, los primeros que una célula plasmática secreta en respuesta a un antígeno, y están asociados a los primeros estadios de la respuesta inmunitaria.
Hoy en día, poco se sabe acerca de la función de las inmunoglobulinas IgD .
Éstas están asociadas a la superficie de las células B, y podrían regular su acción.
Los anticuerpos IgE están implicados en la lucha contra infecciones parasitarias, y también están asociados a las repuestas alérgicas.

Una molécula de IgG típica (Figura 22.8) está formada por dos cadenas polipeptídicas diferentes, cada una presente en dos copias.
La mayor o cadena pesada (H) contiene aproximadamente 440 aminoácidos. 
La secuencia de los primeros 110 aminoácidos del extremo N-tenninal difiere entre diferentes cadenas pesadas, y se denomina región variable (VH) .
Los aminoácidos C-terminales restantes son iguales en todas las cadenas H, y forman la región constante (CH) .
En la especie humana, las cadenas H están codificadas por genes localizados en el brazo largo del cromosoma 14.

Tabla 22.2

Categorías y componentes de las inmunoglobulinas

Clase de Ig.
Cadena ligera.
Cadena pesada.
Tetrámeros.


IgA .
( .
(2(2 .
(2(2 .


IgD .
( .
(2(2 .
(2(2 .


IgE .
( .
(2(2 .
(2(2 .


IgG .
( o ( .
( .
(2(2 .
(2(2 .


IgM .
( .
(2(2 .
(2(2 .


La cadena ligera (L) está formada por 220 aminoácidos, de los que los primeros 110 forman la región variable (VL) (Figura 22.3).
El resto de aminoácidos del extremo C- terminal forman la región constante (CL) de la cadena ligera.
Hay dos tipos de cadenas ligeras: las cadenas kappa , codificadas por genes localizados en el cromosoma humano, y las cadenas lambda , codificadas por genes localizados en el brazo largo del cromosoma 22.

Una molécula de IgG funcional está formada por dos cadenas pesadas y por dos cadenas ligeras, unidas por puentes disulfuro.
Las regiones variables de las cadenas pesadas y ligeras forman el sitio de combinación del anticuerpo .
El sitio de combinación de cada anticuerpo tiene una conformación única que le permite unirse a un antígeno específico, como una llave en una cerradura.

Figura 22.1

Las células madre de la médula ósea generan los precursores de las células T, de los macrófagos y de las células B del sistema inmunitario

Figura 22.2

Diagrama de respuestas durante la respuesta inmunitaria mediada por anticuerpos

Cuando un macrófago encuentra un agente infeccioso como un virus, lo ingiere y lo destruye.
En la superficie del macrófago se muestran algunos de los antígenos víricos, y activa las células T4.
Las células T activadas pueden diferenciarse en células T de memoria y en células T cooperadoras. 
Las células T cooperadoras estimulan las células B en los nódulos linfáticos, que pueden producir un anticuerpo contra el antígeno vírico y activarse mitóticamente.
La progenie de las células B activadas puede diferenciarse en células B de memoria y en células plasmáticas, que producen y secretan grandes cantidades de un solo anticuerpo.

El anticuerpo secretado se une al antígeno vírico, marcando la partícula vírica para que los fagocitos la destruyan.

Figura 22.3

Molécula típica de anticuerpo (inmunoglobulina)

La molécula arene forma de Y y contiene cuatro cadenas polipeptídicas.
Los brazos largos son las cadenas pesadas y los brazos cortos son las cadenas ligeras. 
Cada cadena contiene una región constante y una regían variable.
La región variable forma el sitio de unión del anticuerpo. que interacciona con un antígeno especifico de manera parecida al mecanismo de una llave y su cerradura.

Las cadenas están unidas entre si por puentes disulfuro indicados como . -.- .- .

[1] El receptor antigénico reconoce y se une a un solo tipo de epítopo , que puede estar presente en más de una molécula antigénica diferente

[2] Del mismo modo que el receptor antigénico reconoce un solo tipo de epítopo, cada tipo de anticuerpo responde a un epítopo diferente, teniendo en cuenta que un antígeno puede tener más de un epítopo y que un mismo epítopo puede estar presente en más de un antígeno diferente.


Tecnología del DNA recombinante

CONCEPTOS DEL CAPÍTULO

La tecnología del DNA recombinante depende, en parte, de la capacidad de cortar N' unir segmentos de DNA por secuencias especificas de bases.
Utilizando esta metodología, se pueden transferir segmentos particulares de DNA a virus o a bacterias para amplificarlos, aislarlos e identificarlos.
La utilización de esta tecnología ha conseguido importantes avances en la cartografía de genes, en el diagnóstico de enfermedades, en la producción comercial de productos génicos humanos y en la transferencia de genes entre especies diferentes, tanto en plantas como en animales.

El redescubrimiento de los experimentos de Mendel, realizado independientemente por de Vries, Correns y von Tschermak en 1900, marcó el inicio de la genética como disciplina organizada.
Durante el crecimiento y desarrollo de esta ciencia, varios descubrimientos clave han servido de puntos de inflexión, acelerando todos ellos la velocidad a la que crecen los conocimientos en genética y, a la vez, abriendo nuevos campos de investigación.

Uno de los primeros de estos hitos fue la teoría cromosómica de la herencia.

Este concepto, propuesto por Sutton y Boveri en 1902, lo desarrollaron Morgan y sus colaboradores utilizando la mosca de la fruta ( Drosophila ).
De estas investigaciones provino la comprensión de la genética de la transmisión, de la determinación del sexo, del ligamiento y de la utilización de los cromosomas para cartografiar genes en sus loci citológicos específicos.

Un segundo hito fue el descubrimiento realizado por Avery, MacLeod y McCarty de que el DNA es la macro-molécula que transporta la información genética.
Este descubrimiento estimuló la utilización de virus y de bacterias como organismos para la investigación genética, y condujo al modelo de estructura del DNA de WatsonCrick.
Este modelo ha traído consigo el conocimiento de las bases moleculares del código genético, de la transcripción, de la traducción y de la regulación génica.

Ahora estamos experimentando otra y, quizás, la más profunda transición en la historia de la genética: 
el desarrollo y la aplicación de la tecnología del DNA recombinante.
Esta tecnología se utiliza en investigación básica y en el desarrollo y producción de vacunas, de proteínas terapéuticas y de plantas y animales modificados genéticamente.
También han surgido temores por las epidemias o por los cambios ecológicos a gran escala que podrían resultar .le la liberación al medio de organismos genéticamente modificados.
Este capitulo expone los métodos básicos de la tecnología del DNA recombinante que se utilizan para aislar, replicar y analizar los genes.
En el siguiente capítulo expondremos algunas de las aplicaciones de esta tecnología a la agricultura, a la medicina y a la industria.

Generalidades de la tecnología del DNA recombinante

El término DNA recombinante hace referencia a la creación de nuevas combinaciones de segmentos o de moléculas de DNA que no se encuentran juntas de manera natural.
Aunque el proceso genético de la recombinación produce DNA recombinante, este término se reserva a las moléculas de DNA producidas por la unión de segmentos que provienen de diferentes fuentes biológicas.

La tecnología del DNA recombinante utiliza técnicas que provienen de la bioquímica de los ácidos nucleicos unidas a metodologías genéticas desarrolladas originalmente para la investigación de bacterias y de virus.
Como describiremos a continuación, la utilización del DNA recombinante es una herramienta poderosa para el aislamiento de poblaciones puras de secuencias específicas de DNA a partir de una población de secuencias mezcladas.
Los procedimientos básicos incluyen una serie de pasos:

1. Los fragmentos de DNA se generan utilizando unas enzimas denominadas endonucleasas de restricción , que reconocen y cortan las moléculas de DNA por secuencias nucleotídicas específicas.
2. Los fragmentos producidos mediante la digestión con enzimas de restricción se unen a otras moléculas de DNA que sirven de vectores .
Los vectores pueden replicarse autónomamente en una célula huésped y facilitan la manipulación de la molécula de DNA recombinante recién creada.

3. La molécula de DNA recombinante, formada por un vector que lleva un segmento de DNA insertado, se transfiere a una célula huésped.
Dentro de esta célula, la molécula de DNA recombinante se replica, produciendo docenas de copias idénticas conocidas como clones .

4. Al replicarse las células huésped, las células descendientes heredan el DNA recombinante, creándose una población de células idénticas, que llevan todas la secuencia clonada.
5. Los segmentos de DNA clonados pueden recuperarse de las células huésped, purificarse y analizarse.
6. Potencialmente, el DNA clonado puede transcribirse, su mRNA puede traducirse, y el producto génico puede aislarse y examinarse.

Fabricación del DNA recombinante

El desarrollo de las técnicas de DNA recombinante ofrece nuevas oportunidades para la investigación, ya que simplifica la obtención de grandes cantidades de DNA que codifican genes específicos, y facilita las investigaciones de la organización, estructura y expresión génicas. 
Esta metodología también ha dado un impulso al desarrollo de la naciente industria biotecnológica, que está suministrando un numero creciente de productos al mercado.
El DNA recombinante produce grandes cantidades de copias de segmentos de DNA específicos, incluyendo genes.
El proceso se resume en las siguientes secciones.

Enzimas de restricción

La piedra angular de la tecnología del DNA recombinante es un tipo de enzimas denominadas endonucleasas de restricción .
Estas enzimas, aisladas en bacterias, reciben este nombre debido a que limitan o previenen las infecciones víricas degradando el ácido nucleico invasor.
Las enzimas de restricción reconocen una secuencia específica de nucleótidos (denominada sitio de restricción) de una molécula de DNA de doble cadena, y cortan el DNA por esa secuencia.
El premio Nobel de 1978 se otorgó a Werner Arher, Hamilton Smith y Daniel Nathans por su investigación sobre las enzimas de restricción.
Hasta la fecha, se han aislado y caracterizado casi 200 tipos de enzimas de restricción diferentes.

Las enzimas de restricción se denominan según el organismo en el que se descubrieron, utilizando un sistema alfanumérico.
La enzima EcoRI proviene de Escherichia coli , y se pronuncia "eco- erre-uno". 
HindIII se descubrió en Hemophilus influenzae y se pronuncia " jinde-tres ". 
Hay dos tipos de enzimas de restricción.
Las enzimas de Tipo I cortan las dos cadenas del DNA en una posición aleatoria a cierta distancia del sitio de restricción. 
Las enzimas de Tipo I no se utilizan normalmente en la investigación de DNA recombinante ya que el sitio de corte no es preciso.
Las enzimas de Tipo II reconocen una secuencia específica y cortan las dos cadenas de la molécula de DNA con absoluta precisión dentro de la secuencia reconocida. 
Las enzimas de Tipo II se usan ampliamente en investigación de DNA recombinante puesto que cortan en sitios específicos.

Las secuencias reconocidas por las enzimas de Tipo II son simétricas.
La secuencia de una de las cadenas leída en dirección 5'-3' es la misma que la secuencia de la cadena complementaria leída también en dirección 5'-3'.
Las secuencias que se leen igual en ambas direcciones se denominan palindrómicas .
La .1 muestra el sitio de reconocimiento palindrómico y los puntos de corte de EcoRI.

La enzima EcoRI corta las cadenas de manera escalonada dentro del sitio de reconocimiento, dejando extremos de cadena sencilla.
Estas colas, que tienen secuencias nucleotídicas idénticas, son "pegajosas", ya que pueden unirse a colas complementarias de otros fragmentos de DNA mediante puentes de hidrógeno.

Si moléculas de DNA de diferente origen comparten los mismos sitios de reconocimiento palindrómicos, ambas tendrán colas complementarias de cadena sencilla cuando se traten con una endonucleasa de restricción (Figura 15.2).
Si estos fragmentos se ponen juntos bajo determinadas condiciones, los fragmentos de DNA de distinto origen pueden formar moléculas recombinantes estableciendo puentes de hidrógeno entre los extremos pegajosos.
Luego se utiliza la enzima DNA ligase para unir covalentemente los esqueletos azúcar-fosfato de los dos fragmentos, produciéndose así una molécula de DNA recombinante (.2).

Otras enzimas de restricción de Tipo II, como SmaI, cortan el DNA formando fragmentos romos (.3).
Los fragmentos de DNA con extremos romos también pueden unirse y formar moléculas recombinantes, pero primero deben modificarse (.3).
La enzima desoxinucleotidil transferasa terminal se utiliza para crear extremos de cadena sencilla mediante la adición de "colas" de nucleótidos.
Si a uno de los DNA se le añade una cola de poli- dA , y al otro DNA se le añade una cola de poli-dT , se forman colas complementarias, y los fragmentos pueden unirse mediante puentes de hidrógeno.

Entonces, por ligación, pueden formarse moléculas recombinantes (.3).

La .4 representa algunas enzimas de restricción comunes y sus secuencias de reconocimiento.
Algunas de estas enzimas producen extremos cohesivos o "pegajosos", mientras que otras generan extremos romos.

Vectores

Después de unirse a un vector o vehículo de clonación, un segmento de DNA puede llegar a entrar en una célula huésped y replicarse o clonarse. 
Los vectores son, esencialmente, moléculas de DNA transportadoras.
Para servir de vector, una molécula de DNA debe tener unas determinadas características:
1. Debe poder replicarse independientemente junto con el segmento de DNA que transporta.
2. Debería contener algunos sitios de corte para enzimas de restricción, presentes sólo una vez en el vector.
Estos sitios de restricción se cortan con una enzima de restricción y se utilizan para insertar segmentos de DNA cortados con la misma enzima [1] .

3. Debería tener algún marcador de selección (normalmente genes de resistencia a antibióticos o genes de enzimas que la célula huésped no tenga) para poder distinguir las células huésped que transportan el vector de las que no lo contienen.
4. El vector de la célula huésped debería ser fácil de recuperar.

Actualmente se utilizan tres tipos principales de vectores: los plásmidos , los bacteriófagos y los cósmidos .

Los plásmidos

Los plásmidos son moléculas de DNA de doble cadena extracromosómicas de origen natural que tienen un origen de replicación (ori+) y que se replican autónomamente en las células bacterianas.
En el Capítulo 18 se tratará la genética de los plásmidos y de sus células huésped bacterianas.
En esta sección se pondrá énfasis en la función de los plásmidos como vectores.
Para poder utilizarlos en ingeniería genética, se han modificado o diseñado muchos plásmidos de manera que contengan un número limitado de sitios de restricción y genes de resistencia a antibióticos específicos.

El vector pBR322 fue uno de los primeros plásmidos diseñados que se utilizó en DNA recombinante (Figura 15.5).
Este plásmidos tiene un origen de replicación ( ori +), dos genes de selección (resistencia a los antibióticos ampicilina y tetraciclina), y algunos sitios de restricción únicos.
Dentro del gen de resistencia a tetraciclina hay sitios de restricción únicos para las enzimas BamHI, SphI, SalI, XmaIII y NruI, y dentro del gen de resistencia a ampicilina hay sitios de restricción únicos para las enzimas RruI, PvuI y PstI.
Si se introduce pBR322 en una célula bacteriana sin plásmidos y sensible a antibióticos, la célula se convertir en resistente a la tetraciclina y a la ampicilina. 
Si se inserta un fragmento de DNA en los sitios de restricción RruI, PvuI, o PstI, se inactivará el gen de resistencia a ampicilina, pero el gen de resistencia a tetraciclina seguir activo.
Si este plásmidos recombinante se transfiere a una célula bacteriana que no contenga plásmidos, las células que contengan el plásmidos recombinante podrán identificarse, ya que serán resistentes a tetraciclina y sensibles a ampicilina.

Con el paso del tiempo se han desarrollado vectores plasmídicos más sofisticados, que ofrecen diversas características útiles.
Uno de estos plásmidos, que deriva de pBR322 , es el pUC18 (.6), que tiene las siguientes características:
1. El plásmido tiene la mitad de tamaño que pBR322 , lo que permite insertar fragmentos de DNA más largos.
2. pUC18 se replica produciendo unas 500 copias por célula, lo que significa unas 5 o 10 veces más que las que produce pBR322 .
3. Tiene un gran número de sitios de restricción únicos, agrupados en una región denominada sitio de clonación múltiple ( polylinker ).
4. El plásmidos contiene un fragmento de un gen bacteriano denominado lacZ , y el sitio de clonación múltiple está insertado dentro de este fragmento del gen. 
El gen lacZ normal codifica la beta-galactosidasa, enzima que corta moléculas de azúcar.
Cuando pUC18 se introduce en una célula huésped bacteriana que tiene el gen lacZ mutante, se produce beta-galactosidasa funcional.
La presencia de beta-galactosidasa funcional en una colonia bacteriana puede detectarse mediante un ensayo colorimétrico.
Las células bacterianas que contienen pUC18 forman colonias de color azul cuando crecen en un medio que contiene una substancia denominada X-gal. 
Si hay DNA insertado en el sitio de clonación múltiple, se interrumpe el gen lacZ y se forman colonias blancas.
Las colonias blancas, resistentes a ampicilina, contienen plásmidos con DNA insertado.


Los bacteriófagos lambda y M13

Lambda es un bacteriófago muy utilizado en experimentos de DNA recombinante.
Se han identificado y cartografiado todos los genes de lambda, y se conoce toda la secuencia nucleotídica de su genoma.
El tercio central de su cromosoma es prescindible, y puede reemplazarse por DNA exógeno sin afectar la capacidad del fago para infectar células y formar calvas (Figura 15.7).
Se han desarrollado más de 100 vectores basados en el fago lambda, eliminando diversas porciones del grupo génico central.
Para clonar utilizando el vector lambda, se corta el DNA del fago con una enzima de restricción (por ejemplo, EcoRI), lo que produce un brazo izquierdo, un brazo derecho y una región central.
Se aíslan los brazos y se ligan (utilizando la DNA ligase) a un segmento de DNA obtenido tras cortar DNA genómico con la misma enzima de restricción (en este ejemplo, con EcoRI).

Las moléculas recombinantes resultantes pueden introducirse en células huésped bacterianas por transfección .
Primero, se permeabilizan las células huésped mediante un tratamiento químico, y se mezclan con las moléculas ligadas.
Los vectores que contienen el inserto entran en las células huésped, donde dirigen la síntesis de fagos infectivos, llevando cada uno de ellos el inserto de DNA.
Como alternativa, se mezcla el DNA de lambda que contiene el inserto con los componentes proteicas del fago; de esta mezcla se forman partículas fágicas infectivas.
Los fagos pueden amplificarse haciéndolos crecer en placas sembradas con bacterias, donde se formarán calvas, o bien infectando células en un medio líquido y recogiendo las células lisadas. 

También se utilizan otros bacteriófagos como vectores, entre los que destaca el fago de cadena sencilla M13. 
Cuando M13 infecta a una célula bacteriana, la cadena sencilla (cadena +) se replica y produce una molécula de doble cadena denominada forma replicativa (RF) .
Las moléculas RF pueden considerarse similares a los plásmidos, pudiéndose insertar DNA exógeno en sitios de restricción únicos presentes en el DNA del fago.
Cuando se reinsertan en células bacterianas, las moléculas RF se replican y producen cadenas sencillas (+), en las que hay una cadena del DNA insertado.
La célula hace salir los clones de DNA de cadena sencilla que produce M13, que pueden recuperarse y utilizarse directamente para su secuenciación, o como molde para alteraciones mutacionales de la secuencia clonada. 

Los cósmidos y los vectores transbordadores

Los cósmidos son vectores híbridos construidos utilizando partes del cromosoma del fago lambda y de DNA plasmídico. 
Los cósmidos contienen la secuencia cos del fago lambda, necesaria para el empaquetamiento del DNA del fago dentro de su cubierta proteica, y secuencias plasmídicas de replicación y de genes de resistencia a antibióticos, que permiten identificar las células huésped que los contienen (Figura 15.8).
El DNA de los cósmidos que contiene insertos de DNA se empaqueta en la cápside proteica, de lambda para formar partículas fágicas infectivas. 
Una vez el cósmido entra en la célula huésped, se replica como un plásmidos.
Puesto que la mayoría del genoma de lambda se ha delecionado, los cósmidos pueden transportar insertos de DNA macho más grandes que los que lambda puede llevar.
Los cósmidos pueden transportar casi -50 Kb de- DNA insertado; los vectores fágicos pueden acomodar insertos de DNA de unas 15 Kb; y los plásmidos generalmente están limitados a insertos de 5-10 Kb.

Existen otros vectores híbridos, construidos con orígenes de replicación provenientes de distintas fuentes (por ejemplo, plásmidos y virus animales como SV40), que pueden replicarse en más de un tipo de célula huésped.
Generalmente, estos vectores transbordadores contienen marcadores genéticos que permiten su selección en los dos sistemas huésped, y pueden utilizarse para transportar insertos de DNA entre E. coli y otra células huésped, como levadura, y viceversa.

A menudo, estos vectores se emplean para investigar la expresión génica.
En las siguientes secciones se describir n otros vectores utilizados para aplicaciones específicas. 

Cromosomas artificiales bacterianos

Para cartografiar y analizar genomas eucarióticos complejos, se ha desarrollado un vector multiuso denominado cromosoma artificial bacteriano (BAC) basado en el factor F de bacterias.
Recuerde que en Capítulo 6 se expuso que el factor F es un plásmidos que se replica independientemente y que está implicado en la transferencia de información genética durante la conjugación bacteriana.
Puesto que los factores F pueden transportar fragmentos del cromosoma bacteriano de hasta 1 Mb, han sido diseñados para que funcionen como vectores de DNA eucariótico.
Los vectores BAC tienen los genes de replicación y de número de copias del factor F, e incorporan un marcador de resistencia a un antibiótico y sitios de restricción para insertar el DNA exógeno que se desee clonar.
Además, el sitio de clonación está flanqueado por regiones promotoras que pueden utilizarse para generar moléculas de RNA y expresar así el gen clonado, o para utilizarlas como sonda para paseo cromosómico, y para secuenciar el DNA del inserto clonado (.9).

Figura 15.1

La enzima de restricción EcoRI reconoce y se une a la secuencia palindrómica GAATTC

El corte del DNA en esa sitio produce colas de cadena sencilla complementarias.

La colas de cadena sencilla resultantes pueden unirse con colas complementarias de otros fragmentos de DNA para formar moléculas de DNA recombinante.

Figura 15.2

Para formar moléculas de DNA recombinante, se corta DNA de distintas fuentes con EcoRI y se mezcla para que se unan formando moléculas recombinantes 

Después se utiliza la enzima DNA ligase para unirlos covalentemente en moléculas de DNA recombinante. 

Figura 15.3

Las moléculas de DNA re combinante pueden formarse a partir DNA corta do con enzimas que dejan extremos romos

En este método, se utiliza la enzima desoxinucleotidil transferasa terminal para crear colas complementarias mediante la adición de fragmentos de poli- dA y de poli-dT .
Estas colas sirven para unir el DNA de diferentes fuentes y para crear moléculas de DNA recombinante, que son unidas covalentemente por tratamiento con la DNA ligase. 

Figura 15.4

Algunas enzimas de restricción comunes, con sus sitios de reconocimiento y de corte, el patrón de corte y la fuente de origen

Figura 15.5

Mapa de restricción del plásmidos pBR322 , que muestra las localizaciones de los sitios de las enzimas de restricción que cortan el plásmidos por un solo sitio

Estos sitios pueden utilizarse para insertar los fragmentos de DNA a clonar.

También se muestran las localizaciones de los genes de resistencia a antibióticos.

Figura 15.6

El plásmidos pUC18 ofrece varias ventajas como vector de clonación 

Debido a su pequeño tamaño, puede aceptar fragmentos de DNA relativamente grandes; se replica hasta un alto número de copias, y tiene un gran número de sitios de restricción en el sitio de clonación múltiple, localizado dentro del gen lacZ .
Las bacterias que contienen pUC18 producen colonias de color azul si crecen en un medio que contenga X-gal.
DNA insertado en el sitio de clonación múltiple interrumpe el gen lacZ , resultando en colonias blancas, lo que permite la identificación directa de las colonias que tienen insertos de DNA clonados.

Figura 15.7

Pasos en la clonación utilizando el fago lambda como vector

Se extrae el DNA de una preparación de fago lambda y se elimina d grupo central de genes por tratamiento con una enzima de restricción.
El DNA a clonar se corta con la misma enzima y se liga entre los brazos del cromosoma de lambda.
Luego se empaqueta el cromosoma recombinante dentro de las proteínas del fago para formar un virus recombinante.
Este virus puede infectar células bacterianas y replicar su cromosoma, incluido el inserto de DNA.

Figura 15.8

El cósmido pJB8 contiene un origen de replicación bacteriano (ori), un solo sitio cos (cos), un gen de resistencia a ampicilina ( amp , para seleccionar las colonias que han incorporado el cósmido), y una región que contiene cuatro sitios de restricción para la clonación (BamHI, EcoRI, CIaI y HindIII)

Puesto que el vector es pequeño (5,4 Kb de longitud), puede aceptar segmentos de DNA exógeno de 33 a 46 Kb de longitud.
El sitio cos permite que el cósmido que contenga un inserto grande se empaquete con proteínas de cubierta vírica de lambda como si fuesen cromosomas víricos.
Las cubiertas víricas que contienen un cósmido pueden utilizarse para infectar células huésped adecuadas, y el vector, que transporta el inserto de DNA, se transferirá a la célula huésped. 
Una vez dentro, la secuencia ori permite que el cósmido se replique como un plásmido bacteriano.

[1] Nota del traductor: 
Estos sitios de restricción únicos permiten insertar segmentos de DNA cortados con la misma enzima sin desmembrar el vector, situación que se produciría si se utilizasen sitios de restricción presentes más de una vez en el vector.

